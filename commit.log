commit 0d080f556627f3ad6e3270eba049d1bcb34f6dc7
Merge: b990ddb6 acd44bf9
Author: Wang Qi <wangqi@users.noreply.github.com>
Date:   Sat Dec 27 10:46:04 2025 -0500

    Merge branch 'k2-fsa:master' into master

commit acd44bf9ab0ee83c2189c93b442038c0b4e66786
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Fri Dec 26 15:20:23 2025 +0800

    Fix creating a view of an Ort::Value tensor. (#2939)

diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
index a33252de..2abfc25a 100644
--- a/sherpa-onnx/csrc/onnx-utils.cc
+++ b/sherpa-onnx/csrc/onnx-utils.cc
@@ -189,8 +189,8 @@ Ort::Value View(Ort::Value *v) {
   auto type_and_shape = v->GetTensorTypeAndShapeInfo();
   std::vector<int64_t> shape = type_and_shape.GetShape();
 
-  auto memory_info =
-      Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+  auto memory_info = v->GetTensorMemoryInfo();
+
   switch (type_and_shape.GetElementType()) {
     case ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32:
       return Ort::Value::CreateTensor(

commit 1f5f9633cc71f0426cbcea591fba56ef8a646984
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 25 17:25:56 2025 +0800

    Add C++ runtime and Python API for Google MedASR models (#2935)
    
    This pull request significantly enhances sherpa-onnx by integrating comprehensive support for Google MedASR CTC models. The changes encompass the foundational C++ model implementation, a streamlined Python API for ease of use, and specialized audio feature processing and text post-processing logic to ensure accurate medical speech recognition. This integration also includes updates to the build system and continuous integration tests, alongside a practical Python example, making MedASR models readily available and verifiable within the framework.

diff --git a/.github/scripts/test-python.sh b/.github/scripts/test-python.sh
index 49523aaf..ea8679a2 100755
--- a/.github/scripts/test-python.sh
+++ b/.github/scripts/test-python.sh
@@ -8,6 +8,17 @@ log() {
   echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
 }
 
+log "test Google MedASR"
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+ls -lh sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+
+ls -lh sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs
+
+python3 ./python-api-examples/offline-medasr-ctc-decode-files.py
+rm -rf sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+
 log "test omnilingual ASR"
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
 tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
diff --git a/.github/workflows/export-medasr-ctc-to-onnx.yaml b/.github/workflows/export-medasr-ctc-to-onnx.yaml
index 80aa921f..9bfafe25 100644
--- a/.github/workflows/export-medasr-ctc-to-onnx.yaml
+++ b/.github/workflows/export-medasr-ctc-to-onnx.yaml
@@ -3,7 +3,7 @@ name: export-medasr-ctc-to-onnx
 on:
   push:
     branches:
-      - export-medasr-onnx
+      - cpp-medasr-2
   workflow_dispatch:
 
 concurrency:
@@ -42,7 +42,9 @@ jobs:
         run: |
           cd scripts/medasr
 
-          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/0.wav
+          for i in $(seq 0 5); do
+            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/$i.wav
+          done
 
           curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/transcript.txt
 
@@ -53,7 +55,9 @@ jobs:
         run: |
           cd scripts/medasr
 
-          python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./0.wav
+          for i in $(seq 0 5); do
+            python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./$i.wav
+          done
 
           cat transcript.txt
 
@@ -62,7 +66,9 @@ jobs:
         run: |
           cd scripts/medasr
 
-          python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./0.wav
+          for i in $(seq 0 5); do
+            python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./$i.wav
+          done
 
           cat transcript.txt
 
diff --git a/python-api-examples/offline-medasr-ctc-decode-files.py b/python-api-examples/offline-medasr-ctc-decode-files.py
new file mode 100755
index 00000000..d231eb85
--- /dev/null
+++ b/python-api-examples/offline-medasr-ctc-decode-files.py
@@ -0,0 +1,142 @@
+#!/usr/bin/env python3
+
+"""
+This file shows how to use a non-streaming Google MedASR CTC model from
+https://huggingface.co/google/medasr
+to decode files.
+
+Please download model files from
+https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+
+For instance,
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+"""
+
+import time
+from pathlib import Path
+
+import librosa
+import numpy as np
+import sherpa_onnx
+
+
+def create_recognizer():
+    model = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx"
+    tokens = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt"
+    test_wav_0 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav"
+    test_wav_1 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/1.wav"
+    test_wav_2 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/2.wav"
+    test_wav_3 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/3.wav"
+    test_wav_4 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/4.wav"
+    test_wav_5 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/5.wav"
+
+    for f in [
+        model,
+        tokens,
+        test_wav_0,
+        test_wav_1,
+        test_wav_2,
+        test_wav_3,
+        test_wav_4,
+        test_wav_5,
+    ]:
+        if not Path(f).is_file():
+            print(f"{f} does not exist")
+
+            raise ValueError(
+                """Please download model files from
+                https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+                """
+            )
+    return (
+        sherpa_onnx.OfflineRecognizer.from_medasr_ctc(
+            model=model,
+            tokens=tokens,
+            num_threads=2,
+        ),
+        test_wav_0,
+        test_wav_1,
+        test_wav_2,
+        test_wav_3,
+        test_wav_4,
+        test_wav_5,
+    )
+
+
+def load_audio(filename):
+    audio, sample_rate = librosa.load(filename, sr=16000)
+    assert sample_rate == 16000, sample_rate
+
+    return np.ascontiguousarray(audio)
+
+
+def decode_single_file(recognizer, filename):
+    samples = load_audio(filename)
+
+    start_time = time.time()
+
+    stream = recognizer.create_stream()
+    stream.accept_waveform(sample_rate=16000, waveform=samples)
+    recognizer.decode_stream(stream)
+
+    end_time = time.time()
+    elapsed_seconds = end_time - start_time
+    audio_duration = len(samples) / 16000
+    real_time_factor = elapsed_seconds / audio_duration
+
+    print("---")
+    print(filename)
+    print(stream.result)
+    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
+    print(f"Audio duration in seconds: {audio_duration:.3f}")
+    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
+    print()
+
+
+def decode_multiple_files(recognizer, filenames):
+    streams = []
+
+    start_time = time.time()
+
+    audio_duration = 0
+
+    for filename in filenames:
+        samples = load_audio(filename)
+        audio_duration += len(samples) / 16000
+
+        stream = recognizer.create_stream()
+        stream.accept_waveform(sample_rate=16000, waveform=samples)
+        streams.append(stream)
+
+    recognizer.decode_streams(streams)
+
+    end_time = time.time()
+    elapsed_seconds = end_time - start_time
+    real_time_factor = elapsed_seconds / audio_duration
+
+    for name, stream in zip(filenames, streams):
+        print("---")
+        print(name)
+        print(stream.result)
+        print()
+
+    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
+    print(f"Audio duration in seconds: {audio_duration:.3f}")
+    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
+    print()
+    print()
+
+
+def main():
+    recognizer, *filenames = create_recognizer()
+
+    decode_single_file(recognizer, filenames[0])
+    decode_single_file(recognizer, filenames[1])
+    decode_multiple_files(recognizer, filenames[2:])
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/medasr/export_onnx.py b/scripts/medasr/export_onnx.py
index a0fce456..2f7bd7e7 100755
--- a/scripts/medasr/export_onnx.py
+++ b/scripts/medasr/export_onnx.py
@@ -107,6 +107,7 @@ def main():
         "model_author": "google",
         "maintainer": "k2-fsa",
         "vocab_size": processor.tokenizer.vocab_size,
+        "subsampling_factor": 4,
         "url": "https://github.com/Google-Health/medasr",
         "license": "https://developers.google.com/health-ai-developer-foundations/terms",
     }
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 5c3338fe..d21995b5 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -40,6 +40,8 @@ set(sources
   offline-fire-red-asr-model.cc
   offline-lm-config.cc
   offline-lm.cc
+  offline-medasr-ctc-model-config.cc
+  offline-medasr-ctc-model.cc
   offline-model-config.cc
   offline-moonshine-greedy-search-decoder.cc
   offline-moonshine-model-config.cc
diff --git a/sherpa-onnx/csrc/offline-ctc-model.cc b/sherpa-onnx/csrc/offline-ctc-model.cc
index 4becf1af..4cc5d407 100644
--- a/sherpa-onnx/csrc/offline-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-ctc-model.cc
@@ -21,6 +21,7 @@
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-dolphin-model.h"
+#include "sherpa-onnx/csrc/offline-medasr-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
@@ -126,6 +127,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
     return std::make_unique<OfflineTeleSpeechCtcModel>(config);
   } else if (!config.omnilingual.model.empty()) {
     return std::make_unique<OfflineOmnilingualAsrCtcModel>(config);
+  } else if (!config.medasr.model.empty()) {
+    return std::make_unique<OfflineMedAsrCtcModel>(config);
   }
 
   // TODO(fangjun): Refactor it. We don't need to use model_type here
@@ -192,6 +195,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
     return std::make_unique<OfflineTeleSpeechCtcModel>(mgr, config);
   } else if (!config.omnilingual.model.empty()) {
     return std::make_unique<OfflineOmnilingualAsrCtcModel>(mgr, config);
+  } else if (!config.medasr.model.empty()) {
+    return std::make_unique<OfflineMedAsrCtcModel>(mgr, config);
   }
 
   // TODO(fangjun): Refactor it. We don't need to use model_type here
diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
new file mode 100644
index 00000000..1fdac49d
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
@@ -0,0 +1,40 @@
+// sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
+
+#include <sstream>
+#include <string>
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+void OfflineMedAsrCtcModelConfig::Register(ParseOptions *po) {
+  po->Register(
+      "medasr", &model,
+      "Path to model.onnx from MedASR. Please see "
+      "https://github.com/k2-fsa/sherpa-onnx/pull/2934 for available models");
+}
+
+bool OfflineMedAsrCtcModelConfig::Validate() const {
+  if (!FileExists(model)) {
+    SHERPA_ONNX_LOGE("MedASR model: '%s' does not exist", model.c_str());
+    return false;
+  }
+
+  return true;
+}
+
+std::string OfflineMedAsrCtcModelConfig::ToString() const {
+  std::ostringstream os;
+
+  os << "OfflineMedAsrCtcModelConfig(";
+  os << "model=\"" << model << "\")";
+
+  return os.str();
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
new file mode 100644
index 00000000..02629e1b
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
@@ -0,0 +1,29 @@
+// sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+
+#include <string>
+
+#include "sherpa-onnx/csrc/parse-options.h"
+
+namespace sherpa_onnx {
+
+struct OfflineMedAsrCtcModelConfig {
+  std::string model;
+
+  OfflineMedAsrCtcModelConfig() = default;
+  explicit OfflineMedAsrCtcModelConfig(const std::string &model)
+      : model(model) {}
+
+  void Register(ParseOptions *po);
+  bool Validate() const;
+
+  std::string ToString() const;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model.cc b/sherpa-onnx/csrc/offline-medasr-ctc-model.cc
new file mode 100644
index 00000000..de1255cf
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-medasr-ctc-model.cc
@@ -0,0 +1,198 @@
+// sherpa-onnx/csrc/offline-medasr-ctc-model.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/offline-medasr-ctc-model.h"
+
+#include <algorithm>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/onnx-utils.h"
+#include "sherpa-onnx/csrc/session.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+namespace {
+
+std::vector<int64_t> GetMask(Ort::Value length) {
+  auto shape = length.GetTensorTypeAndShapeInfo().GetShape();
+  if (shape.size() != 1) {
+    SHERPA_ONNX_LOGE("Invalid length dim %zu", shape.size());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  auto batch_size = shape[0];
+
+  const int64_t *p = length.GetTensorData<int64_t>();
+
+  int64_t max_len = *std::max_element(p, p + batch_size);
+
+  std::vector<int64_t> ans(batch_size * max_len, 0);
+
+  int64_t *p_mask = ans.data();
+
+  for (int32_t i = 0; i < batch_size; ++i) {
+    auto len = p[i];
+    std::fill(p_mask, p_mask + len, 1);
+
+    p_mask += max_len;
+  }
+
+  return ans;
+}
+
+}  // namespace
+
+class OfflineMedAsrCtcModel::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR),
+        sess_opts_(GetSessionOptions(config)),
+        allocator_{} {
+    auto buf = ReadFile(config_.medasr.model);
+    Init(buf.data(), buf.size());
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR),
+        sess_opts_(GetSessionOptions(config)),
+        allocator_{} {
+    auto buf = ReadFile(mgr, config_.medasr.model);
+    Init(buf.data(), buf.size());
+  }
+
+  std::vector<Ort::Value> Forward(Ort::Value features,
+                                  Ort::Value features_length) {
+    std::vector<int64_t> mask = GetMask(std::move(features_length));
+
+    auto memory_info =
+        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+
+    std::vector<int64_t> shape =
+        features.GetTensorTypeAndShapeInfo().GetShape();
+    shape.resize(2);
+
+    Ort::Value mask_tensor = Ort::Value::CreateTensor<int64_t>(
+        memory_info, mask.data(), mask.size(), shape.data(), shape.size());
+
+    std::array<Ort::Value, 2> inputs = {std::move(features),
+                                        std::move(mask_tensor)};
+
+    return sess_->Run({}, input_names_ptr_.data(), inputs.data(), inputs.size(),
+                      output_names_ptr_.data(), output_names_ptr_.size());
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+
+  int32_t SubsamplingFactor() const { return subsampling_factor_; }
+
+  OrtAllocator *Allocator() { return allocator_; }
+
+ private:
+  void Init(void *model_data, size_t model_data_length) {
+    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
+                                           sess_opts_);
+
+    GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
+
+    GetOutputNames(sess_.get(), &output_names_, &output_names_ptr_);
+
+    // get meta data
+    Ort::ModelMetadata meta_data = sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+
+    std::string model_type;
+    SHERPA_ONNX_READ_META_DATA_STR(model_type, "model_type");
+    if (model_type != "medasr_ctc") {
+      SHERPA_ONNX_LOGE("Expect model type medasr_ctc. Given: '%s'",
+                       model_type.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(subsampling_factor_,
+                                            "subsampling_factor", 4);
+  }
+
+ private:
+  OfflineModelConfig config_;
+  Ort::Env env_;
+  Ort::SessionOptions sess_opts_;
+  Ort::AllocatorWithDefaultOptions allocator_;
+
+  std::unique_ptr<Ort::Session> sess_;
+
+  std::vector<std::string> input_names_;
+  std::vector<const char *> input_names_ptr_;
+
+  std::vector<std::string> output_names_;
+  std::vector<const char *> output_names_ptr_;
+
+  int32_t vocab_size_ = 0;
+  int32_t subsampling_factor_ = 0;
+};
+
+OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(Manager *mgr,
+                                             const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineMedAsrCtcModel::~OfflineMedAsrCtcModel() = default;
+
+std::vector<Ort::Value> OfflineMedAsrCtcModel::Forward(
+    Ort::Value features, Ort::Value features_length) {
+  return impl_->Forward(std::move(features), std::move(features_length));
+}
+
+int32_t OfflineMedAsrCtcModel::VocabSize() const { return impl_->VocabSize(); }
+
+int32_t OfflineMedAsrCtcModel::SubsamplingFactor() const {
+  return impl_->SubsamplingFactor();
+}
+
+OrtAllocator *OfflineMedAsrCtcModel::Allocator() const {
+  return impl_->Allocator();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model.h b/sherpa-onnx/csrc/offline-medasr-ctc-model.h
new file mode 100644
index 00000000..76085eff
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-medasr-ctc-model.h
@@ -0,0 +1,65 @@
+// sherpa-onnx/csrc/offline-medasr-ctc-model.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "onnxruntime_cxx_api.h"  // NOLINT
+#include "sherpa-onnx/csrc/offline-ctc-model.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+/** This class implements the CTC model from MedASR.
+ *
+ * See
+ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/export_onnx.py
+ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/test_onnx.py
+ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/run.sh
+ *
+ */
+class OfflineMedAsrCtcModel : public OfflineCtcModel {
+ public:
+  explicit OfflineMedAsrCtcModel(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineMedAsrCtcModel(Manager *mgr, const OfflineModelConfig &config);
+
+  ~OfflineMedAsrCtcModel() override;
+
+  /** Run the forward method of the model.
+   *
+   * @param features  A tensor of shape (N, T, C).
+   * @param features_length  A 1-D tensor of shape (N,) containing number of
+   *                         valid frames in `features` before padding.
+   *                         Its dtype is int64_t.
+   *
+   * @return Return a vector containing:
+   *  - log_probs: A 3-D tensor of shape (N, T', vocab_size).
+   *  - log_probs_length A 1-D tensor of shape (N,). Its dtype is int64_t
+   */
+  std::vector<Ort::Value> Forward(Ort::Value features,
+                                  Ort::Value features_length) override;
+
+  /** Return the vocabulary size of the model
+   */
+  int32_t VocabSize() const override;
+
+  int32_t SubsamplingFactor() const override;
+
+  /** Return an allocator for allocating memory
+   */
+  OrtAllocator *Allocator() const override;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
index 49189f9f..cae0ea78 100644
--- a/sherpa-onnx/csrc/offline-model-config.cc
+++ b/sherpa-onnx/csrc/offline-model-config.cc
@@ -25,6 +25,7 @@ void OfflineModelConfig::Register(ParseOptions *po) {
   dolphin.Register(po);
   canary.Register(po);
   omnilingual.Register(po);
+  medasr.Register(po);
 
   po->Register("telespeech-ctc", &telespeech_ctc,
                "Path to model.onnx for telespeech ctc");
@@ -156,6 +157,10 @@ bool OfflineModelConfig::Validate() const {
     return omnilingual.Validate();
   }
 
+  if (!medasr.model.empty()) {
+    return medasr.Validate();
+  }
+
   if (!telespeech_ctc.empty() && !FileExists(telespeech_ctc)) {
     SHERPA_ONNX_LOGE("telespeech_ctc: '%s' does not exist",
                      telespeech_ctc.c_str());
@@ -186,6 +191,7 @@ std::string OfflineModelConfig::ToString() const {
   os << "dolphin=" << dolphin.ToString() << ", ";
   os << "canary=" << canary.ToString() << ", ";
   os << "omnilingual=" << omnilingual.ToString() << ", ";
+  os << "medasr=" << medasr.ToString() << ", ";
   os << "telespeech_ctc=\"" << telespeech_ctc << "\", ";
   os << "tokens=\"" << tokens << "\", ";
   os << "num_threads=" << num_threads << ", ";
diff --git a/sherpa-onnx/csrc/offline-model-config.h b/sherpa-onnx/csrc/offline-model-config.h
index 6ef84edc..dec5c95e 100644
--- a/sherpa-onnx/csrc/offline-model-config.h
+++ b/sherpa-onnx/csrc/offline-model-config.h
@@ -9,6 +9,7 @@
 #include "sherpa-onnx/csrc/offline-canary-model-config.h"
 #include "sherpa-onnx/csrc/offline-dolphin-model-config.h"
 #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
+#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
 #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
 #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
@@ -36,6 +37,7 @@ struct OfflineModelConfig {
   OfflineDolphinModelConfig dolphin;
   OfflineCanaryModelConfig canary;
   OfflineOmnilingualAsrCtcModelConfig omnilingual;
+  OfflineMedAsrCtcModelConfig medasr;
   std::string telespeech_ctc;
 
   std::string tokens;
@@ -71,6 +73,7 @@ struct OfflineModelConfig {
                      const OfflineDolphinModelConfig &dolphin,
                      const OfflineCanaryModelConfig &canary,
                      const OfflineOmnilingualAsrCtcModelConfig &omnilingual,
+                     const OfflineMedAsrCtcModelConfig &medasr,
                      const std::string &telespeech_ctc,
                      const std::string &tokens, int32_t num_threads, bool debug,
                      const std::string &provider, const std::string &model_type,
@@ -89,6 +92,7 @@ struct OfflineModelConfig {
         dolphin(dolphin),
         canary(canary),
         omnilingual(omnilingual),
+        medasr(medasr),
         telespeech_ctc(telespeech_ctc),
         tokens(tokens),
         num_threads(num_threads),
diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
index 11f7b81d..93505043 100644
--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
@@ -38,6 +38,11 @@ OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
       // tdnn models from yesno have a SIL token, we should remove it.
       continue;
     }
+
+    if (sym_table.Contains("</s>") && src.tokens[i] == sym_table["</s>"]) {
+      // Skip </s> for Google MedASR
+      continue;
+    }
     auto sym = sym_table[src.tokens[i]];
     text.append(sym);
 
@@ -58,6 +63,10 @@ OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
     text = sym_table.DecodeByteBpe(text);
   }
 
+  if (!text.empty() && text.front() == ' ') {
+    text.erase(0, 1);
+  }
+
   r.text = std::move(text);
 
   float frame_shift_s = frame_shift_ms / 1000. * subsampling_factor;
@@ -144,6 +153,17 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
       config_.feat_config.normalize_samples = false;
     }
 
+    if (!config_.model_config.medasr.model.empty()) {
+      config_.feat_config.low_freq = 125;
+      config_.feat_config.high_freq = 7500;
+      config_.feat_config.remove_dc_offset = false;
+      config_.feat_config.dither = 0;
+      config_.feat_config.preemph_coeff = 0;
+      config_.feat_config.window_type = "hanning";
+      config_.feat_config.feature_dim = 128;
+      config_.feat_config.snip_edges = true;
+    }
+
     config_.feat_config.nemo_normalize_type =
         model_->FeatureNormalizationMethod();
 
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 7aabf8d8..d894f955 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -217,6 +217,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       !config.model_config.tdnn.model.empty() ||
       !config.model_config.wenet_ctc.model.empty() ||
       !config.model_config.omnilingual.model.empty() ||
+      !config.model_config.medasr.model.empty() ||
       !config.model_config.dolphin.model.empty()) {
     return std::make_unique<OfflineRecognizerCtcImpl>(config);
   }
@@ -547,6 +548,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       !config.model_config.tdnn.model.empty() ||
       !config.model_config.wenet_ctc.model.empty() ||
       !config.model_config.omnilingual.model.empty() ||
+      !config.model_config.medasr.model.empty() ||
       !config.model_config.dolphin.model.empty()) {
     return std::make_unique<OfflineRecognizerCtcImpl>(mgr, config);
   }
diff --git a/sherpa-onnx/python/csrc/CMakeLists.txt b/sherpa-onnx/python/csrc/CMakeLists.txt
index 0b55ae91..69b47c50 100644
--- a/sherpa-onnx/python/csrc/CMakeLists.txt
+++ b/sherpa-onnx/python/csrc/CMakeLists.txt
@@ -14,6 +14,7 @@ set(srcs
   offline-dolphin-model-config.cc
   offline-fire-red-asr-model-config.cc
   offline-lm-config.cc
+  offline-medasr-ctc-model-config.cc
   offline-model-config.cc
   offline-moonshine-model-config.cc
   offline-nemo-enc-dec-ctc-model-config.cc
diff --git a/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
new file mode 100644
index 00000000..8d13a2ef
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
@@ -0,0 +1,22 @@
+// sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
+
+#include <string>
+#include <vector>
+
+#include "sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineMedAsrCtcModelConfig(py::module *m) {
+  using PyClass = OfflineMedAsrCtcModelConfig;
+  py::class_<PyClass>(*m, "OfflineMedAsrCtcModelConfig")
+      .def(py::init<const std::string &>(), py::arg("model"))
+      .def_readwrite("model", &PyClass::model)
+      .def("__str__", &PyClass::ToString);
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
new file mode 100644
index 00000000..f09a78b3
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
@@ -0,0 +1,16 @@
+// sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
+//
+// Copyright (c)  2023  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+#define SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+
+#include "sherpa-onnx/python/csrc/sherpa-onnx.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineMedAsrCtcModelConfig(py::module *m);
+
+}
+
+#endif  // SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
diff --git a/sherpa-onnx/python/csrc/offline-model-config.cc b/sherpa-onnx/python/csrc/offline-model-config.cc
index 6c1286b8..59fa03ab 100644
--- a/sherpa-onnx/python/csrc/offline-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-model-config.cc
@@ -11,6 +11,7 @@
 #include "sherpa-onnx/python/csrc/offline-canary-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-dolphin-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-fire-red-asr-model-config.h"
+#include "sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-nemo-enc-dec-ctc-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
@@ -38,6 +39,7 @@ void PybindOfflineModelConfig(py::module *m) {
   PybindOfflineDolphinModelConfig(m);
   PybindOfflineCanaryModelConfig(m);
   PybindOfflineOmnilingualAsrCtcModelConfig(m);
+  PybindOfflineMedAsrCtcModelConfig(m);
 
   using PyClass = OfflineModelConfig;
   py::class_<PyClass>(*m, "OfflineModelConfig")
@@ -54,9 +56,10 @@ void PybindOfflineModelConfig(py::module *m) {
                     const OfflineDolphinModelConfig &,
                     const OfflineCanaryModelConfig &,
                     const OfflineOmnilingualAsrCtcModelConfig &,
-                    const std::string &, const std::string &, int32_t, bool,
+                    const OfflineMedAsrCtcModelConfig &, const std::string &,
+                    const std::string &, int32_t, bool, const std::string &,
                     const std::string &, const std::string &,
-                    const std::string &, const std::string &>(),
+                    const std::string &>(),
            py::arg("transducer") = OfflineTransducerModelConfig(),
            py::arg("paraformer") = OfflineParaformerModelConfig(),
            py::arg("nemo_ctc") = OfflineNemoEncDecCtcModelConfig(),
@@ -70,6 +73,7 @@ void PybindOfflineModelConfig(py::module *m) {
            py::arg("dolphin") = OfflineDolphinModelConfig(),
            py::arg("canary") = OfflineCanaryModelConfig(),
            py::arg("omnilingual") = OfflineOmnilingualAsrCtcModelConfig(),
+           py::arg("medasr") = OfflineMedAsrCtcModelConfig(),
            py::arg("telespeech_ctc") = "", py::arg("tokens") = "",
            py::arg("num_threads") = 1, py::arg("debug") = false,
            py::arg("provider") = "cpu", py::arg("model_type") = "",
@@ -87,6 +91,7 @@ void PybindOfflineModelConfig(py::module *m) {
       .def_readwrite("dolphin", &PyClass::dolphin)
       .def_readwrite("canary", &PyClass::canary)
       .def_readwrite("omnilingual", &PyClass::omnilingual)
+      .def_readwrite("medasr", &PyClass::medasr)
       .def_readwrite("telespeech_ctc", &PyClass::telespeech_ctc)
       .def_readwrite("tokens", &PyClass::tokens)
       .def_readwrite("num_threads", &PyClass::num_threads)
diff --git a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
index d8bce13a..8e8e73c6 100644
--- a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
@@ -1,4 +1,4 @@
-// sherpa-onnx/python/csrc/offline-wenet-model-config.cc
+// sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
 //
 // Copyright (c)  2023  Xiaomi Corporation
 
diff --git a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
index ea92c46f..b9df4f07 100644
--- a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
+++ b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
@@ -1,4 +1,4 @@
-// sherpa-onnx/python/csrc/offline-wenet-model-config.h
+// sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
 //
 // Copyright (c)  2023  Xiaomi Corporation
 
diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
index 1194c664..7149f87a 100644
--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
@@ -8,6 +8,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
     HomophoneReplacerConfig,
     OfflineCanaryModelConfig,
     OfflineOmnilingualAsrCtcModelConfig,
+    OfflineMedAsrCtcModelConfig,
     OfflineCtcFstDecoderConfig,
     OfflineDolphinModelConfig,
     OfflineFireRedAsrModelConfig,
@@ -536,6 +537,56 @@ class OfflineRecognizer(object):
         self.config = recognizer_config
         return self
 
+    @classmethod
+    def from_medasr_ctc(
+        cls,
+        model: str,
+        tokens: str,
+        num_threads: int = 1,
+        decoding_method: str = "greedy_search",
+        debug: bool = False,
+        provider: str = "cpu",
+    ):
+        """
+        Please refer to
+        `<https://k2-fsa.github.io/sherpa/onnx/medasr/index.html>`_
+        to download pre-trained models.
+
+        Args:
+          model:
+            Path to ``model.onnx``.
+          tokens:
+            Path to ``tokens.txt``. Each line in ``tokens.txt`` contains two
+            columns::
+
+                symbol integer_id
+
+          num_threads:
+            Number of threads for neural network computation.
+          decoding_method:
+            The only supported decoding method is greedy_search.
+          debug:
+            True to show debug messages.
+          provider:
+            onnxruntime execution providers. Valid values are: cpu, cuda, coreml.
+        """
+        self = cls.__new__(cls)
+        model_config = OfflineModelConfig(
+            medasr=OfflineMedAsrCtcModelConfig(model=model),
+            tokens=tokens,
+            num_threads=num_threads,
+            debug=debug,
+            provider=provider,
+        )
+
+        recognizer_config = OfflineRecognizerConfig(
+            model_config=model_config,
+            decoding_method=decoding_method,
+        )
+        self.recognizer = _Recognizer(recognizer_config)
+        self.config = recognizer_config
+        return self
+
     @classmethod
     def from_omnilingual_asr_ctc(
         cls,

commit ee3611d5017f1e8eb49e05627b42041efc3f9491
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 25 13:56:11 2025 +0800

    Export Google MedASR to sherpa-onnx (#2934)
    
    This pull request integrates Google's MedASR models into the "sherpa-onnx" project by providing a complete workflow for exporting these specialized medical speech recognition models to ONNX format. It includes scripts for model conversion, dynamic quantization, and testing, enabling efficient deployment and inference of MedASR within the "sherpa-onnx" environment. The changes ensure proper licensing attribution and metadata embedding for the exported models.

diff --git a/.github/workflows/export-medasr-ctc-to-onnx.yaml b/.github/workflows/export-medasr-ctc-to-onnx.yaml
new file mode 100644
index 00000000..80aa921f
--- /dev/null
+++ b/.github/workflows/export-medasr-ctc-to-onnx.yaml
@@ -0,0 +1,161 @@
+name: export-medasr-ctc-to-onnx
+
+on:
+  push:
+    branches:
+      - export-medasr-onnx
+  workflow_dispatch:
+
+concurrency:
+  group: export-medasr-ctc-to-onnx-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-medasr-ctc-to-onnx:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: export medasr ctc
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [macos-latest]
+        python-version: ["3.10"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Run
+        shell: bash
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        run: |
+          cd scripts/medasr
+          ./run.sh
+
+      - name: Download test data
+        shell: bash
+        run: |
+          cd scripts/medasr
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/0.wav
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/transcript.txt
+
+          ls -lh
+
+      - name: Test fp32
+        shell: bash
+        run: |
+          cd scripts/medasr
+
+          python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./0.wav
+
+          cat transcript.txt
+
+      - name: Test int8
+        shell: bash
+        run: |
+          cd scripts/medasr
+
+          python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./0.wav
+
+          cat transcript.txt
+
+      - name: Collect fp32 files
+        shell: bash
+        run: |
+          cd scripts/medasr
+
+          d=sherpa-onnx-medasr-ctc-en-2025-12-25
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v model.onnx $d
+          cp -v README.md $d
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs/
+          cp -v transcript.txt $d/test_wavs/
+
+          tar cjvf $d.tar.bz2 $d
+
+          ls -lh $d
+          ls -lh *.tar.bz2
+
+          mv $d ../..
+          mv $d.tar.bz2 ../..
+
+      - name: Collect int8 files
+        shell: bash
+        run: |
+          cd scripts/medasr
+
+          d=sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v model.int8.onnx $d
+          cp -v README.md $d
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs/
+          cp -v transcript.txt $d/test_wavs/
+
+          tar cjvf $d.tar.bz2 $d
+
+          ls -lh $d
+          ls -lh *.tar.bz2
+
+          mv $d ../..
+          mv $d.tar.bz2 ../..
+
+      - name: Release
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 5
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            names=(
+             sherpa-onnx-medasr-ctc-en-2025-12-25
+             sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+            )
+            for d in ${names[@]}; do
+              if [ ! -d $d ]; then
+                echo "$d does not exist - skip it"
+                continue;
+              fi
+
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
+              cp -av $d/* ./huggingface
+              cd huggingface
+              git lfs track "*.onnx"
+              git lfs track "*.wav"
+              git status
+              git add .
+              git status
+              git commit -m "add models"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+              cd ..
+            done
diff --git a/scripts/medasr/README.md b/scripts/medasr/README.md
new file mode 100644
index 00000000..654fbd01
--- /dev/null
+++ b/scripts/medasr/README.md
@@ -0,0 +1,24 @@
+---
+license: other
+license_name: health-ai-developer-foundations
+license_link: https://developers.google.com/health-ai-developer-foundations/terms
+language:
+- en
+pipeline_tag: automatic-speech-recognition
+library_name: transformers
+tags:
+- medical-asr
+- radiology
+- medical
+---
+
+# Introduction
+
+This directory includes models sourced from:
+
+https://github.com/Google-Health/medasr
+
+All model files are governed by the Health AI Developer Foundations Terms of Use.
+For full licensing details, please refer to:
+
+https://developers.google.com/health-ai-developer-foundations/terms
diff --git a/scripts/medasr/export_onnx.py b/scripts/medasr/export_onnx.py
new file mode 100755
index 00000000..a0fce456
--- /dev/null
+++ b/scripts/medasr/export_onnx.py
@@ -0,0 +1,128 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+"""
+Make sure you have set the environment variable
+
+    export HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
+
+where hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx is your Huggingface access token.
+"""
+
+from typing import Any, Dict
+
+import onnx
+import torch
+from onnxruntime.quantization import QuantType, quantize_dynamic
+from transformers import AutoModelForCTC, AutoProcessor
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, Any]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    model.metadata_props.clear()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+class Wrapper(torch.nn.Module):
+    def __init__(self, m):
+        super().__init__()
+        self.m = m
+
+    def forward(self, x: torch.Tensor, mask: torch.Tensor):
+        """
+        Args:
+          x: (N, T, C), dtype float32
+          mask: (N, T), dtype int64. Valid positions are 1. Padding positions are 0.
+        Returns:
+          logits: (N, T/4, vocob_size), dtype float32
+          logits_len: (N,), dtype int64
+        """
+        o = self.m(x, mask.bool())
+        logits_len = self.m._get_subsampling_output_length(mask.sum(-1)).to(torch.int64)
+        return o.logits, logits_len
+
+
+def generate_tokens(tokenizer):
+    vocab = tokenizer.get_vocab()
+    id2token = {i: t for t, i in vocab.items()}
+
+    with open("tokens.txt", "w", encoding="utf-8") as f:
+        for i in range(tokenizer.vocab_size):
+            if i == tokenizer.pad_token_id:
+                f.write(f"<blk> {i}\n")
+            else:
+                f.write(f"{id2token[i]} {i}\n")
+    print("saved to tokens.txt")
+
+
+@torch.no_grad()
+def main():
+    model_id = "google/medasr"
+    processor = AutoProcessor.from_pretrained(model_id)
+
+    generate_tokens(processor.tokenizer)
+
+    model = AutoModelForCTC.from_pretrained(model_id)
+
+    w = Wrapper(model)
+    w.eval()
+
+    filename = "model.onnx"
+    x = torch.rand(1, 100, 128)
+    mask = torch.ones(1, x.shape[1], dtype=torch.int64)
+    torch.onnx.export(
+        w,
+        (x, mask),
+        filename,
+        input_names=["x", "mask"],
+        output_names=["logits", "logits_len"],
+        dynamic_axes={
+            "x": {0: "N", 1: "T"},
+            "mask": {0: "N", 1: "T"},
+            "logits": {0: "N", 1: "T_4"},
+            "logits_len": {0: "N"},
+        },
+        opset_version=14,
+        external_data=False,
+        dynamo=False,
+    )
+
+    meta_data = {
+        "model_type": "medasr_ctc",
+        "version": "20251225",
+        "model_author": "google",
+        "maintainer": "k2-fsa",
+        "vocab_size": processor.tokenizer.vocab_size,
+        "url": "https://github.com/Google-Health/medasr",
+        "license": "https://developers.google.com/health-ai-developer-foundations/terms",
+    }
+    add_meta_data(filename=filename, meta_data=meta_data)
+
+    filename_int8 = "model.int8.onnx"
+    quantize_dynamic(
+        model_input=filename,
+        model_output=filename_int8,
+        op_types_to_quantize=["MatMul"],
+        # Note that we have to use QUInt8 here.
+        #
+        # When QInt8 is used, C++ onnxruntime produces incorrect results
+        weight_type=QuantType.QUInt8,
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/medasr/run.sh b/scripts/medasr/run.sh
new file mode 100755
index 00000000..779a13c6
--- /dev/null
+++ b/scripts/medasr/run.sh
@@ -0,0 +1,21 @@
+#!/usr/bin/env bash
+
+if [ -z $HF_TOKEN ]; then
+  echo "Please first run export HF_TOKEN=your_huggingface_access_token."
+  exit 1
+fi
+
+pip install \
+  accelerate \
+  bitsandbytes \
+  git+https://github.com/huggingface/transformers.git@65dc261512cbdb1ee72b88ae5b222f2605aad8e5 \
+  onnx==1.17.0 \
+  onnxruntime==1.17.1 \
+  librosa \
+  onnxscript \
+  "numpy<2" \
+  kaldi-native-fbank
+
+./export_onnx.py
+
+ls -lh
diff --git a/scripts/medasr/test_onnx.py b/scripts/medasr/test_onnx.py
new file mode 100755
index 00000000..3436403f
--- /dev/null
+++ b/scripts/medasr/test_onnx.py
@@ -0,0 +1,163 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import argparse
+import time
+
+import kaldi_native_fbank as knf
+import librosa
+import numpy as np
+import onnxruntime as ort
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--model",
+        type=str,
+        required=True,
+        help="Path to onnx model file",
+    )
+
+    parser.add_argument(
+        "--tokens",
+        type=str,
+        required=True,
+        help="Path to tokens.txt",
+    )
+
+    parser.add_argument(
+        "--wav",
+        type=str,
+        required=True,
+        help="Path to test wav",
+    )
+    return parser.parse_args()
+
+
+class OnnxModel:
+    def __init__(self, filename):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 1
+
+        self.session_opts = session_opts
+
+        self.model = ort.InferenceSession(
+            filename,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+
+    def __call__(self, x, mask):
+        """
+        Args:
+          x: (N, T, C), float32
+          mask: (N, T), int64
+        Returns:
+          logits: (N, T/4, vocab_size), float32
+          logits_len: (N,) int64
+        """
+        logits, logits_len = self.model.run(
+            [
+                self.model.get_outputs()[0].name,
+                self.model.get_outputs()[1].name,
+            ],
+            {
+                self.model.get_inputs()[0].name: x,
+                self.model.get_inputs()[1].name: mask,
+            },
+        )
+
+        return logits, logits_len
+
+
+def load_tokens(tokens):
+    id2token = dict()
+    with open(tokens, encoding="utf-8") as f:
+        for line in f:
+            fields = line.split()
+            if len(fields) == 1:
+                id2token[int(fields[0])] = " "
+            else:
+                t, idx = fields
+                id2token[int(idx)] = t
+    return id2token
+
+
+def compute_feat(samples):
+    opts = knf.FbankOptions()
+    opts.frame_opts.dither = 0
+    opts.frame_opts.snip_edges = True
+    opts.frame_opts.window_type = "hanning"
+    opts.frame_opts.samp_freq = 16000
+    opts.frame_opts.preemph_coeff = 0
+    opts.frame_opts.remove_dc_offset = False
+    opts.mel_opts.num_bins = 128
+
+    online_fbank = knf.OnlineFbank(opts)
+    online_fbank.accept_waveform(16000, samples.tolist())
+    online_fbank.input_finished()
+
+    features = np.stack(
+        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
+    )
+    assert features.dtype == np.float32, features.dtype
+
+    features = np.ascontiguousarray(features)
+
+    return features
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+
+    model = OnnxModel(args.model)
+
+    samples, sample_rate = librosa.load(args.wav, sr=16000)
+
+    start = time.time()
+
+    assert sample_rate == 16000, sample_rate
+    features = compute_feat(samples)
+    mask = np.ones(features.shape[0], dtype=np.int64)[None]
+    features = features[None]
+
+    logits, logits_len = model(features, mask)
+    idx = logits[0, : logits_len[0]].argmax(axis=-1)
+
+    end = time.time()
+    elapsed_seconds = end - start
+    audio_duration = samples.shape[0] / 16000
+    real_time_factor = elapsed_seconds / audio_duration
+
+    print("idx", idx)
+
+    unique_ids = []
+    prev = -1
+    for i in idx.tolist():
+        if i == prev:
+            continue
+        unique_ids.append(i)
+        prev = i
+    print("unique_ids", unique_ids)
+    blank_id = 0
+    ids = [i for i in unique_ids if i != blank_id]
+    print(ids)
+
+    id2token = load_tokens(args.tokens)
+
+    tokens = [id2token[i] for i in ids]
+    text = "".join(tokens)
+    print(text)
+    text = text.replace("", " ")
+    print(text)
+    print(f"RTF: {real_time_factor}")
+
+
+if __name__ == "__main__":
+    main()

commit 6cb50059730ad2adb72d92572e820e2c92b89879
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 24 17:24:36 2025 +0800

    Add Android demo for Paraformer ASR with Qualcomm NPU. (#2932)
    
    This pull request significantly advances the Android demo by integrating Paraformer ASR models with Qualcomm NPU acceleration. The core changes involve extending the model configuration to support QNN-specific parameters for Paraformer, introducing robust asset management utilities for handling multi-file QNN models, and updating the Android application's initialization logic to prepare these models for NPU inference. Additionally, the build scripts have been adjusted to reflect the new model additions and naming conventions, ensuring a comprehensive update for NPU-powered ASR on Android.

diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
index 20d25bee..a9508adb 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
@@ -23,6 +23,23 @@ fun assetExists(assetManager: AssetManager, path: String): Boolean {
     return files.contains(fileName)
 }
 
+fun assetListExists(
+    assetManager: AssetManager,
+    paths: String
+): Boolean {
+    if (paths.isBlank()) return false
+
+    val pathList = paths.split(",")
+        .map { it.trim() }
+        .filter { it.isNotEmpty() }
+
+    if (pathList.isEmpty()) return false
+
+    return pathList.all { path ->
+        assetExists(assetManager, path)
+    }
+}
+
 fun copyAssetToInternalStorage(path: String, context: Context): String {
     val targetRoot = context.filesDir
     val outFile = File(targetRoot, path)
@@ -56,6 +73,23 @@ fun copyAssetToInternalStorage(path: String, context: Context): String {
     return outFile.absolutePath
 }
 
+fun copyAssetListToInternalStorage(
+    paths: String,
+    context: Context
+): String {
+    if (paths.isBlank()) return paths
+
+    val pathList = paths.split(",")
+        .map { it.trim() }
+        .filter { it.isNotEmpty() }
+
+    val copiedPaths = pathList.map { path ->
+        copyAssetToInternalStorage(path, context)
+    }
+
+    return copiedPaths.joinToString(",")
+}
+
 
 object SimulateStreamingAsr {
     private var _recognizer: OfflineRecognizer? = null
@@ -125,7 +159,10 @@ object SimulateStreamingAsr {
                 OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
 
                 // for qnn, we need to copy *.so files from assets folder to sd card
-                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty() && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()) {
+                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()
+                    && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()
+                    && config.modelConfig.paraformer.qnnConfig.backendLib.isEmpty()
+                ) {
                     Log.e(TAG, "You should provide libQnnHtp.so for qnn")
                     throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
                 }
@@ -166,6 +203,25 @@ object SimulateStreamingAsr {
                             config.modelConfig.zipformerCtc.qnnConfig.contextBinary,
                             context
                         )
+                } else if (config.modelConfig.paraformer.model.isNotEmpty()
+                    || assetListExists(
+                        context.assets,
+                        config.modelConfig.paraformer.qnnConfig.contextBinary
+                    )
+                ) {
+                    if (config.modelConfig.paraformer.model.isNotEmpty()) {
+                        config.modelConfig.paraformer.model =
+                            copyAssetListToInternalStorage(
+                                config.modelConfig.paraformer.model,
+                                context
+                            )
+                    }
+
+                    config.modelConfig.paraformer.qnnConfig.contextBinary =
+                        copyAssetListToInternalStorage(
+                            config.modelConfig.paraformer.qnnConfig.contextBinary,
+                            context
+                        )
                 }
 
                 if (config.hr.lexicon.isNotEmpty()) {
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
index ae5e1bd8..a4163b0b 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
@@ -163,7 +163,7 @@ fun HomeScreen() {
                                 offset += windowSize
                                 if (!isSpeechStarted && SimulateStreamingAsr.vad.isSpeechDetected()) {
                                     isSpeechStarted = true
-                                    // offset 0.25s
+                                    // offset 0.4s
                                     speechStartOffset = offset - 6400
                                     if(speechStartOffset < 0) {
                                         speechStartOffset = 0
diff --git a/scripts/apk/generate-asr-2pass-apk-script.py b/scripts/apk/generate-asr-2pass-apk-script.py
index aed2cb94..36bec2d9 100755
--- a/scripts/apk/generate-asr-2pass-apk-script.py
+++ b/scripts/apk/generate-asr-2pass-apk-script.py
@@ -114,7 +114,7 @@ def get_2nd_models():
             """,
         ),
         Model(
-            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
+            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
             idx=15,
             lang="zh_en_ko_ja_yue",
             short_name="sense_voice_2024_07_17_int8",
@@ -122,7 +122,6 @@ def get_2nd_models():
             pushd $model_name
 
             rm -rfv test_wavs
-            rm -fv model.onnx
             rm -fv *.py
 
             ls -lh
@@ -410,7 +409,7 @@ def get_models():
     second_zh = [
         "sherpa-onnx-paraformer-zh-2023-09-14",
         "icefall-asr-zipformer-wenetspeech-20230615",
-        "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
+        "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
         "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09",
         "sherpa-onnx-dolphin-base-ctc-multi-lang-int8-2025-04-02",
         "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10",
@@ -434,7 +433,7 @@ def get_models():
         ),
         (
             "sherpa-onnx-streaming-zipformer-en-20M-2023-02-17",
-            "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
+            "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
         ),
         (
             "sherpa-onnx-streaming-zipformer-en-20M-2023-02-17",
diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
index d70bff23..86f43faf 100755
--- a/scripts/apk/generate-qnn-vad-asr-apk-script.py
+++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
@@ -399,6 +399,38 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-5-seconds-paraformer-zh-2023-03-28-int8",
+            idx=9023,
+            lang="zh",
+            short_name="5-seconds-paraformer_zh_2023_03_28_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-5-seconds-paraformer-zh-2025-10-07-int8",
+            idx=9024,
+            lang="zh",
+            short_name="5-seconds-paraformer_zh_2025_10_07_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
index c84ef99c..e9c8caa8 100755
--- a/scripts/apk/generate-vad-asr-apk-script.py
+++ b/scripts/apk/generate-vad-asr-apk-script.py
@@ -95,7 +95,7 @@ def get_models():
             """,
         ),
         Model(
-            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
+            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
             idx=15,
             lang="zh_en_ko_ja_yue",
             lang2="",
@@ -105,7 +105,6 @@ def get_models():
             pushd $model_name
 
             rm -rfv test_wavs
-            rm -fv model.onnx
             rm -fv *.py
 
             ls -lh
diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
index 3d12d8ec..a1e13fc3 100644
--- a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
@@ -97,7 +97,9 @@ class OfflineParaformerModelQnn::Impl {
 
   template <typename Manager>
   Impl(Manager *mgr, const OfflineModelConfig &config) {
-    SHERPA_ONNX_LOGE("TODO(fangjun): To be implemented");
+    SHERPA_ONNX_LOGE(
+        "Please copy all files from assets to SD card and set assetManager to "
+        "null");
     SHERPA_ONNX_EXIT(-1);
   }
 
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
index 7e99533a..546c0e62 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
@@ -4,9 +4,11 @@ package com.k2fsa.sherpa.onnx;
 
 public class OfflineParaformerModelConfig {
     private final String model;
+    private final QnnConfig qnnConfig;
 
     private OfflineParaformerModelConfig(Builder builder) {
         this.model = builder.model;
+        this.qnnConfig = builder.qnnConfig;
     }
 
     public static Builder builder() {
@@ -17,9 +19,13 @@ public class OfflineParaformerModelConfig {
         return model;
     }
 
+    public QnnConfig getQnnConfig() {
+        return qnnConfig;
+    }
 
     public static class Builder {
         private String model = "";
+        private QnnConfig qnnConfig = QnnConfig.builder().build();
 
         public OfflineParaformerModelConfig build() {
             return new OfflineParaformerModelConfig(this);
@@ -29,5 +35,10 @@ public class OfflineParaformerModelConfig {
             this.model = model;
             return this;
         }
+
+        public Builder setQnnConfig(QnnConfig qnnConfig) {
+            this.qnnConfig = qnnConfig;
+            return this;
+        }
     }
 }
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index 3995d40a..fa2f7dff 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -99,6 +99,22 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.paraformer.model, model,
                               paraformer_config_cls, paraformer_config);
 
+  fid = env->GetFieldID(paraformer_config_cls, "qnnConfig",
+                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
+  jobject qnn_config = env->GetObjectField(paraformer_config, fid);
+  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.paraformer.qnn_config.backend_lib, backendLib,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.paraformer.qnn_config.context_binary, contextBinary,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.paraformer.qnn_config.system_lib,
+                              systemLib, qnn_config_cls, qnn_config);
+
   fid = env->GetFieldID(model_config_cls, "whisper",
                         "Lcom/k2fsa/sherpa/onnx/OfflineWhisperModelConfig;");
   jobject whisper_config = env->GetObjectField(model_config, fid);
@@ -168,8 +184,8 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
 
   fid = env->GetFieldID(sense_voice_config_cls, "qnnConfig",
                         "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
-  jobject qnn_config = env->GetObjectField(sense_voice_config, fid);
-  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
+  qnn_config = env->GetObjectField(sense_voice_config, fid);
+  qnn_config_cls = env->GetObjectClass(qnn_config);
 
   SHERPA_ONNX_JNI_READ_STRING(
       ans.model_config.sense_voice.qnn_config.backend_lib, backendLib,
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index f27dfbc5..7b7958fa 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -22,6 +22,7 @@ data class OfflineTransducerModelConfig(
 
 data class OfflineParaformerModelConfig(
     var model: String = "",
+    var qnnConfig: QnnConfig = QnnConfig(),
 )
 
 data class OfflineNemoEncDecCtcModelConfig(
@@ -405,7 +406,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         15 -> {
-            val modelDir = "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17"
+            val modelDir = "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17"
             return OfflineModelConfig(
                 senseVoice = OfflineSenseVoiceModelConfig(
                     model = "$modelDir/model.int8.onnx",
@@ -722,7 +723,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         42 -> {
-            val modelDir = "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10"
+            val modelDir =
+                "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10"
             return OfflineModelConfig(
                 wenetCtc = OfflineWenetCtcModelConfig(
                     model = "$modelDir/model.int8.onnx",
@@ -753,19 +755,20 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9000 -> {
-            val modelDir = "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
                     model = "$modelDir/libmodel.so",
                     qnnConfig = QnnConfig(
-                      // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
-                      //
-                      // model.bin is created in the first run and is used from the second run
-                      // to speed up the initialization
-                      backendLib = "libQnnHtp.so",
-                      systemLib = "libQnnSystem.so",
-                      contextBinary = "$modelDir/model.bin",
+                        // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
+                        //
+                        // model.bin is created in the first run and is used from the second run
+                        // to speed up the initialization
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
                     ),
                 ),
                 tokens = "$modelDir/tokens.txt",
@@ -774,7 +777,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9001 -> {
-            val modelDir = "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -791,7 +795,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9002 -> {
-            val modelDir = "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -808,7 +813,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9003 -> {
-            val modelDir = "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -824,7 +830,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9004 -> {
-            val modelDir = "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -840,7 +847,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9005 -> {
-            val modelDir = "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -856,7 +864,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9006 -> {
-            val modelDir = "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -872,7 +881,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9007 -> {
-            val modelDir = "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -888,7 +898,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9008 -> {
-            val modelDir = "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -904,7 +915,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9009 -> {
-            val modelDir = "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -920,7 +932,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9010 -> {
-            val modelDir = "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -936,7 +949,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9011 -> {
-            val modelDir = "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -953,7 +967,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9012 -> {
-            val modelDir = "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -970,7 +985,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9013 -> {
-            val modelDir = "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -987,7 +1003,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9014 -> {
-            val modelDir = "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1003,7 +1020,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9015 -> {
-            val modelDir = "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1019,7 +1037,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9016 -> {
-            val modelDir = "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1035,7 +1054,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9017 -> {
-            val modelDir = "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1051,7 +1071,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9018 -> {
-            val modelDir = "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1067,7 +1088,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9019 -> {
-            val modelDir = "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1083,7 +1105,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9020 -> {
-            val modelDir = "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1099,7 +1122,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9021 -> {
-            val modelDir = "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            val modelDir =
+                "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
             return OfflineModelConfig(
                 provider = "qnn",
                 zipformerCtc = OfflineZipformerCtcModelConfig(
@@ -1115,8 +1139,9 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
         }
 
         9022 -> {
-            // for my Xiaomi 17 Pro
-            val modelDir = "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
+            // for Xiaomi 17 Pro
+            val modelDir =
+                "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
             return OfflineModelConfig(
                 provider = "qnn",
                 senseVoice = OfflineSenseVoiceModelConfig(
@@ -1131,6 +1156,61 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                 debug = true,
             )
         }
+
+        9023 -> {
+            val modelDir = "sherpa-onnx-qnn-5-seconds-paraformer-zh-2023-03-28-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                paraformer = OfflineParaformerModelConfig(
+                    model = "$modelDir/libencoder.so,$modelDir/libpredictor.so,$modelDir/libdecoder.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        // The following three *.bin files are generated during the first run
+                        // and are used to replace the corresponding *.so files in later runs
+                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
+
+        9024 -> {
+            val modelDir = "sherpa-onnx-qnn-5-seconds-paraformer-zh-2025-10-07-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                paraformer = OfflineParaformerModelConfig(
+                    model = "$modelDir/libencoder.so,$modelDir/libpredictor.so,$modelDir/libdecoder.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        // The following three *.bin files are generated during the first run
+                        // and are used to replace the corresponding *.so files in later runs
+                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
+
+        9025 -> {
+            // for Xiaomi 17 Pro
+            val modelDir = "sherpa-onnx-qnn-SM8850-binary-5-seconds-paraformer-zh-2023-03-28-int8"
+            return OfflineModelConfig(
+                provider = "qnn",
+                paraformer = OfflineParaformerModelConfig(
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
     }
     return null
 }

commit cdbf0a8efa66eb1aaf0d7c0abe4ce0f7026a0b00
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 24 16:20:56 2025 +0800

    Add C++ runtime for Paraformer ASR models with Qualcomm NPU using QNN (#2931)
    
    This pull request significantly expands the capabilities of the sherpa-onnx project by integrating C++ runtime support for Paraformer ASR models on Qualcomm NPUs. This allows for accelerated and more power-efficient speech recognition on devices equipped with Qualcomm hardware, enhancing the project's reach and performance on mobile and embedded platforms.

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 053c652e..5c3338fe 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -235,6 +235,7 @@ endif()
 if(SHERPA_ONNX_ENABLE_QNN)
   list(APPEND sources
     ./qnn/offline-sense-voice-model-qnn.cc
+    ./qnn/offline-paraformer-model-qnn.cc
     ./qnn/offline-zipformer-ctc-model-qnn.cc
     ./qnn/qnn-backend.cc
     ./qnn/qnn-model.cc
diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
index e7075bc4..cf981ac3 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
@@ -21,6 +21,11 @@ void OfflineParaformerModelConfig::Register(ParseOptions *po) {
       "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
       "If you use RK NPU, it is "
       "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn");
+
+  std::string prefix = "paraformer";
+  ParseOptions p(prefix, po);
+
+  qnn_config.Register(&p);
 }
 
 bool OfflineParaformerModelConfig::Validate() const {
@@ -46,6 +51,13 @@ bool OfflineParaformerModelConfig::Validate() const {
       return false;
     }
 
+    for (const auto &name : filenames) {
+      if (!FileExists(name)) {
+        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
+        return false;
+      }
+    }
+
     return true;
   }
 
@@ -63,11 +75,82 @@ bool OfflineParaformerModelConfig::Validate() const {
       return false;
     }
 
+    for (const auto &name : filenames) {
+      if (!FileExists(name)) {
+        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
+        return false;
+      }
+    }
+
     return true;
   }
 
-  SHERPA_ONNX_LOGE("Please pass *.onnx, *.om, or *.rknn models. Given '%s'",
-                   model.c_str());
+  if (EndsWith(model, ".so")) {
+    std::vector<std::string> filenames;
+    SplitStringToVector(model, ",", false, &filenames);
+    if (filenames.size() != 3 || !EndsWith(filenames[0], "encoder.so") ||
+        !EndsWith(filenames[1], "predictor.so") ||
+        !EndsWith(filenames[2], "decoder.so")) {
+      SHERPA_ONNX_LOGE(
+          "For QNN, you should pass "
+          "/path/libencoder.so,/path/libpredictor.so,/path/libdecoder.so. "
+          "Given '%s'",
+          model.c_str());
+      return false;
+    }
+
+    for (const auto &name : filenames) {
+      if (!FileExists(name)) {
+        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
+        return false;
+      }
+    }
+
+    if (!qnn_config.Validate()) {
+      return false;
+    }
+
+    return true;
+  }
+
+  if (model.empty() && !qnn_config.context_binary.empty()) {
+    // we require that the context_binary exists
+    if (!FileExists(qnn_config.context_binary)) {
+      SHERPA_ONNX_LOGE(
+          "Model is empty, but you provide a context binary that does not "
+          "exist");
+      return false;
+    }
+
+    std::vector<std::string> filenames;
+    SplitStringToVector(model, ",", false, &filenames);
+    if (filenames.size() != 3) {
+      SHERPA_ONNX_LOGE(
+          "For Paraformer with QNN, you should pass "
+          "/path/encoder.bin,/path/predictor.bin,/path/decoder.bin"
+          "Given '%s'",
+          model.c_str());
+      return false;
+    }
+
+    for (const auto &name : filenames) {
+      if (!FileExists(name)) {
+        SHERPA_ONNX_LOGE("Paraformer context binary '%s' does not exist",
+                         name.c_str());
+        return false;
+      }
+    }
+
+    if (!qnn_config.Validate()) {
+      return false;
+    }
+
+    return true;
+  }
+
+  SHERPA_ONNX_LOGE(
+      "Please pass *.onnx, *.om, *.rknn, or *.so models. Given '%s'",
+      model.c_str());
   return false;
 }
 
@@ -75,7 +158,13 @@ std::string OfflineParaformerModelConfig::ToString() const {
   std::ostringstream os;
 
   os << "OfflineParaformerModelConfig(";
-  os << "model=\"" << model << "\")";
+  os << "model=\"" << model << "\"";
+
+  if (!qnn_config.backend_lib.empty()) {
+    os << ", qnn_config=" << qnn_config.ToString();
+  }
+
+  os << ")";
 
   return os.str();
 }
diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.h b/sherpa-onnx/csrc/offline-paraformer-model-config.h
index e4fa94d8..8c7f0567 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model-config.h
+++ b/sherpa-onnx/csrc/offline-paraformer-model-config.h
@@ -7,6 +7,7 @@
 #include <string>
 
 #include "sherpa-onnx/csrc/parse-options.h"
+#include "sherpa-onnx/csrc/qnn-config.h"
 
 namespace sherpa_onnx {
 
@@ -17,8 +18,14 @@ struct OfflineParaformerModelConfig {
   // for rknn,
   // model is
   // "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn"
+  //
+  // for qnn with shared libs, model is
+  // model is
+  // "/path/to/libencoder.so,/path/to/libpredictor.so,/path/to/libdecoder.so"
   std::string model;
 
+  QnnConfig qnn_config;
+
   OfflineParaformerModelConfig() = default;
   explicit OfflineParaformerModelConfig(const std::string &model)
       : model(model) {}
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 38bf3e5b..7aabf8d8 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -58,6 +58,7 @@
 #endif
 
 #if SHERPA_ONNX_ENABLE_QNN
+#include "sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h"
 #include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
 #include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
 #endif
@@ -180,10 +181,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
                !config.model_config.zipformer_ctc.qnn_config.context_binary
                     .empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
+    } else if (!config.model_config.paraformer.model.empty() ||
+               !config.model_config.paraformer.qnn_config.context_binary
+                    .empty()) {
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelQnn>>(
+          config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models and Zipformer CTC models are currently "
-          "supported by qnn for non-streaming ASR.");
+          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
+          "supported by QNN for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
@@ -504,10 +511,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
                     .empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
                                                                     config);
+    } else if (!config.model_config.paraformer.model.empty() ||
+               !config.model_config.paraformer.qnn_config.context_binary
+                    .empty()) {
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelQnn>>(
+          mgr, config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models and Zipformer CTC models are currently "
-          "supported by qnn for non-streaming ASR.");
+          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
+          "supported by QNN for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
index 7b97fbac..cfbab2ad 100644
--- a/sherpa-onnx/csrc/offline-stream.cc
+++ b/sherpa-onnx/csrc/offline-stream.cc
@@ -14,6 +14,7 @@
 #include <utility>
 #include <vector>
 
+#include "Eigen/Core"
 #include "kaldi-native-fbank/csrc/online-feature.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/math.h"
diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
new file mode 100644
index 00000000..3d12d8ec
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
@@ -0,0 +1,654 @@
+// sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <mutex>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/math.h"
+#include "sherpa-onnx/csrc/qnn/macros.h"
+#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
+#include "sherpa-onnx/csrc/qnn/qnn-model.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+class OfflineParaformerModelQnn::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    std::vector<std::string> filenames;
+    SplitStringToVector(config_.paraformer.model, ",", true, &filenames);
+    if (!filenames.empty()) {
+      if (filenames.size() != 3) {
+        SHERPA_ONNX_LOGE("Invalid Paraformer QNN model '%s'",
+                         config_.paraformer.model.c_str());
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+
+    std::vector<std::string> binary_filenames;
+    SplitStringToVector(config_.paraformer.qnn_config.context_binary, ",", true,
+                        &binary_filenames);
+    if (!binary_filenames.empty()) {
+      if (binary_filenames.size() != 3) {
+        SHERPA_ONNX_LOGE(
+            "There should be 3 files for Paraformer context binary. Actual: "
+            "%d. '%s'",
+            static_cast<int32_t>(binary_filenames.size()),
+            config_.paraformer.qnn_config.context_binary.c_str());
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+
+    if (filenames.empty() && binary_filenames.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You need to provide either a model or a context binary for "
+          "Paraformer with QNN");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    bool ok = InitEncoder(filenames.empty() ? "" : filenames[0],
+                          binary_filenames.empty() ? "" : binary_filenames[0]);
+    if (!ok) {
+      SHERPA_ONNX_LOGE(
+          "Failed to init encoder with lib file '%s', context binary: '%s'",
+          filenames.empty() ? "" : filenames[0].c_str(),
+          binary_filenames.empty() ? "" : binary_filenames[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    ok = InitPredictor(filenames.empty() ? "" : filenames[1],
+                       binary_filenames.empty() ? "" : binary_filenames[1]);
+    if (!ok) {
+      SHERPA_ONNX_LOGE(
+          "Failed to init predictor with lib file '%s', context binary: '%s'",
+          filenames.empty() ? "" : filenames[1].c_str(),
+          binary_filenames.empty() ? "" : binary_filenames[1].c_str());
+      return;
+    }
+
+    ok = InitDecoder(filenames.empty() ? "" : filenames[2],
+                     binary_filenames.empty() ? "" : binary_filenames[2]);
+    if (!ok) {
+      SHERPA_ONNX_LOGE(
+          "Failed to init decoder with lib file '%s', context binary: '%s'",
+          filenames.empty() ? "" : filenames[2].c_str(),
+          binary_filenames.empty() ? "" : binary_filenames[2].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) {
+    SHERPA_ONNX_LOGE("TODO(fangjun): To be implemented");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  std::vector<float> Run(std::vector<float> features) {
+    std::lock_guard<std::mutex> lock(mutex_);
+
+    std::vector<float> encoder_out = RunEncoder(std::move(features));
+    std::vector<float> transposed_encoder_out =
+        Transpose(encoder_out.data(), encoder_out_dim1_, encoder_out_dim2_);
+
+    std::vector<float> alphas = RunPredictor(transposed_encoder_out);
+
+    std::vector<float> acoustic_embedding =
+        ComputeAcousticEmbedding(encoder_out, alphas, encoder_out_dim2_);
+
+    int32_t num_tokens = acoustic_embedding.size() / encoder_out_dim2_;
+
+    acoustic_embedding.resize(encoder_out.size());
+
+    std::vector<float> transposed_acoustic_embedding = Transpose(
+        acoustic_embedding.data(), encoder_out_dim1_, encoder_out_dim2_);
+
+    std::vector<float> decoder_out = RunDecoder(
+        transposed_encoder_out, transposed_acoustic_embedding, num_tokens);
+
+    decoder_out = Transpose(decoder_out.data(), vocab_size_, encoder_out_dim1_);
+    decoder_out.resize(num_tokens * vocab_size_);
+    return decoder_out;
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+
+ private:
+  std::vector<float> RunEncoder(std::vector<float> features) const {
+    features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
+
+    encoder_model_->SetInputTensorData("x", features.data(), features.size());
+    encoder_model_->Run();
+    return encoder_model_->GetOutputTensorData("encoder_out");
+  }
+
+  std::vector<float> RunPredictor(
+      const std::vector<float> &transposed_encoder_out) const {
+    predictor_model_->SetInputTensorData("encoder_out",
+                                         transposed_encoder_out.data(),
+                                         transposed_encoder_out.size());
+    predictor_model_->Run();
+    return predictor_model_->GetOutputTensorData("alphas");
+  }
+
+  std::vector<float> RunDecoder(
+      const std::vector<float> &transposed_encoder_out,
+      const std::vector<float> &transposed_acoustic_embedding,
+      int32_t num_tokens) const {
+    std::vector<int32_t> mask(encoder_out_dim1_, 1);
+    std::fill(mask.begin() + num_tokens, mask.end(), 0);
+
+    decoder_model_->SetInputTensorData("encoder_out",
+                                       transposed_encoder_out.data(),
+                                       transposed_encoder_out.size());
+
+    decoder_model_->SetInputTensorData("acoustic_embedding",
+                                       transposed_acoustic_embedding.data(),
+                                       transposed_acoustic_embedding.size());
+
+    decoder_model_->SetInputTensorData("mask", mask.data(), mask.size());
+
+    decoder_model_->Run();
+
+    return decoder_model_->GetOutputTensorData("decoder_out");
+  }
+
+  std::vector<float> ApplyLFR(std::vector<float> in) const {
+    int32_t lfr_window_size = 7;
+    int32_t lfr_window_shift = 6;
+    int32_t in_feat_dim = 80;
+
+    int32_t in_num_frames = in.size() / in_feat_dim;
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
+    int32_t out_num_frames =
+        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+
+    if (out_num_frames > num_input_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          out_num_frames, num_input_frames_);
+
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios.");
+
+      out_num_frames = num_input_frames_;
+    }
+
+    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+
+    std::vector<float> out(num_input_frames_ * out_feat_dim);
+
+    const float *p_in = in.data();
+    float *p_out = out.data();
+
+    for (int32_t i = 0; i != out_num_frames; ++i) {
+      std::copy(p_in, p_in + out_feat_dim, p_out);
+
+      p_out += out_feat_dim;
+      p_in += lfr_window_shift * in_feat_dim;
+    }
+
+    return out;
+  }
+
+  bool InitEncoder(const std::string &lib_filename,
+                   const std::string &context_binary) {
+    encoder_backend_ = std::make_unique<QnnBackend>(
+        config_.paraformer.qnn_config.backend_lib, config_.debug);
+
+    if (context_binary.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from encoder model lib '%s' since context binary is not "
+            "given.",
+            lib_filename.c_str());
+      }
+
+      InitEncoderFromModelLib(lib_filename);
+
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Skip generating encoder context binary since you don't provide a "
+            "path to save it");
+      }
+    } else if (!FileExists(context_binary)) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init encoder from model lib '%s' since context binary '%s' does "
+            "not exist",
+            lib_filename.c_str(), context_binary.c_str());
+      }
+
+      InitEncoderFromModelLib(lib_filename);
+
+      CreateContextBinary(encoder_model_.get(), context_binary);
+    } else {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("Init from encoder context binary '%s'",
+                         context_binary.c_str());
+      }
+      InitEncoderFromContextBinary(context_binary);
+    }
+
+    PostInitEncoder();
+
+    return true;
+  }
+
+  bool InitPredictor(const std::string &lib_filename,
+                     const std::string &context_binary) {
+    predictor_backend_ = std::make_unique<QnnBackend>(
+        config_.paraformer.qnn_config.backend_lib, config_.debug);
+
+    if (context_binary.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from predictor model lib '%s' since context binary is not "
+            "given.",
+            lib_filename.c_str());
+      }
+
+      InitPredictorFromModelLib(lib_filename);
+
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Skip generating predictor context binary since you don't provide "
+            "a path to save it");
+      }
+    } else if (!FileExists(context_binary)) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init predictor from model lib '%s' since context binary '%s' does "
+            "not exist",
+            lib_filename.c_str(), context_binary.c_str());
+      }
+
+      InitPredictorFromModelLib(lib_filename);
+      CreateContextBinary(predictor_model_.get(), context_binary);
+    } else {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("Init from predictor context binary '%s'",
+                         context_binary.c_str());
+      }
+      InitPredictorFromContextBinary(context_binary);
+    }
+
+    PostInitPredictor();
+
+    return true;
+  }
+
+  bool InitDecoder(const std::string &lib_filename,
+                   const std::string &context_binary) {
+    decoder_backend_ = std::make_unique<QnnBackend>(
+        config_.paraformer.qnn_config.backend_lib, config_.debug);
+
+    if (context_binary.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from decoder model lib since context binary is not given");
+      }
+
+      InitDecoderFromModelLib(lib_filename);
+
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Skip generating decoder context binary since you don't provide "
+            "a path to save it");
+      }
+    } else if (!FileExists(context_binary)) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init decoder from model lib since context binary '%s' does not "
+            "exist",
+            context_binary.c_str());
+      }
+
+      InitDecoderFromModelLib(lib_filename);
+      CreateContextBinary(decoder_model_.get(), context_binary);
+    } else {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("Init from decoder context binary '%s'",
+                         context_binary.c_str());
+      }
+      InitDecoderFromContextBinary(context_binary);
+    }
+
+    PostInitDecoder();
+
+    return true;
+  }
+
+  void InitEncoderFromModelLib(const std::string &lib_filename) {
+    encoder_backend_->InitContext();
+    encoder_model_ = std::make_unique<QnnModel>(
+        lib_filename, encoder_backend_.get(), config_.debug);
+  }
+
+  void InitPredictorFromModelLib(const std::string &lib_filename) {
+    predictor_backend_->InitContext();
+    predictor_model_ = std::make_unique<QnnModel>(
+        lib_filename, predictor_backend_.get(), config_.debug);
+  }
+
+  void InitDecoderFromModelLib(const std::string &lib_filename) {
+    decoder_backend_->InitContext();
+    decoder_model_ = std::make_unique<QnnModel>(
+        lib_filename, decoder_backend_.get(), config_.debug);
+  }
+
+  void CreateContextBinary(QnnModel *model, const std::string &context_binary) {
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
+    }
+
+    bool ok = model->SaveBinaryContext(context_binary);
+
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
+                       context_binary.c_str());
+    }
+
+    if (config_.debug && ok) {
+      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
+      SHERPA_ONNX_LOGE(
+          "It should be super fast the next time you init the system.");
+      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
+    }
+  }
+
+  void InitEncoderFromContextBinary(const std::string &context_binary) {
+    if (config_.paraformer.qnn_config.system_lib.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You should provide --paraformer.qnn-system-lib if you also provide "
+          "context binary");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    encoder_model_ = std::make_unique<QnnModel>(
+        context_binary, config_.paraformer.qnn_config.system_lib,
+        encoder_backend_.get(), BinaryContextTag{}, config_.debug);
+  }
+
+  void InitPredictorFromContextBinary(const std::string &context_binary) {
+    if (config_.paraformer.qnn_config.system_lib.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You should provide --paraformer.qnn-system-lib if you also provide "
+          "context binary");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    predictor_model_ = std::make_unique<QnnModel>(
+        context_binary, config_.paraformer.qnn_config.system_lib,
+        predictor_backend_.get(), BinaryContextTag{}, config_.debug);
+  }
+
+  void InitDecoderFromContextBinary(const std::string &context_binary) {
+    if (config_.paraformer.qnn_config.system_lib.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You should provide --paraformer.qnn-system-lib if you also provide "
+          "context binary");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    decoder_model_ = std::make_unique<QnnModel>(
+        context_binary, config_.paraformer.qnn_config.system_lib,
+        decoder_backend_.get(), BinaryContextTag{}, config_.debug);
+  }
+
+  void PostInitEncoder() { CheckEncoderModel(); }
+
+  void PostInitPredictor() { CheckPredictorModel(); }
+
+  void PostInitDecoder() { CheckDecoderModel(); }
+
+  void CheckEncoderModel() {
+    const auto &input_tensor_names = encoder_model_->InputTensorNames();
+    if (input_tensor_names.size() != 1) {
+      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
+                       static_cast<int32_t>(input_tensor_names.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[0] != "x") {
+      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
+                       input_tensor_names[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> x_shape =
+        encoder_model_->TensorShape(input_tensor_names[0]);
+    if (x_shape.size() != 3) {
+      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
+                       static_cast<int32_t>(x_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    num_input_frames_ = x_shape[1];
+    feat_dim_ = x_shape[2];
+
+    if (!encoder_model_->HasTensor("encoder_out")) {
+      SHERPA_ONNX_LOGE("Model does not have output node 'encoder_out'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> encoder_out_shape =
+        encoder_model_->TensorShape("encoder_out");
+
+    encoder_out_dim1_ = encoder_out_shape[1];
+    encoder_out_dim2_ = encoder_out_shape[2];
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("num_input_frames: %d", num_input_frames_);
+      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
+      SHERPA_ONNX_LOGE("encoder_out_dim1: %d", encoder_out_dim1_);
+      SHERPA_ONNX_LOGE("encoder_out_dim2: %d", encoder_out_dim2_);
+    }
+  }
+
+  void CheckPredictorModel() {
+    const auto &input_tensor_names = predictor_model_->InputTensorNames();
+    if (input_tensor_names.size() != 1) {
+      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
+                       static_cast<int32_t>(input_tensor_names.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[0] != "encoder_out") {
+      SHERPA_ONNX_LOGE("The 1st input should be encoder_out, actual '%s'",
+                       input_tensor_names[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> x_shape =
+        predictor_model_->TensorShape(input_tensor_names[0]);
+    if (x_shape.size() != 3) {
+      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
+                       static_cast<int32_t>(x_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[1] != encoder_out_dim2_) {
+      SHERPA_ONNX_LOGE(
+          "The input dim 1 of the predictor should be %d, given: %d",
+          encoder_out_dim2_, x_shape[1]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[2] != encoder_out_dim1_) {
+      SHERPA_ONNX_LOGE(
+          "The input dim 2 of the predictor should be %d, given: %d",
+          encoder_out_dim1_, x_shape[2]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (!predictor_model_->HasTensor("alphas")) {
+      SHERPA_ONNX_LOGE("Model does not have output node 'alphas'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> alphas_shape = predictor_model_->TensorShape("alphas");
+    if (alphas_shape.size() != 2) {
+      SHERPA_ONNX_LOGE("alphas should be 2-d, given: %d",
+                       static_cast<int32_t>(alphas_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (alphas_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("We support only batch size 1 for alphas. Given: %d",
+                       alphas_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (alphas_shape[1] != encoder_out_dim1_) {
+      SHERPA_ONNX_LOGE("Expected output dim %d for alphas. Given: %d",
+                       encoder_out_dim1_, alphas_shape[1]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  void CheckDecoderModel() {
+    const auto &input_tensor_names = decoder_model_->InputTensorNames();
+    if (input_tensor_names.size() != 3) {
+      SHERPA_ONNX_LOGE("Expect 3 input tensors. Actual %d",
+                       static_cast<int32_t>(input_tensor_names.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[0] != "encoder_out") {
+      SHERPA_ONNX_LOGE("The 1st input should be encoder_out, actual '%s'",
+                       input_tensor_names[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[1] != "acoustic_embedding") {
+      SHERPA_ONNX_LOGE(
+          "The 2nd input should be acoustic_embedding, actual '%s'",
+          input_tensor_names[1].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[2] != "mask") {
+      SHERPA_ONNX_LOGE("The 3rd input should be mask, actual '%s'",
+                       input_tensor_names[2].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (!decoder_model_->HasTensor("decoder_out")) {
+      SHERPA_ONNX_LOGE("Model does not have output node 'decoder_out'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> decoder_out_shape =
+        decoder_model_->TensorShape("decoder_out");
+    if (decoder_out_shape.size() != 3) {
+      SHERPA_ONNX_LOGE("decoder_out should be 3-d, given: %d",
+                       static_cast<int32_t>(decoder_out_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (decoder_out_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("We support only batch size 1 for decoder. Given: %d",
+                       decoder_out_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (decoder_out_shape[2] != encoder_out_dim1_) {
+      SHERPA_ONNX_LOGE("Expected output dim %d for decoder_out. Given: %d",
+                       encoder_out_dim1_, decoder_out_shape[2]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    vocab_size_ = decoder_out_shape[1];
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
+    }
+  }
+
+ private:
+  std::mutex mutex_;
+  OfflineModelConfig config_;
+
+  std::unique_ptr<QnnBackend> encoder_backend_;
+  std::unique_ptr<QnnModel> encoder_model_;
+
+  std::unique_ptr<QnnBackend> predictor_backend_;
+  std::unique_ptr<QnnModel> predictor_model_;
+
+  std::unique_ptr<QnnBackend> decoder_backend_;
+  std::unique_ptr<QnnModel> decoder_model_;
+
+  int32_t num_input_frames_ = 0;
+  int32_t feat_dim_ = 0;
+
+  int32_t encoder_out_dim1_ = 0;
+  int32_t encoder_out_dim2_ = 0;
+  int32_t vocab_size_ = 0;
+};
+
+OfflineParaformerModelQnn::~OfflineParaformerModelQnn() = default;
+
+OfflineParaformerModelQnn::OfflineParaformerModelQnn(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineParaformerModelQnn::OfflineParaformerModelQnn(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+std::vector<float> OfflineParaformerModelQnn::Run(
+    std::vector<float> features) const {
+  return impl_->Run(std::move(features));
+}
+
+int32_t OfflineParaformerModelQnn::VocabSize() const {
+  return impl_->VocabSize();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineParaformerModelQnn::OfflineParaformerModelQnn(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineParaformerModelQnn::OfflineParaformerModelQnn(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
new file mode 100644
index 00000000..76ab0be0
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
@@ -0,0 +1,40 @@
+// sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
+#define SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineParaformerModelQnn {
+ public:
+  ~OfflineParaformerModelQnn();
+
+  explicit OfflineParaformerModelQnn(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineParaformerModelQnn(Manager *mgr, const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (num_frames, feature_dim)
+   *                 before applying LFR.
+   * @returns Return a tensor of shape (num_output_frames, vocab_size)
+   */
+  std::vector<float> Run(std::vector<float> features) const;
+
+  int32_t VocabSize() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
index b628dc20..8224c216 100644
--- a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
@@ -7,7 +7,7 @@
 #include <algorithm>
 #include <array>
 #include <memory>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <string>
 #include <utility>
 #include <vector>
@@ -88,6 +88,9 @@ class OfflineSenseVoiceModelQnn::Impl {
     std::lock_guard<std::mutex> lock(mutex_);
 
     features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
 
     int32_t num_frames = features.size() / feat_dim_;
 
@@ -207,6 +210,11 @@ class OfflineSenseVoiceModelQnn::Impl {
     int32_t in_feat_dim = 80;
 
     int32_t in_num_frames = in.size() / in_feat_dim;
+
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
     int32_t out_num_frames =
         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
 

commit c08541ce36e914cae98f22abb92a3e2456c7573b
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 24 11:23:45 2025 +0800

    Optimize computation with Eigen. (#2928)
    
    This pull request introduces the Eigen library to optimize various numerical computations within the project. It systematically replaces existing manual, loop-based implementations for operations such as calculating mean and inverse standard deviation, applying feature normalization, scaling, windowing, and log-mel transformations. The primary goal of this refactoring is to enhance the overall computational performance and improve the readability and maintainability of the codebase by utilizing Eigen's highly optimized matrix and array functionalities.

diff --git a/sherpa-onnx/csrc/math-test.cc b/sherpa-onnx/csrc/math-test.cc
index ca4a967b..1083144a 100644
--- a/sherpa-onnx/csrc/math-test.cc
+++ b/sherpa-onnx/csrc/math-test.cc
@@ -65,4 +65,76 @@ TEST(Scale, Case2InPlace) {
   EXPECT_EQ(src, expected);
 }
 
+/*
+
+import numpy as np
+
+def compute_mean_and_inv_std(p: np.ndarray):
+    mean = p.mean(axis=0)
+    var = np.maximum((p**2).mean(axis=0) - mean**2, 0.0)
+    std = np.sqrt(var)
+    inv_std = 1.0 / (std + 1e-5)
+    return mean.astype(np.float32), inv_std.astype(np.float32)
+
+def dump_cpp_vector(name: str, arr: np.ndarray):
+    flat = arr.flatten()
+    print(f"std::vector<float> {name} = {{")
+    line = ""
+    for i, v in enumerate(flat):
+        line += f"{v:.8f}f, "
+        if (i + 1) % 8 == 0:
+            print("  " + line)
+            line = ""
+    if line:
+        print("  " + line)
+    print("};\n")
+
+np.random.seed(42)
+num_rows, num_cols = 4, 6
+x = np.random.randn(num_rows, num_cols).astype(np.float32)
+
+mean, inv_std = compute_mean_and_inv_std(x)
+
+dump_cpp_vector("x", x)
+dump_cpp_vector("mean", mean)
+dump_cpp_vector("inv_std", inv_std)
+
+ */
+
+TEST(ComputeMeanAndInvStd, Case1) {
+  std::vector<float> x = {
+      0.49671414f,  -0.13826430f, 0.64768857f, 1.52302980f,  -0.23415338f,
+      -0.23413695f, 1.57921278f,  0.76743472f, -0.46947438f, 0.54256004f,
+      -0.46341768f, -0.46572974f, 0.24196227f, -1.91328025f, -1.72491789f,
+      -0.56228751f, -1.01283109f, 0.31424734f, -0.90802407f, -1.41230369f,
+      1.46564877f,  -0.22577630f, 0.06752820f, -1.42474818f,
+  };
+
+  std::vector<float> expected_mean = {
+      0.35246629f, -0.67410338f, -0.02026373f,
+      0.31938151f, -0.41071847f, -0.45259190f,
+  };
+
+  std::vector<float> expected_inv_std = {
+      1.13103926f, 0.94854516f, 0.83320111f,
+      1.24679470f, 2.52932906f, 1.59057319f,
+  };
+
+  std::vector<float> mean;
+  std::vector<float> inv_std;
+
+  int32_t num_rows = 4;
+  int32_t num_cols = 6;
+
+  ComputeMeanAndInvStd(x.data(), num_rows, num_cols, &mean, &inv_std);
+
+  ASSERT_EQ(mean.size(), num_cols);
+  ASSERT_EQ(inv_std.size(), num_cols);
+
+  for (int32_t i = 0; i < num_cols; ++i) {
+    EXPECT_NEAR(mean[i], expected_mean[i], 1e-6f) << "at index " << i;
+    EXPECT_NEAR(inv_std[i], expected_inv_std[i], 1e-6f) << "at index " << i;
+  }
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
index adfcc036..64b7f1ee 100644
--- a/sherpa-onnx/csrc/math.cc
+++ b/sherpa-onnx/csrc/math.cc
@@ -4,6 +4,9 @@
 #include "sherpa-onnx/csrc/math.h"
 
 #include <vector>
+
+#include "Eigen/Dense"
+
 namespace sherpa_onnx {
 
 void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
@@ -72,4 +75,27 @@ std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols) {
   return output;
 }
 
+void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
+                          std::vector<float> *mean,
+                          std::vector<float> *inv_stddev) {
+  using RowMajorMat =
+      Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+
+  Eigen::Map<const RowMajorMat> X(p, num_rows, num_cols);
+
+  Eigen::RowVectorXf mean_vec = X.colwise().mean();
+
+  Eigen::RowVectorXf mean_sq = X.array().square().colwise().mean();
+
+  Eigen::RowVectorXf var = mean_sq.array() - mean_vec.array().square();
+
+  Eigen::RowVectorXf stddev = var.array().max(0.0f).sqrt();
+
+  Eigen::RowVectorXf inv_std = (stddev.array() + 1e-5f).inverse();
+
+  mean->assign(mean_vec.data(), mean_vec.data() + num_cols);
+
+  inv_stddev->assign(inv_std.data(), inv_std.data() + num_cols);
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
index 69956768..ac6eb976 100644
--- a/sherpa-onnx/csrc/math.h
+++ b/sherpa-onnx/csrc/math.h
@@ -147,5 +147,19 @@ std::vector<float> ComputeAcousticEmbedding(
 // Transpose a 2-D matrix in row-major
 std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols);
 
+/* Compute mean and inverse stddev over rows.
+ *
+ * @param p  A pointer to a 2-d array of shape (num_rows, num_cols)
+ * @param num_rows Number of rows
+ * @param num_cols Number of columns
+ * @param mean On return, it contains p.mean(axis=0). You don't need to
+ *             pre-allocate space for it.
+ * @param inv_stddev On return, it contains 1/p.std(axis=0) You don't need to
+ *                   pre-allocate space for it.
+ */
+void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
+                          std::vector<float> *mean,
+                          std::vector<float> *inv_stddev);
+
 }  // namespace sherpa_onnx
 #endif  // SHERPA_ONNX_CSRC_MATH_H_
diff --git a/sherpa-onnx/csrc/offline-dolphin-model.cc b/sherpa-onnx/csrc/offline-dolphin-model.cc
index 843a0127..ef9501ec 100644
--- a/sherpa-onnx/csrc/offline-dolphin-model.cc
+++ b/sherpa-onnx/csrc/offline-dolphin-model.cc
@@ -19,6 +19,7 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
@@ -65,16 +66,15 @@ class OfflineDolphinModel::Impl {
 
   void NormalizeFeatures(float *features, int32_t num_frames,
                          int32_t feat_dim) const {
-    auto p = features;
-    const auto &mean = meta_data_.mean;
-    const auto &invstd = meta_data_.inv_stddev;
-
-    for (int32_t f = 0; f < num_frames; ++f) {
-      for (int32_t d = 0; d < feat_dim; ++d) {
-        p[d] = (p[d] - mean[d]) * invstd[d];
-      }
-      p += feat_dim;
-    }
+    using RowMajorMat =
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+    Eigen::Map<RowMajorMat> x(features, num_frames, feat_dim);
+
+    Eigen::Map<const Eigen::RowVectorXf> mean(meta_data_.mean.data(), feat_dim);
+    Eigen::Map<const Eigen::RowVectorXf> inv_std(meta_data_.inv_stddev.data(),
+                                                 feat_dim);
+    x.array() =
+        (x.array().rowwise() - mean.array()).rowwise() * inv_std.array();
   }
 
   OrtAllocator *Allocator() { return allocator_; }
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
index d5bbc176..bce191b0 100644
--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
@@ -20,6 +20,7 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
@@ -85,19 +86,14 @@ class OfflineOmnilingualAsrCtcModel::Impl {
       return;
     }
 
-    double s = 0;
-    double sq = 0;
-    for (int32_t i = 0; i < feat_dim; ++i) {
-      s += features[i];
-      sq += features[i] * features[i];
-    }
-
-    double mean = s / feat_dim;
-    double inv_stddev = 1 / std::sqrt(sq / feat_dim - mean * mean + 1e-5);
+    // Map the single-row feature vector
+    Eigen::Map<Eigen::ArrayXf> x(features, feat_dim);
+    float mean = x.mean();
+    float var = (x.square().mean() - mean * mean);
+    var = std::max(var, 0.0f);
+    float inv_stddev = 1.0f / std::sqrt(var + 1e-5f);
 
-    for (int32_t i = 0; i < feat_dim; ++i) {
-      features[i] = (features[i] - mean) * inv_stddev;
-    }
+    x = (x - mean) * inv_stddev;
   }
 
  private:
diff --git a/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h b/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
index bda51066..34fc65c6 100644
--- a/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
@@ -12,6 +12,7 @@
 #include <utility>
 #include <vector>
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/offline-fire-red-asr-decoder.h"
 #include "sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.h"
 #include "sherpa-onnx/csrc/offline-fire-red-asr-model.h"
@@ -131,20 +132,19 @@ class OfflineRecognizerFireRedAsrImpl : public OfflineRecognizerImpl {
 
   void ApplyCMVN(std::vector<float> *v) const {
     const auto &meta_data = model_->GetModelMetadata();
-    const auto &mean = meta_data.mean;
-    const auto &inv_stddev = meta_data.inv_stddev;
-    int32_t feat_dim = static_cast<int32_t>(mean.size());
+    const auto &mean_vec = meta_data.mean;
+    const auto &inv_stddev_vec = meta_data.inv_stddev;
+    int32_t feat_dim = static_cast<int32_t>(mean_vec.size());
     int32_t num_frames = static_cast<int32_t>(v->size()) / feat_dim;
-
-    float *p = v->data();
-
-    for (int32_t i = 0; i != num_frames; ++i) {
-      for (int32_t k = 0; k != feat_dim; ++k) {
-        p[k] = (p[k] - mean[k]) * inv_stddev[k];
-      }
-
-      p += feat_dim;
-    }
+    Eigen::Map<
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
+        mat(v->data(), num_frames, feat_dim);
+    Eigen::Map<const Eigen::RowVectorXf> mean(mean_vec.data(), feat_dim);
+    Eigen::Map<const Eigen::RowVectorXf> inv_std(inv_stddev_vec.data(),
+                                                 feat_dim);
+
+    mat.array() =
+        (mat.array().rowwise() - mean.array()).rowwise() * inv_std.array();
   }
 
  private:
diff --git a/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h b/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
index a6fe4cbd..de367686 100644
--- a/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
@@ -11,6 +11,7 @@
 #include <utility>
 #include <vector>
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-paraformer-decoder.h"
 #include "sherpa-onnx/csrc/offline-paraformer-greedy-search-decoder.h"
@@ -242,19 +243,18 @@ class OfflineRecognizerParaformerImpl : public OfflineRecognizerImpl {
   void ApplyCMVN(std::vector<float> *v) const {
     const std::vector<float> &neg_mean = model_->NegativeMean();
     const std::vector<float> &inv_stddev = model_->InverseStdDev();
+    int32_t dim = static_cast<int32_t>(neg_mean.size());
+    int32_t num_frames = static_cast<int32_t>(v->size()) / dim;
 
-    int32_t dim = neg_mean.size();
-    int32_t num_frames = v->size() / dim;
+    Eigen::Map<
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
+        mat(v->data(), num_frames, dim);
 
-    float *p = v->data();
+    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
+    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
 
-    for (int32_t i = 0; i != num_frames; ++i) {
-      for (int32_t k = 0; k != dim; ++k) {
-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
-      }
-
-      p += dim;
-    }
+    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
+                  inv_stddev_vec.array();
   }
 
   OfflineRecognizerConfig config_;
diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
index 02c06b44..056c79f0 100644
--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
@@ -11,6 +11,7 @@
 #include <utility>
 #include <vector>
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-ctc-greedy-search-decoder.h"
 #include "sherpa-onnx/csrc/offline-model-config.h"
@@ -402,22 +403,18 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
 
   void ApplyCMVN(std::vector<float> *v) const {
     const auto &meta_data = model_->GetModelMetadata();
-
     const std::vector<float> &neg_mean = meta_data.neg_mean;
     const std::vector<float> &inv_stddev = meta_data.inv_stddev;
-
-    int32_t dim = neg_mean.size();
-    int32_t num_frames = v->size() / dim;
-
-    float *p = v->data();
-
-    for (int32_t i = 0; i != num_frames; ++i) {
-      for (int32_t k = 0; k != dim; ++k) {
-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
-      }
-
-      p += dim;
-    }
+    int32_t dim = static_cast<int32_t>(neg_mean.size());
+    int32_t num_frames = static_cast<int32_t>(v->size()) / dim;
+    Eigen::Map<
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
+        mat(v->data(), num_frames, dim);
+    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
+
+    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
+    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
+                  inv_stddev_vec.array();
   }
 
   SymbolTable symbol_table_;
diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
index 265c8dbe..7b97fbac 100644
--- a/sherpa-onnx/csrc/offline-stream.cc
+++ b/sherpa-onnx/csrc/offline-stream.cc
@@ -16,51 +16,12 @@
 
 #include "kaldi-native-fbank/csrc/online-feature.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/math.h"
 #include "sherpa-onnx/csrc/offline-recognizer.h"
 #include "sherpa-onnx/csrc/resample.h"
 
 namespace sherpa_onnx {
 
-/* Compute mean and inverse stddev over rows.
- *
- * @param p  A pointer to a 2-d array of shape (num_rows, num_cols)
- * @param num_rows Number of rows
- * @param num_cols Number of columns
- * @param mean On return, it contains p.mean(axis=0)
- * @param inv_stddev On return, it contains 1/p.std(axis=0)
- */
-static void ComputeMeanAndInvStd(const float *p, int32_t num_rows,
-                                 int32_t num_cols, std::vector<float> *mean,
-                                 std::vector<float> *inv_stddev) {
-  std::vector<float> sum(num_cols);
-  std::vector<float> sum_sq(num_cols);
-
-  for (int32_t i = 0; i != num_rows; ++i) {
-    for (int32_t c = 0; c != num_cols; ++c) {
-      auto t = p[c];
-      sum[c] += t;
-      sum_sq[c] += t * t;
-    }
-    p += num_cols;
-  }
-
-  mean->resize(num_cols);
-  inv_stddev->resize(num_cols);
-
-  for (int32_t i = 0; i != num_cols; ++i) {
-    auto t = sum[i] / num_rows;
-    (*mean)[i] = t;
-
-    float stddev = std::sqrt(sum_sq[i] / num_rows - t * t);
-
-    if (stddev != stddev) {
-      stddev = 0;
-    }
-
-    (*inv_stddev)[i] = 1.0f / (stddev + 1e-5f);
-  }
-}
-
 class OfflineStream::Impl {
  public:
   explicit Impl(const FeatureExtractorConfig &config,
@@ -305,17 +266,20 @@ class OfflineStream::Impl {
 
   static void NemoNormalizePerFeature(float *p, int32_t num_frames,
                                       int32_t feature_dim) {
-    std::vector<float> mean;
-    std::vector<float> inv_stddev;
+    using RowMajorMat =
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
 
-    ComputeMeanAndInvStd(p, num_frames, feature_dim, &mean, &inv_stddev);
+    Eigen::Map<RowMajorMat> x(p, num_frames, feature_dim);
 
-    for (int32_t n = 0; n != num_frames; ++n) {
-      for (int32_t i = 0; i != feature_dim; ++i) {
-        p[i] = (p[i] - mean[i]) * inv_stddev[i];
-      }
-      p += feature_dim;
-    }
+    Eigen::RowVectorXf mean = x.colwise().mean();
+    Eigen::RowVectorXf var =
+        (x.array().square().colwise().mean() - mean.array().square())
+            .max(0.0f);  // avoid negative due to FP error
+
+    Eigen::RowVectorXf inv_std = (var.array().sqrt() + 1e-5f).inverse();
+
+    x.array() =
+        (x.array().rowwise() - mean.array()).rowwise() * inv_std.array();
   }
 
  private:
diff --git a/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h b/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
index 2f4b3c47..30ef4c91 100644
--- a/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
+++ b/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
@@ -11,6 +11,7 @@
 #include <utility>
 #include <vector>
 
+#include "Eigen/Dense"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/online-lm.h"
@@ -412,19 +413,18 @@ class OnlineRecognizerParaformerImpl : public OnlineRecognizerImpl {
   void ApplyCMVN(std::vector<float> *v) const {
     const std::vector<float> &neg_mean = model_.NegativeMean();
     const std::vector<float> &inv_stddev = model_.InverseStdDev();
+    int dim = static_cast<int>(neg_mean.size());
+    int num_frames = static_cast<int>(v->size()) / dim;
 
-    int32_t dim = neg_mean.size();
-    int32_t num_frames = v->size() / dim;
+    Eigen::Map<
+        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
+        mat(v->data(), num_frames, dim);
 
-    float *p = v->data();
+    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
+    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
 
-    for (int32_t i = 0; i != num_frames; ++i) {
-      for (int32_t k = 0; k != dim; ++k) {
-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
-      }
-
-      p += dim;
-    }
+    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
+                  inv_stddev_vec.array();
   }
 
   void PositionalEncoding(std::vector<float> *v, int32_t t_offset) const {
diff --git a/sherpa-onnx/csrc/ten-vad-model.cc b/sherpa-onnx/csrc/ten-vad-model.cc
index 69a64237..f86bc2f5 100644
--- a/sherpa-onnx/csrc/ten-vad-model.cc
+++ b/sherpa-onnx/csrc/ten-vad-model.cc
@@ -21,6 +21,7 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
+#include "Eigen/Dense"
 #include "kaldi-native-fbank/csrc/mel-computations.h"
 #include "kaldi-native-fbank/csrc/rfft.h"
 #include "sherpa-onnx/csrc/file-utils.h"
@@ -314,9 +315,10 @@ class TenVadModel::Impl {
   }
 
   static void Scale(const float *samples, int32_t n, float *out) {
-    for (int32_t i = 0; i != n; ++i) {
-      out[i] = samples[i] * 32768;
-    }
+    Eigen::Map<const Eigen::ArrayXf> input(samples, n);
+    Eigen::Map<Eigen::ArrayXf> output(out, n);
+    constexpr float kScale = 32768.0f;
+    output = input * kScale;
   }
 
   void Preemphasis(const float *samples, int32_t n, float *out) {
@@ -333,9 +335,10 @@ class TenVadModel::Impl {
 
   static void ApplyWindow(const float *samples, const float *window, int32_t n,
                           float *out) {
-    for (int32_t i = 0; i != n; ++i) {
-      out[i] = samples[i] * window[i];
-    }
+    Eigen::Map<const Eigen::ArrayXf> samp_vec(samples, n);
+    Eigen::Map<const Eigen::ArrayXf> win_vec(window, n);
+    Eigen::Map<Eigen::ArrayXf> out_vec(out, n);
+    out_vec = samp_vec * win_vec;
   }
 
   static void ComputePowerSpectrum(const float *fft_bins, int32_t n,
@@ -351,16 +354,21 @@ class TenVadModel::Impl {
   }
 
   static void LogMel(const float *in, int32_t n, float *out) {
-    for (int32_t i = 0; i != n; ++i) {
-      // 20.79441541679836 is log(32768*32768)
-      out[i] = logf(in[i] + 1e-10f) - 20.79441541679836f;
-    }
+    Eigen::Map<const Eigen::ArrayXf> input(in, n);
+    Eigen::Map<Eigen::ArrayXf> output(out, n);
+    // 20.79441541679836 is log(32768*32768)
+    constexpr float kLogScale = 20.79441541679836f;
+    output = (input + 1e-10f).log() - kLogScale;
   }
 
   void ApplyNormalization(const float *in, float *out) const {
-    for (int32_t i = 0; i != static_cast<int32_t>(mean_.size()); ++i) {
-      out[i] = (in[i] - mean_[i]) * inv_stddev_[i];
-    }
+    int32_t dim = static_cast<int32_t>(mean_.size());
+
+    Eigen::Map<const Eigen::ArrayXf> input(in, dim);
+    Eigen::Map<Eigen::ArrayXf> output(out, dim);
+    Eigen::Map<const Eigen::ArrayXf> mean_vec(mean_.data(), dim);
+    Eigen::Map<const Eigen::ArrayXf> inv_stddev_vec(inv_stddev_.data(), dim);
+    output = (input - mean_vec) * inv_stddev_vec;
   }
 
   void ComputeFeatures(const float *samples, int32_t n) {

commit 27a0bf4b9c082c519695ac166144151cadb5df40
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 24 09:48:15 2025 +0800

    Add Transpose for a 2-D matrix. (#2926)

diff --git a/scripts/paraformer/rknn/export_decoder_onnx.py b/scripts/paraformer/rknn/export_decoder_onnx.py
index 98ff95f5..cec25683 100755
--- a/scripts/paraformer/rknn/export_decoder_onnx.py
+++ b/scripts/paraformer/rknn/export_decoder_onnx.py
@@ -26,7 +26,7 @@ def get_args():
         "--float-mask",
         type=int,
         default=1,
-        help="1 to use float master. 0 to use int32 mask",
+        help="1 to use float mask. 0 to use int32 mask",
     )
 
     parser.add_argument(
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index e28e9a30..053c652e 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -770,6 +770,7 @@ if(SHERPA_ONNX_ENABLE_TESTS)
     cat-test.cc
     circular-buffer-test.cc
     context-graph-test.cc
+    math-test.cc
     packed-sequence-test.cc
     pad-sequence-test.cc
     regex-lang-test.cc
@@ -784,7 +785,6 @@ if(SHERPA_ONNX_ENABLE_TESTS)
   )
   if(SHERPA_ONNX_ENABLE_TTS)
     list(APPEND sherpa_onnx_test_srcs
-      offline-tts-zipvoice-frontend-test.cc
       piper-phonemize-test.cc
     )
   endif()
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 77d47f39..fc372afd 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -40,8 +40,6 @@
 namespace sherpa_onnx {
 
 namespace {
-// code in this anonymous namespace is written by ChatGPT
-//
 // Please see https://github.com/k2-fsa/sherpa-onnx/pull/2853
 // for why we need to do the replacement
 static const std::vector<std::pair<std::string, std::string>> kReplacements = {
diff --git a/sherpa-onnx/csrc/math-test.cc b/sherpa-onnx/csrc/math-test.cc
new file mode 100644
index 00000000..ca4a967b
--- /dev/null
+++ b/sherpa-onnx/csrc/math-test.cc
@@ -0,0 +1,68 @@
+// sherpa-onnx/csrc/math-test.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/math.h"
+
+#include <vector>
+
+#include "gtest/gtest.h"
+
+namespace sherpa_onnx {
+
+TEST(Transpose, Case1) {
+  // 0 1 2
+  // 3 4 5
+  std::vector<float> in = {0, 1, 2, 3, 4, 5};
+  std::vector<float> out = Transpose(in.data(), 2, 3);
+
+  // 0 3
+  // 1 4
+  // 2 5
+  std::vector<float> expected_out = {0, 3, 1, 4, 2, 5};
+  EXPECT_EQ(out, expected_out);
+}
+
+TEST(Transpose, Case2) {
+  // 0 1
+  // 2 3
+  // 4 5
+  std::vector<float> in = {0, 1, 2, 3, 4, 5};
+  std::vector<float> out = Transpose(in.data(), 3, 2);
+
+  // 0 2 4
+  // 1 3 5
+  std::vector<float> expected_out = {0, 2, 4, 1, 3, 5};
+  EXPECT_EQ(out, expected_out);
+}
+
+TEST(ScaleAdd, Case1) {
+  std::vector<float> src = {1, 2, 3};
+  float scale = 10;
+  std::vector<float> in_out = {5, 6, 0};
+  ScaleAdd(src.data(), scale, src.size(), in_out.data());
+
+  std::vector<float> expected = {10 + 5, 20 + 6, 30 + 0};
+  EXPECT_EQ(in_out, expected);
+}
+
+TEST(Scale, Case1) {
+  std::vector<float> src = {1, 2, 3};
+  float scale = 10;
+  std::vector<float> in_out = {5, 6, 0};
+  Scale(src.data(), scale, src.size(), in_out.data());
+
+  std::vector<float> expected = {10, 20, 30};
+  EXPECT_EQ(in_out, expected);
+}
+
+TEST(Scale, Case2InPlace) {
+  std::vector<float> src = {1, 2, 3};
+  float scale = 10;
+  Scale(src.data(), scale, src.size(), src.data());
+
+  std::vector<float> expected = {10, 20, 30};
+  EXPECT_EQ(src, expected);
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
index d3628191..adfcc036 100644
--- a/sherpa-onnx/csrc/math.cc
+++ b/sherpa-onnx/csrc/math.cc
@@ -6,16 +6,18 @@
 #include <vector>
 namespace sherpa_onnx {
 
-static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
-  for (int32_t i = 0; i < n; ++i) {
-    in_out[i] += scale * src[i];
-  }
+void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
+  Eigen::Map<const Eigen::ArrayXf> src_vec(src, n);
+  Eigen::Map<Eigen::ArrayXf> inout_vec(in_out, n);
+
+  inout_vec += scale * src_vec;
 }
 
-static void Scale(const float *src, float scale, int32_t n, float *out) {
-  for (int32_t i = 0; i < n; ++i) {
-    out[i] = scale * src[i];
-  }
+void Scale(const float *src, float scale, int32_t n, float *out) {
+  Eigen::Map<const Eigen::ArrayXf> src_vec(src, n);
+  Eigen::Map<Eigen::ArrayXf> out_vec(out, n);
+
+  out_vec = scale * src_vec;
 }
 
 // this if for Paraformer
@@ -54,4 +56,20 @@ std::vector<float> ComputeAcousticEmbedding(
   return ans;
 }
 
+std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols) {
+  std::vector<float> output(cols * rows);
+
+  Eigen::Map<const Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
+                                 Eigen::RowMajor>>
+      in(input, rows, cols);
+
+  Eigen::Map<
+      Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
+      out(output.data(), cols, rows);
+
+  out.noalias() = in.transpose();
+
+  return output;
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
index 1edc065d..69956768 100644
--- a/sherpa-onnx/csrc/math.h
+++ b/sherpa-onnx/csrc/math.h
@@ -13,6 +13,8 @@
 #include <numeric>
 #include <vector>
 
+#include "Eigen/Dense"
+
 namespace sherpa_onnx {
 
 // logf(FLT_EPSILON)
@@ -131,10 +133,19 @@ std::vector<int32_t> TopkIndex(const std::vector<std::vector<T>> &vec,
   return TopkIndex(flatten.data(), flatten.size(), topk);
 }
 
+// in_out[i] += src[i] * scale
+void ScaleAdd(const float *src, float scale, int32_t n, float *in_out);
+
+// out[i] = src[i] * scale
+void Scale(const float *src, float scale, int32_t n, float *out);
+
 // For Paraformer
 std::vector<float> ComputeAcousticEmbedding(
     const std::vector<float> &encoder_out, const std::vector<float> &alphas,
     int32_t encoder_dim);
 
+// Transpose a 2-D matrix in row-major
+std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols);
+
 }  // namespace sherpa_onnx
 #endif  // SHERPA_ONNX_CSRC_MATH_H_
diff --git a/sherpa-onnx/csrc/normal-data-generator.cc b/sherpa-onnx/csrc/normal-data-generator.cc
index b62bede3..6c31863c 100644
--- a/sherpa-onnx/csrc/normal-data-generator.cc
+++ b/sherpa-onnx/csrc/normal-data-generator.cc
@@ -2,8 +2,6 @@
 //
 // Copyright      2025  Xiaomi Corporation
 
-// Written by ChatGPT
-
 #include "sherpa-onnx/csrc/normal-data-generator.h"
 
 #include <random>
diff --git a/sherpa-onnx/csrc/normal-data-generator.h b/sherpa-onnx/csrc/normal-data-generator.h
index e250f2b6..17601b4f 100644
--- a/sherpa-onnx/csrc/normal-data-generator.h
+++ b/sherpa-onnx/csrc/normal-data-generator.h
@@ -2,7 +2,6 @@
 //
 // Copyright      2025  Xiaomi Corporation
 
-// Written by ChatGPT
 #ifndef SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
 #define SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
 
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
index a315d75b..1064c145 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
@@ -16,6 +16,7 @@
 #include "kaldi-native-fbank/csrc/stft.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+#include "sherpa-onnx/csrc/math.h"
 #include "sherpa-onnx/csrc/offline-tts-frontend.h"
 #include "sherpa-onnx/csrc/offline-tts-impl.h"
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
@@ -282,17 +283,14 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
     int64_t C = mel_shape[2];
 
     const float *mel_data = mel.GetTensorData<float>();
-    std::vector<float> mel_permuted(C * T);
 
     float inv_feat_scale = 1 / feat_scale;
 
-    for (int64_t c = 0; c < C; ++c) {
-      for (int64_t t = 0; t < T; ++t) {
-        int64_t src_idx = t * C + c;  // src: [T, C] (row major)
-        int64_t dst_idx = c * T + t;  // dst: [C, T] (row major)
-        mel_permuted[dst_idx] = mel_data[src_idx] * inv_feat_scale;
-      }
-    }
+    // mel_permuted is (C, T)
+    std::vector<float> mel_permuted = Transpose(mel_data, T, C);
+
+    Scale(mel_permuted.data(), inv_feat_scale, mel_permuted.size(),
+          mel_permuted.data());
 
     std::array<int64_t, 3> new_shape = {1, C, T};
     Ort::Value mel_new = Ort::Value::CreateTensor<float>(

commit f98a85ea6e1feb6075c0f85e9d602741e1af29ff
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 24 09:15:46 2025 +0800

    Export Paraformer ASR models to QNN (#2925)

diff --git a/.github/scripts/export-qnn/generate_paraformer.py b/.github/scripts/export-qnn/generate_paraformer.py
new file mode 100755
index 00000000..9797eb5e
--- /dev/null
+++ b/.github/scripts/export-qnn/generate_paraformer.py
@@ -0,0 +1,47 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import json
+
+from device_info import soc_info_dict
+from dataclasses import asdict, dataclass
+import itertools
+
+
+@dataclass
+class Config:
+    soc: str  # SM8850
+    soc_id: int  # 87
+    arch: str  # v81
+    input_in_seconds: str
+    framework: str
+
+
+def main():
+
+    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+    framework_list = ["FunASR", "WSChuan-ASR"]
+
+    configs = []
+
+    for name, soc in soc_info_dict.items():
+        for num_seconds, framework in itertools.product(
+            input_in_seconds, framework_list
+        ):
+            configs.append(
+                Config(
+                    soc=name,
+                    soc_id=soc.model.value,
+                    arch=soc.info.arch.name,
+                    input_in_seconds=num_seconds,
+                    framework=framework,
+                )
+            )
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/workflows/export-paraformer-to-qnn.yaml b/.github/workflows/export-paraformer-to-qnn.yaml
new file mode 100644
index 00000000..211dd96b
--- /dev/null
+++ b/.github/workflows/export-paraformer-to-qnn.yaml
@@ -0,0 +1,457 @@
+name: export-paraformer-to-qnn
+
+on:
+  push:
+    branches:
+      - export-paraformer-qnn-2
+  workflow_dispatch:
+
+concurrency:
+  group: export-paraformer-to-qnn-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-qnn/generate_paraformer.py
+          MATRIX=$(python3 .github/scripts/export-qnn/generate_paraformer.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
+  export-paraformer-to-qnn:
+    needs: generate_build_matrix
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
+    runs-on: ubuntu-22.04
+    strategy:
+      fail-fast: false
+      matrix:
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python 3.10
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.10"
+
+      - name: Display NDK HOME
+        shell: bash
+        run: |
+          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
+          ls -lh ${ANDROID_NDK_LATEST_HOME}
+
+      - name: Create directories
+        shell: bash
+        run: |
+          mkdir so binary
+
+      - name: Create Python virtual environment
+        shell: bash
+        run: |
+          python3 -m venv py310
+          which python3
+          source py310/bin/activate
+          which python3
+
+      - name: Show ndk-build help
+        shell: bash
+        run: |
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          ndk-build --help
+
+      - name: Download toolkit
+        shell: bash
+        run: |
+          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.40.0.251030.zip
+          ls -lh v2.40.0.251030.zip
+
+      - name: Unzip toolkit
+        shell: bash
+        run: |
+          unzip v2.40.0.251030.zip
+
+      - name: Show
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---ls -lh qairt---"
+
+          ls -lh qairt
+
+          echo "---"
+
+      - name: Install linux dependencies
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---"
+
+          ls -lh qairt
+
+          cd qairt/2.40.0.251030/bin
+          source envsetup.sh
+
+          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          cd qairt/2.40.0.251030/bin
+          source envsetup.sh
+
+          python3 -m pip install \
+            mock \
+            numpy \
+            opencv-python \
+            optuna \
+            packaging \
+            pandas \
+            paramiko \
+            pathlib2 \
+            pillow \
+            plotly \
+            protobuf \
+            psutil \
+            pydantic \
+            pytest \
+            pyyaml \
+            rich \
+            scikit-optimize \
+            scipy \
+            six \
+            tabulate \
+            typing-extensions \
+            xlsxwriter
+
+          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
+
+          which python3
+
+      - name: Install onnx dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+          python3 -m pip install --upgrade \
+            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+            kaldi_native_fbank \
+            pip \
+            "numpy<2" \
+            onnx==1.17.0 \
+            onnxruntime==1.17.1 \
+            soundfile \
+            librosa \
+            onnxsim \
+            sentencepiece \
+            pyyaml
+
+          which python3
+
+      - name: Show qnn-onnx-converter help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.40.0.251030/bin
+          source envsetup.sh
+          popd
+
+          qnn-onnx-converter --help
+
+      - name: Show qnn-model-lib-generator help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.40.0.251030/bin
+          source envsetup.sh
+          popd
+
+          qnn-model-lib-generator --help
+
+      - name: Show qnn-net-run help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.40.0.251030/bin
+          source envsetup.sh
+          popd
+
+          qnn-net-run --help
+
+      - name: Run Paraformer from FunAsr
+        if: matrix.framework == 'FunASR'
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.40.0.251030/bin
+          source envsetup.sh
+          popd
+
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+
+          export t=${{ matrix.input_in_seconds }}
+          export soc=${{ matrix.soc }}
+
+          dir=$PWD
+
+          cd scripts/paraformer/qnn
+
+          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/am.mvn
+          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/config.yaml
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/tokens.txt
+
+          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/model.pt
+          mv model.pt model_state_dict.pt
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/0.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/1.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/2.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/3-sichuan.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/4-tianjin.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/5-henan.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/6-zh-en.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/8k.wav
+
+          rm -f README.md || true
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/README.md
+
+
+          ./convert_decoder.sh
+
+          ./convert_predictor.sh
+
+          ./convert_encoder.sh
+
+          ls -lh model_libs/*/lib*.so
+
+          ls -lh binary
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "collect results"
+
+          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-paraformer-zh-2023-03-28-int8
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v README.md $d
+          cp -v binary/encoder.bin $d/
+          cp -v binary/predictor.bin $d/
+          cp -v binary/decoder.bin $d/
+
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+
+          mv *.tar.bz2 ../../../binary/
+
+
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+
+              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2023-03-28-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2023-03-28-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v README.md $d
+
+            cp -v model_libs/$p/libencoder*.so $d/libencoder.so
+            cp -v model_libs/$p/libpredictor*.so $d/libpredictor.so
+            cp -v model_libs/$p/libdecoder*.so $d/libdecoder.so
+
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../../../so/
+
+
+      - name: Run Paraformer from WSChuan-ASR
+        if: matrix.framework == 'WSChuan-ASR'
+        shell: bash
+        run: |
+          dir=$PWD
+          source py310/bin/activate
+
+          pushd qairt/2.40.0.251030/bin
+          source envsetup.sh
+          popd
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+          export t=${{ matrix.input_in_seconds }}
+          export soc=${{ matrix.soc }}
+
+          cd scripts/paraformer/qnn
+
+          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/am.mvn
+          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/config.yaml
+          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/tokens.json
+          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/model_state_dict.pt
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/tokens.txt
+
+
+          for i in $(seq 1 16); do
+            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/test_wavs/$i.wav
+          done
+
+          rm -f README.md || true
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/README.md
+
+          ./convert_decoder.sh
+
+          ./convert_predictor.sh
+
+          ./convert_encoder.sh
+
+          ls -lh model_libs/*/lib*.so
+
+          ls -lh binary
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "collect results"
+
+          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-paraformer-zh-2025-10-07-int8
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v README.md $d
+          cp -v binary/encoder.bin $d/
+          cp -v binary/predictor.bin $d/
+          cp -v binary/decoder.bin $d/
+
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+
+          mv *.tar.bz2 ../../../binary/
+
+
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+
+              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2025-10-07-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2025-10-07-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v README.md $d
+
+            cp -v model_libs/$p/libencoder*.so $d/libencoder.so
+            cp -v model_libs/$p/libpredictor*.so $d/libpredictor.so
+            cp -v model_libs/$p/libdecoder*.so $d/libdecoder.so
+
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../../../so/
+
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: ${{ matrix.framework }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
+          path: ./scripts/paraformer/qnn/my-config*/*.json
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./binary/*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn-binary
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
+          tag: asr-models-qnn
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./binary/*.tar.bz2
+          overwrite: true
+          tag: asr-models-qnn-binary
diff --git a/scripts/paraformer/qnn/.gitignore b/scripts/paraformer/qnn/.gitignore
new file mode 100644
index 00000000..8a80e2e8
--- /dev/null
+++ b/scripts/paraformer/qnn/.gitignore
@@ -0,0 +1,2 @@
+*.raw
+*-list.txt
diff --git a/scripts/paraformer/qnn/convert_decoder.sh b/scripts/paraformer/qnn/convert_decoder.sh
new file mode 100755
index 00000000..e16fe97e
--- /dev/null
+++ b/scripts/paraformer/qnn/convert_decoder.sh
@@ -0,0 +1,84 @@
+#!/usr/bin/env bash
+
+if [ -z $t ]; then
+  echo "Please run export t=num_input_seconds"
+  exit 1
+fi
+
+if [ -z $soc ]; then
+  echo "Please run export soc=SM8850, etc."
+  exit 1
+fi
+
+if [ -z $QNN_SDK_ROOT ]; then
+  echo "Please run setup QNN first"
+  exit 1
+fi
+
+echo "Export to onnx with num_seconds $t"
+
+python3 ./export_decoder_onnx.py --input-len-in-seconds $t --opset-version 17 --float-mask 0
+
+ls -lh decoder-*.onnx
+
+python3 ../../pyannote/segmentation/show-onnx.py --filename ./decoder-$t-seconds.onnx
+
+echo "Generate test data"
+
+python3 ./generate_decoder_data.py --input-len-in-seconds $t
+
+ls -lh decoder-*
+
+echo "---"
+cat ./decoder-input-list.txt
+echo "---"
+
+echo "Convert onnx to qnn"
+
+
+qnn-onnx-converter \
+  --input_network ./decoder-$t-seconds.onnx \
+  --output_path ./decoder-$t-seconds-quantized \
+  --input_list ./decoder-input-list.txt \
+  --use_native_input_files  \
+  --input_dtype encoder_out float32 \
+  --input_dtype acoustic_embedding float32 \
+  --input_dtype mask int32 \
+  --act_bitwidth 16 \
+  --bias_bitwidth 32
+
+  # Note(fangjun): It throws an error if we specify the layout for decoder inputs.
+  # --input_layout encoder_out NTF
+
+ls -lh
+
+mv -v decoder-$t-seconds-quantized decoder-$t-seconds-quantized.cpp
+
+python3 ../../qnn/generate_config.py \
+    --soc $soc \
+    --graph-name "decoder_${t}_seconds_quantized" \
+    --output-dir ./my-config-3 \
+    --qnn-sdk-root $QNN_SDK_ROOT
+
+ls -lh my-config-3
+
+head -n100 ./my-config-3/*.json
+
+python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+    -c "decoder-$t-seconds-quantized.cpp" \
+    -b "decoder-$t-seconds-quantized.bin" \
+    -o model_libs
+    # -t x86_64-linux-clang \
+
+ls -lh model_libs/x86_64-linux-clang/
+
+$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+  --model ./model_libs/x86_64-linux-clang/libdecoder-${t}-seconds-quantized.so \
+  --output_dir ./binary \
+  --binary_file decoder \
+  --config_file ./my-config-3/htp_backend_extensions.json
+
+ls -lh binary
+
+echo "Finish exporting decoder"
diff --git a/scripts/paraformer/qnn/convert_encoder.sh b/scripts/paraformer/qnn/convert_encoder.sh
new file mode 100755
index 00000000..ca24217e
--- /dev/null
+++ b/scripts/paraformer/qnn/convert_encoder.sh
@@ -0,0 +1,81 @@
+#!/usr/bin/env bash
+
+if [ -z $t ]; then
+  echo "Please run export t=num_input_seconds"
+  exit 1
+fi
+
+if [ -z $soc ]; then
+  echo "Please run export soc=SM8850, etc."
+  exit 1
+fi
+
+if [ -z $QNN_SDK_ROOT ]; then
+  echo "Please run setup QNN first"
+  exit 1
+fi
+
+echo "Export to onnx with num_seconds $t"
+
+python3 ./export_encoder_onnx.py --input-len-in-seconds $t --opset-version 17
+
+ls -lh encoder-*.onnx
+
+python3 ../../pyannote/segmentation/show-onnx.py --filename ./encoder-$t-seconds.onnx
+
+echo "Generate test data"
+
+python3 ./generate_encoder_data.py --input-len-in-seconds $t
+
+ls -lh encoder-*
+
+echo "---"
+cat ./encoder-input-list.txt
+echo "---"
+
+echo "Convert onnx to qnn"
+
+
+qnn-onnx-converter \
+  --input_network ./encoder-$t-seconds.onnx \
+  --output_path ./encoder-$t-seconds-quantized \
+  --out_node encoder_out \
+  --input_list ./encoder-input-list.txt \
+  --use_native_input_files  \
+  --input_dtype x float32 \
+  --act_bitwidth 16 \
+  --bias_bitwidth 32 \
+  --input_layout x NTF
+
+ls -lh
+
+mv -v encoder-$t-seconds-quantized encoder-$t-seconds-quantized.cpp
+
+python3 ../../qnn/generate_config.py \
+    --soc $soc \
+    --graph-name "encoder_${t}_seconds_quantized" \
+    --output-dir ./my-config \
+    --qnn-sdk-root $QNN_SDK_ROOT
+
+ls -lh my-config
+
+head -n100 ./my-config/*.json
+
+python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+    -c "encoder-$t-seconds-quantized.cpp" \
+    -b "encoder-$t-seconds-quantized.bin" \
+    -o model_libs
+    # -t x86_64-linux-clang \
+
+ls -lh model_libs/x86_64-linux-clang/
+
+$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+  --model ./model_libs/x86_64-linux-clang/libencoder-${t}-seconds-quantized.so \
+  --output_dir ./binary \
+  --binary_file encoder \
+  --config_file ./my-config/htp_backend_extensions.json
+
+ls -lh binary
+
+echo "Finish exporting encoder"
diff --git a/scripts/paraformer/qnn/convert_predictor.sh b/scripts/paraformer/qnn/convert_predictor.sh
new file mode 100755
index 00000000..e419bfda
--- /dev/null
+++ b/scripts/paraformer/qnn/convert_predictor.sh
@@ -0,0 +1,82 @@
+#!/usr/bin/env bash
+
+if [ -z $t ]; then
+  echo "Please run export t=num_input_seconds"
+  exit 1
+fi
+
+if [ -z $soc ]; then
+  echo "Please run export soc=SM8850, etc."
+  exit 1
+fi
+
+if [ -z $QNN_SDK_ROOT ]; then
+  echo "Please run setup QNN first"
+  exit 1
+fi
+
+echo "Export to onnx with num_seconds $t"
+
+python3 ./export_predictor_onnx.py --input-len-in-seconds $t --opset-version 17
+
+ls -lh predictor-*.onnx
+
+python3 ../../pyannote/segmentation/show-onnx.py --filename ./predictor-$t-seconds.onnx
+
+echo "Generate test data"
+
+python3 ./generate_predictor_data.py --input-len-in-seconds $t
+
+ls -lh predictor-*
+
+echo "---"
+cat ./predictor-input-list.txt
+echo "---"
+
+echo "Convert onnx to qnn"
+
+
+qnn-onnx-converter \
+  --input_network ./predictor-$t-seconds.onnx \
+  --output_path ./predictor-$t-seconds-quantized \
+  --input_list ./predictor-input-list.txt \
+  --use_native_input_files  \
+  --input_dtype encoder_out float32 \
+  --act_bitwidth 16 \
+  --bias_bitwidth 32
+
+  # Note(fangjun): It throws an error if we specify the layout for predictor input.
+  # --input_layout encoder_out NTF
+
+ls -lh
+
+mv -v predictor-$t-seconds-quantized predictor-$t-seconds-quantized.cpp
+
+python3 ../../qnn/generate_config.py \
+    --soc $soc \
+    --graph-name "predictor_${t}_seconds_quantized" \
+    --output-dir ./my-config-2 \
+    --qnn-sdk-root $QNN_SDK_ROOT
+
+ls -lh my-config-2
+
+head -n100 ./my-config-2/*.json
+
+python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+    -c "predictor-$t-seconds-quantized.cpp" \
+    -b "predictor-$t-seconds-quantized.bin" \
+    -o model_libs
+    # -t x86_64-linux-clang \
+
+ls -lh model_libs/x86_64-linux-clang/
+
+$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+  --model ./model_libs/x86_64-linux-clang/libpredictor-${t}-seconds-quantized.so \
+  --output_dir ./binary \
+  --binary_file predictor \
+  --config_file ./my-config-2/htp_backend_extensions.json
+
+ls -lh binary
+
+echo "Finish exporting predictor"
diff --git a/scripts/paraformer/qnn/export_decoder_onnx.py b/scripts/paraformer/qnn/export_decoder_onnx.py
new file mode 120000
index 00000000..7e2cce9f
--- /dev/null
+++ b/scripts/paraformer/qnn/export_decoder_onnx.py
@@ -0,0 +1 @@
+../rknn/export_decoder_onnx.py
\ No newline at end of file
diff --git a/scripts/paraformer/qnn/export_encoder_onnx.py b/scripts/paraformer/qnn/export_encoder_onnx.py
new file mode 120000
index 00000000..80cce3c0
--- /dev/null
+++ b/scripts/paraformer/qnn/export_encoder_onnx.py
@@ -0,0 +1 @@
+../rknn/export_encoder_onnx.py
\ No newline at end of file
diff --git a/scripts/paraformer/qnn/export_predictor_onnx.py b/scripts/paraformer/qnn/export_predictor_onnx.py
new file mode 120000
index 00000000..114602cc
--- /dev/null
+++ b/scripts/paraformer/qnn/export_predictor_onnx.py
@@ -0,0 +1 @@
+../rknn/export_predictor_onnx.py
\ No newline at end of file
diff --git a/scripts/paraformer/qnn/generate_decoder_data.py b/scripts/paraformer/qnn/generate_decoder_data.py
new file mode 100755
index 00000000..0b480862
--- /dev/null
+++ b/scripts/paraformer/qnn/generate_decoder_data.py
@@ -0,0 +1,99 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import glob
+from pathlib import Path
+
+import numpy as np
+import torch
+
+from export_encoder_onnx import get_args, get_num_input_frames, load_model
+from export_predictor_onnx import modified_predictor_forward
+from test_onnx import compute_feat, get_acoustic_embedding
+from torch_model import CifPredictorV2
+
+CifPredictorV2.forward = modified_predictor_forward
+
+
+def pad(features, max_len):
+    if features.shape[0] > max_len:
+        return features[:max_len]
+    elif features.shape[0] < max_len:
+        features = np.pad(
+            features,
+            ((0, max_len - features.shape[0]), (0, 0)),
+            mode="constant",
+            constant_values=0,
+        )
+    return features
+
+
+@torch.no_grad()
+def main():
+    args = get_args()
+    print(vars(args))
+
+    input_len_in_seconds = int(args.input_len_in_seconds)
+    num_input_frames = get_num_input_frames(input_len_in_seconds)
+
+    wav_files = glob.glob("*.wav")
+
+    model = load_model()
+
+    name_list = []
+    for w in wav_files:
+        f = compute_feat(w)
+        print(w, f.shape)
+        f = pad(f, num_input_frames)
+        f = f[None]
+        print(f.shape)
+
+        f = torch.from_numpy(f)
+
+        encoder_out = model.encoder(f)
+        alpha = model.predictor(encoder_out)
+
+        acoustic_embedding = get_acoustic_embedding(
+            alpha[0].numpy(), encoder_out[0].numpy()
+        )
+        acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
+        num_tokens = acoustic_embedding.shape[1]
+
+        acoustic_embedding = torch.nn.functional.pad(
+            acoustic_embedding,
+            (0, 0, 0, encoder_out.shape[1] - num_tokens),
+            "constant",
+            0,
+        )
+
+        mask = torch.zeros(1, encoder_out.shape[1], dtype=torch.int32)
+
+        mask[0, :num_tokens] = 1
+
+        # NOTE(Fangjun): We have to transpose the data since QNN expects
+        # (N, C, T) for the decoder model
+        # Not sure why it has such a requirement.
+
+        encoder_out = encoder_out.permute(0, 2, 1).clone().numpy()
+        acoustic_embedding = acoustic_embedding.permute(0, 2, 1).clone().numpy()
+
+        print("inputs: ", encoder_out.shape, acoustic_embedding.shape, mask.shape)
+
+        name = Path(w).stem
+
+        first = f"decoder-input-{name}-0.raw"
+        second = f"decoder-input-{name}-1.raw"
+        third = f"decoder-input-{name}-2.raw"
+        encoder_out.tofile(first)
+        acoustic_embedding.tofile(second)
+        mask.numpy().tofile(third)
+
+        name_list.append((first, second, third))
+
+    with open("decoder-input-list.txt", "w") as f:
+        for first, second, third in name_list:
+            f.write(f"{first} {second} {third}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/paraformer/qnn/generate_encoder_data.py b/scripts/paraformer/qnn/generate_encoder_data.py
new file mode 100755
index 00000000..fd87ddc8
--- /dev/null
+++ b/scripts/paraformer/qnn/generate_encoder_data.py
@@ -0,0 +1,53 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import glob
+from pathlib import Path
+
+import numpy as np
+
+from export_encoder_onnx import get_args, get_num_input_frames
+from test_onnx import compute_feat
+
+
+def pad(features, max_len):
+    if features.shape[0] > max_len:
+        return features[:max_len]
+    elif features.shape[0] < max_len:
+        features = np.pad(
+            features,
+            ((0, max_len - features.shape[0]), (0, 0)),
+            mode="constant",
+            constant_values=0,
+        )
+    return features
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+
+    input_len_in_seconds = int(args.input_len_in_seconds)
+    num_input_frames = get_num_input_frames(input_len_in_seconds)
+
+    wav_files = glob.glob("*.wav")
+    features_name = []
+    for w in wav_files:
+        f = compute_feat(w)
+        print(w, f.shape)
+        f = pad(f, num_input_frames)
+        print(f.shape)
+        print()
+        name = Path(w).stem
+
+        s = f"encoder-input-{name}.raw"
+        f.tofile(s)
+        features_name.append(s)
+
+    with open("encoder-input-list.txt", "w") as f:
+        for line in features_name:
+            f.write(f"{line}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/paraformer/qnn/generate_predictor_data.py b/scripts/paraformer/qnn/generate_predictor_data.py
new file mode 100755
index 00000000..7ed09c93
--- /dev/null
+++ b/scripts/paraformer/qnn/generate_predictor_data.py
@@ -0,0 +1,76 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import glob
+from pathlib import Path
+
+import numpy as np
+import torch
+
+from export_encoder_onnx import get_args, get_num_input_frames, load_model
+from export_predictor_onnx import modified_predictor_forward
+from test_onnx import compute_feat
+from torch_model import CifPredictorV2
+
+CifPredictorV2.forward = modified_predictor_forward
+
+
+def pad(features, max_len):
+    if features.shape[0] > max_len:
+        return features[:max_len]
+    elif features.shape[0] < max_len:
+        features = np.pad(
+            features,
+            ((0, max_len - features.shape[0]), (0, 0)),
+            mode="constant",
+            constant_values=0,
+        )
+    return features
+
+
+@torch.no_grad()
+def main():
+    args = get_args()
+    print(vars(args))
+
+    input_len_in_seconds = int(args.input_len_in_seconds)
+    num_input_frames = get_num_input_frames(input_len_in_seconds)
+
+    wav_files = glob.glob("*.wav")
+
+    model = load_model()
+
+    name_list = []
+    for w in wav_files:
+        f = compute_feat(w)
+        print(w, f.shape)
+        f = pad(f, num_input_frames)
+        f = f[None]
+        print(f.shape)
+
+        f = torch.from_numpy(f)
+
+        encoder_out = model.encoder(f)
+
+        # NOTE(Fangjun): We have to transpose the data since QNN expects
+        # (N, C, T) for the predictor model
+        # Not sure why it has such a requirement.
+
+        encoder_out = encoder_out.transpose(1, 2).clone().numpy()
+
+        print("encoder_out", encoder_out.shape)
+
+        name = Path(w).stem
+
+        s = f"predictor-input-{name}.raw"
+        encoder_out.tofile(s)
+        name_list.append(s)
+        print(encoder_out.shape)
+
+    with open("predictor-input-list.txt", "w") as f:
+        for line in name_list:
+            f.write(f"{line}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/paraformer/qnn/test_onnx.py b/scripts/paraformer/qnn/test_onnx.py
new file mode 120000
index 00000000..dfb3c92a
--- /dev/null
+++ b/scripts/paraformer/qnn/test_onnx.py
@@ -0,0 +1 @@
+../rknn/test_onnx.py
\ No newline at end of file
diff --git a/scripts/paraformer/qnn/test_qnn.py b/scripts/paraformer/qnn/test_qnn.py
new file mode 100755
index 00000000..e914d0d2
--- /dev/null
+++ b/scripts/paraformer/qnn/test_qnn.py
@@ -0,0 +1,122 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import numpy as np
+import torch
+
+from export_encoder_onnx import load_model
+from export_predictor_onnx import modified_predictor_forward
+from test_onnx import get_acoustic_embedding
+from torch_model import CifPredictorV2
+
+CifPredictorV2.forward = modified_predictor_forward
+
+
+def load_tokens():
+    id2token = dict()
+    with open("./tokens.txt") as f:
+        for line in f:
+            fields = line.strip().split()
+            id2token[int(fields[1])] = fields[0]
+    return id2token
+
+
+@torch.no_grad()
+def main():
+    model = load_model()
+    encoder_params = sum(p.numel() for p in model.encoder.parameters())
+    predictor_params = sum(p.numel() for p in model.predictor.parameters())
+    decoder_params = sum(p.numel() for p in model.decoder.parameters())
+    print("encoder params (M)", encoder_params / 1024 / 1024)
+    print("predictor params (M)", predictor_params / 1024 / 1024)
+    print("decoder params (M)", decoder_params / 1024 / 1024)
+
+    features = np.fromfile("./encoder-input-zh.raw", dtype=np.float32).reshape(
+        (1, -1, 560)
+    )
+    features = torch.from_numpy(features)
+    encoder_out = model.encoder(features)
+    encoder_out.permute(0, 2, 1).numpy().tofile("predictor-in.raw")
+
+    alpha = model.predictor(encoder_out)
+
+    acoustic_embedding = get_acoustic_embedding(
+        alpha[0].numpy(), encoder_out[0].numpy()
+    )
+    acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
+
+    num_tokens = acoustic_embedding.shape[1]
+
+    acoustic_embedding = torch.nn.functional.pad(
+        acoustic_embedding,
+        (0, 0, 0, encoder_out.shape[1] - num_tokens),
+        "constant",
+        0,
+    )
+
+    mask = torch.zeros(1, encoder_out.shape[1], dtype=torch.float32)
+
+    mask[0, :num_tokens] = 1
+    logits = model.decoder(encoder_out, acoustic_embedding, mask)
+    print("encoder_out", encoder_out.shape)
+    print("acoustic_embedding", acoustic_embedding.shape)
+    print("mask", mask.shape)
+
+    encoder_out.permute(0, 2, 1).numpy().tofile("encoder_out.raw")
+    acoustic_embedding.permute(0, 2, 1).numpy().tofile("acoustic_embedding.raw")
+    mask.to(torch.int32).numpy().tofile("mask.raw")
+
+    yseq = logits[0, :num_tokens].argmax(axis=-1).tolist()
+    print(yseq, "-->", len(yseq))
+
+    id2token = load_tokens()
+    text = [id2token[i] for i in yseq]
+    print(text)
+
+    if False:
+        qnn_encoder_out = np.fromfile("./encoder_out.raw", dtype=np.float32).reshape(
+            1, -1, 512
+        )
+
+        qnn_encoder_out = torch.from_numpy(qnn_encoder_out)
+
+        qnn_alpha = np.fromfile("./alphas.raw", dtype=np.float32).reshape(1, -1)
+        qnn_alpha = torch.from_numpy(qnn_alpha)
+
+        acoustic_embedding = get_acoustic_embedding(
+            qnn_alpha[0].numpy(), qnn_encoder_out[0].numpy()
+        )
+        acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
+
+        num_tokens = acoustic_embedding.shape[1]
+
+        acoustic_embedding = torch.nn.functional.pad(
+            acoustic_embedding,
+            (0, 0, 0, qnn_encoder_out.shape[1] - num_tokens),
+            "constant",
+            0,
+        )
+
+        mask = torch.zeros(1, qnn_encoder_out.shape[1], dtype=torch.float32)
+
+        mask[0, :num_tokens] = 1
+
+        logits = model.decoder(qnn_encoder_out, acoustic_embedding, mask)
+    else:
+        logits = np.fromfile("./decoder_out.raw", dtype=np.float32).reshape(
+            1,
+            -1,
+            encoder_out.shape[1],
+        )
+        logits = torch.from_numpy(logits)
+        logits = logits.permute(0, 2, 1)
+
+    yseq = logits[0, :num_tokens].argmax(axis=-1).tolist()
+    print(yseq, "-->", len(yseq))
+    text = [id2token[i] for i in yseq]
+    print(text)
+
+
+if __name__ == "__main__":
+    torch.manual_seed(20251013)
+    main()
diff --git a/scripts/paraformer/qnn/torch_model.py b/scripts/paraformer/qnn/torch_model.py
new file mode 120000
index 00000000..2ed3ee10
--- /dev/null
+++ b/scripts/paraformer/qnn/torch_model.py
@@ -0,0 +1 @@
+../rknn/torch_model.py
\ No newline at end of file
diff --git a/scripts/paraformer/rknn/export_decoder_onnx.py b/scripts/paraformer/rknn/export_decoder_onnx.py
index 5d93fa26..98ff95f5 100755
--- a/scripts/paraformer/rknn/export_decoder_onnx.py
+++ b/scripts/paraformer/rknn/export_decoder_onnx.py
@@ -3,7 +3,38 @@
 
 import torch
 
-from export_encoder_onnx import load_model, get_args, get_num_input_frames
+from export_encoder_onnx import load_model, get_num_input_frames
+
+import argparse
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--input-len-in-seconds",
+        type=int,
+        required=True,
+        help="""RKNN/QNN does not support dynamic shape, so we need to hard-code
+        how long the model can process.
+        """,
+    )
+
+    parser.add_argument(
+        "--float-mask",
+        type=int,
+        default=1,
+        help="1 to use float master. 0 to use int32 mask",
+    )
+
+    parser.add_argument(
+        "--opset-version",
+        type=int,
+        default=14,
+    )
+    return parser.parse_args()
 
 
 @torch.no_grad()
@@ -18,12 +49,15 @@ def main():
 
     encoder_out = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
     acoustic_embedding = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
-    mask = torch.ones([num_input_frames], dtype=torch.float32)
+    if args.float_mask == 1:
+        mask = torch.ones([num_input_frames], dtype=torch.float32)
+    else:
+        mask = torch.ones([num_input_frames], dtype=torch.int32)
 
     d = model.decoder(encoder_out, acoustic_embedding)
     print("d", d.shape)
 
-    opset_version = 14
+    opset_version = args.opset_version
     filename = f"decoder-{input_len_in_seconds}-seconds.onnx"
     torch.onnx.export(
         model.decoder,
diff --git a/scripts/paraformer/rknn/export_encoder_onnx.py b/scripts/paraformer/rknn/export_encoder_onnx.py
index e6ba32da..e3ab3386 100755
--- a/scripts/paraformer/rknn/export_encoder_onnx.py
+++ b/scripts/paraformer/rknn/export_encoder_onnx.py
@@ -25,6 +25,12 @@ def get_args():
         how long the model can process.
         """,
     )
+
+    parser.add_argument(
+        "--opset-version",
+        type=int,
+        default=14,
+    )
     return parser.parse_args()
 
 
@@ -155,7 +161,7 @@ def main():
     x = torch.randn(1, num_input_frames, 560, dtype=torch.float32)
     pos_emb = torch.rand(1, x.shape[1], 560, dtype=torch.float32)
 
-    opset_version = 14
+    opset_version = args.opset_version
     filename = f"encoder-{input_len_in_seconds}-seconds.onnx"
     torch.onnx.export(
         model.encoder,
diff --git a/scripts/paraformer/rknn/export_predictor_onnx.py b/scripts/paraformer/rknn/export_predictor_onnx.py
index 45af85ef..1a2e4aaf 100755
--- a/scripts/paraformer/rknn/export_predictor_onnx.py
+++ b/scripts/paraformer/rknn/export_predictor_onnx.py
@@ -6,25 +6,26 @@ import torch
 from export_encoder_onnx import load_model, get_args, get_num_input_frames
 from torch_model import CifPredictorV2
 
-if __name__ == "__main__":
 
-    def modified_predictor_forward(self: CifPredictorV2, hidden: torch.Tensor):
-        h = hidden
-        context = h.transpose(1, 2)
-        queries = self.pad(context)
-        output = torch.relu(self.cif_conv1d(queries))
-        output = output.transpose(1, 2)
+def modified_predictor_forward(self: CifPredictorV2, hidden: torch.Tensor):
+    h = hidden
+    context = h.transpose(1, 2)
+    queries = self.pad(context)
+    output = torch.relu(self.cif_conv1d(queries))
+    output = output.transpose(1, 2)
+
+    output = self.cif_output(output)
+    alphas = torch.sigmoid(output)
+    alphas = torch.nn.functional.relu(
+        alphas * self.smooth_factor - self.noise_threshold
+    )
 
-        output = self.cif_output(output)
-        alphas = torch.sigmoid(output)
-        alphas = torch.nn.functional.relu(
-            alphas * self.smooth_factor - self.noise_threshold
-        )
+    alphas = alphas.squeeze(-1)
 
-        alphas = alphas.squeeze(-1)
+    return alphas
 
-        return alphas
 
+if __name__ == "__main__":
     CifPredictorV2.forward = modified_predictor_forward
 
 
@@ -40,7 +41,7 @@ def main():
 
     x = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
 
-    opset_version = 14
+    opset_version = args.opset_version
     filename = f"predictor-{input_len_in_seconds}-seconds.onnx"
     torch.onnx.export(
         model.predictor,
diff --git a/scripts/paraformer/rknn/test_onnx.py b/scripts/paraformer/rknn/test_onnx.py
index b8f5bf70..aa73dc81 100755
--- a/scripts/paraformer/rknn/test_onnx.py
+++ b/scripts/paraformer/rknn/test_onnx.py
@@ -61,7 +61,7 @@ def compute_feat(filename):
     )
     assert features.data.contiguous is True
     assert features.dtype == np.float32, features.dtype
-    print("features sum", features.sum(), features.shape)
+    #  print("features sum", features.sum(), features.shape)
 
     window_size = 7  # lfr_m
     window_shift = 6  # lfr_n
diff --git a/scripts/sense-voice/rknn/test_nano_torch.py b/scripts/sense-voice/rknn/test_nano_torch.py
index 9616ba68..7e150742 100755
--- a/scripts/sense-voice/rknn/test_nano_torch.py
+++ b/scripts/sense-voice/rknn/test_nano_torch.py
@@ -47,6 +47,8 @@ def load_torch_model():
 @torch.no_grad()
 def main():
     model = load_torch_model()
+    num_params = sum(p.numel() for p in model.parameters())
+    print("num_params (M)", num_params, num_params / 1000000)
 
     samples, sample_rate = test_onnx.load_audio("./zh.wav")
     assert sample_rate == 16000, sample_rate

commit 2d189c4f1904ae977059e3e612da03904d9a59a0
Author: Wei Kang <wkang.pku@gmail.com>
Date:   Tue Dec 23 11:47:44 2025 +0800

    [KWS] Add phone+ppinyin tokenization with lexicon support (for zh-en model) (#2922)

diff --git a/scripts/text2token.py b/scripts/text2token.py
index 71026d98..73f87e28 100755
--- a/scripts/text2token.py
+++ b/scripts/text2token.py
@@ -87,10 +87,19 @@ def get_args():
         "--tokens-type",
         type=str,
         required=True,
-        choices=["cjkchar", "bpe", "cjkchar+bpe", "fpinyin", "ppinyin"],
-        help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin or ppinyin.
+        choices=[
+            "cjkchar",
+            "bpe",
+            "cjkchar+bpe",
+            "fpinyin",
+            "ppinyin",
+            "phone+ppinyin",
+        ],
+        help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin
+        ppinyin or phone+ppinyin.
         fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
         ppinyin means partial pinyin, it splits pinyin into initial and final,
+        phone means English phonemes in CMU dictionary format.
         """,
     )
 
@@ -100,6 +109,12 @@ def get_args():
         help="The path to bpe.model. Only required when tokens-type is bpe or cjkchar+bpe.",
     )
 
+    parser.add_argument(
+        "--lexicon",
+        type=str,
+        help="The path to lexicon.txt. Only required when tokens-type is phone+ppinyin.",
+    )
+
     parser.add_argument(
         "--output",
         type=str,
@@ -134,6 +149,7 @@ def main():
         tokens=args.tokens,
         tokens_type=args.tokens_type,
         bpe_model=args.bpe_model,
+        lexicon=args.lexicon,
     )
     with open(args.output, "w", encoding="utf8") as f:
         for i, txt in enumerate(encoded_texts):
diff --git a/sherpa-onnx/python/sherpa_onnx/cli.py b/sherpa-onnx/python/sherpa_onnx/cli.py
index 0527e20a..edd8eb9e 100644
--- a/sherpa-onnx/python/sherpa_onnx/cli.py
+++ b/sherpa-onnx/python/sherpa_onnx/cli.py
@@ -1,12 +1,13 @@
 # Copyright (c)  2023  Xiaomi Corporation
 
 import logging
+
 try:
     import click
 except ImportError:
-    print('Please run')
-    print('  pip install click')
-    print('before you continue')
+    print("Please run")
+    print("  pip install click")
+    print("before you continue")
     raise
 
 from pathlib import Path
@@ -36,12 +37,21 @@ def cli():
 @click.option(
     "--tokens-type",
     type=click.Choice(
-        ["cjkchar", "bpe", "cjkchar+bpe", "fpinyin", "ppinyin"], case_sensitive=True
+        [
+            "cjkchar",
+            "bpe",
+            "cjkchar+bpe",
+            "fpinyin",
+            "ppinyin",
+            "phone+ppinyin",
+        ],
+        case_sensitive=True,
     ),
     required=True,
-    help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin or ppinyin.
+    help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin or phone+ppinyin.
     fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
     ppinyin means partial pinyin, it splits pinyin into initial and final,
+    phone means English phonemes in CMU dictionary format.
     """,
 )
 @click.option(
@@ -49,8 +59,18 @@ def cli():
     type=str,
     help="The path to bpe.model. Only required when tokens-type is bpe or cjkchar+bpe.",
 )
+@click.option(
+    "--lexicon",
+    type=str,
+    help="The path to lexicon.txt. Only required when tokens-type is phone+ppinyin.",
+)
 def encode_text(
-    input: Path, output: Path, tokens: Path, tokens_type: str, bpe_model: Path
+    input: Path,
+    output: Path,
+    tokens: Path,
+    tokens_type: str,
+    bpe_model: Path,
+    lexicon: Path,
 ):
     """
     Encode the texts given by the INPUT to tokens and write the results to the OUTPUT.
@@ -101,7 +121,11 @@ def encode_text(
             extra_info.append(extra)
 
     encoded_texts = text2token(
-        texts, tokens=tokens, tokens_type=tokens_type, bpe_model=bpe_model
+        texts,
+        tokens=tokens,
+        tokens_type=tokens_type,
+        bpe_model=bpe_model,
+        lexicon=lexicon,
     )
     with open(output, "w", encoding="utf8") as f:
         for i, txt in enumerate(encoded_texts):
diff --git a/sherpa-onnx/python/sherpa_onnx/utils.py b/sherpa-onnx/python/sherpa_onnx/utils.py
index fd36f2c0..de4b4b7c 100644
--- a/sherpa-onnx/python/sherpa_onnx/utils.py
+++ b/sherpa-onnx/python/sherpa_onnx/utils.py
@@ -10,6 +10,7 @@ def text2token(
     tokens: str,
     tokens_type: str = "cjkchar",
     bpe_model: Optional[str] = None,
+    lexicon: Optional[str] = None,
     output_ids: bool = False,
 ) -> List[List[Union[str, int]]]:
     """
@@ -21,12 +22,15 @@ def text2token(
       tokens:
         The path of the tokens.txt.
       tokens_type:
-        The valid values are cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin.
+        The valid values are cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin, phone+ppinyin.
         fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
         ppinyin means partial pinyin, it splits pinyin into initial and final,
+        phone means English phonemes in CMU dictionary format.
       bpe_model:
         The path of the bpe model. Only required when tokens_type is bpe or
         cjkchar+bpe.
+      lexicon:
+        The path of the lexicon.txt. Only required when tokens_type is phone+ppinyin.
       output_ids:
         True to output token ids otherwise tokens.
     Returns:
@@ -64,34 +68,75 @@ def text2token(
         sp = spm.SentencePieceProcessor()
         sp.load(bpe_model)
 
+    phone_table = {}
+    if tokens_type == "phone+ppinyin":
+        assert (
+            lexicon and Path(lexicon).is_file()
+        ), f"File not exists, {lexicon}"
+        with open(lexicon, "r", encoding="utf-8") as f:
+            for line in f:
+                toks = line.strip().split()
+                assert len(toks) >= 2, len(toks)
+                word = toks[0]
+                phones = toks[1:]
+                phone_table[word] = phones
+
     texts_list: List[List[str]] = []
 
+    def to_pinyin(txt: str, out_type: str) -> List[str]:
+        assert out_type in ["ppinyin", "fpinyin"], f"given {out_type}"
+        py = [x[0] for x in pinyin(txt)]
+        if "ppinyin" == out_type:
+            res = []
+            for x in py:
+                initial = to_initials(x, strict=False)
+                final = to_finals_tone(x, strict=False)
+                if initial == "" and final == "":
+                    res.append(x)
+                else:
+                    if initial:
+                        res.append(initial)
+                    if final:
+                        res.append(final)
+            return res
+        else:
+            return py
+
     if tokens_type == "cjkchar":
         texts_list = [list("".join(text.split())) for text in texts]
     elif tokens_type == "bpe":
         texts_list = sp.encode(texts, out_type=str)
-    elif "pinyin" in tokens_type:
+    elif tokens_type == "ppinyin" or tokens_type == "fpinyin":
         for txt in texts:
-            py = [x[0] for x in pinyin(txt)]
-            if "ppinyin" == tokens_type:
-                res = []
-                for x in py:
-                    initial = to_initials(x, strict=False)
-                    final = to_finals_tone(x, strict=False)
-                    if initial == "" and final == "":
-                        res.append(x)
+            texts_list.append(to_pinyin(txt, tokens_type))
+    elif tokens_type == "phone+ppinyin":
+        # CJK(China Japan Korea) unicode range is [U+4E00, U+9FFF], ref:
+        # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
+        pattern = re.compile(r"^[\u4e00-\u9fff]+$")
+        for text in texts:
+            words = text.strip().split()
+            text_list = []
+            skip_text = False
+            for w in words:
+                if w in phone_table:
+                    text_list += phone_table[w]
+                else:
+                    if pattern.fullmatch(w) is None:
+                        print(
+                            f"Word {w} not in lexicon and it is not a CJK character, "
+                            f"skipping text: {text}."
+                        )
+                        skip_text = True
+                        break
                     else:
-                        if initial != "":
-                            res.append(initial)
-                        if final != "":
-                            res.append(final)
-                texts_list.append(res)
-            else:
-                texts_list.append(py)
+                        text_list += to_pinyin(w, "ppinyin")
+            if not skip_text:
+                texts_list.append(text_list)
     else:
         assert (
             tokens_type == "cjkchar+bpe"
-        ), f"Supported tokens_type are cjkchar, bpe, cjkchar+bpe, given {tokens_type}"
+        ), f"Supported tokens_type are cjkchar, bpe, cjkchar+bpe, ppinyin, fpinyin, phone+ppinyin given {tokens_type}"
+
         # CJK(China Japan Korea) unicode range is [U+4E00, U+9FFF], ref:
         # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
         pattern = re.compile(r"([\u4e00-\u9fff])")
diff --git a/sherpa-onnx/python/tests/test_text2token.py b/sherpa-onnx/python/tests/test_text2token.py
index 7bc065b6..c8924d5a 100755
--- a/sherpa-onnx/python/tests/test_text2token.py
+++ b/sherpa-onnx/python/tests/test_text2token.py
@@ -116,6 +116,93 @@ class TestText2Token(unittest.TestCase):
             [685, 736, 275, 178, 179, 921, 736],
         ], encoded_ids
 
+    def test_phone_ppinyin(self):
+        tokens = f"{d}/text2token/tokens_phone_ppinyin.txt"
+        lexicon = f"{d}/text2token/en.phone"
+
+        if not Path(tokens).is_file() or not Path(lexicon).is_file():
+            print(
+                f"No test data found, skipping test_phone_ppinyin().\n"
+                f"You can download the test data by: \n"
+                f"git clone https://github.com/pkufool/sherpa-test-data.git /tmp/sherpa-test-data"
+            )
+            return
+
+        texts = [" GOES TOGETHER", " GOES WITH "]
+        encoded_texts = sherpa_onnx.text2token(
+            texts,
+            tokens=tokens,
+            tokens_type="phone+ppinyin",
+            lexicon=lexicon,
+        )
+        assert encoded_texts == [
+            [
+                "sh",
+                "",
+                "j",
+                "i",
+                "r",
+                "n",
+                "m",
+                "n",
+                "G",
+                "OW1",
+                "Z",
+                "T",
+                "AH0",
+                "G",
+                "EH1",
+                "DH",
+                "ER0",
+            ],
+            [
+                "zh",
+                "ng",
+                "g",
+                "u",
+                "G",
+                "OW1",
+                "Z",
+                "W",
+                "IH1",
+                "DH",
+                "m",
+                "i",
+                "g",
+                "u",
+            ],
+        ], encoded_texts
+
+        encoded_ids = sherpa_onnx.text2token(
+            texts,
+            tokens=tokens,
+            tokens_type="phone+ppinyin",
+            lexicon=lexicon,
+            output_ids=True,
+        )
+        assert encoded_ids == [
+            [
+                139,
+                203,
+                127,
+                107,
+                137,
+                200,
+                130,
+                207,
+                35,
+                50,
+                70,
+                59,
+                9,
+                35,
+                26,
+                24,
+                28,
+            ],
+            [182, 241, 87, 163, 35, 50, 70, 68, 38, 24, 130, 231, 87, 163],
+        ], encoded_ids
+
 
 if __name__ == "__main__":
     unittest.main()

commit 16e399fe63587564c4dcb4d75dab2f745a9dbf4f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 22 18:42:50 2025 +0800

    Epoxrt more zipformer ctc models to qnn (#2921)

diff --git a/.github/scripts/export-qnn/generate_zipformer.py b/.github/scripts/export-qnn/generate_zipformer.py
new file mode 100755
index 00000000..09ae8702
--- /dev/null
+++ b/.github/scripts/export-qnn/generate_zipformer.py
@@ -0,0 +1,52 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import json
+
+from device_info import soc_info_dict
+from dataclasses import asdict, dataclass
+import itertools
+
+
+@dataclass
+class Config:
+    soc: str  # SM8850
+    soc_id: int  # 87
+    arch: str  # v81
+    input_in_seconds: str
+    model_name: str
+
+
+def main():
+
+    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+    model_name_list = ["20250703", "20251222"]
+
+    configs = []
+
+    for name, soc in soc_info_dict.items():
+        for num_seconds, model_name in itertools.product(
+            input_in_seconds, model_name_list
+        ):
+            if model_name == "20251222":
+                if num_seconds not in ["5"]:
+                    # TODO(fangjun): We only upload model-5-seconds.onnx right now
+                    continue
+
+            configs.append(
+                Config(
+                    soc=name,
+                    soc_id=soc.model.value,
+                    arch=soc.info.arch.name,
+                    input_in_seconds=num_seconds,
+                    model_name=model_name,
+                )
+            )
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
index 4b411384..f54b9618 100644
--- a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
+++ b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
@@ -3,7 +3,7 @@ name: export-zipformer-ctc-to-qnn-20250703
 on:
   push:
     branches:
-      - qnn-zipformer-ctc-models
+      - zipformer-qnn-model-2
   workflow_dispatch:
 
 concurrency:
@@ -11,24 +11,50 @@ concurrency:
   cancel-in-progress: true
 
 jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-qnn/generate_zipformer.py
+          MATRIX=$(python3 .github/scripts/export-qnn/generate_zipformer.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
   export-zipformer-ctc-to-qnn-20250703:
+    needs: generate_build_matrix
     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
-    name: ${{ matrix.input_in_seconds }}
-    runs-on: ${{ matrix.os }}
+    name: ${{ matrix.model_name }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
+    runs-on: ubuntu-22.04
     strategy:
       fail-fast: false
       matrix:
-        os: [ubuntu-22.04]
-        python-version: ["3.10"]
-        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
 
     steps:
       - uses: actions/checkout@v4
 
-      - name: Setup Python ${{ matrix.python-version }}
+      - name: Setup Python 3.10
         uses: actions/setup-python@v5
         with:
-          python-version: ${{ matrix.python-version }}
+          python-version: "3.10"
+
+      - name: Create directories
+        shell: bash
+        run: |
+          mkdir so binary
 
       - name: Display NDK HOME
         shell: bash
@@ -175,6 +201,7 @@ jobs:
           qnn-net-run --help
 
       - name: Run ${{ matrix.input_in_seconds }}
+        if: matrix.model_name == '20250703'
         shell: bash
         run: |
           source py310/bin/activate
@@ -185,6 +212,7 @@ jobs:
 
           export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
           export LDFLAGS="-Wl,-z,max-page-size=16384"
+          dir=$PWD
 
           mkdir tmp
 
@@ -244,8 +272,43 @@ jobs:
 
           readelf -lW model_libs/*/lib*.so
 
+          echo "Generate context binary"
+
+          $dir/scripts/qnn/generate_config.py  \
+            --soc ${{ matrix.soc }} \
+            --graph-name "model_${t}_seconds_quantized" \
+            --output-dir ./my-config \
+            --qnn-sdk-root $QNN_SDK_ROOT
+
+          ls -lh my-config
+
+          head -n 1000 my-config/*.json
+
+          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
+            --output_dir ./binary \
+            --binary_file model \
+            --config_file ./my-config/htp_backend_extensions.json
+
+          ls -lh binary/
+
           echo "collect results"
 
+          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-zipformer-ctc-zh-2025-07-03-int8
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+          cp -v binary/model.bin $d/
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+          echo "num_frames=$num_frames" > $d/info.txt
+
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+          mv *.tar.bz2 ../binary/
+
           for p in x86_64-linux-clang aarch64-android; do
             if [[ $p == x86_64-linux-clang ]]; then
               d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-linux-x64
@@ -275,22 +338,181 @@ jobs:
           echo "----show---"
           ls -lh *.tar.bz2
 
-          mv *.tar.bz2 ../
+          mv *.tar.bz2 ../so
+
+      - name: Run ${{ matrix.input_in_seconds }}
+        if: matrix.model_name == '20251222'
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+          dir=$PWD
+
+          mkdir tmp
+
+          cd tmp
+
+          t=${{ matrix.input_in_seconds }}
+          num_frames=$(($t*100))
+
+          echo "num_frames: $num_frames"
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
+          chmod +x generate_test_data.py
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
+
+          ./generate_test_data.py --num-frames $num_frames --wav 0.wav
+          ./generate_test_data.py --num-frames $num_frames --wav 1.wav
+          ./generate_test_data.py --num-frames $num_frames --wav 8k.wav
+
+          echo -e "0.raw\n1.raw\n8k.raw" > input_list.txt
+
+          curl -SL -O https://huggingface.co/csukuangfj/2025-12-22/resolve/main/zipformer-ctc-models/model-$t-seconds.onnx
+
+          python3 ../scripts/pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
+
+
+          echo "export to qnn"
+          echo "----------$t----------"
+
+          qnn-onnx-converter \
+            --input_network model-$t-seconds.onnx \
+            --output_path ./model-$t-seconds-quantized \
+            --out_node log_probs \
+            --input_list ./input_list.txt \
+            --use_native_input_files  \
+            --input_dtype x float32 \
+            --act_bitwidth 16 \
+            --bias_bitwidth 32 \
+            --input_layout x NTF
+
+          ls -lh
+          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
+          echo "----"
+          ls -lh
+
+          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+            -c "model-$t-seconds-quantized.cpp" \
+            -b "model-$t-seconds-quantized.bin" \
+            -o model_libs > /dev/null 2>&1
+
+          ls -lh model_libs/*/
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "Generate context binary"
+
+          $dir/scripts/qnn/generate_config.py  \
+            --soc ${{ matrix.soc }} \
+            --graph-name "model_${t}_seconds_quantized" \
+            --output-dir ./my-config \
+            --qnn-sdk-root $QNN_SDK_ROOT
+
+          ls -lh my-config
+
+          head -n 1000 my-config/*.json
+
+          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
+            --output_dir ./binary \
+            --binary_file model \
+            --config_file ./my-config/htp_backend_extensions.json
+
+          ls -lh binary/
+
+          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-zipformer-ctc-zh-2025-12-22-int8
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+          cp -v binary/model.bin $d/
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+          echo "num_frames=$num_frames" > $d/info.txt
+
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+          mv *.tar.bz2 ../binary/
+
+          echo "collect results"
+
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-12-22-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-12-22-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v model_libs/$p/lib*.so $d/libmodel.so
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+
+            echo "num_frames=$num_frames" > $d/info.txt
+            echo "target=$p" >> $d/info.txt
+
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../so
 
       - uses: actions/upload-artifact@v4
         with:
-          name: ${{ matrix.input_in_seconds }}-seconds
+          name: ${{ matrix.model_name }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
           path: ./tmp/*.json
 
+      - name: Release
+        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn
+
       - name: Release
         if: github.repository_owner == 'csukuangfj'
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
-          file: ./*.tar.bz2
+          file: ./binary/*.tar.bz2
           overwrite: true
           repo_name: k2-fsa/sherpa-onnx
           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn-binary
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
           tag: asr-models-qnn
 
       - name: Release
@@ -298,6 +520,6 @@ jobs:
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
-          file: ./*.tar.bz2
+          file: ./binary/*.tar.bz2
           overwrite: true
-          tag: asr-models-qnn
+          tag: asr-models-qnn-binary
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
index 8a09d884..20d25bee 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
@@ -147,9 +147,19 @@ object SimulateStreamingAsr {
                             config.modelConfig.senseVoice.qnnConfig.contextBinary,
                             context
                         )
-                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
-                    config.modelConfig.zipformerCtc.model =
-                        copyAssetToInternalStorage(config.modelConfig.zipformerCtc.model, context)
+                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty() ||
+                    assetExists(
+                        context.assets,
+                        path = config.modelConfig.zipformerCtc.qnnConfig.contextBinary
+                    )
+                ) {
+                    if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
+                        config.modelConfig.zipformerCtc.model =
+                            copyAssetToInternalStorage(
+                                config.modelConfig.zipformerCtc.model,
+                                context
+                            )
+                    }
 
                     config.modelConfig.zipformerCtc.qnnConfig.contextBinary =
                         copyAssetToInternalStorage(
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index a58b34ef..38bf3e5b 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -176,7 +176,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       return std::make_unique<
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
           config);
-    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+    } else if (!config.model_config.zipformer_ctc.model.empty() ||
+               !config.model_config.zipformer_ctc.qnn_config.context_binary
+                    .empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
     } else {
       SHERPA_ONNX_LOGE(
@@ -497,7 +499,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       return std::make_unique<
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
           mgr, config);
-    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+    } else if (!config.model_config.zipformer_ctc.model.empty() ||
+               !config.model_config.zipformer_ctc.qnn_config.context_binary
+                    .empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
                                                                     config);
     } else {
diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
index 0aaaacbb..33e7880a 100644
--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
@@ -22,13 +22,31 @@ void OfflineZipformerCtcModelConfig::Register(ParseOptions *po) {
 }
 
 bool OfflineZipformerCtcModelConfig::Validate() const {
-  if (!FileExists(model)) {
-    SHERPA_ONNX_LOGE("zipformer CTC model file '%s' does not exist",
-                     model.c_str());
-    return false;
+  if (qnn_config.context_binary.empty()) {
+    if (model.empty()) {
+      SHERPA_ONNX_LOGE("Please provide a Zipformer CTC model");
+      return false;
+    }
+
+    if (!FileExists(model)) {
+      SHERPA_ONNX_LOGE("Zipformer CTC model '%s' does not exist",
+                       model.c_str());
+      return false;
+    }
   }
 
-  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
+  if (model.empty() && !qnn_config.context_binary.empty()) {
+    // we require that the context_binary exists
+    if (!FileExists(qnn_config.context_binary)) {
+      SHERPA_ONNX_LOGE(
+          "Model is empty, but you provide a context binary that does not "
+          "exist");
+      return false;
+    }
+  }
+
+  if (EndsWith(model, ".so") || EndsWith(model, ".bin") ||
+      (model.empty() && !qnn_config.context_binary.empty())) {
     return qnn_config.Validate();
   }
 

commit 94a040e396d5b612de1b54f173434e3489c357b4
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 18 17:43:25 2025 +0800

    Refactor ZipVoice C++ code (#2911)
    
    This pull request introduces a significant refactoring of the ZipVoice C++ implementation, aiming to improve code structure, performance, and maintainability. The changes include the addition of a NormalDataGenerator for better random number generation, splitting the OfflineTtsZipvoiceModel into more modular encoder and decoder components, and optimizing the mel spectrogram computation.

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index d03dd75d..e28e9a30 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -26,6 +26,7 @@ set(sources
   keyword-spotter.cc
   lodr-fst.cc
   math.cc
+  normal-data-generator.cc
   offline-canary-model-config.cc
   offline-canary-model.cc
   offline-ctc-fst-decoder-config.cc
diff --git a/sherpa-onnx/csrc/normal-data-generator.cc b/sherpa-onnx/csrc/normal-data-generator.cc
new file mode 100644
index 00000000..b62bede3
--- /dev/null
+++ b/sherpa-onnx/csrc/normal-data-generator.cc
@@ -0,0 +1,46 @@
+// sherpa-onnx/csrc/normal-data-generator.cc
+//
+// Copyright      2025  Xiaomi Corporation
+
+// Written by ChatGPT
+
+#include "sherpa-onnx/csrc/normal-data-generator.h"
+
+#include <random>
+#include <thread>
+
+namespace sherpa_onnx {
+
+// Helper type hidden in translation unit
+struct RNGHolder {
+  std::mt19937 rng;
+  std::normal_distribution<float> dist;
+
+  RNGHolder()
+      : rng([] {
+          std::random_device rd;
+          std::seed_seq seq{rd(),
+                            static_cast<unsigned>(std::hash<std::thread::id>{}(
+                                std::this_thread::get_id()))};
+          return std::mt19937(seq);
+        }()),
+        dist() {}
+};
+
+NormalDataGenerator::NormalDataGenerator(float mean /* = 0.0f */,
+                                         float stddev /* = 1.0f */)
+    : mean_(mean), stddev_(stddev) {}
+
+void NormalDataGenerator::Fill(float *data, std::size_t size) const {
+  // One RNGHolder per thread
+  static thread_local RNGHolder holder;
+
+  holder.dist.param(
+      std::normal_distribution<float>::param_type(mean_, stddev_));
+
+  for (std::size_t i = 0; i < size; ++i) {
+    data[i] = holder.dist(holder.rng);
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/normal-data-generator.h b/sherpa-onnx/csrc/normal-data-generator.h
new file mode 100644
index 00000000..e250f2b6
--- /dev/null
+++ b/sherpa-onnx/csrc/normal-data-generator.h
@@ -0,0 +1,26 @@
+// sherpa-onnx/csrc/normal-data-generator.h
+//
+// Copyright      2025  Xiaomi Corporation
+
+// Written by ChatGPT
+#ifndef SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
+#define SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
+
+#include <cstddef>
+
+namespace sherpa_onnx {
+
+class NormalDataGenerator {
+ public:
+  explicit NormalDataGenerator(float mean = 0.0f, float stddev = 1.0f);
+
+  // Fill pre-allocated memory
+  void Fill(float *data, std::size_t size) const;
+
+ private:
+  float mean_;
+  float stddev_;
+};
+
+}  // namespace sherpa_onnx
+#endif  // SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
index 8e9861f5..a315d75b 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
@@ -33,6 +33,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
         model_(std::make_unique<OfflineTtsZipvoiceModel>(config.model)),
         vocoder_(Vocoder::Create(config.model)) {
     InitFrontend();
+
+    PostInit();
   }
 
   template <typename Manager>
@@ -41,6 +43,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
         model_(std::make_unique<OfflineTtsZipvoiceModel>(mgr, config.model)),
         vocoder_(Vocoder::Create(mgr, config.model)) {
     InitFrontend(mgr);
+
+    PostInit();
   }
 
   int32_t SampleRate() const override {
@@ -99,6 +103,33 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
   }
 
  private:
+  void PostInit() { InitMelBanks(); }
+
+  void InitMelBanks() {
+    const auto &meta = model_->GetMetaData();
+    int32_t sample_rate = meta.sample_rate;
+    int32_t n_fft = meta.n_fft;
+    int32_t hop_length = meta.hop_length;
+    int32_t win_length = meta.window_length;
+    int32_t num_mels = meta.num_mels;
+
+    knf::FrameExtractionOptions frame_opts;
+    frame_opts.samp_freq = sample_rate;
+    frame_opts.frame_length_ms = win_length * 1000 / sample_rate;
+    frame_opts.frame_shift_ms = hop_length * 1000 / sample_rate;
+    frame_opts.window_type = "hanning";
+
+    knf::MelBanksOptions mel_opts;
+    mel_opts.num_bins = num_mels;
+    mel_opts.low_freq = 0;
+    mel_opts.high_freq = sample_rate / 2;
+    mel_opts.is_librosa = true;
+    mel_opts.use_slaney_mel_scale = false;
+    mel_opts.norm = "";
+
+    mel_banks_ = std::make_unique<knf::MelBanks>(mel_opts, frame_opts, 1.0f);
+  }
+
   template <typename Manager>
   void InitFrontend(Manager *mgr) {
     frontend_ = std::make_unique<MatchaTtsLexicon>(
@@ -112,9 +143,9 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
         config_.model.zipvoice.data_dir, config_.model.debug, true);
   }
 
-  std::vector<int32_t> ComputeMelSpectrogram(
-      const std::vector<float> &_samples, int32_t sample_rate,
-      std::vector<float> *prompt_features) const {
+  void ComputeMelSpectrogram(const std::vector<float> &_samples,
+                             int32_t sample_rate, float feat_scale,
+                             std::vector<float> *prompt_features) const {
     const auto &meta = model_->GetMetaData();
     if (sample_rate != meta.sample_rate) {
       SHERPA_ONNX_LOGE(
@@ -131,19 +162,18 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
           sample_rate, meta.sample_rate, lowpass_cutoff, lowpass_filter_width);
       std::vector<float> samples;
       resampler->Resample(_samples.data(), _samples.size(), true, &samples);
-      return ComputeMelSpectrogram(samples, prompt_features);
-    } else {
-      // Use the original samples if the sample rate matches
-      return ComputeMelSpectrogram(_samples, prompt_features);
+      ComputeMelSpectrogram(samples, feat_scale, prompt_features);
+      return;
     }
+
+    ComputeMelSpectrogram(_samples, feat_scale, prompt_features);
   }
 
-  std::vector<int32_t> ComputeMelSpectrogram(
-      const std::vector<float> &samples,
-      std::vector<float> *prompt_features) const {
+  void ComputeMelSpectrogram(const std::vector<float> &samples,
+                             float feat_scale,
+                             std::vector<float> *prompt_features) const {
     const auto &meta = model_->GetMetaData();
 
-    int32_t sample_rate = meta.sample_rate;
     int32_t n_fft = meta.n_fft;
     int32_t hop_length = meta.hop_length;
     int32_t win_length = meta.window_length;
@@ -161,46 +191,23 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
     int32_t num_frames = stft_result.num_frames;
     int32_t fft_bins = n_fft / 2 + 1;
 
-    knf::FrameExtractionOptions frame_opts;
-    frame_opts.samp_freq = sample_rate;
-    frame_opts.frame_length_ms = win_length * 1000 / sample_rate;
-    frame_opts.frame_shift_ms = hop_length * 1000 / sample_rate;
-    frame_opts.window_type = "hanning";
-
-    knf::MelBanksOptions mel_opts;
-    mel_opts.num_bins = num_mels;
-    mel_opts.low_freq = 0;
-    mel_opts.high_freq = sample_rate / 2;
-    mel_opts.is_librosa = true;
-    mel_opts.use_slaney_mel_scale = false;
-    mel_opts.norm = "";
-
-    knf::MelBanks mel_banks(mel_opts, frame_opts, 1.0f);
+    prompt_features->resize(num_frames * num_mels);
+    float *p = prompt_features->data();
 
-    prompt_features->clear();
-    prompt_features->reserve(num_frames * num_mels);
+    std::vector<float> magnitude_spectrum(fft_bins);
 
-    for (int32_t i = 0; i < num_frames; ++i) {
-      std::vector<float> magnitude_spectrum(fft_bins);
+    for (int32_t i = 0; i < num_frames; ++i, p += num_mels) {
       for (int32_t k = 0; k < fft_bins; ++k) {
         float real = stft_result.real[i * fft_bins + k];
         float imag = stft_result.imag[i * fft_bins + k];
         magnitude_spectrum[k] = std::sqrt(real * real + imag * imag);
       }
-      std::vector<float> mel_features(num_mels, 0.0f);
-      mel_banks.Compute(magnitude_spectrum.data(), mel_features.data());
-      for (auto &v : mel_features) {
-        v = std::log(v + 1e-10f);
+
+      mel_banks_->Compute(magnitude_spectrum.data(), p);
+
+      for (int32_t j = 0; j < num_mels; ++j) {
+        p[j] = std::log(p[j] + 1e-10f) * feat_scale;
       }
-      // Instead of push_back a vector, push elements individually
-      prompt_features->insert(prompt_features->end(), mel_features.begin(),
-                              mel_features.end());
-    }
-    if (num_frames == 0) {
-      SHERPA_ONNX_LOGE("No frames extracted from the prompt audio");
-      return {0, 0};
-    } else {
-      return {num_frames, num_mels};
     }
   }
 
@@ -214,12 +221,14 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
 
     std::array<int64_t, 2> tokens_shape = {1,
                                            static_cast<int64_t>(tokens.size())};
+
     Ort::Value tokens_tensor = Ort::Value::CreateTensor(
         memory_info, const_cast<int64_t *>(tokens.data()), tokens.size(),
         tokens_shape.data(), tokens_shape.size());
 
     std::array<int64_t, 2> prompt_tokens_shape = {
         1, static_cast<int64_t>(prompt_tokens.size())};
+
     Ort::Value prompt_tokens_tensor = Ort::Value::CreateTensor(
         memory_info, const_cast<int64_t *>(prompt_tokens.data()),
         prompt_tokens.size(), prompt_tokens_shape.data(),
@@ -230,7 +239,7 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
 
     // Scale prompt_samples
     std::vector<float> prompt_samples_scaled = prompt_samples;
-    float prompt_rms = 0.0f;
+    double prompt_rms = 0.0;
     double sum_sq = 0.0;
     // Compute RMS of prompt_samples
     for (float s : prompt_samples_scaled) {
@@ -238,23 +247,24 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
     }
     prompt_rms = std::sqrt(sum_sq / prompt_samples_scaled.size());
     if (prompt_rms < target_rms && prompt_rms > 0.0f) {
-      float scale = target_rms / static_cast<float>(prompt_rms);
+      float scale = target_rms / prompt_rms;
       for (auto &s : prompt_samples_scaled) {
         s *= scale;
       }
     }
 
     std::vector<float> prompt_features;
-    auto res_shape = ComputeMelSpectrogram(prompt_samples_scaled, sample_rate,
-                                           &prompt_features);
 
-    int32_t num_frames = res_shape[0];
-    int32_t mel_dim = res_shape[1];
+    int32_t mel_dim = model_->GetMetaData().num_mels;
 
-    if (feat_scale != 1.0f) {
-      for (auto &item : prompt_features) {
-        item *= feat_scale;
-      }
+    ComputeMelSpectrogram(prompt_samples_scaled, sample_rate, feat_scale,
+                          &prompt_features);
+
+    int32_t num_frames = prompt_features.size() / mel_dim;
+
+    if (num_frames == 0) {
+      SHERPA_ONNX_LOGE("No frames extracted from the prompt audio");
+      return {};
     }
 
     std::array<int64_t, 3> shape = {1, num_frames, mel_dim};
@@ -268,16 +278,19 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
 
     // Assume mel_shape = {1, T, C}
     std::vector<int64_t> mel_shape = mel.GetTensorTypeAndShapeInfo().GetShape();
-    int64_t T = mel_shape[1], C = mel_shape[2];
+    int64_t T = mel_shape[1];
+    int64_t C = mel_shape[2];
 
-    float *mel_data = mel.GetTensorMutableData<float>();
+    const float *mel_data = mel.GetTensorData<float>();
     std::vector<float> mel_permuted(C * T);
 
+    float inv_feat_scale = 1 / feat_scale;
+
     for (int64_t c = 0; c < C; ++c) {
       for (int64_t t = 0; t < T; ++t) {
         int64_t src_idx = t * C + c;  // src: [T, C] (row major)
         int64_t dst_idx = c * T + t;  // dst: [C, T] (row major)
-        mel_permuted[dst_idx] = mel_data[src_idx] / feat_scale;
+        mel_permuted[dst_idx] = mel_data[src_idx] * inv_feat_scale;
       }
     }
 
@@ -304,6 +317,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
   std::unique_ptr<OfflineTtsZipvoiceModel> model_;
   std::unique_ptr<Vocoder> vocoder_;
   std::unique_ptr<OfflineTtsFrontend> frontend_;
+
+  std::unique_ptr<knf::MelBanks> mel_banks_;
 };
 
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
index dd512caa..56131c3b 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
@@ -22,7 +22,6 @@ struct OfflineTtsZipvoiceModelMetaData {
   int32_t window_length = 1024;
   int32_t num_mels = 100;
   int32_t use_espeak = 1;
-  int32_t use_pinyin = 1;
 };
 
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
index cdd8bc86..2890d982 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
@@ -24,6 +24,7 @@
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/normal-data-generator.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/session.h"
 #include "sherpa-onnx/csrc/text-utils.h"
@@ -37,9 +38,11 @@ class OfflineTtsZipvoiceModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    auto text_buf = ReadFile(config.zipvoice.encoder);
-    auto fm_buf = ReadFile(config.zipvoice.decoder);
-    Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
+    auto buf = ReadFile(config.zipvoice.encoder);
+    InitEncoder(buf.data(), buf.size());
+
+    buf = ReadFile(config.zipvoice.decoder);
+    InitDecoder(buf.data(), buf.size());
   }
 
   template <typename Manager>
@@ -48,9 +51,11 @@ class OfflineTtsZipvoiceModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    auto text_buf = ReadFile(mgr, config.zipvoice.encoder);
-    auto fm_buf = ReadFile(mgr, config.zipvoice.decoder);
-    Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
+    auto buf = ReadFile(mgr, config.zipvoice.encoder);
+    InitEncoder(buf.data(), buf.size());
+
+    buf = ReadFile(mgr, config.zipvoice.decoder);
+    InitDecoder(buf.data(), buf.size());
   }
 
   const OfflineTtsZipvoiceModelMetaData &GetMetaData() const {
@@ -59,44 +64,19 @@ class OfflineTtsZipvoiceModel::Impl {
 
   Ort::Value Run(Ort::Value tokens, Ort::Value prompt_tokens,
                  Ort::Value prompt_features, float speed, int32_t num_steps) {
-    auto memory_info =
-        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
-
     std::vector<int64_t> tokens_shape =
         tokens.GetTensorTypeAndShapeInfo().GetShape();
+
     int64_t batch_size = tokens_shape[0];
-    if (batch_size != 1) {
-      SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
-                       static_cast<int32_t>(batch_size));
-      SHERPA_ONNX_EXIT(-1);
-    }
 
     std::vector<int64_t> prompt_feat_shape =
         prompt_features.GetTensorTypeAndShapeInfo().GetShape();
 
     int64_t prompt_feat_len = prompt_feat_shape[1];
-    int64_t prompt_feat_len_shape = 1;
-    Ort::Value prompt_feat_len_tensor = Ort::Value::CreateTensor<int64_t>(
-        memory_info, &prompt_feat_len, 1, &prompt_feat_len_shape, 1);
-
-    int64_t speed_shape = 1;
-    Ort::Value speed_tensor = Ort::Value::CreateTensor<float>(
-        memory_info, &speed, 1, &speed_shape, 1);
-
-    std::vector<Ort::Value> text_inputs;
-    text_inputs.reserve(4);
-    text_inputs.push_back(std::move(tokens));
-    text_inputs.push_back(std::move(prompt_tokens));
-    text_inputs.push_back(std::move(prompt_feat_len_tensor));
-    text_inputs.push_back(std::move(speed_tensor));
 
-    // forward encoder
-    auto text_out =
-        text_sess_->Run({}, text_input_names_ptr_.data(), text_inputs.data(),
-                        text_inputs.size(), text_output_names_ptr_.data(),
-                        text_output_names_ptr_.size());
-
-    Ort::Value &text_condition = text_out[0];
+    Ort::Value text_condition =
+        RunEncoder(std::move(tokens), std::move(prompt_tokens),
+                   View(&prompt_features), speed);
 
     std::vector<int64_t> text_cond_shape =
         text_condition.GetTensorTypeAndShapeInfo().GetShape();
@@ -105,25 +85,25 @@ class OfflineTtsZipvoiceModel::Impl {
     int64_t feat_dim = meta_data_.feat_dim;
 
     std::vector<float> x_data(batch_size * num_frames * feat_dim);
-    std::random_device rd;
-    std::default_random_engine rng(rd());
-    std::normal_distribution<float> norm(0, 1);
-    for (auto &v : x_data) {
-      v = norm(rng);
-    }
+
+    normal_gen_.Fill(x_data.data(), x_data.size());
+
+    auto memory_info =
+        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
 
     std::vector<int64_t> x_shape = {batch_size, num_frames, feat_dim};
     Ort::Value x = Ort::Value::CreateTensor<float>(
         memory_info, x_data.data(), x_data.size(), x_shape.data(),
         x_shape.size());
 
-    std::vector<float> speech_cond_data(batch_size * num_frames * feat_dim,
-                                        0.0f);
+    std::vector<float> speech_cond_data(batch_size * num_frames * feat_dim);
     const float *src = prompt_features.GetTensorData<float>();
     float *dst = speech_cond_data.data();
-    std::memcpy(dst, src,
-                batch_size * prompt_feat_len * feat_dim * sizeof(float));
+    std::copy(src, src + batch_size * prompt_feat_len * feat_dim, dst);
+    prompt_features = Ort::Value{nullptr};
+
     std::vector<int64_t> speech_cond_shape = {batch_size, num_frames, feat_dim};
+
     Ort::Value speech_condition = Ort::Value::CreateTensor<float>(
         memory_info, speech_cond_data.data(), speech_cond_data.size(),
         speech_cond_shape.data(), speech_cond_shape.size());
@@ -141,125 +121,125 @@ class OfflineTtsZipvoiceModel::Impl {
     Ort::Value guidance_scale_tensor = Ort::Value::CreateTensor<float>(
         memory_info, &guidance_scale, 1, &guidance_scale_shape, 1);
 
-    std::vector<Ort::Value> fm_inputs;
-    fm_inputs.reserve(5);
-    // fm_inputs[0] is t tensor, will set in for loop
-    fm_inputs.emplace_back(nullptr);
-    fm_inputs.push_back(std::move(x));
-    fm_inputs.push_back(std::move(text_condition));
-    fm_inputs.push_back(std::move(speech_condition));
-    fm_inputs.push_back(std::move(guidance_scale_tensor));
+    float *x_ptr = x.GetTensorMutableData<float>();
+
+    int64_t N = batch_size * num_frames * feat_dim;
 
     for (int32_t step = 0; step < num_steps; ++step) {
-      float t_val = timesteps[step];
-      int64_t t_shape = 1;
-      Ort::Value t_tensor =
-          Ort::Value::CreateTensor<float>(memory_info, &t_val, 1, &t_shape, 1);
-      fm_inputs[0] = std::move(t_tensor);
-      auto fm_out = fm_sess_->Run(
-          {}, fm_input_names_ptr_.data(), fm_inputs.data(), fm_inputs.size(),
-          fm_output_names_ptr_.data(), fm_output_names_ptr_.size());
-      Ort::Value &v = fm_out[0];
+      float t = timesteps[step];
+
+      Ort::Value v =
+          RunDecoder(t, View(&x), View(&text_condition),
+                     View(&speech_condition), View(&guidance_scale_tensor));
 
       float delta_t = timesteps[step + 1] - timesteps[step];
-      float *x_ptr = fm_inputs[1].GetTensorMutableData<float>();
+
       const float *v_ptr = v.GetTensorData<float>();
-      int64_t N = batch_size * num_frames * feat_dim;
       for (int64_t i = 0; i < N; ++i) {
         x_ptr[i] += v_ptr[i] * delta_t;
       }
     }
 
-    int64_t keep_frames = num_frames - prompt_feat_len;
-    std::vector<float> out_data(batch_size * keep_frames * feat_dim);
-    x = std::move(fm_inputs[1]);
-    const float *x_ptr = x.GetTensorData<float>();
-    for (int64_t b = 0; b < batch_size; ++b) {
-      std::memcpy(out_data.data() + b * keep_frames * feat_dim,
-                  x_ptr + (b * num_frames + prompt_feat_len) * feat_dim,
-                  keep_frames * feat_dim * sizeof(float));
-    }
-    std::vector<int64_t> out_shape = {batch_size, keep_frames, feat_dim};
+    int64_t kept_frames = num_frames - prompt_feat_len;
+
+    std::vector<int64_t> out_shape = {batch_size, kept_frames, feat_dim};
 
     Ort::Value ans = Ort::Value::CreateTensor<float>(
         allocator_, out_shape.data(), out_shape.size());
 
-    std::copy(out_data.begin(), out_data.end(),
-              ans.GetTensorMutableData<float>());
+    float *p_out = ans.GetTensorMutableData<float>();
+
+    for (int64_t b = 0; b < batch_size; ++b) {
+      auto begin = x_ptr + (b * num_frames + prompt_feat_len) * feat_dim;
+      auto end = begin + kept_frames * feat_dim;
+      std::copy(begin, end, p_out);
+      p_out += kept_frames * feat_dim;
+    }
 
     return ans;
   }
 
  private:
-  void Init(void *encoder_data, size_t encoder_data_length, void *fm_model_data,
-            size_t fm_model_data_length) {
-    // Init encoder model
-    text_sess_ = std::make_unique<Ort::Session>(
+  void InitEncoder(void *encoder_data, size_t encoder_data_length) {
+    encoder_sess_ = std::make_unique<Ort::Session>(
         env_, encoder_data, encoder_data_length, sess_opts_);
-    GetInputNames(text_sess_.get(), &text_input_names_, &text_input_names_ptr_);
-    GetOutputNames(text_sess_.get(), &text_output_names_,
-                   &text_output_names_ptr_);
-
-    // Init flow-matching model
-    fm_sess_ = std::make_unique<Ort::Session>(env_, fm_model_data,
-                                              fm_model_data_length, sess_opts_);
-    GetInputNames(fm_sess_.get(), &fm_input_names_, &fm_input_names_ptr_);
-    GetOutputNames(fm_sess_.get(), &fm_output_names_, &fm_output_names_ptr_);
+    GetInputNames(encoder_sess_.get(), &encoder_input_names_,
+                  &encoder_names_ptr_);
+    GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
+                   &encoder_output_names_ptr_);
 
     Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
-
-    Ort::ModelMetadata meta_data = text_sess_->GetModelMetadata();
+    Ort::ModelMetadata meta_data = encoder_sess_->GetModelMetadata();
     SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.use_espeak, "use_espeak",
                                             1);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.use_pinyin, "use_pinyin",
-                                            1);
-
-    meta_data = fm_sess_->GetModelMetadata();
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.version, "version", 1);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.feat_dim, "feat_dim",
-                                            100);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.sample_rate,
-                                            "sample_rate", 24000);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.n_fft, "n_fft", 1024);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.hop_length, "hop_length",
-                                            256);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.window_length,
-                                            "window_length", 1024);
-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.num_mels, "num_mels",
-                                            100);
 
     if (config_.debug) {
       std::ostringstream os;
 
       os << "---encoder---\n";
-      Ort::ModelMetadata text_meta_data = text_sess_->GetModelMetadata();
+      Ort::ModelMetadata text_meta_data = encoder_sess_->GetModelMetadata();
       PrintModelMetadata(os, text_meta_data);
 
       os << "----------input names----------\n";
       int32_t i = 0;
-      for (const auto &s : text_input_names_) {
+      for (const auto &s : encoder_input_names_) {
         os << i << " " << s << "\n";
         ++i;
       }
       os << "----------output names----------\n";
       i = 0;
-      for (const auto &s : text_output_names_) {
+      for (const auto &s : encoder_output_names_) {
         os << i << " " << s << "\n";
         ++i;
       }
 
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+  }
+
+  void InitDecoder(void *decoder_data, size_t decoder_data_length) {
+    decoder_sess_ = std::make_unique<Ort::Session>(
+        env_, decoder_data, decoder_data_length, sess_opts_);
+    GetInputNames(decoder_sess_.get(), &decoder_input_names_,
+                  &decoder_input_names_ptr_);
+    GetOutputNames(decoder_sess_.get(), &decoder_output_names_,
+                   &decoder_output_names_ptr_);
+
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    auto meta_data = decoder_sess_->GetModelMetadata();
+
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.version, "version", 1);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.feat_dim, "feat_dim",
+                                            100);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.sample_rate,
+                                            "sample_rate", 24000);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.n_fft, "n_fft", 1024);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.hop_length, "hop_length",
+                                            256);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.window_length,
+                                            "window_length", 1024);
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.num_mels, "num_mels",
+                                            100);
+
+    if (config_.debug) {
+      std::ostringstream os;
+
       os << "---decoder---\n";
       PrintModelMetadata(os, meta_data);
 
       os << "----------input names----------\n";
-      i = 0;
-      for (const auto &s : fm_input_names_) {
+      int32_t i = 0;
+      for (const auto &s : decoder_input_names_) {
         os << i << " " << s << "\n";
         ++i;
       }
       os << "----------output names----------\n";
       i = 0;
-      for (const auto &s : fm_output_names_) {
+      for (const auto &s : decoder_output_names_) {
         os << i << " " << s << "\n";
         ++i;
       }
@@ -272,28 +252,97 @@ class OfflineTtsZipvoiceModel::Impl {
     }
   }
 
+  Ort::Value RunEncoder(Ort::Value tokens, Ort::Value prompt_tokens,
+                        Ort::Value prompt_features, float speed) {
+    auto memory_info =
+        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+
+    std::vector<int64_t> tokens_shape =
+        tokens.GetTensorTypeAndShapeInfo().GetShape();
+
+    int64_t batch_size = tokens_shape[0];
+    if (batch_size != 1) {
+      SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
+                       static_cast<int32_t>(batch_size));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int64_t> prompt_feat_shape =
+        prompt_features.GetTensorTypeAndShapeInfo().GetShape();
+
+    int64_t prompt_feat_len = prompt_feat_shape[1];
+    int64_t prompt_feat_len_shape = 1;
+    Ort::Value prompt_feat_len_tensor = Ort::Value::CreateTensor<int64_t>(
+        memory_info, &prompt_feat_len, 1, &prompt_feat_len_shape, 1);
+
+    int64_t speed_shape = 1;
+    Ort::Value speed_tensor = Ort::Value::CreateTensor<float>(
+        memory_info, &speed, 1, &speed_shape, 1);
+
+    std::vector<Ort::Value> encoder_inputs;
+    encoder_inputs.reserve(4);
+    encoder_inputs.push_back(std::move(tokens));
+    encoder_inputs.push_back(std::move(prompt_tokens));
+    encoder_inputs.push_back(std::move(prompt_feat_len_tensor));
+    encoder_inputs.push_back(std::move(speed_tensor));
+
+    auto encoder_out = encoder_sess_->Run(
+        {}, encoder_names_ptr_.data(), encoder_inputs.data(),
+        encoder_inputs.size(), encoder_output_names_ptr_.data(),
+        encoder_output_names_ptr_.size());
+
+    return std::move(encoder_out[0]);
+  }
+
+  Ort::Value RunDecoder(float t, Ort::Value x, Ort::Value text_condition,
+                        Ort::Value speech_condition,
+                        Ort::Value guidance_scale_tensor) {
+    auto memory_info =
+        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+
+    int64_t t_shape = 1;
+    Ort::Value t_tensor =
+        Ort::Value::CreateTensor<float>(memory_info, &t, 1, &t_shape, 1);
+
+    std::vector<Ort::Value> decoder_inputs;
+    decoder_inputs.reserve(5);
+    decoder_inputs.emplace_back(std::move(t_tensor));
+    decoder_inputs.push_back(std::move(x));
+    decoder_inputs.push_back(std::move(text_condition));
+    decoder_inputs.push_back(std::move(speech_condition));
+    decoder_inputs.push_back(std::move(guidance_scale_tensor));
+
+    auto decoder_out = decoder_sess_->Run(
+        {}, decoder_input_names_ptr_.data(), decoder_inputs.data(),
+        decoder_inputs.size(), decoder_output_names_ptr_.data(),
+        decoder_output_names_ptr_.size());
+
+    return std::move(decoder_out[0]);
+  }
+
  private:
   OfflineTtsModelConfig config_;
   Ort::Env env_;
   Ort::SessionOptions sess_opts_;
   Ort::AllocatorWithDefaultOptions allocator_;
 
-  std::unique_ptr<Ort::Session> text_sess_;
-  std::unique_ptr<Ort::Session> fm_sess_;
+  std::unique_ptr<Ort::Session> encoder_sess_;
+  std::unique_ptr<Ort::Session> decoder_sess_;
 
-  std::vector<std::string> text_input_names_;
-  std::vector<const char *> text_input_names_ptr_;
+  std::vector<std::string> encoder_input_names_;
+  std::vector<const char *> encoder_names_ptr_;
 
-  std::vector<std::string> text_output_names_;
-  std::vector<const char *> text_output_names_ptr_;
+  std::vector<std::string> encoder_output_names_;
+  std::vector<const char *> encoder_output_names_ptr_;
 
-  std::vector<std::string> fm_input_names_;
-  std::vector<const char *> fm_input_names_ptr_;
+  std::vector<std::string> decoder_input_names_;
+  std::vector<const char *> decoder_input_names_ptr_;
 
-  std::vector<std::string> fm_output_names_;
-  std::vector<const char *> fm_output_names_ptr_;
+  std::vector<std::string> decoder_output_names_;
+  std::vector<const char *> decoder_output_names_ptr_;
 
   OfflineTtsZipvoiceModelMetaData meta_data_;
+  NormalDataGenerator normal_gen_;
 };
 
 OfflineTtsZipvoiceModel::OfflineTtsZipvoiceModel(

commit 241cb9b3b1674d82d38f9395837dc8468a115cc4
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 18 12:46:52 2025 +0800

    Fix publishing NPM packages (#2909)

diff --git a/.github/workflows/build-wheels-macos-universal2.yaml b/.github/workflows/build-wheels-macos-universal2.yaml
index 4980189b..070e5862 100644
--- a/.github/workflows/build-wheels-macos-universal2.yaml
+++ b/.github/workflows/build-wheels-macos-universal2.yaml
@@ -149,7 +149,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [macos-latest, macos-13]
+        os: [macos-latest, macos-15-intel]
 
     steps:
       - uses: actions/checkout@v4
diff --git a/.github/workflows/build-wheels-macos-x64.yaml b/.github/workflows/build-wheels-macos-x64.yaml
index 71600412..7b0e84ef 100644
--- a/.github/workflows/build-wheels-macos-x64.yaml
+++ b/.github/workflows/build-wheels-macos-x64.yaml
@@ -150,7 +150,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [macos-13]
+        os: [macos-15-intel]
 
     steps:
       - uses: actions/checkout@v4
diff --git a/.github/workflows/jar.yaml b/.github/workflows/jar.yaml
index 0039da08..23725abc 100644
--- a/.github/workflows/jar.yaml
+++ b/.github/workflows/jar.yaml
@@ -32,7 +32,7 @@ jobs:
           - os: macos-latest
             arch: "arm64"
 
-          - os: macos-13
+          - os: macos-15-intel
             arch: "x64"
 
           - os: windows-2022
@@ -108,7 +108,7 @@ jobs:
           rm -rf $src*
 
       - name: Download libs ${{ matrix.os }} ${{ matrix.arch }}
-        if: ${{ matrix.os == 'macos-13' && matrix.arch == 'x64' }}
+        if: ${{ matrix.os == 'macos-15-intel' && matrix.arch == 'x64' }}
         shell: bash
         run: |
           SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
@@ -179,7 +179,7 @@ jobs:
             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-linux-x64-$SHERPA_ONNX_VERSION.jar
           elif [[ $os == "macos-latest" && $arch == "arm64" ]]; then
             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-osx-aarch64-$SHERPA_ONNX_VERSION.jar
-          elif [[ $os == "macos-13" && $arch == "x64" ]]; then
+          elif [[ $os == "macos-15-intel" && $arch == "x64" ]]; then
             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-osx-x64-$SHERPA_ONNX_VERSION.jar
           elif [[ $os == "windows-2022" && $arch == "x64" ]]; then
             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-win-x64-$SHERPA_ONNX_VERSION.jar
@@ -234,7 +234,7 @@ jobs:
             native_jar=sherpa-onnx-native-lib-linux-x64-$SHERPA_ONNX_VERSION.jar
           elif [[ $os == "macos-latest" && $arch == "arm64" ]]; then
             native_jar=sherpa-onnx-native-lib-osx-aarch64-$SHERPA_ONNX_VERSION.jar
-          elif [[ $os == "macos-13" && $arch == "x64" ]]; then
+          elif [[ $os == "macos-15-intel" && $arch == "x64" ]]; then
             native_jar=sherpa-onnx-native-lib-osx-x64-$SHERPA_ONNX_VERSION.jar
           elif [[ $os == "windows-2022" && $arch == "x64" ]]; then
             native_jar=sherpa-onnx-native-lib-win-x64-$SHERPA_ONNX_VERSION.jar
diff --git a/.github/workflows/jni.yaml b/.github/workflows/jni.yaml
index 8369006d..d61ea118 100644
--- a/.github/workflows/jni.yaml
+++ b/.github/workflows/jni.yaml
@@ -26,7 +26,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [ubuntu-latest, macos-latest, macos-13]
+        os: [ubuntu-latest, macos-latest, macos-15-intel]
 
     steps:
       - uses: actions/checkout@v4
diff --git a/.github/workflows/lazarus.yaml b/.github/workflows/lazarus.yaml
index 693b54cd..384080b2 100644
--- a/.github/workflows/lazarus.yaml
+++ b/.github/workflows/lazarus.yaml
@@ -30,7 +30,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [ubuntu-22.04, macos-latest, macos-13, windows-2022]
+        os: [ubuntu-22.04, macos-latest, macos-15-intel, windows-2022]
 
     steps:
       - uses: actions/checkout@v4
@@ -128,7 +128,7 @@ jobs:
         run: |
           cd lazarus-examples/generate_subtitles
           os=${{ matrix.os }}
-          if [[ $os == macos-13 ]]; then
+          if [[ $os == macos-15-intel ]]; then
             lazbuild --verbose --build-mode=Release --widgetset=cocoa ./generate_subtitles.lpi
           elif [[ $os == macos-latest ]]; then
             lazbuild --verbose --build-mode=Release --widgetset=cocoa --cpu=aarch64 ./generate_subtitles.lpi
@@ -191,7 +191,7 @@ jobs:
           ls -lh windows-x64
 
       - name: Collect generating subtitles (macos)
-        if: matrix.os == 'macos-13' || matrix.os == 'macos-latest'
+        if: matrix.os == 'macos-15-intel' || matrix.os == 'macos-latest'
         shell: bash
         run: |
           SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
@@ -228,7 +228,7 @@ jobs:
           path: /tmp/macos-arm64
 
       - uses: actions/upload-artifact@v4
-        if: matrix.os == 'macos-13'
+        if: matrix.os == 'macos-15-intel'
         with:
           name: macos-x64
           path: /tmp/macos-x64
diff --git a/.github/workflows/npm-addon-linux-aarch64.yaml b/.github/workflows/npm-addon-linux-aarch64.yaml
index 2be4902c..840dab0f 100644
--- a/.github/workflows/npm-addon-linux-aarch64.yaml
+++ b/.github/workflows/npm-addon-linux-aarch64.yaml
@@ -54,6 +54,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Show .npmrc
@@ -132,12 +133,8 @@ jobs:
 
               ls -lh ./sherpa-onnx-node
 
-              export NODE_AUTH_TOKEN=${{ secrets.NPM_TOKEN }}
-
               cd ./sherpa-onnx-node
               cp -v /shared/.npmrc ./
-              npm install
-              npm ci
+              # https://docs.npmjs.com/trusted-publishers
               ls -lh
-              # see https://docs.npmjs.com/generating-provenance-statements
-              npm publish --provenance --access public
+              npm publish --access public
diff --git a/.github/workflows/npm-addon-linux-x64.yaml b/.github/workflows/npm-addon-linux-x64.yaml
index 9a63108e..ed30b5d1 100644
--- a/.github/workflows/npm-addon-linux-x64.yaml
+++ b/.github/workflows/npm-addon-linux-x64.yaml
@@ -41,6 +41,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Display node version
@@ -123,11 +124,7 @@ jobs:
 
       - name: Publish
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
         run: |
           cd ./sherpa-onnx-node
-          npm install
-          npm ci
-          # see https://docs.npmjs.com/generating-provenance-statements
-          npm publish --provenance --access public
+          # https://docs.npmjs.com/trusted-publishers
+          npm publish --access public
diff --git a/.github/workflows/npm-addon-macos.yaml b/.github/workflows/npm-addon-macos.yaml
index 4623be69..30d852c3 100644
--- a/.github/workflows/npm-addon-macos.yaml
+++ b/.github/workflows/npm-addon-macos.yaml
@@ -20,7 +20,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [macos-13, macos-14]
+        os: [macos-15-intel, macos-14]
         python-version: ["3.8"]
 
     steps:
@@ -46,6 +46,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Display node version
@@ -117,11 +118,7 @@ jobs:
 
       - name: Publish
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
         run: |
           cd ./sherpa-onnx-node
-          npm install
-          npm ci
-          # see https://docs.npmjs.com/generating-provenance-statements
-          npm publish --provenance --access public
+          # https://docs.npmjs.com/trusted-publishers
+          npm publish --access public
diff --git a/.github/workflows/npm-addon-win-x64.yaml b/.github/workflows/npm-addon-win-x64.yaml
index 8655c5af..2b2b847c 100644
--- a/.github/workflows/npm-addon-win-x64.yaml
+++ b/.github/workflows/npm-addon-win-x64.yaml
@@ -41,6 +41,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Display node version
@@ -118,11 +119,7 @@ jobs:
 
       - name: Publish
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
         run: |
           cd ./sherpa-onnx-node
-          npm install
-          npm ci
-          # see https://docs.npmjs.com/generating-provenance-statements
-          npm publish --provenance --access public
+          # https://docs.npmjs.com/trusted-publishers
+          npm publish --access public
diff --git a/.github/workflows/npm-addon-win-x86.yaml b/.github/workflows/npm-addon-win-x86.yaml
index 85dc95a8..e6426c7f 100644
--- a/.github/workflows/npm-addon-win-x86.yaml
+++ b/.github/workflows/npm-addon-win-x86.yaml
@@ -43,7 +43,7 @@ jobs:
         with:
           registry-url: 'https://registry.npmjs.org'
           architecture: 'x86'
-          node-version: 16
+          node-version: '16'
 
       - name: Display node version
         shell: bash
@@ -158,6 +158,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Display node version
@@ -179,10 +180,7 @@ jobs:
 
       - name: Publish
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN2 }}
-          NPM_TOKEN: ${{ secrets.NPM_TOKEN2 }}
-          token: ${{ secrets.NPM_TOKEN2 }}
         run: |
           cd /tmp/files/sherpa-onnx-node
+          # https://docs.npmjs.com/trusted-publishers
           npm publish --access public
diff --git a/.github/workflows/npm-addon.yaml b/.github/workflows/npm-addon.yaml
index 3a14d5e2..56513842 100644
--- a/.github/workflows/npm-addon.yaml
+++ b/.github/workflows/npm-addon.yaml
@@ -42,6 +42,7 @@ jobs:
       - uses: actions/setup-node@v4
         with:
           registry-url: 'https://registry.npmjs.org'
+          node-version: '24'
 
       - name: Display node version
         shell: bash
@@ -86,11 +87,7 @@ jobs:
 
       - name: Publish
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
         run: |
           cd ./sherpa-onnx-node
-          npm install
-          npm ci
-          # see https://docs.npmjs.com/generating-provenance-statements
-          npm publish --provenance --access public
+          # https://docs.npmjs.com/trusted-publishers
+          npm publish --access public
diff --git a/.github/workflows/npm.yaml b/.github/workflows/npm.yaml
index b72c894f..490b281b 100644
--- a/.github/workflows/npm.yaml
+++ b/.github/workflows/npm.yaml
@@ -54,6 +54,7 @@ jobs:
 
       - uses: actions/setup-node@v4
         with:
+          node-version: '24'
           registry-url: 'https://registry.npmjs.org'
 
       - name: Display node version
@@ -99,14 +100,10 @@ jobs:
 
       - name: Build nodejs package
         shell: bash
-        env:
-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
         run: |
           cd scripts/nodejs
 
           git diff
 
-          npm install
-          npm ci
-          # see https://docs.npmjs.com/generating-provenance-statements
+          # https://docs.npmjs.com/trusted-publishers
           npm publish --provenance --access public
diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
index 6f4c55e4..905af440 100644
--- a/.github/workflows/pascal.yaml
+++ b/.github/workflows/pascal.yaml
@@ -27,7 +27,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [ubuntu-latest, macos-latest, macos-13, windows-2022, ubuntu-22.04-arm]
+        os: [ubuntu-latest, macos-latest, macos-15-intel, windows-2022, ubuntu-22.04-arm]
 
     steps:
       - uses: actions/checkout@v4
@@ -53,7 +53,7 @@ jobs:
           sudo apt-get install -q -y fpc
 
       - name: Install Free pascal compiler (macos)
-        if: matrix.os == 'macos-latest' || matrix.os == 'macos-13'
+        if: matrix.os == 'macos-latest' || matrix.os == 'macos-15-intel'
         shell: bash
         run: |
           brew install fpc
diff --git a/.github/workflows/run-python-test-macos.yaml b/.github/workflows/run-python-test-macos.yaml
index 2de0c6db..749f1d7e 100644
--- a/.github/workflows/run-python-test-macos.yaml
+++ b/.github/workflows/run-python-test-macos.yaml
@@ -31,10 +31,10 @@ jobs:
         # macos-14 is for arm64
         # macos-14-large is for x64
         include:
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.8"
 
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.9"
           - os: macos-14
             python-version: "3.10"
diff --git a/.github/workflows/swift.yaml b/.github/workflows/swift.yaml
index 55d13bd8..73c81103 100644
--- a/.github/workflows/swift.yaml
+++ b/.github/workflows/swift.yaml
@@ -39,7 +39,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [macos-latest, macos-13]
+        os: [macos-latest, macos-15-intel]
 
     steps:
       - uses: actions/checkout@v4
@@ -72,7 +72,7 @@ jobs:
           ./build-swift-macos.sh
 
       - name: Copy files
-        if: matrix.os == 'macos-13' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        if: matrix.os == 'macos-15-intel' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
         shell: bash
         run: |
           SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
@@ -88,7 +88,7 @@ jobs:
           tar cjvf ${dst}.tar.bz2 $dst
 
       - name: Release pre-compiled binaries and libs for macOS
-        if: matrix.os == 'macos-13' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        if: matrix.os == 'macos-15-intel' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
diff --git a/.github/workflows/test-build-wheel.yaml b/.github/workflows/test-build-wheel.yaml
index 35d2f907..d164148e 100644
--- a/.github/workflows/test-build-wheel.yaml
+++ b/.github/workflows/test-build-wheel.yaml
@@ -52,14 +52,14 @@ jobs:
           - os: ubuntu-24.04-arm
             python-version: "3.13"
 
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.8"
 
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.9"
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.10"
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.11"
 
           - os: macos-latest
diff --git a/.github/workflows/test-go-package.yaml b/.github/workflows/test-go-package.yaml
index b6657569..05c11845 100644
--- a/.github/workflows/test-go-package.yaml
+++ b/.github/workflows/test-go-package.yaml
@@ -28,7 +28,7 @@ jobs:
             arch: amd64
           - os: ubuntu-22.04-arm
             arch: arm64
-          - os: macos-13
+          - os: macos-15-intel
             arch: amd64
           - os: macos-14
             arch: arm64
diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
index e362418e..53faad2d 100644
--- a/.github/workflows/test-go.yaml
+++ b/.github/workflows/test-go.yaml
@@ -24,7 +24,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [macos-latest, macos-13, ubuntu-latest, windows-2022, ubuntu-22.04-arm]
+        os: [macos-latest, macos-15-intel, ubuntu-latest, windows-2022, ubuntu-22.04-arm]
 
     steps:
       - uses: actions/checkout@v4
diff --git a/.github/workflows/test-pip-install.yaml b/.github/workflows/test-pip-install.yaml
index a8c71ed3..fc904b1a 100644
--- a/.github/workflows/test-pip-install.yaml
+++ b/.github/workflows/test-pip-install.yaml
@@ -56,14 +56,14 @@ jobs:
           - os: ubuntu-24.04-arm
             python-version: "3.13"
 
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.8"
 
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.9"
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.10"
-          - os: macos-13
+          - os: macos-15-intel
             python-version: "3.11"
 
           - os: macos-14

commit 5ce3d6d93a5f4fe11657bf11a6bf3a022eeef22f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 17 20:36:53 2025 +0800

    Release v1.12.20 (#2907)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 439b0db9..21d55bf8 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,25 @@
+## 1.12.20
+
+* Refactor axcl examples. (#2867)
+* Update README to include Axera NPU (#2870)
+* Add CI for Axera NPU (#2872)
+* Refactor sense voice impl (#2873)
+* Refactor Paraformer Impl (#2874)
+* Remove unused lock file (#2875)
+* Load QNN context binary for faster startup (#2877)
+* Export models to Ascend 910B4 (#2878)
+* Optimize streaming output results when VAD does not detect human voice for a long time (#2876)
+* Build APKs for MatchaTTS Chinese+English (#2882)
+* Publish WASM spaces for MatchaTTS Chinese+English model (#2885)
+* Add script for testing zipvoice onnx models (#2887)
+* upload zipvoice onnx models (#2890)
+* Remove cppinyin from zipvoice (#2892)
+* Fix building errors (#2893)
+* Use a shorter name for Zipvoice models. (#2894)
+* Export GigaAM v3 to sherpa-onnx (#2901)
+* Fix typos in URL (#2905)
+* Support Fun-ASR-Nano-2512 (#2906)
+
 ## 1.12.19
 
 * Fix building without TTS for C API (#2838)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 57cb8e7f..1c34ad66 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.19")
+set(SHERPA_ONNX_VERSION "1.12.20")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index 2ac752f2..df6b6c48 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index 2ac752f2..df6b6c48 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index d7942a39..e3d6bc56 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-v1.12.19-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.19-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-v1.12.20-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.20-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.19.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.20.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index dafdd504..1af7f56d 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index 29f894c1..e0de0cd4 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index c0b95fdf..da437a02 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.19'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.20'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index 2ac752f2..df6b6c48 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index 571a805f..bef7ef4a 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index ac309729..e9d0f91a 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.19")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.20")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index fe528faa..a627d6e7 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index 12e210ee..5e84ec0e 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index fc0c03e1..0aad0cf2 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 0b6ba6c0..221b89ea 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 6a8978b9..9a5e8f26 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251205
-        versionName = "1.12.19"
+        versionCode = 20251217
+        versionName = "1.12.20"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index 66bf1f41..7d3afbcb 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index 66bf1f41..7d3afbcb 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 3a29c0b1..1066d5ff 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251205
-        versionName "1.12.19"
+        versionCode 20251217
+        versionName "1.12.20"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index df5c6037..064723a8 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.19</string>
+	<string>1.12.20</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index b77879a4..a45be760 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index 396260c2..8b2f37a7 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index 04eeb417..ce9b8725 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index fcae6b01..ff195c35 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index 8a1fadb3..3090ff9e 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index ede5c6e4..95bb5c6d 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index f42abe91..f2d5ab5e 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index 0564cda2..a46585b6 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index 377dd7bb..1884ef3b 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index cb072fd3..a9c47b68 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index d581ebee..7b6e0112 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 403fd353..c73ea2ae 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index 70fbb929..ef743d3f 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.19
+version: 1.12.20
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index e3527e8a..7c3cd89b 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.19
+version: 1.12.20
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index d98a541b..2956a40c 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.19
+version: 1.12.20
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.19
+  sherpa_onnx: ^1.12.20
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index b65ccfeb..ac02b6aa 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.19
+version: 1.12.20
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.19
+  sherpa_onnx_android: ^1.12.20
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.19
+  sherpa_onnx_macos: ^1.12.20
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.19
+  sherpa_onnx_linux: ^1.12.20
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.19
+  sherpa_onnx_windows: ^1.12.20
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.19
+  sherpa_onnx_ios: ^1.12.20
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index ffd952bf..a821c033 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.19'
+  s.version          = '1.12.20'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index ec1ea5fd..ecefd905 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.19'
+  s.version          = '1.12.20'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index 2a5e5dbf..7c18da38 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.19';
+export const HAR_VERSION = '1.12.20';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index dde223ef..01087a2d 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.19",
+    "sherpa_onnx": "1.12.20",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index f3e43a76..fe2c2ae5 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.19",
+  "version": "1.12.20",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index c703cba9..02da1ac7 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.19"
+    "sherpa_onnx": "1.12.20"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 21ccbcef..82551df9 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.19",
+    "sherpa_onnx": "1.12.20",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 21ccbcef..82551df9 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.19",
+    "sherpa_onnx": "1.12.20",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 21ccbcef..82551df9 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.19",
+    "sherpa_onnx": "1.12.20",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index af0510b9..672af848 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.19.har
+Please download ./sherpa_onnx-v1.12.20.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index c24b1baf..2f78fe76 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.19",
+    "sherpa_onnx": "1.12.20",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index 5591d3ac..43a41983 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-1.12.19.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-1.12.20.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.19.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.19 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.20.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.20 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index 970c9cc3..d9480032 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x86-v1.12.19.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x86-v1.12.19.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x86-v1.12.19.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x86-v1.12.20.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x86-v1.12.20.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x86-v1.12.20.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 4d2d2280..1588a030 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20251127
-new_version_code=20251205
+old_version_code=20251205
+new_version_code=20251217
 
-old_version="1\.12\.18"
-new_version="1\.12\.19"
+old_version="1\.12\.19"
+new_version="1\.12\.20"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index d24f1105..429c86dd 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.19"
+    "sherpa-onnx-node": "^1.12.20"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index af3538c6..710a5447 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.19",
+    "sherpa-onnx": "^1.12.20",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index 0d84b015..dfbb89f2 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.19</version>
+    <version>1.12.20</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index e9a561da..29b11fb0 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.19",
+    version="1.12.20",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.19",
+        "sherpa-onnx-core==1.12.20",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index d053d293..8ac69af9 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.19",
+    version="1.12.20",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 63991827..3eeb89a7 100644
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.19"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.20"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index 315d3d2e..2d8a55a9 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Fri Dec 5 11:45:41 2025";
+  static const char *date = "Wed Dec 17 19:57:55 2025";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "e6a6599f";
+  static const char *sha1 = "3290e1ce";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.19";
+  static const char *version = "1.12.20";
   return version;
 }
 

commit 3290e1ce5660a384cc4d41353546b1343bc16467
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 17 19:57:55 2025 +0800

    Support Fun-ASR-Nano-2512 (#2906)
    
    This pull request significantly expands the capabilities of sherpa-onnx by integrating the Fun-ASR-Nano-2512 model. The changes encompass the entire pipeline, from model conversion and quantization using new Python scripts to specialized C++ inference logic that correctly interprets the Nano model's unique output structure and metadata. This ensures seamless support for this new, highly optimized ASR model, which boasts features like far-field high-noise recognition and multi-dialect/multi-language support.

diff --git a/.github/workflows/export-sense-voice-to-onnx.yaml b/.github/workflows/export-sense-voice-to-onnx.yaml
index 1c3e9172..f094f780 100644
--- a/.github/workflows/export-sense-voice-to-onnx.yaml
+++ b/.github/workflows/export-sense-voice-to-onnx.yaml
@@ -26,10 +26,31 @@ jobs:
         with:
           python-version: ${{ matrix.python-version }}
 
+      - name: Install dependencies
+        shell: bash
+        run: |
+          sudo apt-get install -y -qq sox libsox-fmt-mp3
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          pip install \
+            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+            onnx==1.17.0 \
+            onnxruntime==1.17.1 \
+            soundfile \
+            kaldi-native-fbank \
+            librosa
+
+          pip install  "numpy<2"
+
       - name: Download test_wavs
         shell: bash
         run: |
           sudo apt-get install -y -qq sox libsox-fmt-mp3
+
+          cd scripts/sense-voice
+
           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/zh.mp3
           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/en.mp3
           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/ja.mp3
@@ -44,13 +65,90 @@ jobs:
           sox ko.mp3 -r 16k ko.wav
           sox yue.mp3 -r 16k yue.wav
 
+
+      - name: Run
+        shell: bash
+        run: |
+          cd scripts/sense-voice
+          curl -SL -O https://huggingface.co/csukuangfj/funasr-nano-with-ctc/resolve/main/model.pt
+          curl -SL -O https://huggingface.co/csukuangfj/funasr-nano-with-ctc/resolve/main/tokens.txt
+          ls -lh
+          ./export_onnx_nano.py
+
+          ls -lh
+
+          d=sherpa-onnx-sense-voice-funasr-nano-2025-12-17
+          d2=sherpa-onnx-sense-voice-funasr-nano-int8-2025-12-17
+          mkdir -p $d $d2
+
+          cp README-nano.md $d/README.md
+          cp README-nano.md $d2/README.md
+
+          mv model.onnx $d/
+          mv model.int8.onnx $d2/
+
+          for m in $d $d2; do
+            mkdir -p $m/test_wavs
+            cp -v *.wav $m/test_wavs
+            cp -v tokens.txt $m/
+
+            ls -lh $m
+
+            tar cjfv $m.tar.bz2 $m
+
+            ls -lh $m.tar.bz2
+            mv $m.tar.bz2 ../../
+            mv $m ../../
+          done
+
+      - name: Publish v3 to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 5
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            names=(
+              sherpa-onnx-sense-voice-funasr-nano-2025-12-17
+              sherpa-onnx-sense-voice-funasr-nano-int8-2025-12-17
+            )
+            for d in ${names[@]}; do
+              if [ ! -d $d ]; then
+                echo "$d does not exist - skip it"
+                continue;
+              fi
+
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
+              cp -av $d/* ./huggingface
+              cd huggingface
+              git lfs track "*.onnx"
+              git lfs track "*.wav"
+              git status
+              git add .
+              git status
+              git commit -m "add models"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+              cd ..
+            done
+
       - name: Run
         shell: bash
+        if: false
         run: |
           cd scripts/sense-voice
           ./run.sh
 
       - name: Publish to huggingface
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
diff --git a/scripts/sense-voice/README-nano.md b/scripts/sense-voice/README-nano.md
new file mode 100644
index 00000000..15aba991
--- /dev/null
+++ b/scripts/sense-voice/README-nano.md
@@ -0,0 +1,33 @@
+# Introduction
+
+This directory contains models converted from
+https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512
+
+## Core Features
+
+> From  https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512
+
+    - Far-field High-noise Recognition: Deeply optimized for far-distance sound pickup and high-noise scenarios (such as conference rooms, in-vehicle environments, industrial sites, etc.), improving recognition accuracy to 93%.
+
+    - Chinese Dialects and Regional Accents:
+
+        - Supports 7 major dialects: Wu, Cantonese, Min, Hakka, Gan, Xiang, Jin
+        - Covers 26 regional accents: including Henan, Shaanxi, Hubei, Sichuan, Chongqing, Yunnan, Guizhou, Guangdong, Guangxi and more than 20 other regions
+
+    - Multi-language Free Speech: Supports recognition of 31 languages, with focused optimization on East and Southeast Asian languages, supporting free language switching and mixed recognition.
+    - Music Background Lyric Recognition: Enhanced speech recognition performance under music background interference, supporting accurate recognition of lyric content in songs.
+
+
+
+## 
+
+> From https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512/blob/main/README_zh.md
+
+    -   **93%**
+    - 
+
+        -  7 
+        -  26  20 
+
+    -   31 
+    -  
diff --git a/scripts/sense-voice/adaptor.py b/scripts/sense-voice/adaptor.py
new file mode 120000
index 00000000..998f6034
--- /dev/null
+++ b/scripts/sense-voice/adaptor.py
@@ -0,0 +1 @@
+rknn/adaptor.py
\ No newline at end of file
diff --git a/scripts/sense-voice/export-onnx.py b/scripts/sense-voice/export-onnx.py
index 0153cebd..831a6577 100755
--- a/scripts/sense-voice/export-onnx.py
+++ b/scripts/sense-voice/export-onnx.py
@@ -7,6 +7,8 @@ https://hf-mirror.com/yuekai/model_repo_sense_voice_small/blob/main/export_onnx.
 as a reference while writing this file.
 
 Thanks to https://github.com/yuekaizhang for making the file public.
+
+You should install FunASR before you run this file.
 """
 
 import os
@@ -120,7 +122,9 @@ def display_params(params):
 
 @torch.no_grad()
 def main():
-    model, params = SenseVoiceSmall.from_pretrained(model="iic/SenseVoiceSmall", device="cpu")
+    model, params = SenseVoiceSmall.from_pretrained(
+        model="iic/SenseVoiceSmall", device="cpu"
+    )
     model.eval()
 
     display_params(params)
diff --git a/scripts/sense-voice/export_onnx_nano.py b/scripts/sense-voice/export_onnx_nano.py
new file mode 100755
index 00000000..96350f50
--- /dev/null
+++ b/scripts/sense-voice/export_onnx_nano.py
@@ -0,0 +1,112 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import argparse
+import os
+from typing import Any, Dict
+
+import onnx
+import torch
+from onnxruntime.quantization import QuantType, quantize_dynamic
+
+from test_nano_torch import load_tokens, load_torch_model
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--opset-version",
+        type=int,
+        default=13,
+    )
+    return parser.parse_args()
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, Any]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+@torch.no_grad()
+def main():
+    args = get_args()
+    print(vars(args))
+    id2tokens = load_tokens()
+
+    vocab_size = len(id2tokens)
+    blank_id = vocab_size - 1
+
+    print("loading model")
+
+    model = load_torch_model()
+    model.eval()
+
+    x = torch.randn(1, 30, 560, dtype=torch.float32)
+
+    opset_version = args.opset_version
+    filename = "model.onnx"
+    torch.onnx.export(
+        model,
+        x,
+        filename,
+        opset_version=opset_version,
+        input_names=["x"],
+        output_names=["logits"],
+        dynamic_axes={
+            "x": {1: "T"},
+        },
+    )
+
+    model_author = "FunAudioLLM"
+    comment = os.environ.get("comment", "FunAudioLLM/Fun-ASR-Nano-2512")
+    url = "https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512"
+
+    meta_data = {
+        "lfr_window_size": 7,
+        "lfr_window_shift": 6,
+        "normalize_samples": 0,  # input should be in the range [-32768, 32767]
+        "model_type": "sense_voice_ctc",
+        "version": "1",
+        "model_author": model_author,
+        "maintainer": "k2-fsa",
+        "vocab_size": vocab_size,
+        "blank_id": blank_id,
+        "comment": comment,
+        "url": url,
+    }
+    add_meta_data(filename=filename, meta_data=meta_data)
+
+    filename_int8 = "model.int8.onnx"
+    quantize_dynamic(
+        model_input=filename,
+        model_output=filename_int8,
+        op_types_to_quantize=["MatMul"],
+        # Note that we have to use QUInt8 here.
+        #
+        # When QInt8 is used, C++ onnxruntime produces incorrect results
+        weight_type=QuantType.QUInt8,
+    )
+
+
+if __name__ == "__main__":
+    torch.manual_seed(20251217)
+    main()
diff --git a/scripts/sense-voice/nano.py b/scripts/sense-voice/nano.py
new file mode 120000
index 00000000..be26565e
--- /dev/null
+++ b/scripts/sense-voice/nano.py
@@ -0,0 +1 @@
+rknn/nano.py
\ No newline at end of file
diff --git a/scripts/sense-voice/rknn/adaptor.py b/scripts/sense-voice/rknn/adaptor.py
new file mode 100644
index 00000000..5b84004d
--- /dev/null
+++ b/scripts/sense-voice/rknn/adaptor.py
@@ -0,0 +1,248 @@
+import torch
+from torch import nn
+
+import torch_model
+
+
+class MultiHeadedAttention(nn.Module):
+    """
+    This class is copied and modified from
+    https://github.com/modelscope/FunASR/blob/main/funasr/models/transformer/attention.py
+    """
+
+    def __init__(self, n_head, n_feat, dropout_rate):
+        super().__init__()
+        assert n_feat % n_head == 0
+
+        # We assume d_v always equals d_k
+        self.d_k = n_feat // n_head
+        self.h = n_head
+        self.linear_q = nn.Linear(n_feat, n_feat)
+        self.linear_k = nn.Linear(n_feat, n_feat)
+        self.linear_v = nn.Linear(n_feat, n_feat)
+        self.linear_out = nn.Linear(n_feat, n_feat)
+        self.attn = None
+        self.dropout = nn.Dropout(p=dropout_rate)
+
+    def forward_qkv(self, query, key, value):
+        """Transform query, key and value.
+
+        Args:
+            query (torch.Tensor): Query tensor (#batch, time1, size).
+            key (torch.Tensor): Key tensor (#batch, time2, size).
+            value (torch.Tensor): Value tensor (#batch, time2, size).
+
+        Returns:
+            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).
+            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).
+            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).
+
+        """
+        n_batch = query.size(0)
+        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
+        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
+        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
+        q = q.transpose(1, 2)  # (batch, head, time1, d_k)
+        k = k.transpose(1, 2)  # (batch, head, time2, d_k)
+        v = v.transpose(1, 2)  # (batch, head, time2, d_k)
+
+        return q, k, v
+
+    def forward_attention(self, value, scores, mask):
+        """Compute attention context vector.
+
+        Args:
+            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).
+            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).
+            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).
+
+        Returns:
+            torch.Tensor: Transformed value (#batch, time1, d_model)
+                weighted by the attention score (#batch, time1, time2).
+
+        """
+        n_batch = value.size(0)
+        if mask is not None:
+            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)
+
+            min_value = -float(
+                "inf"
+            )  # min_value = float(np.finfo(torch.tensor(0, dtype=qk.dtype).numpy().dtype).min)
+            scores = scores.masked_fill(mask, min_value)
+            attn = torch.softmax(scores, dim=-1).masked_fill(
+                mask, 0.0
+            )  # (batch, head, time1, time2)
+        else:
+            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)
+
+        p_attn = self.dropout(attn)
+        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)
+        x = (
+            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
+        )  # (batch, time1, d_model)
+
+        return self.linear_out(x)  # (batch, time1, d_model)
+
+    def forward(self, query, key, value, mask):
+        """Compute scaled dot product attention.
+
+        Args:
+            query (torch.Tensor): Query tensor (#batch, time1, size).
+            key (torch.Tensor): Key tensor (#batch, time2, size).
+            value (torch.Tensor): Value tensor (#batch, time2, size).
+            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
+                (#batch, time1, time2).
+
+        Returns:
+            torch.Tensor: Output tensor (#batch, time1, d_model).
+
+        """
+        q, k, v = self.forward_qkv(query, key, value)
+        #  scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
+        scores = torch.matmul(q, k.transpose(-2, -1)) * self.d_k ** (-0.5)
+
+        return self.forward_attention(v, scores, mask)
+
+
+class EncoderLayer(nn.Module):
+    """
+    This class is copied and modified from
+    https://github.com/modelscope/FunASR/blob/main/funasr/models/transformer/encoder.py
+    """
+
+    def __init__(
+        self,
+        size,
+        self_attn,
+        feed_forward,
+        dropout_rate,
+        normalize_before=True,
+        concat_after=False,
+        stochastic_depth_rate=0.0,
+    ):
+        super().__init__()
+
+        self.self_attn = self_attn
+        self.feed_forward = feed_forward
+        self.norm1 = nn.LayerNorm(size, eps=1e-12)
+        self.norm2 = nn.LayerNorm(size, eps=1e-12)
+        self.dropout = nn.Dropout(dropout_rate)
+        self.size = size
+        self.normalize_before = normalize_before
+        self.concat_after = concat_after
+        if self.concat_after:
+            self.concat_linear = nn.Linear(size + size, size)
+        self.stochastic_depth_rate = stochastic_depth_rate
+
+    def forward(self, x, mask=None, cache=None):
+        """Compute encoded features.
+
+        Args:
+            x_input (torch.Tensor): Input tensor (#batch, time, size).
+            mask (torch.Tensor): Mask tensor for the input (#batch, time).
+            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).
+
+        Returns:
+            torch.Tensor: Output tensor (#batch, time, size).
+            torch.Tensor: Mask tensor (#batch, time).
+
+        """
+        skip_layer = False
+        # with stochastic depth, residual connection `x + f(x)` becomes
+        # `x <- x + 1 / (1 - p) * f(x)` at training time.
+        stoch_layer_coeff = 1.0
+
+        if skip_layer:
+            if cache is not None:
+                x = torch.cat([cache, x], dim=1)
+            return x, mask
+
+        residual = x
+        if self.normalize_before:
+            x = self.norm1(x)
+
+        if cache is None:
+            x_q = x
+        else:
+            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)
+            x_q = x[:, -1:, :]
+            residual = residual[:, -1:, :]
+            mask = None if mask is None else mask[:, -1:, :]
+
+        if self.concat_after:
+            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)
+            x = residual + stoch_layer_coeff * self.concat_linear(x_concat)
+        else:
+            x = residual + stoch_layer_coeff * self.dropout(
+                self.self_attn(x_q, x, x, mask)
+            )
+        if not self.normalize_before:
+            x = self.norm1(x)
+
+        residual = x
+        if self.normalize_before:
+            x = self.norm2(x)
+        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))
+        if not self.normalize_before:
+            x = self.norm2(x)
+
+        if cache is not None:
+            x = torch.cat([cache, x], dim=1)
+
+        return x, mask
+
+
+class Transformer(nn.Module):
+    # This class is copied and modified from
+    # https://github.com/modelscope/FunASR/blob/main/funasr/models/llm_asr/adaptor.py
+    def __init__(
+        self,
+        downsample_rate=1,
+        encoder_dim=512,
+        llm_dim=512,
+        ffn_dim: int = 2048,
+        n_layer: int = 5,
+        **kwargs
+    ):
+        super().__init__()
+        assert downsample_rate == 1, downsample_rate
+        self.k = downsample_rate
+        self.encoder_dim = encoder_dim
+        self.llm_dim = llm_dim
+        self.linear1 = nn.Linear(self.encoder_dim * self.k, ffn_dim)
+        self.relu = nn.ReLU()
+        self.linear2 = nn.Linear(ffn_dim, self.llm_dim)
+
+        self.blocks = None
+        if n_layer > 0:
+            self.blocks = nn.ModuleList(
+                [
+                    EncoderLayer(
+                        llm_dim,
+                        MultiHeadedAttention(
+                            kwargs.get("attention_heads", 8),
+                            llm_dim,
+                            kwargs.get("attention_dropout_rate", 0.0),
+                        ),
+                        torch_model.PositionwiseFeedForward(
+                            llm_dim,
+                            llm_dim // 4,
+                            kwargs.get("dropout_rate", 0.0),
+                        ),
+                        kwargs.get("dropout_rate", 0.0),
+                    )
+                    for i in range(n_layer)
+                ]
+            )
+
+    def forward(self, x):
+        x = self.linear1(x)
+        x = self.relu(x)
+        x = self.linear2(x)
+
+        masks = None
+
+        if self.blocks is not None:
+            for layer, block in enumerate(self.blocks):
+                x, masks = block(x, masks)
+        return x
diff --git a/scripts/sense-voice/rknn/nano.py b/scripts/sense-voice/rknn/nano.py
new file mode 100755
index 00000000..aae5b96f
--- /dev/null
+++ b/scripts/sense-voice/rknn/nano.py
@@ -0,0 +1,31 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from torch import nn
+
+import adaptor
+import torch_model
+
+
+class Nano(nn.Module):
+    def __init__(self, vocab_size: int = 60515):
+        super().__init__()
+        self.audio_encoder = torch_model.SenseVoiceEncoderSmall()
+        self.ctc_decoder = adaptor.Transformer()
+        # blank is 60514, i.e., the last token id
+        self.ctc = torch_model.CTC(
+            odim=vocab_size,
+            encoder_output_size=self.audio_encoder.output_size,
+        )
+
+    def forward(self, x):
+        """
+        Args:
+          x: (N, T, C)
+        Returns:
+          - logits: (N, T, vocab_size)
+        """
+        encoder_out = self.audio_encoder(x)
+        encoder_out = self.ctc_decoder(encoder_out)
+        logits = self.ctc.ctc_lo(encoder_out)
+        return logits
diff --git a/scripts/sense-voice/rknn/test_nano_torch.py b/scripts/sense-voice/rknn/test_nano_torch.py
new file mode 100755
index 00000000..9616ba68
--- /dev/null
+++ b/scripts/sense-voice/rknn/test_nano_torch.py
@@ -0,0 +1,78 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import base64
+from pathlib import Path
+
+import torch
+
+import nano
+import test_onnx
+
+
+def load_tokens(filename: str = "./tokens.txt"):
+    id2token = dict()
+    with open(filename, encoding="utf-8") as f:
+        for line in f:
+            try:
+                f = line.strip().split()
+                if len(f) == 2:
+                    t, i = f
+                else:
+                    t = " "
+                    i = f[0]
+                id2token[int(i)] = t
+            except Exception as ex:
+                print(ex)
+                raise
+    return id2token
+
+
+def load_torch_model():
+    if not Path("./model.pt").is_file():
+        raise ValueError(
+            "Please download files from https://huggingface.co/csukuangfj/funasr-nano-with-ctc"
+        )
+    model = nano.Nano()
+
+    state_dict = torch.load("./model.pt", map_location="cpu")
+    model.load_state_dict(state_dict, strict=True)
+    model.eval()
+
+    del state_dict
+
+    return model
+
+
+@torch.no_grad()
+def main():
+    model = load_torch_model()
+
+    samples, sample_rate = test_onnx.load_audio("./zh.wav")
+    assert sample_rate == 16000, sample_rate
+
+    features = test_onnx.compute_feat(samples=samples, sample_rate=sample_rate)
+    x = torch.from_numpy(features)[None]
+    logits = model(x)
+
+    idx = logits.squeeze(0).argmax(dim=-1)
+    print(idx)
+    idx = torch.unique_consecutive(idx).tolist()
+    print(idx)
+
+    id2token = load_tokens("./tokens.txt")
+    blank_id = len(id2token) - 1
+
+    idx = [i for i in idx if i != blank_id]
+    print(idx)
+
+    s = b""
+    for i in idx:
+        s += base64.b64decode(id2token[i])
+
+    text = s.decode().strip()
+    print(text)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/sense-voice/rknn/test_onnx.py b/scripts/sense-voice/rknn/test_onnx.py
index d4ac38bc..eda42584 100755
--- a/scripts/sense-voice/rknn/test_onnx.py
+++ b/scripts/sense-voice/rknn/test_onnx.py
@@ -1,12 +1,16 @@
 #!/usr/bin/env python3
 # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
 
+"""
+Note: This is for testing the onnx models that would be later used to export
+to RKNN
+"""
+
 import argparse
 from typing import Tuple
 
 import kaldi_native_fbank as knf
 import numpy as np
-import onnxruntime
 import onnxruntime as ort
 import soundfile as sf
 import torch
@@ -132,7 +136,7 @@ def load_tokens(filename):
 def compute_feat(
     samples,
     sample_rate,
-    max_len: int,
+    max_len: int = -1,
     window_size: int = 7,  # lfr_m
     window_shift: int = 6,  # lfr_n
 ):
@@ -162,17 +166,19 @@ def compute_feat(
 
     print("features.shape", features.shape)
 
-    if features.shape[0] > max_len:
-        features = features[:max_len]
-    elif features.shape[0] < max_len:
-        features = np.pad(
-            features,
-            ((0, max_len - features.shape[0]), (0, 0)),
-            mode="constant",
-            constant_values=0,
-        )
+    if max_len > 0:
+        if features.shape[0] > max_len:
+            features = features[:max_len]
+        elif features.shape[0] < max_len:
+            features = np.pad(
+                features,
+                ((0, max_len - features.shape[0]), (0, 0)),
+                mode="constant",
+                constant_values=0,
+            )
 
     print("features.shape", features.shape)
+    features = np.ascontiguousarray(features)
 
     return features
 
diff --git a/scripts/sense-voice/rknn/torch_model.py b/scripts/sense-voice/rknn/torch_model.py
index cdd3bd37..700fabfd 100644
--- a/scripts/sense-voice/rknn/torch_model.py
+++ b/scripts/sense-voice/rknn/torch_model.py
@@ -344,17 +344,6 @@ class LayerNorm(nn.LayerNorm):
         return output.type_as(input)
 
 
-def sequence_mask(lengths, maxlen=None, dtype=torch.float32, device=None):
-    if maxlen is None:
-        maxlen = lengths.max()
-    row_vector = torch.arange(0, maxlen, 1).to(lengths.device)
-    matrix = torch.unsqueeze(lengths, dim=-1)
-    mask = row_vector < matrix
-    mask = mask.detach()
-
-    return mask.type(dtype).to(device) if device is not None else mask.type(dtype)
-
-
 class SenseVoiceEncoderSmall(nn.Module):
     def __init__(self):
         super().__init__()
diff --git a/scripts/sense-voice/test_nano_torch.py b/scripts/sense-voice/test_nano_torch.py
new file mode 120000
index 00000000..14404d3c
--- /dev/null
+++ b/scripts/sense-voice/test_nano_torch.py
@@ -0,0 +1 @@
+rknn/test_nano_torch.py
\ No newline at end of file
diff --git a/scripts/sense-voice/test_onnx.py b/scripts/sense-voice/test_onnx.py
new file mode 120000
index 00000000..68a6a695
--- /dev/null
+++ b/scripts/sense-voice/test_onnx.py
@@ -0,0 +1 @@
+rknn/test_onnx.py
\ No newline at end of file
diff --git a/scripts/sense-voice/test_onnx_nano.py b/scripts/sense-voice/test_onnx_nano.py
new file mode 100755
index 00000000..7bb1fd5a
--- /dev/null
+++ b/scripts/sense-voice/test_onnx_nano.py
@@ -0,0 +1,146 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+"""
+=========./model.onnx==========
+NodeArg(name='x', type='tensor(float)', shape=[1, 'T', 560])
+-----
+NodeArg(name='logits', type='tensor(float)', shape=['Addlogits_dim_0', 'Addlogits_dim_1', 60515])
+
+=========./model.int8.onnx==========
+NodeArg(name='x', type='tensor(float)', shape=[1, 'T', 560])
+-----
+NodeArg(name='logits', type='tensor(float)', shape=['Addlogits_dim_0', 'Addlogits_dim_1', 60515])
+"""
+
+import argparse
+import base64
+from typing import Tuple
+
+from test_onnx import compute_feat, load_audio
+
+import onnxruntime as ort
+import librosa
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--model",
+        type=str,
+        required=True,
+        help="Path to model.onnx",
+    )
+
+    parser.add_argument(
+        "--tokens",
+        type=str,
+        required=True,
+        help="Path to tokens.txt",
+    )
+
+    parser.add_argument(
+        "--wave",
+        type=str,
+        required=True,
+        help="The input wave to be recognized",
+    )
+
+    return parser.parse_args()
+
+
+class OnnxModel:
+    def __init__(self, filename):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 1
+
+        self.session_opts = session_opts
+
+        self.model = ort.InferenceSession(
+            filename,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+
+        meta = self.model.get_modelmeta().custom_metadata_map
+
+        self.window_size = int(meta["lfr_window_size"])  # lfr_m
+        self.window_shift = int(meta["lfr_window_shift"])  # lfr_n
+        self.blank_id = int(meta["blank_id"])
+
+    def __call__(self, x):
+        logits = self.model.run(
+            [
+                self.model.get_outputs()[0].name,
+            ],
+            {
+                self.model.get_inputs()[0].name: x,
+            },
+        )[0]
+
+        return logits
+
+
+def load_tokens(filename: str):
+    ans = dict()
+    i = 0
+    with open(filename, encoding="utf-8") as f:
+        for line in f:
+            ans[i] = line.strip().split()[0]
+            i += 1
+    return ans
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+    samples, sample_rate = load_audio(args.wave)
+    if sample_rate != 16000:
+        samples = librosa.resample(samples, orig_sr=sample_rate, target_sr=16000)
+        sample_rate = 16000
+
+    model = OnnxModel(filename=args.model)
+
+    features = compute_feat(
+        samples=samples,
+        sample_rate=sample_rate,
+        window_size=model.window_size,
+        window_shift=model.window_shift,
+    )
+
+    logits = model(
+        x=features[None],
+    )
+
+    idx = logits[0].argmax(axis=-1)
+    print("initial ids", idx)
+    id2token = load_tokens(args.tokens)
+    blank_id = model.blank_id
+    print("blank_id", blank_id)
+
+    unique_ids = []
+    prev = -1
+    for i in idx:
+        if i == prev:
+            continue
+        unique_ids.append(i)
+        prev = i
+    print("unique_ids", unique_ids)
+
+    ids = [i for i in unique_ids if i != blank_id]
+
+    print("ids without blank", ids)
+    s = b""
+    for i in ids:
+        s += base64.b64decode(id2token[i])
+
+    text = s.decode().strip()
+    print(text)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/sense-voice/torch_model.py b/scripts/sense-voice/torch_model.py
new file mode 120000
index 00000000..5c7bba09
--- /dev/null
+++ b/scripts/sense-voice/torch_model.py
@@ -0,0 +1 @@
+rknn/torch_model.py
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
index b172d76c..02c06b44 100644
--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
@@ -24,14 +24,18 @@ namespace sherpa_onnx {
 
 OfflineRecognitionResult ConvertSenseVoiceResult(
     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor) {
+    int32_t frame_shift_ms, int32_t subsampling_factor,
+    bool is_funasr_nano = false) {
   OfflineRecognitionResult r;
   r.tokens.reserve(src.tokens.size());
   r.timestamps.reserve(src.timestamps.size());
 
   std::string text;
 
-  for (int32_t i = 4; i < src.tokens.size(); ++i) {
+  // Funasr NanO does not support emotion, event, language, etc.
+  int32_t start = is_funasr_nano ? 0 : 4;
+
+  for (int32_t i = start; i < src.tokens.size(); ++i) {
     auto sym = sym_table[src.tokens[i]];
     text.append(sym);
 
@@ -41,18 +45,20 @@ OfflineRecognitionResult ConvertSenseVoiceResult(
 
   float frame_shift_s = frame_shift_ms / 1000. * subsampling_factor;
 
-  for (int32_t i = 4; i < src.timestamps.size(); ++i) {
-    float time = frame_shift_s * (src.timestamps[i] - 4);
+  for (int32_t i = start; i < src.timestamps.size(); ++i) {
+    float time = frame_shift_s * (src.timestamps[i] - start);
     r.timestamps.push_back(time);
   }
 
   r.words = std::move(src.words);
 
-  // parse lang, emotion and event from tokens.
-  if (src.tokens.size() >= 3) {
-    r.lang = sym_table[src.tokens[0]];
-    r.emotion = sym_table[src.tokens[1]];
-    r.event = sym_table[src.tokens[2]];
+  if (!is_funasr_nano) {
+    // parse lang, emotion and event from tokens.
+    if (src.tokens.size() >= 3) {
+      r.lang = sym_table[src.tokens[0]];
+      r.emotion = sym_table[src.tokens[1]];
+      r.event = sym_table[src.tokens[2]];
+    }
   }
 
   return r;
@@ -75,7 +81,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
       SHERPA_ONNX_EXIT(-1);
     }
 
-    InitFeatConfig();
+    PostInit();
   }
 
   template <typename Manager>
@@ -95,7 +101,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
       SHERPA_ONNX_EXIT(-1);
     }
 
-    InitFeatConfig();
+    PostInit();
   }
 
   std::unique_ptr<OfflineStream> CreateStream() const override {
@@ -103,12 +109,21 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
   }
 
   void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    if (meta_data.is_funasr_nano) {
+      for (int32_t i = 0; i < n; ++i) {
+        DecodeOneStreamFunAsrNano(ss[i]);
+      }
+
+      return;
+    }
+
     if (n == 1) {
       DecodeOneStream(ss[0]);
       return;
     }
 
-    const auto &meta_data = model_->GetModelMetadata();
     // 1. Apply LFR
     // 2. Apply CMVN
     //
@@ -229,6 +244,47 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
   OfflineRecognizerConfig GetConfig() const override { return config_; }
 
  private:
+  void DecodeOneStreamFunAsrNano(OfflineStream *s) const {
+    const auto &meta_data = model_->GetModelMetadata();
+    auto memory_info =
+        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+
+    int32_t feat_dim = config_.feat_config.feature_dim * meta_data.window_size;
+    std::vector<float> f = s->GetFrames();
+    f = ApplyLFR(f);
+
+    int32_t num_frames = f.size() / feat_dim;
+    std::array<int64_t, 3> shape = {1, num_frames, feat_dim};
+    Ort::Value x = Ort::Value::CreateTensor(memory_info, f.data(), f.size(),
+                                            shape.data(), shape.size());
+
+    Ort::Value logits{nullptr};
+    try {
+      logits = model_->Forward(std::move(x));
+    } catch (const Ort::Exception &ex) {
+      SHERPA_ONNX_LOGE("\n\nCaught exception:\n\n%s\n\nReturn an empty result",
+                       ex.what());
+      return;
+    }
+
+    int64_t new_num_frames = logits.GetTensorTypeAndShapeInfo().GetShape()[1];
+    int64_t num_frame_shape = 1;
+    Ort::Value logits_length = Ort::Value::CreateTensor(
+        memory_info, &new_num_frames, 1, &num_frame_shape, 1);
+
+    auto results =
+        decoder_->Decode(std::move(logits), std::move(logits_length));
+
+    int32_t frame_shift_ms = 10;
+    int32_t subsampling_factor = meta_data.window_shift;
+    auto r = ConvertSenseVoiceResult(results[0], symbol_table_, frame_shift_ms,
+                                     subsampling_factor, true);
+
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
   void DecodeOneStream(OfflineStream *s) const {
     const auto &meta_data = model_->GetModelMetadata();
 
@@ -299,6 +355,15 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
     s->SetResult(r);
   }
 
+  void PostInit() {
+    InitFeatConfig();
+
+    const auto &meta_data = model_->GetModelMetadata();
+    if (meta_data.is_funasr_nano) {
+      symbol_table_.ApplyBase64Decode();
+    }
+  }
+
   void InitFeatConfig() {
     const auto &meta_data = model_->GetModelMetadata();
 
@@ -307,6 +372,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
     config_.feat_config.high_freq = 0;
     config_.feat_config.snip_edges = true;
   }
+
   std::vector<float> ApplyLFR(const std::vector<float> &in) const {
     const auto &meta_data = model_->GetModelMetadata();
 
diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
index bdaeb855..e5bfcb94 100644
--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
@@ -21,7 +21,8 @@ namespace sherpa_onnx {
 // defined in ../offline-recognizer-sense-voice-impl.h
 OfflineRecognitionResult ConvertSenseVoiceResult(
     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor);
+    int32_t frame_shift_ms, int32_t subsampling_factor,
+    bool is_funasr_nano /*= false*/);
 
 template <typename SenseVoiceModel>
 class OfflineRecognizerSenseVoiceTplImpl : public OfflineRecognizerImpl {
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h b/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
index f8c858ce..207f329b 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
@@ -45,6 +45,8 @@ struct OfflineSenseVoiceModelMetaData {
 
   std::vector<float> neg_mean;    // not used in rk npu and ascend npu
   std::vector<float> inv_stddev;  // not used in rk npu and ascend npu
+
+  bool is_funasr_nano = false;
 };
 
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.cc b/sherpa-onnx/csrc/offline-sense-voice-model.cc
index 588cdd37..94f0296e 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model.cc
@@ -63,6 +63,12 @@ class OfflineSenseVoiceModel::Impl {
     return std::move(ans[0]);
   }
 
+  Ort::Value Forward(Ort::Value features) {
+    auto ans = sess_->Run({}, input_names_ptr_.data(), &features, 1,
+                          output_names_ptr_.data(), output_names_ptr_.size());
+    return std::move(ans[0]);
+  }
+
   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
     return meta_data_;
   }
@@ -91,37 +97,47 @@ class OfflineSenseVoiceModel::Impl {
     }
 
     Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+
+    std::string comment;
+    SHERPA_ONNX_READ_META_DATA_STR_ALLOW_EMPTY(comment, "comment");
+
+    meta_data_.is_funasr_nano = Contains(comment, "Nano");
+
     SHERPA_ONNX_READ_META_DATA(meta_data_.vocab_size, "vocab_size");
+    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.blank_id, "blank_id", 0);
+
     SHERPA_ONNX_READ_META_DATA(meta_data_.window_size, "lfr_window_size");
     SHERPA_ONNX_READ_META_DATA(meta_data_.window_shift, "lfr_window_shift");
     SHERPA_ONNX_READ_META_DATA(meta_data_.normalize_samples,
                                "normalize_samples");
 
-    SHERPA_ONNX_READ_META_DATA(meta_data_.with_itn_id, "with_itn");
+    if (!meta_data_.is_funasr_nano) {
+      SHERPA_ONNX_READ_META_DATA(meta_data_.with_itn_id, "with_itn");
 
-    SHERPA_ONNX_READ_META_DATA(meta_data_.without_itn_id, "without_itn");
+      SHERPA_ONNX_READ_META_DATA(meta_data_.without_itn_id, "without_itn");
 
-    int32_t lang_auto = 0;
-    int32_t lang_zh = 0;
-    int32_t lang_en = 0;
-    int32_t lang_ja = 0;
-    int32_t lang_ko = 0;
-    int32_t lang_yue = 0;
+      int32_t lang_auto = 0;
+      int32_t lang_zh = 0;
+      int32_t lang_en = 0;
+      int32_t lang_ja = 0;
+      int32_t lang_ko = 0;
+      int32_t lang_yue = 0;
 
-    SHERPA_ONNX_READ_META_DATA(lang_auto, "lang_auto");
-    SHERPA_ONNX_READ_META_DATA(lang_zh, "lang_zh");
-    SHERPA_ONNX_READ_META_DATA(lang_en, "lang_en");
-    SHERPA_ONNX_READ_META_DATA(lang_ja, "lang_ja");
-    SHERPA_ONNX_READ_META_DATA(lang_ko, "lang_ko");
-    SHERPA_ONNX_READ_META_DATA(lang_yue, "lang_yue");
+      SHERPA_ONNX_READ_META_DATA(lang_auto, "lang_auto");
+      SHERPA_ONNX_READ_META_DATA(lang_zh, "lang_zh");
+      SHERPA_ONNX_READ_META_DATA(lang_en, "lang_en");
+      SHERPA_ONNX_READ_META_DATA(lang_ja, "lang_ja");
+      SHERPA_ONNX_READ_META_DATA(lang_ko, "lang_ko");
+      SHERPA_ONNX_READ_META_DATA(lang_yue, "lang_yue");
 
-    meta_data_.lang2id = {
-        {"auto", lang_auto}, {"zh", lang_zh}, {"en", lang_en},
-        {"ja", lang_ja},     {"ko", lang_ko}, {"yue", lang_yue},
-    };
+      meta_data_.lang2id = {
+          {"auto", lang_auto}, {"zh", lang_zh}, {"en", lang_en},
+          {"ja", lang_ja},     {"ko", lang_ko}, {"yue", lang_yue},
+      };
 
-    SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.neg_mean, "neg_mean");
-    SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.inv_stddev, "inv_stddev");
+      SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.neg_mean, "neg_mean");
+      SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.inv_stddev, "inv_stddev");
+    }
   }
 
  private:
@@ -159,6 +175,10 @@ Ort::Value OfflineSenseVoiceModel::Forward(Ort::Value features,
                         std::move(language), std::move(text_norm));
 }
 
+Ort::Value OfflineSenseVoiceModel::Forward(Ort::Value features) const {
+  return impl_->Forward(std::move(features));
+}
+
 const OfflineSenseVoiceModelMetaData &OfflineSenseVoiceModel::GetModelMetadata()
     const {
   return impl_->GetModelMetadata();
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.h b/sherpa-onnx/csrc/offline-sense-voice-model.h
index e82680c5..87904657 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model.h
+++ b/sherpa-onnx/csrc/offline-sense-voice-model.h
@@ -39,6 +39,13 @@ class OfflineSenseVoiceModel {
   Ort::Value Forward(Ort::Value features, Ort::Value features_length,
                      Ort::Value language, Ort::Value text_norm) const;
 
+  /** For FunASR-Nano
+   *
+   * @param features A tensor of shape (1, T, C) with dtype float32
+   * @return Return logits of shape (1, T, C) with dtype float32
+   */
+  Ort::Value Forward(Ort::Value features) const;
+
   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
 
   /** Return an allocator for allocating memory
diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
index eafd4958..92925cb5 100644
--- a/sherpa-onnx/csrc/symbol-table.cc
+++ b/sherpa-onnx/csrc/symbol-table.cc
@@ -234,7 +234,14 @@ std::ostream &operator<<(std::ostream &os, const SymbolTable &symbol_table) {
 void SymbolTable::ApplyBase64Decode() {
   sym2id_.clear();
   for (auto &p : id2sym_) {
-    p.second = Base64Decode(p.second);
+    if (p.second == " ") {
+      // for FunASR nano models, there is an empty string in the tokens.txt,
+      // which is converted to " " while reading it in sherpa-onnx. We convert
+      // it back to "" here
+      p.second = "";
+    } else {
+      p.second = Base64Decode(p.second);
+    }
     sym2id_[p.second] = p.first;
   }
 }

commit 5f0049bd6aae3bf90eb3d5a0d13df4427bee1306
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 17 14:24:01 2025 +0800

    Fix typos in URL (#2905)

diff --git a/scripts/nemo/GigaAM/run-ctc-v2.sh b/scripts/nemo/GigaAM/run-ctc-v2.sh
index 4dc4e3f8..667bd908 100755
--- a/scripts/nemo/GigaAM/run-ctc-v2.sh
+++ b/scripts/nemo/GigaAM/run-ctc-v2.sh
@@ -17,7 +17,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam
diff --git a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
index a03a350c..c4af3c2c 100755
--- a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
+++ b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
@@ -17,7 +17,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam
diff --git a/scripts/nemo/GigaAM/run-ctc-v3.sh b/scripts/nemo/GigaAM/run-ctc-v3.sh
index 0273935a..be0a4da9 100755
--- a/scripts/nemo/GigaAM/run-ctc-v3.sh
+++ b/scripts/nemo/GigaAM/run-ctc-v3.sh
@@ -17,7 +17,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam
diff --git a/scripts/nemo/GigaAM/run-rnnt-v2.sh b/scripts/nemo/GigaAM/run-rnnt-v2.sh
index bc9fa82e..af833876 100755
--- a/scripts/nemo/GigaAM/run-rnnt-v2.sh
+++ b/scripts/nemo/GigaAM/run-rnnt-v2.sh
@@ -18,7 +18,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam
diff --git a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
index fd13ceb3..8972f956 100755
--- a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
+++ b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
@@ -18,7 +18,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam
diff --git a/scripts/nemo/GigaAM/run-rnnt-v3.sh b/scripts/nemo/GigaAM/run-rnnt-v3.sh
index 42dcbcb5..1c7f5e1f 100755
--- a/scripts/nemo/GigaAM/run-rnnt-v3.sh
+++ b/scripts/nemo/GigaAM/run-rnnt-v3.sh
@@ -18,7 +18,7 @@ function install_gigaam() {
 
 function download_files() {
   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
 }
 
 install_gigaam

commit 79d7e135c42060a587d64382267de754c7b39a83
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Dec 16 13:49:14 2025 +0800

    Export GigaAM v3 to sherpa-onnx (#2901)

diff --git a/.github/workflows/export-nemo-giga-am-to-onnx.yaml b/.github/workflows/export-nemo-giga-am-to-onnx.yaml
index 2636d73c..defae973 100644
--- a/.github/workflows/export-nemo-giga-am-to-onnx.yaml
+++ b/.github/workflows/export-nemo-giga-am-to-onnx.yaml
@@ -1,6 +1,9 @@
 name: export-nemo-giga-am-to-onnx
 
 on:
+  push:
+    branches:
+      - export-giga-am-v3
   workflow_dispatch:
 
 concurrency:
@@ -27,6 +30,7 @@ jobs:
           python-version: ${{ matrix.python-version }}
 
       - name: Run CTC
+        if: false
         shell: bash
         run: |
           pushd scripts/nemo/GigaAM
@@ -53,6 +57,7 @@ jobs:
           tar cjvf ${d}.tar.bz2 $d
 
       - name: Run Transducer
+        if: false
         shell: bash
         run: |
           pushd scripts/nemo/GigaAM
@@ -82,6 +87,7 @@ jobs:
           tar cjvf ${d}.tar.bz2 $d
 
       - name: Run CTC v2
+        if: false
         shell: bash
         run: |
           pushd scripts/nemo/GigaAM
@@ -107,6 +113,7 @@ jobs:
           tar cjvf ${d}.tar.bz2 $d
 
       - name: Run Transducer v2
+        if: false
         shell: bash
         run: |
           pushd scripts/nemo/GigaAM
@@ -134,6 +141,123 @@ jobs:
 
           tar cjvf ${d}.tar.bz2 $d
 
+      - name: Run CTC v3
+        if: true
+        shell: bash
+        run: |
+          pushd scripts/nemo/GigaAM
+          ./run-ctc-v3.sh
+          popd
+
+          d=sherpa-onnx-nemo-ctc-giga-am-v3-russian-2025-12-16
+          mkdir $d
+          mkdir $d/test_wavs
+          ls -lh scripts/nemo/GigaAM/v3_ctc.onnx
+          rm scripts/nemo/GigaAM/v3_ctc.onnx
+          cp -v scripts/nemo/GigaAM/*.md $d/
+          mv -v scripts/nemo/GigaAM/*.int8.onnx $d/
+          cp -v scripts/nemo/GigaAM/LICENSE $d/
+          mv -v scripts/nemo/GigaAM/tokens.txt $d/
+          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
+          mv -v scripts/nemo/GigaAM/run-ctc-v3.sh $d/
+          mv -v scripts/nemo/GigaAM/*-ctc-v3.py $d/
+          cp -v scripts/nemo/GigaAM/test-onnx-ctc.py $d/
+
+          ls -lh scripts/nemo/GigaAM/
+
+          ls -lh $d
+
+          tar cjvf ${d}.tar.bz2 $d
+
+          ls -lh *.tar.bz2
+
+      - name: Run CTC v3 with punctuations
+        if: true
+        shell: bash
+        run: |
+          pushd scripts/nemo/GigaAM
+          ./run-ctc-v3-punct.sh
+          popd
+
+          d=sherpa-onnx-nemo-ctc-punct-giga-am-v3-russian-2025-12-16
+          mkdir $d
+          mkdir $d/test_wavs
+          rm scripts/nemo/GigaAM/v3_e2e_ctc.onnx
+          cp -v scripts/nemo/GigaAM/*.md $d/
+          mv -v scripts/nemo/GigaAM/*.int8.onnx $d/
+          cp -v scripts/nemo/GigaAM/LICENSE $d/
+          mv -v scripts/nemo/GigaAM/tokens.txt $d/
+          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
+          mv -v scripts/nemo/GigaAM/run-ctc-v3-punct.sh $d/
+          mv -v scripts/nemo/GigaAM/*-ctc-v3-punct.py $d/
+          cp -v scripts/nemo/GigaAM/test-onnx-ctc.py $d/
+
+          ls -lh scripts/nemo/GigaAM/
+
+          ls -lh $d
+
+          tar cjvf ${d}.tar.bz2 $d
+
+          ls -lh *.tar.bz2
+
+      - name: Run Transducer v3
+        if: false
+        shell: bash
+        run: |
+          pushd scripts/nemo/GigaAM
+          ./run-rnnt-v3.sh
+          popd
+
+          d=sherpa-onnx-nemo-transducer-giga-am-v3-russian-2025-12-16
+          mkdir $d
+          mkdir $d/test_wavs
+
+          mv -v scripts/nemo/GigaAM/encoder.int8.onnx $d/
+          mv -v scripts/nemo/GigaAM/decoder.onnx $d/
+          mv -v scripts/nemo/GigaAM/joiner.onnx $d/
+
+          cp -v scripts/nemo/GigaAM/*.md $d/
+          cp -v scripts/nemo/GigaAM/LICENSE $d/
+          mv -v scripts/nemo/GigaAM/tokens.txt $d/
+          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
+          mv -v scripts/nemo/GigaAM/run-rnnt-v3.sh $d/
+          cp -v scripts/nemo/GigaAM/test-onnx-rnnt.py $d/
+
+          ls -lh scripts/nemo/GigaAM/
+
+          ls -lh $d
+
+          tar cjvf ${d}.tar.bz2 $d
+
+      - name: Run Transducer v3 with punctuations
+        if: false
+        shell: bash
+        run: |
+          pushd scripts/nemo/GigaAM
+          ./run-rnnt-v3-punct.sh
+          popd
+
+          d=sherpa-onnx-nemo-transducer-punct-giga-am-v3-russian-2025-12-16
+          mkdir $d
+          mkdir $d/test_wavs
+
+          mv -v scripts/nemo/GigaAM/encoder.int8.onnx $d/
+          mv -v scripts/nemo/GigaAM/decoder.onnx $d/
+          mv -v scripts/nemo/GigaAM/joiner.onnx $d/
+
+          cp -v scripts/nemo/GigaAM/*.md $d/
+          cp -v scripts/nemo/GigaAM/LICENSE $d/
+          mv -v scripts/nemo/GigaAM/tokens.txt $d/
+          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
+          mv -v scripts/nemo/GigaAM/run-rnnt-v3-punct.sh $d/
+          cp -v scripts/nemo/GigaAM/test-onnx-rnnt.py $d/
+
+          ls -lh scripts/nemo/GigaAM/
+
+          ls -lh $d
+
+          tar cjvf ${d}.tar.bz2 $d
+
       - name: Release
         if: github.repository_owner == 'csukuangfj'
         uses: svenstaro/upload-release-action@v2
@@ -155,6 +279,7 @@ jobs:
           tag: asr-models
 
       - name: Publish to huggingface (CTC)
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -182,6 +307,7 @@ jobs:
             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
 
       - name: Publish to huggingface (Transducer)
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -209,6 +335,7 @@ jobs:
             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
 
       - name: Publish v2 to huggingface (CTC)
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -236,6 +363,7 @@ jobs:
             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
 
       - name: Publish v2 to huggingface (Transducer)
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -261,3 +389,44 @@ jobs:
             git status
             git commit -m "add models"
             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+
+      - name: Publish v3 to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 5
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            names=(
+             sherpa-onnx-nemo-ctc-giga-am-v3-russian-2025-12-16
+             sherpa-onnx-nemo-ctc-punct-giga-am-v3-russian-2025-12-16
+             sherpa-onnx-nemo-transducer-giga-am-v3-russian-2025-12-16
+             sherpa-onnx-nemo-transducer-punct-giga-am-v3-russian-2025-12-16
+            )
+            for d in ${names[@]}; do
+              if [ ! -d $d ]; then
+                echo "$d does not exist - skip it"
+                continue;
+              fi
+
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
+              cp -av $d/* ./huggingface
+              cd huggingface
+              git lfs track "*.onnx"
+              git lfs track "*.wav"
+              git status
+              git add .
+              git status
+              git commit -m "add models"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+              cd ..
+            done
diff --git a/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py b/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py
new file mode 100755
index 00000000..b6f801de
--- /dev/null
+++ b/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py
@@ -0,0 +1,88 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import gigaam
+import onnx
+import torch
+from onnxruntime.quantization import QuantType, quantize_dynamic
+
+"""
+==========Input==========
+NodeArg(name='features', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
+NodeArg(name='feature_lengths', type='tensor(int64)', shape=['batch_size'])
+==========Output==========
+NodeArg(name='log_probs', type='tensor(float)', shape=['batch_size', 'seq_len', 257])
+"""
+
+
+def add_meta_data(filename: str, meta_data: dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+"""
+{'model_class': 'ctc', 'sample_rate': 16000, 'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor',
+'sample_rate': 16000, 'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
+'mel_norm': None, 'center': False}, 'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64,
+'n_layers': 16, 'd_model': 768, 'subsampling': 'conv1d', 'subs_kernel_size': 5, 'subsampling_factor': 4,
+'ff_expansion_factor': 4, 'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
+'conv_kernel_size': 5, 'flash_attn': False, 'conv_norm_type': 'layer_norm'}, 'head': {'_target_':
+'gigaam.decoder.CTCHead', 'feat_in': 768, 'num_classes': 257}, 'decoding': {'_target_':
+'gigaam.decoding.CTCGreedyDecoding', 'vocabulary': None,
+'model_path': '/root/.cache/gigaam/v3_e2e_ctc_tokenizer.model'},
+'model_name': 'v3_e2e_ctc', 'hashes': {'model': 'c15fd0dbca70363a146016d197ee0e2a',
+'tokenizer': '2a9cd0c246db42d076e92abb31055deb'}}
+"""
+
+
+def main() -> None:
+    model_name = "v3_e2e_ctc"
+    model = gigaam.load_model(model_name)
+
+    # <blk> is the last token
+    sp = model.decoding.tokenizer.model
+    with open("./tokens.txt", "w", encoding="utf-8") as f:
+        for i in range(sp.vocab_size()):
+            f.write(f"{sp.id_to_piece(i)} {i}\n")
+
+        f.write(f"<blk> {i+1}\n")
+        print("Saved to tokens.txt")
+    model.to_onnx(".")
+    meta_data = {
+        "vocab_size": sp.vocab_size() + 1,
+        "normalize_type": "",
+        "subsampling_factor": 4,
+        "model_type": "EncDecCTCModel",
+        "version": "1",
+        "model_author": "https://github.com/salute-developers/GigaAM",
+        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
+        "language": "Russian",
+        "comment": "v3 with puncutations",
+        "is_giga_am": 1,
+    }
+    add_meta_data(f"./{model_name}.onnx", meta_data)
+    quantize_dynamic(
+        model_input=f"./{model_name}.onnx",
+        model_output="./model.int8.onnx",
+        weight_type=QuantType.QUInt8,
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/nemo/GigaAM/export-onnx-ctc-v3.py b/scripts/nemo/GigaAM/export-onnx-ctc-v3.py
new file mode 100755
index 00000000..227f30af
--- /dev/null
+++ b/scripts/nemo/GigaAM/export-onnx-ctc-v3.py
@@ -0,0 +1,87 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import gigaam
+import onnx
+import torch
+from onnxruntime.quantization import QuantType, quantize_dynamic
+
+"""
+NodeArg(name='features', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
+NodeArg(name='feature_lengths', type='tensor(int64)', shape=['batch_size'])
+-----
+NodeArg(name='log_probs', type='tensor(float)', shape=['batch_size', 'seq_len', 34])
+"""
+
+
+def add_meta_data(filename: str, meta_data: dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+"""
+{'model_class': 'ctc', 'sample_rate': 16000,
+'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000, 'features': 64,
+'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320, 'mel_norm': None, 'center': False},
+'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16, 'd_model': 768,
+'subsampling': 'conv1d', 'subs_kernel_size': 5, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
+'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16, 'conv_kernel_size': 5,
+'flash_attn': False, 'conv_norm_type': 'layer_norm'}, 'head': {'_target_': 'gigaam.decoder.CTCHead',
+'feat_in': 768, 'num_classes': 34}, 'decoding': {'_target_': 'gigaam.decoding.CTCGreedyDecoding',
+'vocabulary': [' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',
+'', '', '', '', '', '', '', '', '', '', '', '', '', '']}, 'model_name': 'v3_ctc',
+'hashes': {'model': '1bdc12052560591b7cdf35bef02619fa'}}
+"""
+
+
+def main() -> None:
+    model_name = "v3_ctc"
+    model = gigaam.load_model(model_name)
+
+    # use characters
+    # space is 0
+    # <blk> is the last token
+    with open("./tokens.txt", "w", encoding="utf-8") as f:
+        for i, s in enumerate(model.cfg["decoding"]["vocabulary"]):
+            f.write(f"{s} {i}\n")
+        f.write(f"<blk> {i+1}\n")
+        print("Saved to tokens.txt")
+    model.to_onnx(".")
+    meta_data = {
+        "vocab_size": len(model.cfg["decoding"]["vocabulary"]) + 1,
+        "normalize_type": "",
+        "subsampling_factor": 4,
+        "model_type": "EncDecCTCModel",
+        "version": "1",
+        "model_author": "https://github.com/salute-developers/GigaAM",
+        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
+        "language": "Russian",
+        "comment": "v3",
+        "is_giga_am": 1,
+    }
+    add_meta_data(f"./{model_name}.onnx", meta_data)
+    quantize_dynamic(
+        model_input=f"./{model_name}.onnx",
+        model_output="./model.int8.onnx",
+        weight_type=QuantType.QUInt8,
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py b/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py
new file mode 100755
index 00000000..b2791aae
--- /dev/null
+++ b/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py
@@ -0,0 +1,176 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+import os
+
+import gigaam
+import onnx
+import torch
+from gigaam.utils import onnx_converter
+from onnxruntime.quantization import QuantType, quantize_dynamic
+from torch import Tensor
+
+# encoder input length should be of int64
+# encder output length can be int64 or int32
+
+"""
+==========Input==========
+NodeArg(name='audio_signal', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
+NodeArg(name='length', type='tensor(int64)', shape=['batch_size'])
+==========Output==========
+NodeArg(name='encoded', type='tensor(float)', shape=['batch_size', 768, 'Transposeencoded_dim_2'])
+NodeArg(name='encoded_len', type='tensor(int32)', shape=['batch_size'])
+==========Input==========
+NodeArg(name='x', type='tensor(int32)', shape=[1, 1])
+NodeArg(name='unused_x_len.1', type='tensor(int32)', shape=[1])
+NodeArg(name='h.1', type='tensor(float)', shape=[1, 1, 320])
+NodeArg(name='c.1', type='tensor(float)', shape=[1, 1, 320])
+==========Output==========
+NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
+NodeArg(name='unused_x_len', type='tensor(int32)', shape=[1])
+NodeArg(name='h', type='tensor(float)', shape=[1, 1, 320])
+NodeArg(name='c', type='tensor(float)', shape=[1, 1, 320])
+==========Input==========
+NodeArg(name='enc', type='tensor(float)', shape=[1, 768, 1])
+NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
+==========Output==========
+NodeArg(name='joint', type='tensor(float)', shape=[1, 1, 1, 1025])
+"""
+
+
+def add_meta_data(filename: str, meta_data: dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+class EncoderWrapper(torch.nn.Module):
+    def __init__(self, m):
+        super().__init__()
+        self.m = m
+
+    def forward(self, audio_signal: Tensor, length: Tensor):
+        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/encoder.py#L499
+        out, out_len = self.m.encoder(audio_signal, length)
+
+        return out, out_len.to(torch.int64)
+
+    def to_onnx(self, dir_path: str = "."):
+        onnx_converter(
+            model_name=f"{self.m.cfg.model_name}_encoder",
+            out_dir=dir_path,
+            module=self.m.encoder,
+            dynamic_axes=self.m.encoder.dynamic_axes(),
+        )
+
+
+class DecoderWrapper(torch.nn.Module):
+    def __init__(self, m):
+        super().__init__()
+        self.m = m
+
+    def forward(self, x: Tensor, unused_x_len: Tensor, h: Tensor, c: Tensor):
+        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/decoder.py#L110C17-L110C54
+        emb = self.m.head.decoder.embed(x)
+        g, (h, c) = self.m.head.decoder.lstm(emb.transpose(0, 1), (h, c))
+        return g.permute(1, 2, 0), unused_x_len + 1, h, c
+
+    def to_onnx(self, dir_path: str = "."):
+        label, hidden_h, hidden_c = self.m.head.decoder.input_example()
+        label = label.to(torch.int32)
+        label_len = torch.zeros(1, dtype=torch.int32)
+
+        onnx_converter(
+            model_name=f"{self.m.cfg.model_name}_decoder",
+            out_dir=dir_path,
+            module=self,
+            dynamic_axes=self.m.encoder.dynamic_axes(),
+            inputs=(label, label_len, hidden_h, hidden_c),
+            input_names=["x", "unused_x_len.1", "h.1", "c.1"],
+            output_names=["dec", "unused_x_len", "h", "c"],
+        )
+
+
+"""
+{'model_class': 'rnnt', 'sample_rate': 16000,
+'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000,
+'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
+'mel_norm': None, 'center': False},
+'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16,
+'d_model': 768, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
+'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
+'conv_kernel_size': 5, 'flash_attn': False, 'subs_kernel_size': 5,
+'subsampling': 'conv1d', 'conv_norm_type': 'layer_norm'},
+'head': {'_target_': 'gigaam.decoder.RNNTHead',
+'decoder': {'pred_hidden': 320, 'pred_rnn_layers': 1, 'num_classes': 1025},
+'joint': {'enc_hidden': 768, 'pred_hidden': 320, 'joint_hidden': 320, 'num_classes': 1025}},
+'decoding': {'_target_': 'gigaam.decoding.RNNTGreedyDecoding',
+'vocabulary': None, 'model_path': '/root/.cache/gigaam/v3_e2e_rnnt_tokenizer.model'}, 'model_name': 'v3_e2e_rnnt', 'hashes': {'model': '72e2a9b5c7caad963b2bbfd2f298c252', 'tokenizer': '3b3bf8370e882885d79731592fc99f98'}}
+"""
+
+
+def main() -> None:
+    model_name = "v3_e2e_rnnt"
+    model = gigaam.load_model(model_name)
+
+    # <blk> is the last token
+    sp = model.decoding.tokenizer.model
+    with open("./tokens.txt", "w", encoding="utf-8") as f:
+        for i in range(sp.vocab_size()):
+            f.write(f"{sp.id_to_piece(i)} {i}\n")
+
+        f.write(f"<blk> {i+1}\n")
+        print("Saved to tokens.txt")
+
+    EncoderWrapper(model).to_onnx(".")
+    DecoderWrapper(model).to_onnx(".")
+
+    onnx_converter(
+        model_name=f"{model.cfg.model_name}_joint",
+        out_dir=".",
+        module=model.head.joint,
+    )
+    meta_data = {
+        # vocab_size does not include the blank
+        # we will increase vocab_size by 1 in the c++ code
+        "vocab_size": model.cfg["head"]["decoder"]["num_classes"] - 1,
+        "pred_rnn_layers": model.cfg["head"]["decoder"]["pred_rnn_layers"],
+        "pred_hidden": model.cfg["head"]["decoder"]["pred_hidden"],
+        "normalize_type": "",
+        "subsampling_factor": 4,
+        "model_type": "EncDecRNNTBPEModel",
+        "version": "3",
+        "model_author": "https://github.com/salute-developers/GigaAM",
+        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
+        "language": "Russian",
+        "comment": "v3",
+        "is_giga_am": 1,
+    }
+
+    add_meta_data(f"./{model_name}_encoder.onnx", meta_data)
+    quantize_dynamic(
+        model_input=f"./{model_name}_encoder.onnx",
+        model_output="./encoder.int8.onnx",
+        weight_type=QuantType.QUInt8,
+    )
+    os.rename(f"./{model_name}_decoder.onnx", "decoder.onnx")
+    os.rename(f"./{model_name}_joint.onnx", "joiner.onnx")
+    os.remove(f"./{model_name}_encoder.onnx")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py b/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py
new file mode 100755
index 00000000..72e604f5
--- /dev/null
+++ b/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py
@@ -0,0 +1,178 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+import os
+
+import gigaam
+import onnx
+import torch
+from gigaam.utils import onnx_converter
+from onnxruntime.quantization import QuantType, quantize_dynamic
+from torch import Tensor
+
+# encoder input length should be of int64
+# encder output length can be int64 or int32
+
+"""
+==========Input==========
+NodeArg(name='audio_signal', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
+NodeArg(name='length', type='tensor(int64)', shape=['batch_size'])
+==========Output==========
+NodeArg(name='encoded', type='tensor(float)', shape=['batch_size', 768, 'Transposeencoded_dim_2'])
+NodeArg(name='encoded_len', type='tensor(int32)', shape=['batch_size'])
+==========Input==========
+NodeArg(name='x', type='tensor(int32)', shape=[1, 1])
+NodeArg(name='unused_x_len.1', type='tensor(int32)', shape=[1])
+NodeArg(name='h.1', type='tensor(float)', shape=[1, 1, 320])
+NodeArg(name='c.1', type='tensor(float)', shape=[1, 1, 320])
+==========Output==========
+NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
+NodeArg(name='unused_x_len', type='tensor(int32)', shape=[1])
+NodeArg(name='h', type='tensor(float)', shape=[1, 1, 320])
+NodeArg(name='c', type='tensor(float)', shape=[1, 1, 320])
+==========Input==========
+NodeArg(name='enc', type='tensor(float)', shape=[1, 768, 1])
+NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
+==========Output==========
+NodeArg(name='joint', type='tensor(float)', shape=[1, 1, 1, 34])
+"""
+
+
+def add_meta_data(filename: str, meta_data: dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    onnx.save(model, filename)
+
+
+class EncoderWrapper(torch.nn.Module):
+    def __init__(self, m):
+        super().__init__()
+        self.m = m
+
+    def forward(self, audio_signal: Tensor, length: Tensor):
+        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/encoder.py#L499
+        out, out_len = self.m.encoder(audio_signal, length)
+
+        return out, out_len.to(torch.int64)
+
+    def to_onnx(self, dir_path: str = "."):
+        onnx_converter(
+            model_name=f"{self.m.cfg.model_name}_encoder",
+            out_dir=dir_path,
+            module=self.m.encoder,
+            dynamic_axes=self.m.encoder.dynamic_axes(),
+        )
+
+
+class DecoderWrapper(torch.nn.Module):
+    def __init__(self, m):
+        super().__init__()
+        self.m = m
+
+    def forward(self, x: Tensor, unused_x_len: Tensor, h: Tensor, c: Tensor):
+        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/decoder.py#L110C17-L110C54
+        emb = self.m.head.decoder.embed(x)
+        g, (h, c) = self.m.head.decoder.lstm(emb.transpose(0, 1), (h, c))
+        return g.permute(1, 2, 0), unused_x_len + 1, h, c
+
+    def to_onnx(self, dir_path: str = "."):
+        label, hidden_h, hidden_c = self.m.head.decoder.input_example()
+        label = label.to(torch.int32)
+        label_len = torch.zeros(1, dtype=torch.int32)
+
+        onnx_converter(
+            model_name=f"{self.m.cfg.model_name}_decoder",
+            out_dir=dir_path,
+            module=self,
+            dynamic_axes=self.m.encoder.dynamic_axes(),
+            inputs=(label, label_len, hidden_h, hidden_c),
+            input_names=["x", "unused_x_len.1", "h.1", "c.1"],
+            output_names=["dec", "unused_x_len", "h", "c"],
+        )
+
+
+"""
+{'model_class': 'rnnt', 'sample_rate': 16000,
+'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000,
+'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
+'mel_norm': None, 'center': False},
+'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16,
+'d_model': 768, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
+'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
+'conv_kernel_size': 5, 'flash_attn': False, 'subs_kernel_size': 5,
+'subsampling': 'conv1d', 'conv_norm_type': 'layer_norm'},
+'head': {'_target_': 'gigaam.decoder.RNNTHead',
+'decoder': {'pred_hidden': 320, 'pred_rnn_layers': 1, 'num_classes': 34},
+'joint': {'enc_hidden': 768, 'pred_hidden': 320, 'joint_hidden': 320, 'num_classes': 34}},
+'decoding': {'_target_': 'gigaam.decoding.RNNTGreedyDecoding',
+'vocabulary': [' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '',
+'', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']},
+'model_name': 'v3_rnnt', 'hashes': {'model': 'be62a7bc46de1311ec288d3bf8ee2818'}}
+"""
+
+
+def main() -> None:
+    model_name = "v3_rnnt"
+    model = gigaam.load_model(model_name)
+
+    # use characters
+    # space is 0
+    # <blk> is the last token
+    with open("./tokens.txt", "w", encoding="utf-8") as f:
+        for i, s in enumerate(model.cfg["decoding"]["vocabulary"]):
+            f.write(f"{s} {i}\n")
+        f.write(f"<blk> {i+1}\n")
+        print("Saved to tokens.txt")
+
+    EncoderWrapper(model).to_onnx(".")
+    DecoderWrapper(model).to_onnx(".")
+
+    onnx_converter(
+        model_name=f"{model.cfg.model_name}_joint",
+        out_dir=".",
+        module=model.head.joint,
+    )
+    meta_data = {
+        # vocab_size does not include the blank
+        # we will increase vocab_size by 1 in the c++ code
+        "vocab_size": model.cfg["head"]["decoder"]["num_classes"] - 1,
+        "pred_rnn_layers": model.cfg["head"]["decoder"]["pred_rnn_layers"],
+        "pred_hidden": model.cfg["head"]["decoder"]["pred_hidden"],
+        "normalize_type": "",
+        "subsampling_factor": 4,
+        "model_type": "EncDecRNNTBPEModel",
+        "version": "3",
+        "model_author": "https://github.com/salute-developers/GigaAM",
+        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
+        "language": "Russian",
+        "comment": "v3",
+        "is_giga_am": 1,
+    }
+
+    add_meta_data(f"./{model_name}_encoder.onnx", meta_data)
+    quantize_dynamic(
+        model_input=f"./{model_name}_encoder.onnx",
+        model_output="./encoder.int8.onnx",
+        weight_type=QuantType.QUInt8,
+    )
+    os.rename(f"./{model_name}_decoder.onnx", "decoder.onnx")
+    os.rename(f"./{model_name}_joint.onnx", "joiner.onnx")
+    os.remove(f"./{model_name}_encoder.onnx")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
new file mode 100755
index 00000000..a03a350c
--- /dev/null
+++ b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
@@ -0,0 +1,28 @@
+#!/usr/bin/env bash
+
+set -ex
+
+function install_gigaam() {
+  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
+  python3 get-pip.py
+  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
+  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
+
+  BRANCH='main'
+  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
+
+  python3 -m pip install -qq kaldi-native-fbank
+  pip install numpy==1.26.4
+}
+
+function download_files() {
+  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+}
+
+install_gigaam
+download_files
+
+python3 ./export-onnx-ctc-v3-punct.py
+ls -lh
+python3 ./test-onnx-ctc.py
diff --git a/scripts/nemo/GigaAM/run-ctc-v3.sh b/scripts/nemo/GigaAM/run-ctc-v3.sh
new file mode 100755
index 00000000..0273935a
--- /dev/null
+++ b/scripts/nemo/GigaAM/run-ctc-v3.sh
@@ -0,0 +1,28 @@
+#!/usr/bin/env bash
+
+set -ex
+
+function install_gigaam() {
+  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
+  python3 get-pip.py
+  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
+  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
+
+  BRANCH='main'
+  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
+
+  python3 -m pip install -qq kaldi-native-fbank
+  pip install numpy==1.26.4
+}
+
+function download_files() {
+  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+}
+
+install_gigaam
+download_files
+
+python3 ./export-onnx-ctc-v3.py
+ls -lh
+python3 ./test-onnx-ctc.py
diff --git a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
new file mode 100755
index 00000000..fd13ceb3
--- /dev/null
+++ b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
@@ -0,0 +1,29 @@
+#!/usr/bin/env bash
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+set -ex
+
+function install_gigaam() {
+  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
+  python3 get-pip.py
+  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
+  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
+
+  BRANCH='main'
+  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
+
+  python3 -m pip install -qq kaldi-native-fbank
+  pip install numpy==1.26.4
+}
+
+function download_files() {
+  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+}
+
+install_gigaam
+download_files
+
+python3 ./export-onnx-rnnt-v3-punct.py
+ls -lh
+python3 ./test-onnx-rnnt.py
diff --git a/scripts/nemo/GigaAM/run-rnnt-v3.sh b/scripts/nemo/GigaAM/run-rnnt-v3.sh
new file mode 100755
index 00000000..42dcbcb5
--- /dev/null
+++ b/scripts/nemo/GigaAM/run-rnnt-v3.sh
@@ -0,0 +1,29 @@
+#!/usr/bin/env bash
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+set -ex
+
+function install_gigaam() {
+  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
+  python3 get-pip.py
+  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
+  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
+
+  BRANCH='main'
+  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
+
+  python3 -m pip install -qq kaldi-native-fbank
+  pip install numpy==1.26.4
+}
+
+function download_files() {
+  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
+}
+
+install_gigaam
+download_files
+
+python3 ./export-onnx-rnnt-v3.py
+ls -lh
+python3 ./test-onnx-rnnt.py

commit a6a36e8cef858aa4f252f037a94463f20cc48d62
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 11 20:30:19 2025 +0800

    Use a shorter name for Zipvoice models. (#2894)

diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
index 2ad7c6b2..9009be24 100644
--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
@@ -216,8 +216,8 @@ final class SherpaOnnxOfflineTtsKittenModelConfig extends Struct {
 
 final class SherpaOnnxOfflineTtsZipVoiceModelConfig extends Struct {
   external Pointer<Utf8> tokens;
-  external Pointer<Utf8> textModel;
-  external Pointer<Utf8> flowMatchingModel;
+  external Pointer<Utf8> encoder;
+  external Pointer<Utf8> decoder;
   external Pointer<Utf8> vocoder;
   external Pointer<Utf8> dataDir;
   external Pointer<Utf8> lexicon;
diff --git a/flutter/sherpa_onnx/lib/src/tts.dart b/flutter/sherpa_onnx/lib/src/tts.dart
index dc1b1a8a..5e622ffd 100644
--- a/flutter/sherpa_onnx/lib/src/tts.dart
+++ b/flutter/sherpa_onnx/lib/src/tts.dart
@@ -36,14 +36,14 @@ class OfflineTtsVitsModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'model': model,
-        'lexicon': lexicon,
-        'tokens': tokens,
-        'dataDir': dataDir,
-        'noiseScale': noiseScale,
-        'noiseScaleW': noiseScaleW,
-        'lengthScale': lengthScale,
-      };
+    'model': model,
+    'lexicon': lexicon,
+    'tokens': tokens,
+    'dataDir': dataDir,
+    'noiseScale': noiseScale,
+    'noiseScaleW': noiseScaleW,
+    'lengthScale': lengthScale,
+  };
 
   final String model;
   final String lexicon;
@@ -85,14 +85,14 @@ class OfflineTtsMatchaModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'acousticModel': acousticModel,
-        'vocoder': vocoder,
-        'lexicon': lexicon,
-        'tokens': tokens,
-        'dataDir': dataDir,
-        'noiseScale': noiseScale,
-        'lengthScale': lengthScale,
-      };
+    'acousticModel': acousticModel,
+    'vocoder': vocoder,
+    'lexicon': lexicon,
+    'tokens': tokens,
+    'dataDir': dataDir,
+    'noiseScale': noiseScale,
+    'lengthScale': lengthScale,
+  };
 
   final String acousticModel;
   final String vocoder;
@@ -134,14 +134,14 @@ class OfflineTtsKokoroModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'model': model,
-        'voices': voices,
-        'tokens': tokens,
-        'dataDir': dataDir,
-        'lengthScale': lengthScale,
-        'lexicon': lexicon,
-        'lang': lang,
-      };
+    'model': model,
+    'voices': voices,
+    'tokens': tokens,
+    'dataDir': dataDir,
+    'lengthScale': lengthScale,
+    'lexicon': lexicon,
+    'lang': lang,
+  };
 
   final String model;
   final String voices;
@@ -178,12 +178,12 @@ class OfflineTtsKittenModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'model': model,
-        'voices': voices,
-        'tokens': tokens,
-        'dataDir': dataDir,
-        'lengthScale': lengthScale,
-      };
+    'model': model,
+    'voices': voices,
+    'tokens': tokens,
+    'dataDir': dataDir,
+    'lengthScale': lengthScale,
+  };
 
   final String model;
   final String voices;
@@ -195,8 +195,8 @@ class OfflineTtsKittenModelConfig {
 class OfflineTtsZipVoiceModelConfig {
   const OfflineTtsZipVoiceModelConfig({
     this.tokens = '',
-    this.textModel = '',
-    this.flowMatchingModel = '',
+    this.encoder = '',
+    this.decoder = '',
     this.vocoder = '',
     this.dataDir = '',
     this.lexicon = '',
@@ -209,8 +209,8 @@ class OfflineTtsZipVoiceModelConfig {
   factory OfflineTtsZipVoiceModelConfig.fromJson(Map<String, dynamic> json) {
     return OfflineTtsZipVoiceModelConfig(
       tokens: json['tokens'] as String? ?? '',
-      textModel: json['textModel'] as String? ?? '',
-      flowMatchingModel: json['flowMatchingModel'] as String? ?? '',
+      encoder: json['encoder'] as String? ?? '',
+      decoder: json['decoder'] as String? ?? '',
       vocoder: json['vocoder'] as String? ?? '',
       dataDir: json['dataDir'] as String? ?? '',
       lexicon: json['lexicon'] as String? ?? '',
@@ -223,25 +223,25 @@ class OfflineTtsZipVoiceModelConfig {
 
   @override
   String toString() {
-    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
+    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, encoder: $encoder, decoder: $decoder, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
   }
 
   Map<String, dynamic> toJson() => {
-        'tokens': tokens,
-        'textModel': textModel,
-        'flowMatchingModel': flowMatchingModel,
-        'vocoder': vocoder,
-        'dataDir': dataDir,
-        'lexicon': lexicon,
-        'featScale': featScale,
-        'tShift': tShift,
-        'targetRms': targetRms,
-        'guidanceScale': guidanceScale,
-      };
+    'tokens': tokens,
+    'encoder': encoder,
+    'decoder': decoder,
+    'vocoder': vocoder,
+    'dataDir': dataDir,
+    'lexicon': lexicon,
+    'featScale': featScale,
+    'tShift': tShift,
+    'targetRms': targetRms,
+    'guidanceScale': guidanceScale,
+  };
 
   final String tokens;
-  final String textModel;
-  final String flowMatchingModel;
+  final String encoder;
+  final String decoder;
   final String vocoder;
   final String dataDir;
   final String lexicon;
@@ -266,15 +266,20 @@ class OfflineTtsModelConfig {
   factory OfflineTtsModelConfig.fromJson(Map<String, dynamic> json) {
     return OfflineTtsModelConfig(
       vits: OfflineTtsVitsModelConfig.fromJson(
-          json['vits'] as Map<String, dynamic>? ?? const {}),
+        json['vits'] as Map<String, dynamic>? ?? const {},
+      ),
       matcha: OfflineTtsMatchaModelConfig.fromJson(
-          json['matcha'] as Map<String, dynamic>? ?? const {}),
+        json['matcha'] as Map<String, dynamic>? ?? const {},
+      ),
       kokoro: OfflineTtsKokoroModelConfig.fromJson(
-          json['kokoro'] as Map<String, dynamic>? ?? const {}),
+        json['kokoro'] as Map<String, dynamic>? ?? const {},
+      ),
       kitten: OfflineTtsKittenModelConfig.fromJson(
-          json['kitten'] as Map<String, dynamic>? ?? const {}),
+        json['kitten'] as Map<String, dynamic>? ?? const {},
+      ),
       zipvoice: OfflineTtsZipVoiceModelConfig.fromJson(
-          json['zipvoice'] as Map<String, dynamic>? ?? const {}),
+        json['zipvoice'] as Map<String, dynamic>? ?? const {},
+      ),
       numThreads: json['numThreads'] as int? ?? 1,
       debug: json['debug'] as bool? ?? true,
       provider: json['provider'] as String? ?? 'cpu',
@@ -287,15 +292,15 @@ class OfflineTtsModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'vits': vits.toJson(),
-        'matcha': matcha.toJson(),
-        'kokoro': kokoro.toJson(),
-        'kitten': kitten.toJson(),
-        'zipvoice': zipvoice.toJson(),
-        'numThreads': numThreads,
-        'debug': debug,
-        'provider': provider,
-      };
+    'vits': vits.toJson(),
+    'matcha': matcha.toJson(),
+    'kokoro': kokoro.toJson(),
+    'kitten': kitten.toJson(),
+    'zipvoice': zipvoice.toJson(),
+    'numThreads': numThreads,
+    'debug': debug,
+    'provider': provider,
+  };
 
   final OfflineTtsVitsModelConfig vits;
   final OfflineTtsMatchaModelConfig matcha;
@@ -318,8 +323,9 @@ class OfflineTtsConfig {
 
   factory OfflineTtsConfig.fromJson(Map<String, dynamic> json) {
     return OfflineTtsConfig(
-      model:
-          OfflineTtsModelConfig.fromJson(json['model'] as Map<String, dynamic>),
+      model: OfflineTtsModelConfig.fromJson(
+        json['model'] as Map<String, dynamic>,
+      ),
       ruleFsts: json['ruleFsts'] as String? ?? '',
       maxNumSenetences: json['maxNumSenetences'] as int? ?? 1,
       ruleFars: json['ruleFars'] as String? ?? '',
@@ -333,12 +339,12 @@ class OfflineTtsConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'model': model.toJson(),
-        'ruleFsts': ruleFsts,
-        'maxNumSenetences': maxNumSenetences,
-        'ruleFars': ruleFars,
-        'silenceScale': silenceScale,
-      };
+    'model': model.toJson(),
+    'ruleFsts': ruleFsts,
+    'maxNumSenetences': maxNumSenetences,
+    'ruleFars': ruleFars,
+    'silenceScale': silenceScale,
+  };
 
   final OfflineTtsModelConfig model;
   final String ruleFsts;
@@ -348,10 +354,7 @@ class OfflineTtsConfig {
 }
 
 class GeneratedAudio {
-  GeneratedAudio({
-    required this.samples,
-    required this.sampleRate,
-  });
+  GeneratedAudio({required this.samples, required this.sampleRate});
 
   final Float32List samples;
   final int sampleRate;
@@ -378,8 +381,8 @@ class OfflineTts {
     c.ref.model.vits.noiseScaleW = config.model.vits.noiseScaleW;
     c.ref.model.vits.lengthScale = config.model.vits.lengthScale;
 
-    c.ref.model.matcha.acousticModel =
-        config.model.matcha.acousticModel.toNativeUtf8();
+    c.ref.model.matcha.acousticModel = config.model.matcha.acousticModel
+        .toNativeUtf8();
     c.ref.model.matcha.vocoder = config.model.matcha.vocoder.toNativeUtf8();
     c.ref.model.matcha.lexicon = config.model.matcha.lexicon.toNativeUtf8();
     c.ref.model.matcha.tokens = config.model.matcha.tokens.toNativeUtf8();
@@ -402,8 +405,8 @@ class OfflineTts {
     c.ref.model.kitten.lengthScale = config.model.kitten.lengthScale;
 
     c.ref.model.zipvoice.tokens = config.model.zipvoice.tokens.toNativeUtf8();
-    c.ref.model.zipvoice.textModel = config.model.zipvoice.textModel.toNativeUtf8();
-    c.ref.model.zipvoice.flowMatchingModel = config.model.zipvoice.flowMatchingModel.toNativeUtf8();
+    c.ref.model.zipvoice.encoder = config.model.zipvoice.encoder.toNativeUtf8();
+    c.ref.model.zipvoice.decoder = config.model.zipvoice.decoder.toNativeUtf8();
     c.ref.model.zipvoice.vocoder = config.model.zipvoice.vocoder.toNativeUtf8();
     c.ref.model.zipvoice.dataDir = config.model.zipvoice.dataDir.toNativeUtf8();
     c.ref.model.zipvoice.lexicon = config.model.zipvoice.lexicon.toNativeUtf8();
@@ -430,8 +433,8 @@ class OfflineTts {
     calloc.free(c.ref.model.zipvoice.lexicon);
     calloc.free(c.ref.model.zipvoice.dataDir);
     calloc.free(c.ref.model.zipvoice.vocoder);
-    calloc.free(c.ref.model.zipvoice.flowMatchingModel);
-    calloc.free(c.ref.model.zipvoice.textModel);
+    calloc.free(c.ref.model.zipvoice.decoder);
+    calloc.free(c.ref.model.zipvoice.encoder);
     calloc.free(c.ref.model.zipvoice.tokens);
 
     calloc.free(c.ref.model.kitten.dataDir);
@@ -470,12 +473,15 @@ class OfflineTts {
     ptr = nullptr;
   }
 
-  GeneratedAudio generate(
-      {required String text, int sid = 0, double speed = 1.0}) {
+  GeneratedAudio generate({
+    required String text,
+    int sid = 0,
+    double speed = 1.0,
+  }) {
     final Pointer<Utf8> textPtr = text.toNativeUtf8();
     final p =
         SherpaOnnxBindings.offlineTtsGenerate?.call(ptr, textPtr, sid, speed) ??
-            nullptr;
+        nullptr;
     calloc.free(textPtr);
 
     if (p == nullptr) {
@@ -491,26 +497,35 @@ class OfflineTts {
     return GeneratedAudio(samples: newSamples, sampleRate: sampleRate);
   }
 
-  GeneratedAudio generateWithCallback(
-      {required String text,
-      int sid = 0,
-      double speed = 1.0,
-      required int Function(Float32List samples) callback}) {
+  GeneratedAudio generateWithCallback({
+    required String text,
+    int sid = 0,
+    double speed = 1.0,
+    required int Function(Float32List samples) callback,
+  }) {
     // see
     // https://github.com/dart-lang/sdk/issues/54276#issuecomment-1846109285
     // https://stackoverflow.com/questions/69537440/callbacks-in-dart-dartffi-only-supports-calling-static-dart-functions-from-nat
     // https://github.com/dart-lang/sdk/blob/main/tests/ffi/isolate_local_function_callbacks_test.dart#L46
     final wrapper =
-        NativeCallable<SherpaOnnxGeneratedAudioCallbackNative>.isolateLocal(
-            (Pointer<Float> samples, int n) {
-      final s = samples.asTypedList(n);
-      final newSamples = Float32List.fromList(s);
-      return callback(newSamples);
-    }, exceptionalReturn: 0);
+        NativeCallable<SherpaOnnxGeneratedAudioCallbackNative>.isolateLocal((
+          Pointer<Float> samples,
+          int n,
+        ) {
+          final s = samples.asTypedList(n);
+          final newSamples = Float32List.fromList(s);
+          return callback(newSamples);
+        }, exceptionalReturn: 0);
 
     final Pointer<Utf8> textPtr = text.toNativeUtf8();
-    final p = SherpaOnnxBindings.offlineTtsGenerateWithCallback
-            ?.call(ptr, textPtr, sid, speed, wrapper.nativeFunction) ??
+    final p =
+        SherpaOnnxBindings.offlineTtsGenerateWithCallback?.call(
+          ptr,
+          textPtr,
+          sid,
+          speed,
+          wrapper.nativeFunction,
+        ) ??
         nullptr;
 
     calloc.free(textPtr);
diff --git a/go-api-examples/non-streaming-tts/main.go b/go-api-examples/non-streaming-tts/main.go
index 05e8a9e3..978ea6de 100644
--- a/go-api-examples/non-streaming-tts/main.go
+++ b/go-api-examples/non-streaming-tts/main.go
@@ -50,10 +50,10 @@ func main() {
 	flag.Float32Var(&config.Model.Kitten.LengthScale, "kitten-length-scale", 1.0, "length_scale for kitten. small -> faster; large -> slower")
 
 	flag.StringVar(&config.Model.Zipvoice.Tokens, "zipvoice-tokens", "", "Path to tokens.txt for ZipVoice")
-	flag.StringVar(&config.Model.Zipvoice.TextModel, "zipvoice-text-model", "", "Path to ZipVoice text encoder model")
-	flag.StringVar(&config.Model.Zipvoice.FlowMatchingModel, "zipvoice-flow-matching-model", "", "Path to ZipVoice flow-matching decoder")
+	flag.StringVar(&config.Model.Zipvoice.Encoder, "zipvoice-encoder", "", "Path to ZipVoice text encoder model")
+	flag.StringVar(&config.Model.Zipvoice.Decoder, "zipvoice-decoder", "", "Path to ZipVoice flow-matching decoder")
 	flag.StringVar(&config.Model.Zipvoice.DataDir, "zipvoice-data-dir", "", "Path to espeak-ng-data")
-	flag.StringVar(&config.Model.Zipvoice.PinyinDict, "zipvoice-pinyin-dict", "", "Path to pinyin.raw (for zh)")
+	flag.StringVar(&config.Model.Zipvoice.Lexicon, "zipvoice-lexicon", "", "Path to lexicon.txt (for zh)")
 	flag.StringVar(&config.Model.Zipvoice.Vocoder, "zipvoice-vocoder", "", "Path to vocoder (e.g., vocos_24khz.onnx)")
 
 	flag.Float32Var(&config.Model.Zipvoice.FeatScale, "zipvoice-feat-scale", 0.1, "Feature scale for ZipVoice")
diff --git a/go-api-examples/non-streaming-tts/run-zipvoice.sh b/go-api-examples/non-streaming-tts/run-zipvoice.sh
index 43493e5e..4b0125d7 100755
--- a/go-api-examples/non-streaming-tts/run-zipvoice.sh
+++ b/go-api-examples/non-streaming-tts/run-zipvoice.sh
@@ -3,26 +3,30 @@
 set -ex
 
 # to download more models
-if [ ! -f ./sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx ]; then
-  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
-  tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
-  rm sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+if [ ! -f ./sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+  tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+  rm sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+fi
+
+if [ ! -f vocos_24khz.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
 fi
 
 go mod tidy
 go build
 
 ./non-streaming-tts \
-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
-  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
-  --zipvoice-pinyin-dict sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
-  --zipvoice-tokens sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
-  --zipvoice-vocoder sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
-  --prompt-audio sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
+  --zipvoice-encoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
+  --zipvoice-decoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
+  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
+  --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
+  --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
+  --zipvoice-vocoder ./vocos_24khz.onnx \
+  --prompt-audio sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
   --zipvoice-num-steps 4 \
   --num-threads 4 \
   --output-filename=./test-zipvoice.wav \
-  --prompt-text "" \
-  ""
+  --prompt-text ", . ." \
+  ", . . , ."
 
diff --git a/python-api-examples/offline-zeroshot-tts.py b/python-api-examples/offline-zeroshot-tts.py
index 746fbb3d..db2d7b3c 100755
--- a/python-api-examples/offline-zeroshot-tts.py
+++ b/python-api-examples/offline-zeroshot-tts.py
@@ -16,8 +16,8 @@ tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
 wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
 
 python3 ./python-api-examples/offline-zeroshot-tts.py \
-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
+  --zipvoice-encoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
+  --zipvoice-decoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
   --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
   --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
   --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
@@ -49,17 +49,17 @@ def add_zipvoice_args(parser):
     )
 
     parser.add_argument(
-        "--zipvoice-text-model",
+        "--zipvoice-encoder",
         type=str,
         default="",
-        help="Path to zipvoice text model.",
+        help="Path to zipvoice text encoder model.",
     )
 
     parser.add_argument(
-        "--zipvoice-flow-matching-model",
+        "--zipvoice-decoder",
         type=str,
         default="",
-        help="Path to zipvoice flow matching model.",
+        help="Path to zipvoice flow matching decoder model.",
     )
 
     parser.add_argument(
@@ -235,8 +235,8 @@ def main():
         model=sherpa_onnx.OfflineTtsModelConfig(
             zipvoice=sherpa_onnx.OfflineTtsZipvoiceModelConfig(
                 tokens=args.zipvoice_tokens,
-                text_model=args.zipvoice_text_model,
-                flow_matching_model=args.zipvoice_flow_matching_model,
+                encoder=args.zipvoice_encoder,
+                decoder=args.zipvoice_decoder,
                 data_dir=args.zipvoice_data_dir,
                 lexicon=args.zipvoice_lexicon,
                 vocoder=args.zipvoice_vocoder,
diff --git a/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs b/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
index 96d226ac..82ddeb5b 100644
--- a/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
+++ b/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
@@ -10,11 +10,11 @@ namespace SherpaOnnx
         public OfflineTtsZipVoiceModelConfig()
         {
             Tokens = "";
-            TextModel = "";
-            FlowMatchingModel = "";
+            Encoder = "";
+            Decoder = "";
             Vocoder = "";
             DataDir = "";
-            PinyinDict = "";
+            Lexicon = "";
 
             FeatScale = 0.1F;
             Tshift = 0.5F;
@@ -25,10 +25,10 @@ namespace SherpaOnnx
         public string Tokens;
 
         [MarshalAs(UnmanagedType.LPStr)]
-        public string TextModel;
+        public string Encoder;
 
         [MarshalAs(UnmanagedType.LPStr)]
-        public string FlowMatchingModel;
+        public string Decoder;
 
         [MarshalAs(UnmanagedType.LPStr)]
         public string Vocoder;
@@ -37,7 +37,7 @@ namespace SherpaOnnx
         public string DataDir;
 
         [MarshalAs(UnmanagedType.LPStr)]
-        public string PinyinDict;
+        public string Lexicon;
 
         public float FeatScale;
         public float Tshift;
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index e91fa835..3e754f06 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -960,12 +960,12 @@ type OfflineTtsKittenModelConfig struct {
 }
 
 type OfflineTtsZipvoiceModelConfig struct {
-	Tokens            string // Path to tokens.txt for ZipVoice
-	TextModel         string // Path to text encoder (e.g. text_encoder.onnx)
-	FlowMatchingModel string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
-	DataDir           string // Path to espeak-ng-data
-	Lexicon           string // Path to lexicon.txt (needed for zh)
-	Vocoder           string // Path to vocoder (e.g. vocos_24khz.onnx)
+	Tokens  string // Path to tokens.txt for ZipVoice
+	Encoder string // Path to text encoder (e.g. encoder.onnx)
+	Decoder string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
+	DataDir string // Path to espeak-ng-data
+	Lexicon string // Path to lexicon.txt (needed for zh)
+	Vocoder string // Path to vocoder (e.g. vocos_24khz.onnx)
 
 	FeatScale     float32 // Feature scale
 	TShift        float32 // t-shift (<1 shifts to smaller t)
@@ -1136,11 +1136,11 @@ func NewOfflineTts(config *OfflineTtsConfig) *OfflineTts {
 	c.model.zipvoice.tokens = C.CString(config.Model.Zipvoice.Tokens)
 	defer C.free(unsafe.Pointer(c.model.zipvoice.tokens))
 
-	c.model.zipvoice.text_model = C.CString(config.Model.Zipvoice.TextModel)
-	defer C.free(unsafe.Pointer(c.model.zipvoice.text_model))
+	c.model.zipvoice.encoder = C.CString(config.Model.Zipvoice.Encoder)
+	defer C.free(unsafe.Pointer(c.model.zipvoice.encoder))
 
-	c.model.zipvoice.flow_matching_model = C.CString(config.Model.Zipvoice.FlowMatchingModel)
-	defer C.free(unsafe.Pointer(c.model.zipvoice.flow_matching_model))
+	c.model.zipvoice.decoder = C.CString(config.Model.Zipvoice.Decoder)
+	defer C.free(unsafe.Pointer(c.model.zipvoice.decoder))
 
 	c.model.zipvoice.vocoder = C.CString(config.Model.Zipvoice.Vocoder)
 	defer C.free(unsafe.Pointer(c.model.zipvoice.vocoder))
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index bfb30355..c00286ac 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -1242,10 +1242,10 @@ static sherpa_onnx::OfflineTtsConfig GetOfflineTtsConfig(
   // zipvoice
   tts_config.model.zipvoice.tokens =
       SHERPA_ONNX_OR(config->model.zipvoice.tokens, "");
-  tts_config.model.zipvoice.text_model =
-      SHERPA_ONNX_OR(config->model.zipvoice.text_model, "");
-  tts_config.model.zipvoice.flow_matching_model =
-      SHERPA_ONNX_OR(config->model.zipvoice.flow_matching_model, "");
+  tts_config.model.zipvoice.encoder =
+      SHERPA_ONNX_OR(config->model.zipvoice.encoder, "");
+  tts_config.model.zipvoice.decoder =
+      SHERPA_ONNX_OR(config->model.zipvoice.decoder, "");
   tts_config.model.zipvoice.vocoder =
       SHERPA_ONNX_OR(config->model.zipvoice.vocoder, "");
   tts_config.model.zipvoice.data_dir =
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 38d99c5a..f622c26d 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -1068,8 +1068,8 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsKittenModelConfig {
 
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsZipvoiceModelConfig {
   const char *tokens;
-  const char *text_model;
-  const char *flow_matching_model;
+  const char *encoder;
+  const char *decoder;
   const char *vocoder;
   const char *data_dir;
   const char *lexicon;
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index f1f2feec..25bb9f36 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -414,9 +414,8 @@ OfflineTts OfflineTts::Create(const OfflineTtsConfig &config) {
   c.model.kitten.length_scale = config.model.kitten.length_scale;
 
   c.model.zipvoice.tokens = config.model.zipvoice.tokens.c_str();
-  c.model.zipvoice.text_model = config.model.zipvoice.text_model.c_str();
-  c.model.zipvoice.flow_matching_model =
-      config.model.zipvoice.flow_matching_model.c_str();
+  c.model.zipvoice.encoder = config.model.zipvoice.encoder.c_str();
+  c.model.zipvoice.decoder = config.model.zipvoice.decoder.c_str();
   c.model.zipvoice.vocoder = config.model.zipvoice.vocoder.c_str();
   c.model.zipvoice.data_dir = config.model.zipvoice.data_dir.c_str();
   c.model.zipvoice.lexicon = config.model.zipvoice.lexicon.c_str();
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index f7144c50..7f46946c 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -428,8 +428,8 @@ struct OfflineTtsKittenModelConfig {
 
 struct OfflineTtsZipvoiceModelConfig {
   std::string tokens;
-  std::string text_model;
-  std::string flow_matching_model;
+  std::string encoder;
+  std::string decoder;
   std::string vocoder;
   std::string data_dir;
   std::string lexicon;
diff --git a/sherpa-onnx/csrc/offline-tts-impl.cc b/sherpa-onnx/csrc/offline-tts-impl.cc
index 4ebf3b1d..2c94ae59 100644
--- a/sherpa-onnx/csrc/offline-tts-impl.cc
+++ b/sherpa-onnx/csrc/offline-tts-impl.cc
@@ -42,8 +42,8 @@ std::unique_ptr<OfflineTtsImpl> OfflineTtsImpl::Create(
     return std::make_unique<OfflineTtsVitsImpl>(config);
   } else if (!config.model.matcha.acoustic_model.empty()) {
     return std::make_unique<OfflineTtsMatchaImpl>(config);
-  } else if (!config.model.zipvoice.text_model.empty() &&
-             !config.model.zipvoice.flow_matching_model.empty()) {
+  } else if (!config.model.zipvoice.encoder.empty() &&
+             !config.model.zipvoice.decoder.empty()) {
     return std::make_unique<OfflineTtsZipvoiceImpl>(config);
   } else if (!config.model.kokoro.model.empty()) {
     return std::make_unique<OfflineTtsKokoroImpl>(config);
@@ -63,8 +63,8 @@ std::unique_ptr<OfflineTtsImpl> OfflineTtsImpl::Create(
     return std::make_unique<OfflineTtsVitsImpl>(mgr, config);
   } else if (!config.model.matcha.acoustic_model.empty()) {
     return std::make_unique<OfflineTtsMatchaImpl>(mgr, config);
-  } else if (!config.model.zipvoice.text_model.empty() &&
-             !config.model.zipvoice.flow_matching_model.empty()) {
+  } else if (!config.model.zipvoice.encoder.empty() &&
+             !config.model.zipvoice.decoder.empty()) {
     return std::make_unique<OfflineTtsZipvoiceImpl>(mgr, config);
   } else if (!config.model.kokoro.model.empty()) {
     return std::make_unique<OfflineTtsKokoroImpl>(mgr, config);
diff --git a/sherpa-onnx/csrc/offline-tts-model-config.cc b/sherpa-onnx/csrc/offline-tts-model-config.cc
index df7bf06f..176560ad 100644
--- a/sherpa-onnx/csrc/offline-tts-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-model-config.cc
@@ -41,7 +41,7 @@ bool OfflineTtsModelConfig::Validate() const {
     return matcha.Validate();
   }
 
-  if (!zipvoice.flow_matching_model.empty()) {
+  if (!zipvoice.decoder.empty()) {
     return zipvoice.Validate();
   }
 
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
index 70de8667..712590ff 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
@@ -18,10 +18,9 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
   po->Register("zipvoice-data-dir", &data_dir,
                "Path to the directory containing dict for espeak-ng.");
   po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
-  po->Register("zipvoice-text-model", &text_model,
-               "Path to zipvoice text model");
-  po->Register("zipvoice-flow-matching-model", &flow_matching_model,
-               "Path to zipvoice flow-matching model, i.e., the decoder model");
+  po->Register("zipvoice-encoder", &encoder, "Path to zipvoice text model");
+  po->Register("zipvoice-decoder", &decoder,
+               "Path to zipvoice flow-matching decoder model");
   po->Register("zipvoice-vocoder", &vocoder, "Path to zipvoice vocoder");
   po->Register("zipvoice-feat-scale", &feat_scale,
                "Feature scale for ZipVoice (default: 0.1)");
@@ -46,23 +45,23 @@ bool OfflineTtsZipvoiceModelConfig::Validate() const {
     return false;
   }
 
-  if (text_model.empty()) {
-    SHERPA_ONNX_LOGE("Please provide --zipvoice-text-model");
+  if (encoder.empty()) {
+    SHERPA_ONNX_LOGE("Please provide --zipvoice-encoder");
     return false;
   }
-  if (!FileExists(text_model)) {
-    SHERPA_ONNX_LOGE("--zipvoice-text-model: '%s' does not exist",
-                     text_model.c_str());
+  if (!FileExists(encoder)) {
+    SHERPA_ONNX_LOGE("--zipvoice-encoder: '%s' does not exist",
+                     encoder.c_str());
     return false;
   }
 
-  if (flow_matching_model.empty()) {
-    SHERPA_ONNX_LOGE("Please provide --zipvoice-flow-matching-model");
+  if (decoder.empty()) {
+    SHERPA_ONNX_LOGE("Please provide --zipvoice-decoder");
     return false;
   }
-  if (!FileExists(flow_matching_model)) {
-    SHERPA_ONNX_LOGE("--zipvoice-flow-matching-model: '%s' does not exist",
-                     flow_matching_model.c_str());
+  if (!FileExists(decoder)) {
+    SHERPA_ONNX_LOGE("--zipvoice-decoder: '%s' does not exist",
+                     decoder.c_str());
     return false;
   }
 
@@ -126,8 +125,8 @@ std::string OfflineTtsZipvoiceModelConfig::ToString() const {
 
   os << "OfflineTtsZipvoiceModelConfig(";
   os << "tokens=\"" << tokens << "\", ";
-  os << "text_model=\"" << text_model << "\", ";
-  os << "flow_matching_model=\"" << flow_matching_model << "\", ";
+  os << "encoder=\"" << encoder << "\", ";
+  os << "decoder=\"" << decoder << "\", ";
   os << "vocoder=\"" << vocoder << "\", ";
   os << "data_dir=\"" << data_dir << "\", ";
   os << "lexicon=\"" << lexicon << "\", ";
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
index 702760d0..eb837ece 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
@@ -14,8 +14,8 @@ namespace sherpa_onnx {
 
 struct OfflineTtsZipvoiceModelConfig {
   std::string tokens;
-  std::string text_model;
-  std::string flow_matching_model;  // decoder
+  std::string encoder;
+  std::string decoder;
   std::string vocoder;
 
   std::string data_dir;
@@ -29,14 +29,14 @@ struct OfflineTtsZipvoiceModelConfig {
   OfflineTtsZipvoiceModelConfig() = default;
 
   OfflineTtsZipvoiceModelConfig(
-      const std::string &tokens, const std::string &text_model,
-      const std::string &flow_matching_model, const std::string &vocoder,
+      const std::string &tokens, const std::string &encoder,
+      const std::string &decoder, const std::string &vocoder,
       const std::string &data_dir, const std::string &lexicon,
       float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
       float guidance_scale = 1.0)
       : tokens(tokens),
-        text_model(text_model),
-        flow_matching_model(flow_matching_model),
+        encoder(encoder),
+        decoder(decoder),
         vocoder(vocoder),
         data_dir(data_dir),
         lexicon(lexicon),
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
index 8b4d321b..cdd8bc86 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
@@ -37,8 +37,8 @@ class OfflineTtsZipvoiceModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    auto text_buf = ReadFile(config.zipvoice.text_model);
-    auto fm_buf = ReadFile(config.zipvoice.flow_matching_model);
+    auto text_buf = ReadFile(config.zipvoice.encoder);
+    auto fm_buf = ReadFile(config.zipvoice.decoder);
     Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
   }
 
@@ -48,8 +48,8 @@ class OfflineTtsZipvoiceModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    auto text_buf = ReadFile(mgr, config.zipvoice.text_model);
-    auto fm_buf = ReadFile(mgr, config.zipvoice.flow_matching_model);
+    auto text_buf = ReadFile(mgr, config.zipvoice.encoder);
+    auto fm_buf = ReadFile(mgr, config.zipvoice.decoder);
     Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
   }
 
@@ -90,7 +90,7 @@ class OfflineTtsZipvoiceModel::Impl {
     text_inputs.push_back(std::move(prompt_feat_len_tensor));
     text_inputs.push_back(std::move(speed_tensor));
 
-    // forward text-encoder
+    // forward encoder
     auto text_out =
         text_sess_->Run({}, text_input_names_ptr_.data(), text_inputs.data(),
                         text_inputs.size(), text_output_names_ptr_.data(),
@@ -191,11 +191,11 @@ class OfflineTtsZipvoiceModel::Impl {
   }
 
  private:
-  void Init(void *text_model_data, size_t text_model_data_length,
-            void *fm_model_data, size_t fm_model_data_length) {
-    // Init text-encoder model
+  void Init(void *encoder_data, size_t encoder_data_length, void *fm_model_data,
+            size_t fm_model_data_length) {
+    // Init encoder model
     text_sess_ = std::make_unique<Ort::Session>(
-        env_, text_model_data, text_model_data_length, sess_opts_);
+        env_, encoder_data, encoder_data_length, sess_opts_);
     GetInputNames(text_sess_.get(), &text_input_names_, &text_input_names_ptr_);
     GetOutputNames(text_sess_.get(), &text_output_names_,
                    &text_output_names_ptr_);
@@ -231,7 +231,7 @@ class OfflineTtsZipvoiceModel::Impl {
     if (config_.debug) {
       std::ostringstream os;
 
-      os << "---zipvoice text-encoder model---\n";
+      os << "---encoder---\n";
       Ort::ModelMetadata text_meta_data = text_sess_->GetModelMetadata();
       PrintModelMetadata(os, text_meta_data);
 
@@ -248,7 +248,7 @@ class OfflineTtsZipvoiceModel::Impl {
         ++i;
       }
 
-      os << "---zipvoice flow-matching model---\n";
+      os << "---decoder---\n";
       PrintModelMetadata(os, meta_data);
 
       os << "----------input names----------\n";
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
index ad2e3352..67ea1b19 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
@@ -25,21 +25,23 @@ Offline/Non-streaming zero-shot text-to-speech with sherpa-onnx
 
 Usage example:
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
-tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
 
 ./bin/sherpa-onnx-offline-zeroshot-tts \
-  --zipvoice-flow-matching-model=sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
-  --zipvoice-text-model=sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
-  --zipvoice-data-dir=sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
-  --zipvoice-pinyin-dict=sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
-  --zipvoice-tokens=sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
-  --zipvoice-vocoder=sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
-  --prompt-audio=sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
+  --zipvoice-encoder=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
+  --zipvoice-decoder=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
+  --zipvoice-data-dir=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
+  --zipvoice-lexicon=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
+  --zipvoice-tokens=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
+  --zipvoice-vocoder=./vocos_24khz.onnx \
+  --prompt-audio=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
   --num-steps=4 \
   --num-threads=4 \
-  --prompt-text="" \
-  ""
+  --prompt-text=", . ." \
+  ", . . , ."
 
 It will generate a file ./generated.wav as specified by --output-filename.
 )usage";
diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
index 5d86b3c9..257aa30e 100644
--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
+++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
@@ -103,11 +103,11 @@ type
 
   TSherpaOnnxOfflineTtsZipVoiceModelConfig = record
     Tokens: AnsiString;
-    TextModel: AnsiString;
-    FlowMatchingModel: AnsiString;
+    Encoder: AnsiString;
+    Decoder: AnsiString;
     Vocoder: AnsiString;
     DataDir: AnsiString;
-    PinyinDict: AnsiString;
+    Lexicon: AnsiString;
     FeatScale: Single;
     Tshift: Single;
     TargetRms: Single;
@@ -983,11 +983,11 @@ type
 
   SherpaOnnxOfflineTtsZipVoiceModelConfig = record
     Tokens: PAnsiChar;
-    TextModel: PAnsiChar;
-    FlowMatchingModel: PAnsiChar;
+    Encoder: PAnsiChar;
+    Decoder: PAnsiChar;
     Vocoder: PAnsiChar;
     DataDir: PAnsiChar;
-    PinyinDict: PAnsiChar;
+    Lexicon: PAnsiChar;
     FeatScale: cfloat;
     Tshift: cfloat;
     TargetRms: cfloat;
@@ -2423,18 +2423,18 @@ function TSherpaOnnxOfflineTtsZipVoiceModelConfig.ToString: AnsiString;
 begin
   Result := Format('TSherpaOnnxOfflineTtsZipVoiceModelConfig(' +
     'Tokens := %s, ' +
-    'TextModel := %s, ' +
-    'FlowMatchingModel := %s, ' +
+    'Encoder := %s, ' +
+    'Decoder := %s, ' +
     'Vocoder := %s, ' +
     'DataDir := %s, ' +
-    'PinyinDict := %s, ' +
+    'Lexicon := %s, ' +
     'FeatScale := %.2f, ' +
     'Tshift := %.2f, ' +
     'TargetRms := %.2f, ' +
     'GuidanceScale := %.2f' +
     ')',
-    [Self.Tokens, Self.TextModel, Self.FlowMatchingModel, Self.Vocoder,
-     Self.DataDir, Self.PinyinDict, Self.FeatScale, Self.Tshift,
+    [Self.Tokens, Self.Encoder, Self.Decoder, Self.Vocoder,
+     Self.DataDir, Self.Lexicon, Self.FeatScale, Self.Tshift,
      Self.TargetRms, Self.GuidanceScale]);
 end;
 
@@ -2528,11 +2528,11 @@ begin
   C.Model.Kitten.LengthScale := Config.Model.Kitten.LengthScale;
 
   C.Model.ZipVoice.Tokens := PAnsiChar(Config.Model.ZipVoice.Tokens);
-  C.Model.ZipVoice.TextModel := PAnsiChar(Config.Model.ZipVoice.TextModel);
-  C.Model.ZipVoice.FlowMatchingModel := PAnsiChar(Config.Model.ZipVoice.FlowMatchingModel);
+  C.Model.ZipVoice.Encoder := PAnsiChar(Config.Model.ZipVoice.Encoder);
+  C.Model.ZipVoice.Decoder := PAnsiChar(Config.Model.ZipVoice.Decoder);
   C.Model.ZipVoice.Vocoder := PAnsiChar(Config.Model.ZipVoice.Vocoder);
   C.Model.ZipVoice.DataDir := PAnsiChar(Config.Model.ZipVoice.DataDir);
-  C.Model.ZipVoice.PinyinDict := PAnsiChar(Config.Model.ZipVoice.PinyinDict);
+  C.Model.ZipVoice.Lexicon := PAnsiChar(Config.Model.ZipVoice.Lexicon);
   C.Model.ZipVoice.FeatScale := Config.Model.ZipVoice.FeatScale;
   C.Model.ZipVoice.Tshift := Config.Model.ZipVoice.Tshift;
   C.Model.ZipVoice.TargetRms := Config.Model.ZipVoice.TargetRms;
diff --git a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
index dc6e8fb2..55bb5ade 100644
--- a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
@@ -19,14 +19,14 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
                     const std::string &, const std::string &,
                     const std::string &, const std::string &, float, float,
                     float, float>(),
-           py::arg("tokens"), py::arg("text_model"),
-           py::arg("flow_matching_model"), py::arg("vocoder"),
-           py::arg("data_dir") = "", py::arg("lexicon") = "",
-           py::arg("feat_scale") = 0.1, py::arg("t_shift") = 0.5,
-           py::arg("target_rms") = 0.1, py::arg("guidance_scale") = 1.0)
+           py::arg("tokens"), py::arg("encoder"), py::arg("decoder"),
+           py::arg("vocoder"), py::arg("data_dir") = "",
+           py::arg("lexicon") = "", py::arg("feat_scale") = 0.1,
+           py::arg("t_shift") = 0.5, py::arg("target_rms") = 0.1,
+           py::arg("guidance_scale") = 1.0)
       .def_readwrite("tokens", &PyClass::tokens)
-      .def_readwrite("text_model", &PyClass::text_model)
-      .def_readwrite("flow_matching_model", &PyClass::flow_matching_model)
+      .def_readwrite("encoder", &PyClass::encoder)
+      .def_readwrite("decoder", &PyClass::decoder)
       .def_readwrite("vocoder", &PyClass::vocoder)
       .def_readwrite("data_dir", &PyClass::data_dir)
       .def_readwrite("lexicon", &PyClass::lexicon)
diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index ee40fbde..1b1ca137 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -929,8 +929,8 @@ func sherpaOnnxOfflineTtsKittenModelConfig(
 
 func sherpaOnnxOfflineTtsZipvoiceModelConfig(
   tokens: String = "",
-  textModel: String = "",
-  flowMatchingModel: String = "",
+  encoder: String = "",
+  decoder: String = "",
   vocoder: String = "",
   dataDir: String = "",
   lexicon: String = "",
@@ -941,8 +941,8 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
 ) -> SherpaOnnxOfflineTtsZipvoiceModelConfig {
   return SherpaOnnxOfflineTtsZipvoiceModelConfig(
     tokens: toCPointer(tokens),
-    text_model: toCPointer(textModel),
-    flow_matching_model: toCPointer(flowMatchingModel),
+    encoder: toCPointer(encoder),
+    decoder: toCPointer(decoder),
     vocoder: toCPointer(vocoder),
     data_dir: toCPointer(dataDir),
     lexicon: toCPointer(lexicon),
diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
index 2c1b0c36..2234d1b6 100644
--- a/wasm/tts/sherpa-onnx-tts.js
+++ b/wasm/tts/sherpa-onnx-tts.js
@@ -264,15 +264,14 @@ function initSherpaOnnxOfflineTtsKittenModelConfig(config, Module) {
 
 function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   const tokensLen = Module.lengthBytesUTF8(config.tokens || '') + 1;
-  const textModelLen = Module.lengthBytesUTF8(config.textModel || '') + 1;
-  const flowMatchingModelLen =
-      Module.lengthBytesUTF8(config.flowMatchingModel || '') + 1;
+  const encoderLen = Module.lengthBytesUTF8(config.encoder || '') + 1;
+  const decoderLen = Module.lengthBytesUTF8(config.decoder || '') + 1;
   const vocoderLen = Module.lengthBytesUTF8(config.vocoder || '') + 1;
   const dataDirLen = Module.lengthBytesUTF8(config.dataDir || '') + 1;
   const lexiconLen = Module.lengthBytesUTF8(config.lexicon || '') + 1;
 
-  const n = tokensLen + textModelLen + flowMatchingModelLen + vocoderLen +
-      dataDirLen + lexiconLen;
+  const n = tokensLen + encoderLen + decoderLen + vocoderLen + dataDirLen +
+      lexiconLen;
 
   const buffer = Module._malloc(n);
 
@@ -283,12 +282,11 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   Module.stringToUTF8(config.tokens || '', buffer + offset, tokensLen);
   offset += tokensLen;
 
-  Module.stringToUTF8(config.textModel || '', buffer + offset, textModelLen);
-  offset += textModelLen;
+  Module.stringToUTF8(config.encoder || '', buffer + offset, encoderLen);
+  offset += encoderLen;
 
-  Module.stringToUTF8(
-      config.flowMatchingModel || '', buffer + offset, flowMatchingModelLen);
-  offset += flowMatchingModelLen;
+  Module.stringToUTF8(config.decoder || '', buffer + offset, decoderLen);
+  offset += decoderLen;
 
   Module.stringToUTF8(config.vocoder || '', buffer + offset, vocoderLen);
   offset += vocoderLen;
@@ -304,10 +302,10 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   offset += tokensLen;
 
   Module.setValue(ptr + 4, buffer + offset, 'i8*');
-  offset += textModelLen;
+  offset += encoderLen;
 
   Module.setValue(ptr + 8, buffer + offset, 'i8*');
-  offset += flowMatchingModelLen;
+  offset += decoderLen;
 
   Module.setValue(ptr + 12, buffer + offset, 'i8*');
   offset += vocoderLen;
@@ -377,8 +375,8 @@ function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
   if (!('offlineTtsZipVoiceModelConfig' in config)) {
     config.offlineTtsZipVoiceModelConfig = {
       tokens: '',
-      textModel: '',
-      flowMatchingModel: '',
+      encoder: '',
+      decoder: '',
       vocoder: '',
       dataDir: '',
       lexicon: '',
diff --git a/wasm/tts/sherpa-onnx-wasm-main-tts.cc b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
index 99deb46c..09e06d69 100644
--- a/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+++ b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
@@ -75,8 +75,8 @@ void MyPrint(SherpaOnnxOfflineTtsConfig *tts_config) {
 
   fprintf(stdout, "----------zipvoice model config----------\n");
   fprintf(stdout, "tokens: %s\n", zipvoice->tokens);
-  fprintf(stdout, "text_model: %s\n", zipvoice->text_model);
-  fprintf(stdout, "flow_matching_model: %s\n", zipvoice->flow_matching_model);
+  fprintf(stdout, "encoder: %s\n", zipvoice->encoder);
+  fprintf(stdout, "decoder: %s\n", zipvoice->decoder);
   fprintf(stdout, "vocoder: %s\n", zipvoice->vocoder);
   fprintf(stdout, "data_dir: %s\n", zipvoice->data_dir);
   fprintf(stdout, "lexicon: %s\n", zipvoice->lexicon);

commit 1e4ac57888414f1cfbf6ce5fd46f582a0b74e55a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 11 19:15:53 2025 +0800

    Fix building errors (#2893)
    
    This pull request addresses and resolves building errors by refactoring the OfflineTtsZipVoiceModelConfig across various language bindings. The core change involves renaming the pinyinDict parameter to lexicon to standardize terminology and improve clarity in text-to-speech configurations. Additionally, the Python example script for offline zero-shot TTS has been updated to align with these changes, including new model paths and example content, ensuring the examples remain functional and up-to-date.

diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
index 8ce77797..2ad7c6b2 100644
--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
@@ -220,7 +220,7 @@ final class SherpaOnnxOfflineTtsZipVoiceModelConfig extends Struct {
   external Pointer<Utf8> flowMatchingModel;
   external Pointer<Utf8> vocoder;
   external Pointer<Utf8> dataDir;
-  external Pointer<Utf8> pinyinDict;
+  external Pointer<Utf8> lexicon;
 
   @Float()
   external double featScale;
diff --git a/flutter/sherpa_onnx/lib/src/tts.dart b/flutter/sherpa_onnx/lib/src/tts.dart
index be3fce65..dc1b1a8a 100644
--- a/flutter/sherpa_onnx/lib/src/tts.dart
+++ b/flutter/sherpa_onnx/lib/src/tts.dart
@@ -199,7 +199,7 @@ class OfflineTtsZipVoiceModelConfig {
     this.flowMatchingModel = '',
     this.vocoder = '',
     this.dataDir = '',
-    this.pinyinDict = '',
+    this.lexicon = '',
     this.featScale = 0.1,
     this.tShift = 0.5,
     this.targetRms = 0.1,
@@ -213,7 +213,7 @@ class OfflineTtsZipVoiceModelConfig {
       flowMatchingModel: json['flowMatchingModel'] as String? ?? '',
       vocoder: json['vocoder'] as String? ?? '',
       dataDir: json['dataDir'] as String? ?? '',
-      pinyinDict: json['pinyinDict'] as String? ?? '',
+      lexicon: json['lexicon'] as String? ?? '',
       featScale: (json['featScale'] as num?)?.toDouble() ?? 0.1,
       tShift: (json['tShift'] as num?)?.toDouble() ?? 0.5,
       targetRms: (json['targetRms'] as num?)?.toDouble() ?? 0.1,
@@ -223,7 +223,7 @@ class OfflineTtsZipVoiceModelConfig {
 
   @override
   String toString() {
-    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, pinyinDict: $pinyinDict, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
+    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
   }
 
   Map<String, dynamic> toJson() => {
@@ -232,7 +232,7 @@ class OfflineTtsZipVoiceModelConfig {
         'flowMatchingModel': flowMatchingModel,
         'vocoder': vocoder,
         'dataDir': dataDir,
-        'pinyinDict': pinyinDict,
+        'lexicon': lexicon,
         'featScale': featScale,
         'tShift': tShift,
         'targetRms': targetRms,
@@ -244,7 +244,7 @@ class OfflineTtsZipVoiceModelConfig {
   final String flowMatchingModel;
   final String vocoder;
   final String dataDir;
-  final String pinyinDict;
+  final String lexicon;
   final double featScale;
   final double tShift;
   final double targetRms;
@@ -406,7 +406,7 @@ class OfflineTts {
     c.ref.model.zipvoice.flowMatchingModel = config.model.zipvoice.flowMatchingModel.toNativeUtf8();
     c.ref.model.zipvoice.vocoder = config.model.zipvoice.vocoder.toNativeUtf8();
     c.ref.model.zipvoice.dataDir = config.model.zipvoice.dataDir.toNativeUtf8();
-    c.ref.model.zipvoice.pinyinDict = config.model.zipvoice.pinyinDict.toNativeUtf8();
+    c.ref.model.zipvoice.lexicon = config.model.zipvoice.lexicon.toNativeUtf8();
     c.ref.model.zipvoice.featScale = config.model.zipvoice.featScale;
     c.ref.model.zipvoice.tShift = config.model.zipvoice.tShift;
     c.ref.model.zipvoice.targetRms = config.model.zipvoice.targetRms;
@@ -427,7 +427,7 @@ class OfflineTts {
     calloc.free(c.ref.ruleFsts);
     calloc.free(c.ref.model.provider);
 
-    calloc.free(c.ref.model.zipvoice.pinyinDict);
+    calloc.free(c.ref.model.zipvoice.lexicon);
     calloc.free(c.ref.model.zipvoice.dataDir);
     calloc.free(c.ref.model.zipvoice.vocoder);
     calloc.free(c.ref.model.zipvoice.flowMatchingModel);
diff --git a/python-api-examples/offline-zeroshot-tts.py b/python-api-examples/offline-zeroshot-tts.py
index 805f06fe..746fbb3d 100755
--- a/python-api-examples/offline-zeroshot-tts.py
+++ b/python-api-examples/offline-zeroshot-tts.py
@@ -10,21 +10,23 @@ Usage:
 
 Example (zipvoice)
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
-tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
 
 python3 ./python-api-examples/offline-zeroshot-tts.py \
-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
-  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
-  --zipvoice-pinyin-dict sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
-  --zipvoice-tokens sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
-  --zipvoice-vocoder sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
-  --prompt-audio sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
+  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
+  --zipvoice-text-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
+  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
+  --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
+  --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
+  --zipvoice-vocoder vocos_24khz.onnx \
+  --prompt-audio sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
   --zipvoice-num-steps 4 \
   --num-threads 4 \
-  --prompt-text "" \
-  ""
+  --prompt-text ", . ." \
+  ", . . , ."
 """
 
 import argparse
@@ -68,10 +70,10 @@ def add_zipvoice_args(parser):
     )
 
     parser.add_argument(
-        "--zipvoice-pinyin-dict",
+        "--zipvoice-lexicon",
         type=str,
         default="",
-        help="Path to the pinyin dictionary.",
+        help="Path to the lexicon.txt",
     )
 
     parser.add_argument(
@@ -236,7 +238,7 @@ def main():
                 text_model=args.zipvoice_text_model,
                 flow_matching_model=args.zipvoice_flow_matching_model,
                 data_dir=args.zipvoice_data_dir,
-                pinyin_dict=args.zipvoice_pinyin_dict,
+                lexicon=args.zipvoice_lexicon,
                 vocoder=args.zipvoice_vocoder,
                 feat_scale=args.zipvoice_feat_scale,
                 t_shift=args.zipvoice_t_shift,
@@ -268,9 +270,7 @@ def main():
     end = time.time()
 
     if len(audio.samples) == 0:
-        print(
-            "Error in generating audios. Please read previous error messages."
-        )
+        print("Error in generating audios. Please read previous error messages.")
         return
 
     elapsed_seconds = end - start
@@ -287,9 +287,7 @@ def main():
     print(f"The text is '{args.text}'")
     print(f"Elapsed seconds: {elapsed_seconds:.3f}")
     print(f"Audio duration in seconds: {audio_duration:.3f}")
-    print(
-        f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}"
-    )
+    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
 
 
 if __name__ == "__main__":
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 721db857..e91fa835 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -484,7 +484,7 @@ type OfflineModelConfig struct {
 	ZipformerCtc OfflineZipformerCtcModelConfig
 	Canary       OfflineCanaryModelConfig
 	WenetCtc     OfflineWenetCtcModelConfig
-	Omnilingual     OfflineOmnilingualAsrCtcModelConfig
+	Omnilingual  OfflineOmnilingualAsrCtcModelConfig
 	Tokens       string // Path to tokens.txt
 
 	// Number of threads to use for neural network computation
@@ -964,7 +964,7 @@ type OfflineTtsZipvoiceModelConfig struct {
 	TextModel         string // Path to text encoder (e.g. text_encoder.onnx)
 	FlowMatchingModel string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
 	DataDir           string // Path to espeak-ng-data
-	PinyinDict        string // Path to pinyin.raw (needed for zh)
+	Lexicon           string // Path to lexicon.txt (needed for zh)
 	Vocoder           string // Path to vocoder (e.g. vocos_24khz.onnx)
 
 	FeatScale     float32 // Feature scale
@@ -1148,8 +1148,8 @@ func NewOfflineTts(config *OfflineTtsConfig) *OfflineTts {
 	c.model.zipvoice.data_dir = C.CString(config.Model.Zipvoice.DataDir)
 	defer C.free(unsafe.Pointer(c.model.zipvoice.data_dir))
 
-	c.model.zipvoice.pinyin_dict = C.CString(config.Model.Zipvoice.PinyinDict)
-	defer C.free(unsafe.Pointer(c.model.zipvoice.pinyin_dict))
+	c.model.zipvoice.lexicon = C.CString(config.Model.Zipvoice.Lexicon)
+	defer C.free(unsafe.Pointer(c.model.zipvoice.lexicon))
 
 	c.model.zipvoice.feat_scale = C.float(config.Model.Zipvoice.FeatScale)
 	c.model.zipvoice.t_shift = C.float(config.Model.Zipvoice.TShift)
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 99422a06..bfb30355 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -703,9 +703,11 @@ const SherpaOnnxOfflineRecognizerResult *SherpaOnnxGetOfflineStreamResult(
       r->durations = nullptr;
     }
 
-    if (!result.ys_log_probs.empty() && result.ys_log_probs.size() == r->count) {
+    if (!result.ys_log_probs.empty() &&
+        result.ys_log_probs.size() == r->count) {
       r->ys_log_probs = new float[r->count];
-      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(), r->ys_log_probs);
+      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(),
+                r->ys_log_probs);
     } else {
       r->ys_log_probs = nullptr;
     }
@@ -1248,8 +1250,8 @@ static sherpa_onnx::OfflineTtsConfig GetOfflineTtsConfig(
       SHERPA_ONNX_OR(config->model.zipvoice.vocoder, "");
   tts_config.model.zipvoice.data_dir =
       SHERPA_ONNX_OR(config->model.zipvoice.data_dir, "");
-  tts_config.model.zipvoice.pinyin_dict =
-      SHERPA_ONNX_OR(config->model.zipvoice.pinyin_dict, "");
+  tts_config.model.zipvoice.lexicon =
+      SHERPA_ONNX_OR(config->model.zipvoice.lexicon, "");
   tts_config.model.zipvoice.feat_scale =
       SHERPA_ONNX_OR(config->model.zipvoice.feat_scale, 0.1f);
   tts_config.model.zipvoice.t_shift =
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index e3f7e016..38d99c5a 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -1072,7 +1072,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsZipvoiceModelConfig {
   const char *flow_matching_model;
   const char *vocoder;
   const char *data_dir;
-  const char *pinyin_dict;
+  const char *lexicon;
   float feat_scale;
   float t_shift;
   float target_rms;
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index a7e35c55..f1f2feec 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -419,7 +419,7 @@ OfflineTts OfflineTts::Create(const OfflineTtsConfig &config) {
       config.model.zipvoice.flow_matching_model.c_str();
   c.model.zipvoice.vocoder = config.model.zipvoice.vocoder.c_str();
   c.model.zipvoice.data_dir = config.model.zipvoice.data_dir.c_str();
-  c.model.zipvoice.pinyin_dict = config.model.zipvoice.pinyin_dict.c_str();
+  c.model.zipvoice.lexicon = config.model.zipvoice.lexicon.c_str();
   c.model.zipvoice.feat_scale = config.model.zipvoice.feat_scale;
   c.model.zipvoice.t_shift = config.model.zipvoice.t_shift;
   c.model.zipvoice.target_rms = config.model.zipvoice.target_rms;
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index 77401e67..f7144c50 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -432,7 +432,7 @@ struct OfflineTtsZipvoiceModelConfig {
   std::string flow_matching_model;
   std::string vocoder;
   std::string data_dir;
-  std::string pinyin_dict;
+  std::string lexicon;
 
   float feat_scale = 0.1;
   float t_shift = 0.5;
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
index 1226adf8..70de8667 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
@@ -18,9 +18,6 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
   po->Register("zipvoice-data-dir", &data_dir,
                "Path to the directory containing dict for espeak-ng.");
   po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
-  po->Register("zipvoice-pinyin-dict", &pinyin_dict,
-               "Path to the pinyin dictionary for cppinyin (i.e converting "
-               "Chinese into phones).");
   po->Register("zipvoice-text-model", &text_model,
                "Path to zipvoice text model");
   po->Register("zipvoice-flow-matching-model", &flow_matching_model,
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
index 5b5d0c24..702760d0 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
@@ -21,9 +21,6 @@ struct OfflineTtsZipvoiceModelConfig {
   std::string data_dir;
   std::string lexicon;
 
-  // Used for converting Chinese characters to pinyin
-  std::string pinyin_dict;
-
   float feat_scale = 0.1;
   float t_shift = 0.5;
   float target_rms = 0.1;
@@ -35,15 +32,14 @@ struct OfflineTtsZipvoiceModelConfig {
       const std::string &tokens, const std::string &text_model,
       const std::string &flow_matching_model, const std::string &vocoder,
       const std::string &data_dir, const std::string &lexicon,
-      const std::string &pinyin_dict, float feat_scale = 0.1,
-      float t_shift = 0.5, float target_rms = 0.1, float guidance_scale = 1.0)
+      float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
+      float guidance_scale = 1.0)
       : tokens(tokens),
         text_model(text_model),
         flow_matching_model(flow_matching_model),
         vocoder(vocoder),
         data_dir(data_dir),
         lexicon(lexicon),
-        pinyin_dict(pinyin_dict),
         feat_scale(feat_scale),
         t_shift(t_shift),
         target_rms(target_rms),
diff --git a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
index 53451077..dc6e8fb2 100644
--- a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
@@ -21,7 +21,7 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
                     float, float>(),
            py::arg("tokens"), py::arg("text_model"),
            py::arg("flow_matching_model"), py::arg("vocoder"),
-           py::arg("data_dir") = "", py::arg("pinyin_dict") = "",
+           py::arg("data_dir") = "", py::arg("lexicon") = "",
            py::arg("feat_scale") = 0.1, py::arg("t_shift") = 0.5,
            py::arg("target_rms") = 0.1, py::arg("guidance_scale") = 1.0)
       .def_readwrite("tokens", &PyClass::tokens)
@@ -29,7 +29,7 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
       .def_readwrite("flow_matching_model", &PyClass::flow_matching_model)
       .def_readwrite("vocoder", &PyClass::vocoder)
       .def_readwrite("data_dir", &PyClass::data_dir)
-      .def_readwrite("pinyin_dict", &PyClass::pinyin_dict)
+      .def_readwrite("lexicon", &PyClass::lexicon)
       .def_readwrite("feat_scale", &PyClass::feat_scale)
       .def_readwrite("t_shift", &PyClass::t_shift)
       .def_readwrite("target_rms", &PyClass::target_rms)
diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index b18b8e1a..ee40fbde 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -933,7 +933,7 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
   flowMatchingModel: String = "",
   vocoder: String = "",
   dataDir: String = "",
-  pinyinDict: String = "",
+  lexicon: String = "",
   featScale: Float = 0.1,
   tShift: Float = 0.5,
   targetRms: Float = 0.1,
@@ -945,7 +945,7 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
     flow_matching_model: toCPointer(flowMatchingModel),
     vocoder: toCPointer(vocoder),
     data_dir: toCPointer(dataDir),
-    pinyin_dict: toCPointer(pinyinDict),
+    lexicon: toCPointer(lexicon),
     feat_scale: featScale,
     t_shift: tShift,
     target_rms: targetRms,
diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
index e24bbc3f..2c1b0c36 100644
--- a/wasm/tts/sherpa-onnx-tts.js
+++ b/wasm/tts/sherpa-onnx-tts.js
@@ -269,10 +269,10 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
       Module.lengthBytesUTF8(config.flowMatchingModel || '') + 1;
   const vocoderLen = Module.lengthBytesUTF8(config.vocoder || '') + 1;
   const dataDirLen = Module.lengthBytesUTF8(config.dataDir || '') + 1;
-  const pinyinDictLen = Module.lengthBytesUTF8(config.pinyinDict || '') + 1;
+  const lexiconLen = Module.lengthBytesUTF8(config.lexicon || '') + 1;
 
   const n = tokensLen + textModelLen + flowMatchingModelLen + vocoderLen +
-      dataDirLen + pinyinDictLen;
+      dataDirLen + lexiconLen;
 
   const buffer = Module._malloc(n);
 
@@ -296,8 +296,8 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   Module.stringToUTF8(config.dataDir || '', buffer + offset, dataDirLen);
   offset += dataDirLen;
 
-  Module.stringToUTF8(config.pinyinDict || '', buffer + offset, pinyinDictLen);
-  offset += pinyinDictLen;
+  Module.stringToUTF8(config.lexicon || '', buffer + offset, lexiconLen);
+  offset += lexiconLen;
 
   offset = 0;
   Module.setValue(ptr, buffer + offset, 'i8*');
@@ -316,7 +316,7 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   offset += dataDirLen;
 
   Module.setValue(ptr + 20, buffer + offset, 'i8*');
-  offset += pinyinDictLen;
+  offset += lexiconLen;
 
   Module.setValue(ptr + 24, config.featScale || 0.1, 'float');
   Module.setValue(ptr + 28, config.tShift || 0.5, 'float');
@@ -381,7 +381,7 @@ function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
       flowMatchingModel: '',
       vocoder: '',
       dataDir: '',
-      pinyinDict: '',
+      lexicon: '',
       featScale: 0.1,
       tShift: 0.5,
       targetRMS: 0.1,
diff --git a/wasm/tts/sherpa-onnx-wasm-main-tts.cc b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
index e6707135..99deb46c 100644
--- a/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+++ b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
@@ -79,7 +79,7 @@ void MyPrint(SherpaOnnxOfflineTtsConfig *tts_config) {
   fprintf(stdout, "flow_matching_model: %s\n", zipvoice->flow_matching_model);
   fprintf(stdout, "vocoder: %s\n", zipvoice->vocoder);
   fprintf(stdout, "data_dir: %s\n", zipvoice->data_dir);
-  fprintf(stdout, "pinyin_dict: %s\n", zipvoice->pinyin_dict);
+  fprintf(stdout, "lexicon: %s\n", zipvoice->lexicon);
   fprintf(stdout, "feat scale: %.3f\n", zipvoice->feat_scale);
   fprintf(stdout, "t_shift: %.3f\n", zipvoice->t_shift);
   fprintf(stdout, "target_rms: %.3f\n", zipvoice->target_rms);

commit 1b59efdbd23acbd61242d0d7cb0ff02bab6d3382
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 11 18:57:22 2025 +0800

    Remove cppinyin from zipvoice (#2892)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 84be55ea..57cb8e7f 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -496,8 +496,6 @@ if(SHERPA_ONNX_ENABLE_WEBSOCKET)
 endif()
 
 if(SHERPA_ONNX_ENABLE_TTS)
-  include(cppinyin)
-
   include(espeak-ng-for-piper)
   set(ESPEAK_NG_DIR ${espeak_ng_SOURCE_DIR})
   message(STATUS "ESPEAK_NG_DIR: ${ESPEAK_NG_DIR}")
diff --git a/build-ios.sh b/build-ios.sh
index c2bba819..6e23ea0c 100755
--- a/build-ios.sh
+++ b/build-ios.sh
@@ -127,7 +127,7 @@ cmake --build build/os64 --target install
 echo "Generate xcframework"
 
 mkdir -p "build/simulator/lib"
-for f in libcppinyin_core.a libkaldi-native-fbank-core.a libkissfft-float.a libsherpa-onnx-c-api.a libsherpa-onnx-core.a \
+for f in libkaldi-native-fbank-core.a libkissfft-float.a libsherpa-onnx-c-api.a libsherpa-onnx-core.a \
          libsherpa-onnx-fstfar.a libssentencepiece_core.a \
          libsherpa-onnx-fst.a libsherpa-onnx-kaldifst-core.a libkaldi-decoder-core.a \
          libucd.a libpiper_phonemize.a libespeak-ng.a; do
@@ -139,7 +139,6 @@ done
 # Merge archive first, because the following xcodebuild create xcframework
 # cannot accept multi archive with the same architecture.
 libtool -static -o build/simulator/libsherpa-onnx.a \
-  build/simulator/lib/libcppinyin_core.a \
   build/simulator/lib/libkaldi-native-fbank-core.a \
   build/simulator/lib/libkissfft-float.a \
   build/simulator/lib/libsherpa-onnx-c-api.a \
@@ -154,7 +153,6 @@ libtool -static -o build/simulator/libsherpa-onnx.a \
   build/simulator/lib/libssentencepiece_core.a
 
 libtool -static -o build/os64/libsherpa-onnx.a \
-  build/os64/lib/libcppinyin_core.a \
   build/os64/lib/libkaldi-native-fbank-core.a \
   build/os64/lib/libkissfft-float.a \
   build/os64/lib/libsherpa-onnx-c-api.a \
diff --git a/build-swift-macos.sh b/build-swift-macos.sh
index 8fef5da7..1e1e8e9c 100755
--- a/build-swift-macos.sh
+++ b/build-swift-macos.sh
@@ -27,7 +27,6 @@ make install
 rm -fv ./install/include/cargs.h
 
 libtool -static -o ./install/lib/libsherpa-onnx.a \
-  ./install/lib/libcppinyin_core.a \
   ./install/lib/libsherpa-onnx-c-api.a \
   ./install/lib/libsherpa-onnx-core.a \
   ./install/lib/libkaldi-native-fbank-core.a \
diff --git a/c-api-examples/Makefile b/c-api-examples/Makefile
index 6480e985..cbaaf63c 100644
--- a/c-api-examples/Makefile
+++ b/c-api-examples/Makefile
@@ -4,7 +4,7 @@ CUR_DIR :=$(shell pwd)
 CFLAGS := -I ../ -I ../build/_deps/cargs-src/include/
 LDFLAGS := -L ../build/lib
 LDFLAGS += -L ../build/_deps/onnxruntime-src/lib
-LDFLAGS += -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lcargs -lonnxruntime -lcppinyin_core
+LDFLAGS += -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lcargs -lonnxruntime
 LDFLAGS += -framework Foundation
 LDFLAGS += -lc++
 LDFLAGS += -Wl,-rpath,${CUR_DIR}/../build/lib
diff --git a/cmake/cppinyin.cmake b/cmake/cppinyin.cmake
deleted file mode 100644
index 9e2c92b5..00000000
--- a/cmake/cppinyin.cmake
+++ /dev/null
@@ -1,82 +0,0 @@
-function(download_cppinyin)
-  include(FetchContent)
-
-  set(cppinyin_URL "https://github.com/pkufool/cppinyin/archive/refs/tags/v0.10.tar.gz")
-  set(cppinyin_URL2 "https://gh-proxy.com/https://github.com/pkufool/cppinyin/archive/refs/tags/v0.10.tar.gz")
-  set(cppinyin_HASH "SHA256=abe6584d7ee56829e8f4b5fbda3b50ecdf49a13be8e413a78d1b0d5d5c019982")
-
-  # If you don't have access to the Internet,
-  # please pre-download cppinyin
-  set(possible_file_locations
-    $ENV{HOME}/Downloads/cppinyin-0.10.tar.gz
-    ${CMAKE_SOURCE_DIR}/cppinyin-0.10.tar.gz
-    ${CMAKE_BINARY_DIR}/cppinyin-0.10.tar.gz
-    /tmp/cppinyin-0.10.tar.gz
-    /star-fj/fangjun/download/github/cppinyin-0.10.tar.gz
-  )
-
-  foreach(f IN LISTS possible_file_locations)
-    if(EXISTS ${f})
-      set(cppinyin_URL  "${f}")
-      file(TO_CMAKE_PATH "${cppinyin_URL}" cppinyin_URL)
-      message(STATUS "Found local downloaded cppinyin: ${cppinyin_URL}")
-      set(cppinyin_URL2)
-      break()
-    endif()
-  endforeach()
-
-  set(CPPINYIN_ENABLE_TESTS OFF CACHE BOOL "" FORCE)
-  set(CPPINYIN_BUILD_PYTHON OFF CACHE BOOL "" FORCE)
-
-  FetchContent_Declare(cppinyin
-    URL
-      ${cppinyin_URL}
-      ${cppinyin_URL2}
-    URL_HASH
-      ${cppinyin_HASH}
-  )
-
-  FetchContent_GetProperties(cppinyin)
-  if(NOT cppinyin_POPULATED)
-    message(STATUS "Downloading cppinyin ${cppinyin_URL}")
-    FetchContent_Populate(cppinyin)
-
-    file(REMOVE ${cppinyin_SOURCE_DIR}/CMakeLists.txt)
-    configure_file(
-        ${CMAKE_CURRENT_LIST_DIR}/cppinyin.patch
-        ${cppinyin_SOURCE_DIR}/CMakeLists.txt
-        COPYONLY
-    )
-  endif()
-
-  message(STATUS "cppinyin is downloaded to ${cppinyin_SOURCE_DIR}")
-
-  if(BUILD_SHARED_LIBS)
-    set(_build_shared_libs_bak ${BUILD_SHARED_LIBS})
-    set(BUILD_SHARED_LIBS OFF)
-  endif()
-
-  add_subdirectory(${cppinyin_SOURCE_DIR} ${cppinyin_BINARY_DIR} EXCLUDE_FROM_ALL)
-
-  if(_build_shared_libs_bak)
-    set_target_properties(cppinyin_core
-      PROPERTIES
-        POSITION_INDEPENDENT_CODE ON
-        C_VISIBILITY_PRESET hidden
-        CXX_VISIBILITY_PRESET hidden
-    )
-    set(BUILD_SHARED_LIBS ON)
-  endif()
-
-  target_include_directories(cppinyin_core
-    PUBLIC
-      ${cppinyin_SOURCE_DIR}/
-  )
-
-  if(NOT BUILD_SHARED_LIBS)
-    install(TARGETS cppinyin_core DESTINATION lib)
-  endif()
-
-endfunction()
-
-download_cppinyin()
diff --git a/cmake/cppinyin.patch b/cmake/cppinyin.patch
deleted file mode 100644
index ec146874..00000000
--- a/cmake/cppinyin.patch
+++ /dev/null
@@ -1,81 +0,0 @@
-cmake_minimum_required(VERSION 3.12 FATAL_ERROR)
-
-project(cppinyin)
-
-set(CPPINYIN_VERSION "0.10")
-
-set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/lib")
-set(CMAKE_LIBRARY_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/lib")
-set(CMAKE_RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin")
-
-set(CMAKE_SKIP_BUILD_RPATH FALSE)
-set(BUILD_RPATH_USE_ORIGIN TRUE)
-set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
-
-if(NOT APPLE)
-  set(CPPINYIN_RPATH_ORIGIN "$ORIGIN")
-else()
-  set(CPPINYIN_RPATH_ORIGIN "@loader_path")
-endif()
-
-set(CMAKE_INSTALL_RPATH ${CPPINYIN_RPATH_ORIGIN})
-set(CMAKE_BUILD_RPATH ${CPPINYIN_RPATH_ORIGIN})
-
-option(CPPINYIN_ENABLE_TESTS "Whether to build tests" OFF)
-option(CPPINYIN_BUILD_PYTHON "Whether to build Python" OFF)
-option(BUILD_SHARED_LIBS "Whether to build shared libraries" ON)
-
-if(NOT CMAKE_BUILD_TYPE)
-  message(STATUS "No CMAKE_BUILD_TYPE given, default to Release")
-  set(CMAKE_BUILD_TYPE Release)
-endif()
-
-list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake/Modules)
-list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)
-
-include(CheckCXXCompilerFlag)
-if(NOT WIN32)
-  check_cxx_compiler_flag("-std=c++14" CPPINYIN_COMPILER_SUPPORTS_CXX14)
-else()
-  # windows x86 or x86_64
-  check_cxx_compiler_flag("/std:c++14" CPPINYIN_COMPILER_SUPPORTS_CXX14)
-endif()
-if(NOT CPPINYIN_COMPILER_SUPPORTS_CXX14)
-  message(FATAL_ERROR "
-    cppinyin requires a compiler supporting at least C++14.
-    If you are using GCC, please upgrade it to at least version 7.0.
-    If you are using Clang, please upgrade it to at least version 3.4.")
-endif()
-
-if(NOT CMAKE_CXX_STANDARD)
-  set(CMAKE_CXX_STANDARD 14 CACHE STRING "The C++ version to be used.")
-endif()
-set(CMAKE_CXX_EXTENSIONS OFF)
-message(STATUS "C++ Standard version: ${CMAKE_CXX_STANDARD}")
-
-if(CPPINYIN_BUILD_PYTHON)
-  include(pybind11)
-endif()
-
-include_directories(${CMAKE_SOURCE_DIR})
-
-if(WIN32)
-  # disable various warnings for MSVC
-  # 4244: 'initializing': conversion from 'float' to 'int32_t',
-  # 4267: 'argument': conversion from 'size_t' to 'uint32_t', possible loss of data
-  set(disabled_warnings
-      /wd4244
-      /wd4267
-  )
-  message(STATUS "Disabled warnings: ${disabled_warnings}")
-  foreach(w IN LISTS disabled_warnings)
-    string(APPEND CMAKE_CXX_FLAGS " ${w} ")
-  endforeach()
-endif()
-
-if(CPPINYIN_ENABLE_TESTS)
-  include(googletest)
-  enable_testing()
-endif()
-
-add_subdirectory(cppinyin)
diff --git a/cmake/sherpa-onnx-static.pc.in b/cmake/sherpa-onnx-static.pc.in
index 061905fc..42beb084 100644
--- a/cmake/sherpa-onnx-static.pc.in
+++ b/cmake/sherpa-onnx-static.pc.in
@@ -22,4 +22,4 @@ Cflags: -I"${includedir}"
 # Note: -lcargs is required only for the following file
 # https://github.com/k2-fsa/sherpa-onnx/blob/master/c-api-examples/decode-file-c-api.c
 # We add it here so that users don't need to specify -lcargs when compiling decode-file-c-api.c
-Libs: -L"${libdir}" -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lonnxruntime -lssentencepiece_core -lcppinyin_core -Wl,-rpath,${libdir} @SHERPA_ONNX_PKG_WITH_CARGS@ @SHERPA_ONNX_PKG_CONFIG_EXTRA_LIBS@
+Libs: -L"${libdir}" -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lonnxruntime -lssentencepiece_core -Wl,-rpath,${libdir} @SHERPA_ONNX_PKG_WITH_CARGS@ @SHERPA_ONNX_PKG_CONFIG_EXTRA_LIBS@
diff --git a/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props b/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
index cc0aa014..9b35d475 100644
--- a/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
+++ b/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
@@ -15,7 +15,6 @@
         sherpa-onnx-fst.lib;
         kaldi-native-fbank-core.lib;
         kissfft-float.lib;
-        cppinyin_core.lib;
         onnxruntime.lib;
         piper_phonemize.lib;
         espeak-ng.lib;
diff --git a/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props b/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
index cc0aa014..9b35d475 100644
--- a/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
+++ b/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
@@ -15,7 +15,6 @@
         sherpa-onnx-fst.lib;
         kaldi-native-fbank-core.lib;
         kissfft-float.lib;
-        cppinyin_core.lib;
         onnxruntime.lib;
         piper_phonemize.lib;
         espeak-ng.lib;
diff --git a/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props b/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
index cc0aa014..9b35d475 100644
--- a/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
+++ b/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
@@ -15,7 +15,6 @@
         sherpa-onnx-fst.lib;
         kaldi-native-fbank-core.lib;
         kissfft-float.lib;
-        cppinyin_core.lib;
         onnxruntime.lib;
         piper_phonemize.lib;
         espeak-ng.lib;
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 2a7e7433..d03dd75d 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -261,7 +261,6 @@ if(SHERPA_ONNX_ENABLE_TTS)
     offline-tts-model-config.cc
     offline-tts-vits-model-config.cc
     offline-tts-vits-model.cc
-    offline-tts-zipvoice-frontend.cc
     offline-tts-zipvoice-model-config.cc
     offline-tts-zipvoice-model.cc
     offline-tts.cc
@@ -417,7 +416,6 @@ target_link_libraries(sherpa-onnx-core fstfar fst)
 
 if(SHERPA_ONNX_ENABLE_TTS)
   target_link_libraries(sherpa-onnx-core
-    cppinyin_core
     piper_phonemize)
 endif()
 
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 873918d0..77d47f39 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -123,8 +123,12 @@ std::vector<std::string> SplitTokensUTF8(const std::string &s) {
 }
 
 std::vector<std::string> ProcessPhonemes(
-    const std::vector<std::vector<char32_t>> &phonemes) {
+    const std::vector<std::vector<char32_t>> &phonemes, bool skip_replacement) {
   auto tokens = ConvertPhonemesToUTF8(phonemes);
+  if (skip_replacement) {
+    return tokens;
+  }
+
   std::string joined = Join(tokens);
   std::string replaced = ApplyReplacements(joined);
   return SplitTokensUTF8(replaced);
@@ -139,8 +143,8 @@ void CallPhonemizeEspeak(const std::string &text,
 class MatchaTtsLexicon::Impl {
  public:
   Impl(const std::string &lexicon, const std::string &tokens,
-       const std::string &data_dir, bool debug)
-      : debug_(debug) {
+       const std::string &data_dir, bool debug, bool skip_replacement)
+      : debug_(debug), skip_replacement_(skip_replacement) {
     if (lexicon.empty()) {
       SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
       SHERPA_ONNX_EXIT(-1);
@@ -163,8 +167,8 @@ class MatchaTtsLexicon::Impl {
 
   template <typename Manager>
   Impl(Manager *mgr, const std::string &lexicon, const std::string &tokens,
-       const std::string &data_dir, bool debug)
-      : debug_(debug) {
+       const std::string &data_dir, bool debug, bool skip_replacement)
+      : debug_(debug), skip_replacement_(skip_replacement) {
     if (lexicon.empty()) {
       SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
       SHERPA_ONNX_EXIT(-1);
@@ -360,7 +364,7 @@ class MatchaTtsLexicon::Impl {
         std::vector<std::vector<piper::Phoneme>> phonemes;
         CallPhonemizeEspeak(w, config, &phonemes);
 
-        auto pp = ProcessPhonemes(phonemes);
+        auto pp = ProcessPhonemes(phonemes, skip_replacement_);
 
         for (const auto &p : pp) {
           if (token2id_.count(p)) {
@@ -477,20 +481,25 @@ class MatchaTtsLexicon::Impl {
   std::unordered_map<int32_t, std::string> id2token_;
 
   bool debug_ = false;
+  bool skip_replacement_ = false;
 };  // namespace sherpa_onnx
 
 MatchaTtsLexicon::~MatchaTtsLexicon() = default;
 
 MatchaTtsLexicon::MatchaTtsLexicon(const std::string &lexicon,
                                    const std::string &tokens,
-                                   const std::string &data_dir, bool debug)
-    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug)) {}
+                                   const std::string &data_dir, bool debug,
+                                   bool skip_replacement)
+    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug,
+                                   skip_replacement)) {}
 
 template <typename Manager>
 MatchaTtsLexicon::MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
                                    const std::string &tokens,
-                                   const std::string &data_dir, bool debug)
-    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug)) {}
+                                   const std::string &data_dir, bool debug,
+                                   bool skip_replacement)
+    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug,
+                                   skip_replacement)) {}
 
 std::vector<TokenIDs> MatchaTtsLexicon::ConvertTextToTokenIds(
     const std::string &text, const std::string & /*unused_voice = ""*/) const {
@@ -502,7 +511,7 @@ template MatchaTtsLexicon::MatchaTtsLexicon(AAssetManager *mgr,
                                             const std::string &lexicon,
                                             const std::string &tokens,
                                             const std::string &data_dir,
-                                            bool debug);
+                                            bool debug, bool skip_replacement);
 #endif
 
 #if __OHOS__
@@ -510,7 +519,7 @@ template MatchaTtsLexicon::MatchaTtsLexicon(NativeResourceManager *mgr,
                                             const std::string &lexicon,
                                             const std::string &tokens,
                                             const std::string &data_dir,
-                                            bool debug);
+                                            bool debug, bool skip_replacement);
 #endif
 
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.h b/sherpa-onnx/csrc/matcha-tts-lexicon.h
index f9da31a6..f23df439 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.h
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.h
@@ -20,12 +20,13 @@ class MatchaTtsLexicon : public OfflineTtsFrontend {
   ~MatchaTtsLexicon() override;
 
   MatchaTtsLexicon(const std::string &lexicon, const std::string &tokens,
-                   const std::string &data_dir, bool debug);
+                   const std::string &data_dir, bool debug,
+                   bool skip_replacement);
 
   template <typename Manager>
   MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
                    const std::string &tokens, const std::string &data_dir,
-                   bool debug);
+                   bool debug, bool skip_replacement);
 
   std::vector<TokenIDs> ConvertTextToTokenIds(
       const std::string &text,
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
index 92d7efff..9440e25a 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
@@ -386,7 +386,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
     if (meta_data.is_zh_en) {
       frontend_ = std::make_unique<MatchaTtsLexicon>(
           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
-          config_.model.matcha.data_dir, config_.model.debug);
+          config_.model.matcha.data_dir, config_.model.debug, false);
     } else if (meta_data.jieba) {
       frontend_ = std::make_unique<CharacterLexicon>(
           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
@@ -407,7 +407,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
     if (meta_data.is_zh_en) {
       frontend_ = std::make_unique<MatchaTtsLexicon>(
           config_.model.matcha.lexicon, config_.model.matcha.tokens,
-          config_.model.matcha.data_dir, config_.model.debug);
+          config_.model.matcha.data_dir, config_.model.debug, false);
     } else if (meta_data.jieba) {
       frontend_ = std::make_unique<CharacterLexicon>(
           config_.model.matcha.lexicon, config_.model.matcha.tokens,
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
deleted file mode 100644
index 0cf582c7..00000000
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+++ /dev/null
@@ -1,81 +0,0 @@
-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
-
-#include <string>
-#include <vector>
-
-#include "espeak-ng/speak_lib.h"
-#include "gtest/gtest.h"
-#include "phoneme_ids.hpp"  // NOLINT
-#include "phonemize.hpp"    // NOLINT
-#include "sherpa-onnx/csrc/file-utils.h"
-#include "sherpa-onnx/csrc/macros.h"
-
-namespace sherpa_onnx {
-
-TEST(ZipVoiceFrontend, Case1) {
-  std::string data_dir = "../zipvoice/espeak-ng-data";
-  if (!FileExists(data_dir + "/en_dict")) {
-    SHERPA_ONNX_LOGE("%s/en_dict does not exist. Skipping test",
-                     data_dir.c_str());
-    return;
-  }
-
-  if (!FileExists(data_dir + "/phontab")) {
-    SHERPA_ONNX_LOGE("%s/phontab does not exist. Skipping test",
-                     data_dir.c_str());
-    return;
-  }
-
-  if (!FileExists(data_dir + "/phonindex")) {
-    SHERPA_ONNX_LOGE("%s/phonindex does not exist. Skipping test",
-                     data_dir.c_str());
-    return;
-  }
-
-  if (!FileExists(data_dir + "/phondata")) {
-    SHERPA_ONNX_LOGE("%s/phondata does not exist. Skipping test",
-                     data_dir.c_str());
-    return;
-  }
-
-  if (!FileExists(data_dir + "/intonations")) {
-    SHERPA_ONNX_LOGE("%s/intonations does not exist. Skipping test",
-                     data_dir.c_str());
-    return;
-  }
-
-  std::string pinyin_dict = data_dir + "/../pinyin.dict";
-  if (!FileExists(pinyin_dict)) {
-    SHERPA_ONNX_LOGE("%s does not exist. Skipping test", pinyin_dict.c_str());
-    return;
-  }
-
-  std::string tokens_file = data_dir + "/../tokens.txt";
-  if (!FileExists(tokens_file)) {
-    SHERPA_ONNX_LOGE("%s does not exist. Skipping test", tokens_file.c_str());
-    return;
-  }
-
-  auto frontend = OfflineTtsZipvoiceFrontend(
-      tokens_file, data_dir, pinyin_dict,
-      OfflineTtsZipvoiceModelMetaData{.use_espeak = true, .use_pinyin = true},
-      true);
-
-  std::string text = "how are you doing?";
-  std::vector<sherpa_onnx::TokenIDs> ans =
-      frontend.ConvertTextToTokenIds(text, "en-us");
-
-  text = "";
-  ans = frontend.ConvertTextToTokenIds(text, "en-us");
-
-  text =
-      "<pin1><yin2> [S1]and hello "
-      "world[S2]";
-  ans = frontend.ConvertTextToTokenIds(text, "en-us");
-}
-
-}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
deleted file mode 100644
index 26ca18f4..00000000
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+++ /dev/null
@@ -1,386 +0,0 @@
-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#include <algorithm>
-#include <cctype>
-#include <codecvt>
-#include <fstream>
-#include <locale>
-#include <memory>
-#include <regex>
-#include <sstream>
-#include <string>
-#include <strstream>
-#include <unordered_map>
-#include <utility>
-#include <vector>
-
-#if __ANDROID_API__ >= 9
-#include "android/asset_manager.h"
-#include "android/asset_manager_jni.h"
-#endif
-
-#if __OHOS__
-#include "rawfile/raw_file_manager.h"
-#endif
-
-#include "cppinyin/csrc/cppinyin.h"
-#include "espeak-ng/speak_lib.h"
-#include "phoneme_ids.hpp"  // NOLINT
-#include "phonemize.hpp"    // NOLINT
-#include "sherpa-onnx/csrc/file-utils.h"
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
-#include "sherpa-onnx/csrc/text-utils.h"
-
-namespace sherpa_onnx {
-
-void CallPhonemizeEspeak(const std::string &text,
-                         piper::eSpeakPhonemeConfig &config,  // NOLINT
-                         std::vector<std::vector<piper::Phoneme>> *phonemes);
-
-static std::unordered_map<std::string, int32_t> ReadTokens(std::istream &is) {
-  std::unordered_map<std::string, int32_t> token2id;
-
-  std::string line;
-  std::string sym;
-  int32_t id = 0;
-  while (std::getline(is, line)) {
-    std::istringstream iss(line);
-    iss >> sym;
-    if (iss.eof()) {
-      id = atoi(sym.c_str());
-      sym = " ";
-    } else {
-      iss >> id;
-    }
-    // eat the trailing \r\n on windows
-    iss >> std::ws;
-    if (!iss.eof()) {
-      SHERPA_ONNX_LOGE("Error when reading tokens: %s", line.c_str());
-      exit(-1);
-    }
-
-    if (token2id.count(sym)) {
-      SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
-                       sym.c_str(), line.c_str(), token2id.at(sym));
-      exit(-1);
-    }
-    token2id.insert({sym, id});
-  }
-  return token2id;
-}
-
-static std::string MapPunctuations(
-    const std::string &text,
-    const std::unordered_map<std::string, std::string> &punct_map) {
-  std::string result = text;
-  for (const auto &kv : punct_map) {
-    // Replace all occurrences of kv.first with kv.second
-    size_t pos = 0;
-    while ((pos = result.find(kv.first, pos)) != std::string::npos) {
-      result.replace(pos, kv.first.length(), kv.second);
-      pos += kv.second.length();
-    }
-  }
-  return result;
-}
-
-static void ProcessPinyin(
-    const std::string &pinyin, const cppinyin::PinyinEncoder *pinyin_encoder,
-    const std::unordered_map<std::string, int32_t> &token2id,
-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
-  auto initial = pinyin_encoder->ToInitial(pinyin);
-  if (!initial.empty()) {
-    // append '0' to fix the conflict with espeak token
-    initial = initial + "0";
-    if (token2id.count(initial)) {
-      tokens_ids->push_back(token2id.at(initial));
-      tokens->push_back(initial);
-    } else {
-      SHERPA_ONNX_LOGE("Skip unknown initial %s", initial.c_str());
-    }
-  }
-  auto final_t = pinyin_encoder->ToFinal(pinyin);
-  if (!final_t.empty()) {
-    if (!std::isdigit(final_t.back())) {
-      final_t = final_t + "5";  // use 5 for neutral tone
-    }
-    if (token2id.count(final_t)) {
-      tokens_ids->push_back(token2id.at(final_t));
-      tokens->push_back(final_t);
-    } else {
-      SHERPA_ONNX_LOGE("Skip unknown final %s", final_t.c_str());
-    }
-  }
-}
-
-static void TokenizeZh(const std::string &words,
-                       const cppinyin::PinyinEncoder *pinyin_encoder,
-                       const std::unordered_map<std::string, int32_t> &token2id,
-                       std::vector<int64_t> *token_ids,
-                       std::vector<std::string> *tokens) {
-  std::vector<std::string> pinyins;
-  pinyin_encoder->Encode(words, &pinyins, "number" /*tone*/, false /*partial*/);
-  for (const auto &pinyin : pinyins) {
-    if (pinyin_encoder->ValidPinyin(pinyin, "number" /*tone*/)) {
-      ProcessPinyin(pinyin, pinyin_encoder, token2id, token_ids, tokens);
-    } else {
-      auto wstext = ToWideString(pinyin);
-      for (auto &wc : wstext) {
-        auto c = ToString(std::wstring(1, wc));
-        if (token2id.count(c)) {
-          token_ids->push_back(token2id.at(c));
-          tokens->push_back(c);
-        } else {
-          SHERPA_ONNX_LOGE("Skip unknown character %s", c.c_str());
-        }
-      }
-    }
-  }
-}
-
-static void TokenizeEn(const std::string &words,
-                       const std::unordered_map<std::string, int32_t> &token2id,
-                       const std::string &voice,
-                       std::vector<int64_t> *token_ids,
-                       std::vector<std::string> *tokens) {
-  piper::eSpeakPhonemeConfig config;
-  // ./bin/espeak-ng-bin --path  ./install/share/espeak-ng-data/ --voices
-  // to list available voices
-  config.voice = voice;  // e.g., voice is en-us
-
-  std::vector<std::vector<piper::Phoneme>> phonemes;
-
-  CallPhonemizeEspeak(words, config, &phonemes);
-
-  for (const auto &p : phonemes) {
-    for (const auto &ph : p) {
-      auto token = Utf32ToUtf8(std::u32string(1, ph));
-      if (token2id.count(token)) {
-        token_ids->push_back(token2id.at(token));
-        tokens->push_back(token);
-      } else {
-        SHERPA_ONNX_LOGE("Skip unknown phoneme %s", token.c_str());
-      }
-    }
-  }
-}
-
-static void TokenizeTag(
-    const std::string &words,
-    const std::unordered_map<std::string, int32_t> &token2id,
-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
-  // in zipvoice tags are all in upper case
-  std::string tag = ToUpperAscii(words);
-  if (token2id.count(tag)) {
-    tokens_ids->push_back(token2id.at(tag));
-    tokens->push_back(tag);
-  } else {
-    SHERPA_ONNX_LOGE("Skip unknown tag %s", tag.c_str());
-  }
-}
-
-static void TokenizePinyin(
-    const std::string &words, const cppinyin::PinyinEncoder *pinyin_encoder,
-    const std::unordered_map<std::string, int32_t> &token2id,
-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
-  // words are in the form of <ha3>, <ha4>
-  std::string pinyin = words.substr(1, words.size() - 2);
-  if (!pinyin.empty()) {
-    if (pinyin[pinyin.size() - 1] == '5') {
-      pinyin = pinyin.substr(0, pinyin.size() - 1);  // remove the tone
-    }
-    if (pinyin_encoder->ValidPinyin(pinyin, "number" /*tone*/)) {
-      ProcessPinyin(pinyin, pinyin_encoder, token2id, tokens_ids, tokens);
-    } else {
-      SHERPA_ONNX_LOGE("Invalid pinyin %s", pinyin.c_str());
-    }
-  }
-}
-
-OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
-    const std::string &tokens, const std::string &data_dir,
-    const std::string &pinyin_dict,
-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug /*= false*/)
-    : debug_(debug), meta_data_(meta_data) {
-  std::ifstream is(tokens);
-  token2id_ = ReadTokens(is);
-  if (meta_data_.use_pinyin) {
-    pinyin_encoder_ = std::make_unique<cppinyin::PinyinEncoder>(pinyin_dict);
-  } else {
-    pinyin_encoder_ = nullptr;
-  }
-  if (meta_data_.use_espeak) {
-    // We should copy the directory of espeak-ng-data from the asset to
-    // some internal or external storage and then pass the directory to
-    // data_dir.
-    InitEspeak(data_dir);
-  }
-}
-
-template <typename Manager>
-OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
-    Manager *mgr, const std::string &tokens, const std::string &data_dir,
-    const std::string &pinyin_dict,
-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug)
-    : debug_(debug), meta_data_(meta_data) {
-  auto buf = ReadFile(mgr, tokens);
-  std::istrstream is(buf.data(), buf.size());
-  token2id_ = ReadTokens(is);
-  if (meta_data_.use_pinyin) {
-    auto buf = ReadFile(mgr, pinyin_dict);
-    std::istringstream iss(std::string(buf.begin(), buf.end()));
-    pinyin_encoder_ = std::make_unique<cppinyin::PinyinEncoder>(iss);
-  } else {
-    pinyin_encoder_ = nullptr;
-  }
-  if (meta_data_.use_espeak) {
-    // We should copy the directory of espeak-ng-data from the asset to
-    // some internal or external storage and then pass the directory to
-    // data_dir.
-    InitEspeak(data_dir);
-  }
-}
-
-std::vector<TokenIDs> OfflineTtsZipvoiceFrontend::ConvertTextToTokenIds(
-    const std::string &_text, const std::string &voice) const {
-  std::string text = _text;
-  if (meta_data_.use_espeak) {
-    text = ToLowerAscii(_text);
-  }
-
-  text = MapPunctuations(text, punct_map_);
-
-  auto wstext = ToWideString(text);
-
-  std::vector<std::string> parts;
-  // Match <...>, [...], or single character
-  std::wregex part_pattern(LR"([<\[].*?[>\]]|.)");
-  auto words_begin =
-      std::wsregex_iterator(wstext.begin(), wstext.end(), part_pattern);
-  auto words_end = std::wsregex_iterator();
-  for (std::wsregex_iterator i = words_begin; i != words_end; ++i) {
-    parts.push_back(ToString(i->str()));
-  }
-
-  // types are en, zh, tag, pinyin, other
-  // tag is [...]
-  // pinyin is <...>
-  // other is any other text that does not match the above, normally numbers and
-  // punctuations
-  std::vector<std::string> types;
-  for (auto &word : parts) {
-    if (word.size() == 1 && std::isalpha(word[0])) {
-      // single character, e.g., 'a', 'b', 'c'
-      types.push_back("en");
-    } else if (word.size() > 1 && word[0] == '<' && word.back() == '>') {
-      // e.g., <ha3>, <ha4>
-      types.push_back("pinyin");
-    } else if (word.size() > 1 && word[0] == '[' && word.back() == ']') {
-      types.push_back("tag");
-    } else if (ContainsCJK(word)) {  // word contains one CJK characters
-      types.push_back("zh");
-    } else {
-      types.push_back("other");
-    }
-  }
-
-  std::vector<std::pair<std::string, std::string>> parts_with_types;
-  std::ostringstream oss;
-  std::string t_lang;
-  oss.str("");
-  std::ostringstream debug_oss;
-  if (debug_) {
-    debug_oss << "Text : " << _text << ", Parts with types: \n";
-  }
-  for (int32_t i = 0; i < types.size(); ++i) {
-    if (i == 0) {
-      oss << parts[i];
-      t_lang = types[i];
-    } else {
-      if (t_lang == "other" && (types[i] != "tag" && types[i] != "pinyin")) {
-        // combine into current type if the previous part is "other"
-        // do not combine with "tag" or "pinyin"
-        oss << parts[i];
-        t_lang = types[i];
-      } else {
-        if ((t_lang == types[i] || types[i] == "other") && t_lang != "pinyin" &&
-            t_lang != "tag") {
-          // same language or other, continue
-          // do not combine other into "pinyin" or "tag"
-          oss << parts[i];
-        } else {
-          // different language, start a new sentence
-          std::string part = oss.str();
-          oss.str("");
-          parts_with_types.emplace_back(part, t_lang);
-          if (debug_) {
-            debug_oss << "(" << part << ", " << t_lang << "),";
-          }
-          oss << parts[i];
-          t_lang = types[i];
-        }
-      }
-    }
-  }
-
-  std::string part = oss.str();
-  oss.str("");
-  parts_with_types.emplace_back(part, t_lang);
-  if (debug_) {
-    debug_oss << "(" << part << ", " << t_lang << ")\n";
-    SHERPA_ONNX_LOGE("%s", debug_oss.str().c_str());
-    debug_oss.str("");
-  }
-
-  std::vector<int64_t> token_ids;
-  std::vector<std::string> tokens;  // for debugging
-  for (const auto &pt : parts_with_types) {
-    if (pt.second == "zh") {
-      TokenizeZh(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
-                 &tokens);
-    } else if (pt.second == "en") {
-      TokenizeEn(pt.first, token2id_, "en-us", &token_ids, &tokens);
-    } else if (pt.second == "pinyin") {
-      TokenizePinyin(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
-                     &tokens);
-    } else if (pt.second == "tag") {
-      TokenizeTag(pt.first, token2id_, &token_ids, &tokens);
-    } else {
-      SHERPA_ONNX_LOGE("Unexpected type: %s", pt.second.c_str());
-      exit(-1);
-    }
-  }
-  if (debug_) {
-    debug_oss << "Tokens and IDs: \n";
-    for (int32_t i = 0; i < tokens.size(); i++) {
-      debug_oss << "(" << tokens[i] << ", " << token_ids[i] << "),";
-    }
-    debug_oss << "\n";
-    SHERPA_ONNX_LOGE("%s", debug_oss.str().c_str());
-  }
-
-  std::vector<TokenIDs> ans;
-  ans.push_back(TokenIDs(std::move(token_ids)));
-  return ans;
-}
-
-#if __ANDROID_API__ >= 9
-template OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
-    AAssetManager *mgr, const std::string &tokens, const std::string &data_dir,
-    const std::string &pinyin_dict,
-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug = false);
-
-#endif
-
-#if __OHOS__
-template OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
-    NativeResourceManager *mgr, const std::string &tokens,
-    const std::string &data_dir, const std::string &pinyin_dict,
-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug = false);
-
-#endif
-
-}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
deleted file mode 100644
index 1e47103a..00000000
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
+++ /dev/null
@@ -1,62 +0,0 @@
-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#ifndef SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
-#define SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
-#include <cstdint>
-#include <memory>
-#include <string>
-#include <unordered_map>
-#include <vector>
-
-#include "cppinyin/csrc/cppinyin.h"
-#include "sherpa-onnx/csrc/offline-tts-frontend.h"
-#include "sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h"
-
-namespace sherpa_onnx {
-
-class OfflineTtsZipvoiceFrontend : public OfflineTtsFrontend {
- public:
-  OfflineTtsZipvoiceFrontend(const std::string &tokens,
-                             const std::string &data_dir,
-                             const std::string &pinyin_dict,
-                             const OfflineTtsZipvoiceModelMetaData &meta_data,
-                             bool debug = false);
-
-  template <typename Manager>
-  OfflineTtsZipvoiceFrontend(Manager *mgr, const std::string &tokens,
-                             const std::string &data_dir,
-                             const std::string &pinyin_dict,
-                             const OfflineTtsZipvoiceModelMetaData &meta_data,
-                             bool debug = false);
-
-  /** Convert a string to token IDs.
-   *
-   * @param text The input text.
-   *             Example 1: "This is the first sample sentence; this is the
-   *             second one." Example 2: ""
-   * @param voice Optional. It is for espeak-ng.
-   *
-   * @return Return a vector-of-vector of token IDs. Each subvector contains
-   *         a sentence that can be processed independently.
-   *         If a frontend does not support splitting the text into
-   * sentences, the resulting vector contains only one subvector.
-   */
-  std::vector<TokenIDs> ConvertTextToTokenIds(
-      const std::string &text, const std::string &voice = "") const override;
-
- private:
-  bool debug_ = false;
-  std::unordered_map<std::string, int32_t> token2id_;
-  const std::unordered_map<std::string, std::string> punct_map_ = {
-      {"", ","}, {"", "."}, {"", "!"},  {"", "?"},     {"", ";"},
-      {"", ":"}, {"", ","}, {"", "'"},   {"", "\""},     {"", "\""},
-      {"", "'"},  {"", ""},  {"", ""}, {"", ""}, {"...", ""}};
-  OfflineTtsZipvoiceModelMetaData meta_data_;
-  std::unique_ptr<cppinyin::PinyinEncoder> pinyin_encoder_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
index ff16bf60..8e9861f5 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
@@ -15,9 +15,9 @@
 #include "kaldi-native-fbank/csrc/mel-computations.h"
 #include "kaldi-native-fbank/csrc/stft.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
 #include "sherpa-onnx/csrc/offline-tts-frontend.h"
 #include "sherpa-onnx/csrc/offline-tts-impl.h"
-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-model.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
@@ -83,8 +83,16 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
     }
 
     // we assume batch size is 1
-    std::vector<int64_t> tokens = text_token_ids[0].tokens;
-    std::vector<int64_t> prompt_tokens = prompt_token_ids[0].tokens;
+    std::vector<int64_t> tokens;
+    for (const auto &t : text_token_ids) {
+      tokens.insert(tokens.end(), t.tokens.begin(), t.tokens.end());
+    }
+
+    std::vector<int64_t> prompt_tokens;
+    for (const auto &t : prompt_token_ids) {
+      prompt_tokens.insert(prompt_tokens.end(), t.tokens.begin(),
+                           t.tokens.end());
+    }
 
     return Process(tokens, prompt_tokens, prompt_samples, sample_rate, speed,
                    num_steps);
@@ -93,28 +101,15 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
  private:
   template <typename Manager>
   void InitFrontend(Manager *mgr) {
-    const auto &meta_data = model_->GetMetaData();
-    frontend_ = std::make_unique<OfflineTtsZipvoiceFrontend>(
-        mgr, config_.model.zipvoice.tokens, config_.model.zipvoice.data_dir,
-        config_.model.zipvoice.pinyin_dict, meta_data, config_.model.debug);
+    frontend_ = std::make_unique<MatchaTtsLexicon>(
+        mgr, config_.model.zipvoice.lexicon, config_.model.zipvoice.tokens,
+        config_.model.zipvoice.data_dir, config_.model.debug, true);
   }
 
   void InitFrontend() {
-    const auto &meta_data = model_->GetMetaData();
-
-    if (meta_data.use_pinyin && config_.model.zipvoice.pinyin_dict.empty()) {
-      SHERPA_ONNX_LOGE(
-          "Please provide --zipvoice-pinyin-dict for converting Chinese into "
-          "pinyin.");
-      exit(-1);
-    }
-    if (meta_data.use_espeak && config_.model.zipvoice.data_dir.empty()) {
-      SHERPA_ONNX_LOGE("Please provide --zipvoice-data-dir for espeak-ng.");
-      exit(-1);
-    }
-    frontend_ = std::make_unique<OfflineTtsZipvoiceFrontend>(
-        config_.model.zipvoice.tokens, config_.model.zipvoice.data_dir,
-        config_.model.zipvoice.pinyin_dict, meta_data, config_.model.debug);
+    frontend_ = std::make_unique<MatchaTtsLexicon>(
+        config_.model.zipvoice.lexicon, config_.model.zipvoice.tokens,
+        config_.model.zipvoice.data_dir, config_.model.debug, true);
   }
 
   std::vector<int32_t> ComputeMelSpectrogram(
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
index 453bd6f6..1226adf8 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
@@ -17,13 +17,14 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
                "Path to tokens.txt for ZipVoice models");
   po->Register("zipvoice-data-dir", &data_dir,
                "Path to the directory containing dict for espeak-ng.");
+  po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
   po->Register("zipvoice-pinyin-dict", &pinyin_dict,
                "Path to the pinyin dictionary for cppinyin (i.e converting "
                "Chinese into phones).");
   po->Register("zipvoice-text-model", &text_model,
                "Path to zipvoice text model");
   po->Register("zipvoice-flow-matching-model", &flow_matching_model,
-               "Path to zipvoice flow-matching model");
+               "Path to zipvoice flow-matching model, i.e., the decoder model");
   po->Register("zipvoice-vocoder", &vocoder, "Path to zipvoice vocoder");
   po->Register("zipvoice-feat-scale", &feat_scale,
                "Feature scale for ZipVoice (default: 0.1)");
@@ -96,12 +97,6 @@ bool OfflineTtsZipvoiceModelConfig::Validate() const {
     }
   }
 
-  if (!pinyin_dict.empty() && !FileExists(pinyin_dict)) {
-    SHERPA_ONNX_LOGE("--zipvoice-pinyin-dict: '%s' does not exist",
-                     pinyin_dict.c_str());
-    return false;
-  }
-
   if (feat_scale <= 0) {
     SHERPA_ONNX_LOGE("--zipvoice-feat-scale must be positive. Given: %f",
                      feat_scale);
@@ -138,7 +133,7 @@ std::string OfflineTtsZipvoiceModelConfig::ToString() const {
   os << "flow_matching_model=\"" << flow_matching_model << "\", ";
   os << "vocoder=\"" << vocoder << "\", ";
   os << "data_dir=\"" << data_dir << "\", ";
-  os << "pinyin_dict=\"" << pinyin_dict << "\", ";
+  os << "lexicon=\"" << lexicon << "\", ";
   os << "feat_scale=" << feat_scale << ", ";
   os << "t_shift=" << t_shift << ", ";
   os << "target_rms=" << target_rms << ", ";
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
index ef43bf41..5b5d0c24 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
@@ -15,12 +15,11 @@ namespace sherpa_onnx {
 struct OfflineTtsZipvoiceModelConfig {
   std::string tokens;
   std::string text_model;
-  std::string flow_matching_model;
+  std::string flow_matching_model;  // decoder
   std::string vocoder;
 
-  // If data_dir is given, lexicon is ignored
-  // data_dir is for piper-phonemize, which uses espeak-ng
   std::string data_dir;
+  std::string lexicon;
 
   // Used for converting Chinese characters to pinyin
   std::string pinyin_dict;
@@ -35,14 +34,15 @@ struct OfflineTtsZipvoiceModelConfig {
   OfflineTtsZipvoiceModelConfig(
       const std::string &tokens, const std::string &text_model,
       const std::string &flow_matching_model, const std::string &vocoder,
-      const std::string &data_dir, const std::string &pinyin_dict,
-      float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
-      float guidance_scale = 1.0)
+      const std::string &data_dir, const std::string &lexicon,
+      const std::string &pinyin_dict, float feat_scale = 0.1,
+      float t_shift = 0.5, float target_rms = 0.1, float guidance_scale = 1.0)
       : tokens(tokens),
         text_model(text_model),
         flow_matching_model(flow_matching_model),
         vocoder(vocoder),
         data_dir(data_dir),
+        lexicon(lexicon),
         pinyin_dict(pinyin_dict),
         feat_scale(feat_scale),
         t_shift(t_shift),
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
index 05f324c9..8b4d321b 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
@@ -68,7 +68,7 @@ class OfflineTtsZipvoiceModel::Impl {
     if (batch_size != 1) {
       SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
                        static_cast<int32_t>(batch_size));
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 
     std::vector<int64_t> prompt_feat_shape =
@@ -108,7 +108,10 @@ class OfflineTtsZipvoiceModel::Impl {
     std::random_device rd;
     std::default_random_engine rng(rd());
     std::normal_distribution<float> norm(0, 1);
-    for (auto &v : x_data) v = norm(rng);
+    for (auto &v : x_data) {
+      v = norm(rng);
+    }
+
     std::vector<int64_t> x_shape = {batch_size, num_frames, feat_dim};
     Ort::Value x = Ort::Value::CreateTensor<float>(
         memory_info, x_data.data(), x_data.size(), x_shape.data(),
diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
index 2357724d..5d86b3c9 100644
--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
+++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
@@ -698,7 +698,6 @@ const
      {$linklib sherpa-onnx-kaldifst-core}
      {$linklib sherpa-onnx-fstfar}
      {$linklib sherpa-onnx-fst}
-     {$linklib cppinyin_core}
      {$linklib kissfft-float}
      {$linklib kaldi-native-fbank-core}
      {$linklib piper_phonemize}

commit 42dcaf194109bc8e8e748118da3ac76e5731b00a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 11 18:03:13 2025 +0800

    upload zipvoice onnx models (#2890)

diff --git a/.github/workflows/upload-models.yaml b/.github/workflows/upload-models.yaml
index f5922704..a596c26b 100644
--- a/.github/workflows/upload-models.yaml
+++ b/.github/workflows/upload-models.yaml
@@ -4,7 +4,6 @@ on:
   push:
     branches:
       - upload-models
-      # - upload-more-models
   workflow_dispatch:
 
 concurrency:
@@ -31,6 +30,10 @@ jobs:
           git config --global user.email "csukuangfj@gmail.com"
           git config --global user.name "Fangjun Kuang"
 
+      - name: Setup tmate session
+        if: false
+        uses: mxschmitt/action-tmate@v3
+
       - name: Streaming zipformer from Banafo/Kroko-ASR
         if: false
         shell: bash
@@ -178,7 +181,7 @@ jobs:
           ls -lh *.tar.bz2
 
       - name: wenetspeech chuan paraformer
-        if: true
+        if: false
         shell: bash
         run: |
           git lfs install
@@ -482,7 +485,14 @@ jobs:
               popd
             done
 
+      - uses: actions/upload-artifact@v4
+        if: false
+        with:
+          name: here
+          path: ./*.tar.bz2
+
       - name: Release
+        if: false
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
diff --git a/.github/workflows/upload-zipvoice-models.yaml b/.github/workflows/upload-zipvoice-models.yaml
new file mode 100644
index 00000000..cc209c63
--- /dev/null
+++ b/.github/workflows/upload-zipvoice-models.yaml
@@ -0,0 +1,135 @@
+name: upload-zipvoice-models
+
+on:
+  push:
+    branches:
+      - upload-zipvoice-onnx-models
+  workflow_dispatch:
+
+concurrency:
+  group: upload-zipvoice-models-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  upload-zipvoice-models:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: upload zipvoice models
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        python-version: ["3.10"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: git config
+        shell: bash
+        run: |
+          git config --global user.email "csukuangfj@gmail.com"
+          git config --global user.name "Fangjun Kuang"
+
+      - name: Setup Python 3.10
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.10"
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          python3 -m pip install --upgrade pip pypinyin
+
+      - name: sherpa-onnx-zipvoice-distill-zh-en-emilia-int8
+        shell: bash
+        run: |
+          echo "Generate lexicon.txt"
+
+          python3 ./scripts/zipvoice/zh-en/generate_lexicon.py
+
+          d=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia
+          mkdir $d
+
+          cp lexicon.txt $d
+
+          pushd $d
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/prompt.txt
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female-2.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/leijun-1.wav
+
+
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/fm_decoder_int8.onnx
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/text_encoder_int8.onnx
+
+          mv fm_decoder_int8.onnx decoder.int8.onnx
+          mv text_encoder_int8.onnx encoder.int8.onnx
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
+          tar xf espeak-ng-data.tar.bz2
+          rm espeak-ng-data.tar.bz2
+
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/tokens.txt
+          mkdir test_wavs
+          mv *.wav test_wavs
+
+          mv prompt.txt test_wavs
+
+          ls -lh
+          popd
+          tar cjfv $d.tar.bz2 $d
+          rm -rf $d
+          ls -lh $d.tar.bz2
+
+      - name: sherpa-onnx-zipvoice-distill-zh-en-emilia-fp32
+        shell: bash
+        run: |
+          echo "Generate lexicon.txt"
+
+          python3 ./scripts/zipvoice/zh-en/generate_lexicon.py
+
+          d=sherpa-onnx-zipvoice-distill-fp32-zh-en-emilia
+          mkdir $d
+
+          cp lexicon.txt $d
+
+          pushd $d
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/prompt.txt
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female-2.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/leijun-1.wav
+
+
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/fm_decoder.onnx
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/text_encoder.onnx
+
+          mv fm_decoder.onnx decoder.onnx
+          mv text_encoder.onnx encoder.onnx
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
+          tar xf espeak-ng-data.tar.bz2
+          rm espeak-ng-data.tar.bz2
+
+          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/tokens.txt
+          mkdir test_wavs
+          mv *.wav test_wavs
+
+          mv prompt.txt test_wavs
+
+          ls -lh
+          popd
+          tar cjfv $d.tar.bz2 $d
+          rm -rf $d
+          ls -lh $d.tar.bz2
+
+      - name: Release
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: tts-models

commit 7f6ec017d3a336dc9e5eb7ed22adb0381bc640ba
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Dec 11 14:26:18 2025 +0800

    Add script for testing zipvoice onnx models (#2887)

diff --git a/scripts/zipvoice/zh-en/generate_lexicon.py b/scripts/zipvoice/zh-en/generate_lexicon.py
new file mode 100755
index 00000000..2f2211e0
--- /dev/null
+++ b/scripts/zipvoice/zh-en/generate_lexicon.py
@@ -0,0 +1,90 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+
+from pypinyin import Style, lazy_pinyin, load_phrases_dict, phrases_dict, pinyin_dict
+from pypinyin.contrib.tone_convert import to_finals_tone3, to_initials
+
+load_phrases_dict(
+    {
+        "": [["hang2"], ["zhang3"]],
+        "": [["yin2"], ["hang2"], ["hang2"], ["zhang3"]],
+    }
+)
+user_defined = {
+    "": ["wei1", "tiao2"],
+    "": ["zhe4", "ge4"],
+    "": ["fang1", "bian2", "de1"],
+}
+
+
+def get_initial_final(token):
+    if isinstance(token, list):
+        ans = ""
+        sep = ""
+        for t in token:
+            ans += sep + get_initial_final(t)
+            sep = " "
+        return ans
+
+    initial = to_initials(token, strict=False)
+
+    final = to_finals_tone3(
+        token,
+        strict=False,
+        neutral_tone_with_five=True,
+    )
+
+    ans = ""
+    if initial:
+        ans = initial + "0"
+
+    if final:
+        ans += f" {final}"
+
+    return ans
+
+
+def main():
+    filename = "lexicon.txt"
+
+    word_dict = pinyin_dict.pinyin_dict
+    phrases = phrases_dict.phrases_dict
+
+    with open(filename, "w", encoding="utf-8") as f:
+        for key in word_dict:
+            if not (0x4E00 <= key <= 0x9FFF):
+                continue
+
+            w = chr(key)
+            token = lazy_pinyin(
+                w,
+                style=Style.TONE3,
+                tone_sandhi=True,
+                neutral_tone_with_five=True,
+            )[0]
+
+            initial_final = get_initial_final(token)
+
+            f.write(f"{w} {initial_final}\n")
+
+        for key, value in user_defined.items():
+            initial_final = get_initial_final(value)
+            f.write(f"{key} {initial_final}\n")
+
+        for key in phrases:
+            if key in user_defined:
+                continue
+            token = lazy_pinyin(
+                key,
+                style=Style.TONE3,
+                tone_sandhi=True,
+                neutral_tone_with_five=True,
+            )
+            initial_final = get_initial_final(token)
+
+            f.write(f"{key} {initial_final}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/zipvoice/zh-en/test_onnx.py b/scripts/zipvoice/zh-en/test_onnx.py
new file mode 100755
index 00000000..4e9fbc8c
--- /dev/null
+++ b/scripts/zipvoice/zh-en/test_onnx.py
@@ -0,0 +1,381 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+
+import kaldi_native_fbank as knf
+import numpy as np
+import onnxruntime as ort
+import soundfile as sf
+
+
+def compute_features(samples):
+    stft_config = knf.StftConfig(
+        n_fft=1024,
+        hop_length=256,
+        win_length=1024,
+        center=True,
+        window_type="hann",
+    )
+    knf_stft = knf.Stft(stft_config)
+    stft_result = knf_stft(samples.tolist())
+    real = np.array(stft_result.real, dtype=np.float32).reshape(
+        stft_result.num_frames, -1
+    )
+    imag = np.array(stft_result.imag, dtype=np.float32).reshape(
+        stft_result.num_frames, -1
+    )
+
+    mag = np.sqrt(real * real + imag * imag).astype(np.float32)
+
+    mel_opts = knf.MelBanksOptions()
+    mel_opts.num_bins = 100
+    mel_opts.low_freq = 0
+    mel_opts.high_freq = 24000 // 2
+    mel_opts.is_librosa = True
+    mel_opts.norm = ""
+    mel_opts.use_slaney_mel_scale = False
+
+    frame_opts = knf.FrameExtractionOptions()
+    frame_opts.samp_freq = 24000
+    #  frame_opts.frame_length_ms = 1024 * 1000 / 24000
+    #  frame_opts.frame_shift_ms = 256 * 1000 / 24000
+
+    mel_filters = knf.MelBanks(mel_opts, frame_opts)
+    mel_features = np.zeros((mag.shape[0], 100))
+    for i in range(mag.shape[0]):
+        mel_features[i] = mel_filters.compute(mag[i])
+    print("sum", np.sum(mel_features), np.mean(mel_features))
+
+    mel_features = np.log(mel_features + 1e-10)
+    return mel_features
+
+
+class OnnxModel:
+    def __init__(
+        self,
+        text_encoder_path: str,
+        fm_decoder_path: str,
+        num_thread: int = 1,
+    ):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = num_thread
+        session_opts.intra_op_num_threads = num_thread
+
+        self.session_opts = session_opts
+
+        self.init_text_encoder(text_encoder_path)
+        self.init_fm_decoder(fm_decoder_path)
+
+    def init_text_encoder(self, model_path: str):
+        self.text_encoder = ort.InferenceSession(
+            model_path,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+
+    def init_fm_decoder(self, model_path: str):
+        self.fm_decoder = ort.InferenceSession(
+            model_path,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+        meta = self.fm_decoder.get_modelmeta().custom_metadata_map
+        self.feat_dim = int(meta["feat_dim"])
+
+    def run_text_encoder(
+        self,
+        tokens: np.ndarray,
+        prompt_tokens: np.ndarray,
+        prompt_features_len: np.ndarray,
+        speed: np.ndarray,
+    ) -> np.ndarray:
+        out = self.text_encoder.run(
+            [
+                self.text_encoder.get_outputs()[0].name,
+            ],
+            {
+                self.text_encoder.get_inputs()[0].name: tokens,
+                self.text_encoder.get_inputs()[1].name: prompt_tokens,
+                self.text_encoder.get_inputs()[2].name: prompt_features_len,
+                self.text_encoder.get_inputs()[3].name: speed,
+            },
+        )
+        return out[0]
+
+    def run_fm_decoder(
+        self,
+        t: np.ndarray,
+        x: np.ndarray,
+        text_condition: np.ndarray,
+        speech_condition: np.ndarray,
+        guidance_scale: np.ndarray,
+    ) -> np.ndarray:
+        out = self.fm_decoder.run(
+            [
+                self.fm_decoder.get_outputs()[0].name,
+            ],
+            {
+                self.fm_decoder.get_inputs()[0].name: t,
+                self.fm_decoder.get_inputs()[1].name: x,
+                self.fm_decoder.get_inputs()[2].name: text_condition,
+                self.fm_decoder.get_inputs()[3].name: speech_condition,
+                self.fm_decoder.get_inputs()[4].name: guidance_scale,
+            },
+        )
+        return out[0]
+
+
+class OnnxVocosModel:
+    def __init__(
+        self,
+        filename: str,
+    ):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 1
+
+        self.session_opts = session_opts
+        self.model = ort.InferenceSession(
+            filename,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+        print(f"vocos {self.model.get_modelmeta().custom_metadata_map}")
+
+        print("----------vocos----------")
+        for i in self.model.get_inputs():
+            print(i)
+
+        print("-----")
+
+        for i in self.model.get_outputs():
+            print(i)
+        print()
+
+    def __call__(self, x: np.ndarray):
+        """
+        Args:
+          x: (N, feat_dim, num_frames)
+        Returns:
+          mag: (N, n_fft/2+1, num_frames)
+          x: (N, n_fft/2+1, num_frames)
+          y: (N, n_fft/2+1, num_frames)
+
+        The complex spectrum is mag * (x + j*y)
+        """
+        assert x.ndim == 3, x.shape
+        assert x.shape[0] == 1, x.shape
+
+        mag, x, y = self.model.run(
+            [
+                self.model.get_outputs()[0].name,
+                self.model.get_outputs()[1].name,
+                self.model.get_outputs()[2].name,
+            ],
+            {
+                self.model.get_inputs()[0].name: x,
+            },
+        )
+
+        return mag, x, y
+
+
+def get_phones(text):
+    if text[-1] != ".":
+        text += "."
+
+    word2tokens = dict()
+    with open("./lexicon.txt", encoding="utf-8") as f:
+        for line in f:
+            fields = line.split()
+            word = fields[0]
+            tokens = fields[1:]
+            word2tokens[word] = tokens
+
+    token2id = dict()
+    with open("./tokens.txt", encoding="utf-8") as f:
+        for line in f:
+            fields = line.strip().split()
+            if len(fields) == 1:
+                token2id[" "] = int(fields[0])
+            else:
+                token2id[fields[0]] = int(fields[1])
+
+    tokens = []
+    for w in text:
+        if w in word2tokens:
+            tokens += word2tokens[w]
+        else:
+            tokens.append(w)
+    ids = []
+    for t in tokens:
+        if t in token2id:
+            ids.append(token2id[t])
+        else:
+            print(f"skip {t}")
+
+    return ids
+
+
+def compute_rms(features):
+    return np.sqrt(np.mean(np.square(features)))
+
+
+def get_timestamps(num_steps, t_shift=1):
+    steps = np.linspace(0, 1, num_steps + 1)
+    if t_shift != 1:
+        steps = t_shift * steps / (1 + (t_shift - 1) * steps)
+
+    return steps.tolist()
+
+
+def trim_leading_silence_energy(samples, frame_size=2048, hop=512, energy_thresh=0.5):
+    energies = [
+        np.sum(np.abs(samples[i : i + frame_size]) ** 2)
+        for i in range(0, len(samples) - frame_size, hop)
+    ]
+    #  print(energies)
+    # First frame whose energy exceeds threshold
+    frame_index = next((i for i, e in enumerate(energies) if e > energy_thresh), 0)
+    frame_index = max(frame_index - 3, 0)
+    start_sample = frame_index * hop
+    return samples[start_sample:]
+
+
+def main():
+    vocoder = OnnxVocosModel("./vocos_24khz.onnx")
+
+    prompt_text = ", ! , "
+    prompt_wav_filename = "news-female.wav"
+
+    prompt_text = ", , , ."
+    prompt_wav_filename = "news-female-2.wav"
+
+    prompt_text = ", . ."
+    prompt_wav_filename = "leijun-1.wav"
+
+    prompt_ids = get_phones(prompt_text)
+
+    text = ", . . , ."
+
+    ids = get_phones(text)
+
+    data, sample_rate = sf.read(
+        prompt_wav_filename,
+        always_2d=True,
+        dtype="float32",
+    )
+    data = data[:, 0]  # use only the first channel
+    samples = np.ascontiguousarray(data)
+    if sample_rate != 24000:
+        import librosa
+
+        samples = librosa.resample(
+            samples,
+            orig_sr=sample_rate,
+            target_sr=24000,
+        )
+        sample_rate = 24000
+
+    assert len(samples.shape) == 1, samples.shape
+
+    rms = compute_rms(samples)
+    print("rms", rms)
+
+    target_rms = 0.1
+    if rms < target_rms:
+        samples = samples * target_rms / rms
+    new_rms = compute_rms(samples)
+
+    print("new_rms", new_rms)
+
+    prompt_features = compute_features(samples)
+    print("features.shape", prompt_features.shape)
+
+    feat_scale = 0.1
+    prompt_features = prompt_features * feat_scale
+
+    model = OnnxModel(
+        text_encoder_path="./text_encoder_int8.onnx",
+        fm_decoder_path="./fm_decoder_int8.onnx",
+    )
+
+    tokens = np.array([ids], dtype=np.int64)
+    assert len(tokens.shape) == 2, tokens.shape
+
+    prompt_tokens = np.array([prompt_ids], dtype=np.int64)
+    assert len(prompt_tokens.shape) == 2, prompt_tokens.shape
+    prompt_features_len = np.array(prompt_features.shape[0], dtype=np.int64)
+    speed = np.array(1.0, dtype=np.float32)
+
+    print(tokens.shape, prompt_tokens.shape, prompt_features_len)
+
+    text_condition = model.run_text_encoder(
+        tokens=tokens,
+        prompt_tokens=prompt_tokens,
+        prompt_features_len=prompt_features_len,
+        speed=speed,
+    )
+
+    x = np.random.randn(*text_condition.shape).astype(np.float32)
+
+    speech_condition = np.pad(
+        prompt_features,
+        pad_width=((0, x.shape[1] - prompt_features.shape[0]), (0, 0)),
+        mode="constant",
+        constant_values=0,
+    )[None].astype(np.float32)
+
+    print(speech_condition.shape, prompt_features.shape)
+
+    guidance_scale = np.array(1.0, dtype=np.float32)
+
+    num_steps = 8
+    steps = get_timestamps(num_steps=num_steps, t_shift=0.5)
+    for i in range(num_steps):
+        t = np.array(steps[i], dtype=np.float32)
+        v = model.run_fm_decoder(
+            t=t,
+            x=x,
+            text_condition=text_condition,
+            speech_condition=speech_condition,
+            guidance_scale=guidance_scale,
+        )
+        x = x + v * (steps[i + 1] - steps[i])
+    print("prompt_features", prompt_features.shape)
+    x = x[:, prompt_features.shape[0] :]
+    print("x", x.shape)
+
+    x = x / feat_scale
+    mel = x.transpose(0, 2, 1)
+    mag, x, y = vocoder(mel)
+    print("mag", mag.shape, x.shape, y.shape)
+
+    stft_result = knf.StftResult(
+        real=(mag * x)[0].transpose().reshape(-1).tolist(),
+        imag=(mag * y)[0].transpose().reshape(-1).tolist(),
+        num_frames=mag.shape[2],
+    )
+    config = knf.StftConfig(
+        n_fft=1024,
+        hop_length=256,
+        win_length=1024,
+        window_type="hann",
+        center=True,
+        pad_mode="reflect",
+        normalized=False,
+    )
+    istft = knf.IStft(config)
+    audio_vocos = istft(stft_result)
+
+    audio_vocos = np.array(audio_vocos)
+    audio_vocos = trim_leading_silence_energy(audio_vocos)
+
+    #  if rms < target_rms:
+    #      audio_vocos = audio_vocos / target_rms * rms
+
+    sf.write("generated.wav", audio_vocos, sample_rate, "PCM_16")
+
+
+if __name__ == "__main__":
+    main()

commit 7faab3deeb7ae7f105f8b9444e55fd1fab0b3516
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Dec 10 18:19:35 2025 +0800

    Publish WASM spaces for MatchaTTS Chinese+English model (#2885)

diff --git a/.github/workflows/wasm-simd-hf-space-de-tts.yaml b/.github/workflows/wasm-simd-hf-space-de-tts.yaml
deleted file mode 100644
index cf265378..00000000
--- a/.github/workflows/wasm-simd-hf-space-de-tts.yaml
+++ /dev/null
@@ -1,169 +0,0 @@
-name: wasm-simd-hf-space-de-tts
-
-on:
-  push:
-    branches:
-      - wasm
-    tags:
-      - 'v[0-9]+.[0-9]+.[0-9]+*'
-
-  workflow_dispatch:
-
-concurrency:
-  group: wasm-simd-hf-space-de-tts-${{ github.ref }}
-  cancel-in-progress: true
-
-jobs:
-  wasm-simd-hf-space-de-tts:
-    runs-on: ${{ matrix.os }}
-    strategy:
-      fail-fast: false
-      matrix:
-        os: [ubuntu-latest]
-
-    steps:
-      - uses: actions/checkout@v4
-        with:
-          fetch-depth: 0
-
-      - name: Update version
-        shell: bash
-        run: |
-          ./new-release.sh
-          git diff .
-
-      - name: Install emsdk
-        uses: mymindstorm/setup-emsdk@v14
-        with:
-          version: 3.1.53
-          actions-cache-folder: 'emsdk-cache'
-
-      - name: View emsdk version
-        shell: bash
-        run: |
-          emcc -v
-          echo "--------------------"
-          emcc --check
-
-      - name: Download model files
-        shell: bash
-        run: |
-          cd wasm/tts/assets
-          ls -lh
-          echo "----------"
-          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
-          tar xf vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
-          rm vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
-
-          mv -v vits-piper-de_DE-thorsten_emotional-medium/de_DE-thorsten_emotional-medium.onnx ./model.onnx
-          mv -v vits-piper-de_DE-thorsten_emotional-medium/tokens.txt ./
-          mv vits-piper-de_DE-thorsten_emotional-medium/espeak-ng-data ./
-
-          rm -rf vits-piper-de_DE-thorsten_emotional-medium
-
-          ls -lh
-
-      - name: Build sherpa-onnx for WebAssembly
-        shell: bash
-        run: |
-          ./build-wasm-simd-tts.sh
-
-      - name: collect files
-        shell: bash
-        run: |
-          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-          mv build-wasm-simd-tts/install/bin/wasm/tts sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
-          ls -lh sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
-          tar cjfv sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts.tar.bz2 ./sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
-
-      - name: Upload wasm files
-        uses: actions/upload-artifact@v4
-        with:
-          name: sherpa-onnx-wasm-simd-de-tts
-          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
-
-      - name: Release
-        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
-        uses: svenstaro/upload-release-action@v2
-        with:
-          file_glob: true
-          overwrite: true
-          file: ./*.tar.bz2
-
-      - name: Publish to ModelScope
-        # if: false
-        env:
-          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
-        uses: nick-fields/retry@v2
-        with:
-          max_attempts: 20
-          timeout_seconds: 200
-          shell: bash
-          command: |
-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-            git config --global user.email "csukuangfj@gmail.com"
-            git config --global user.name "Fangjun Kuang"
-
-            rm -rf ms
-            export GIT_LFS_SKIP_SMUDGE=1
-            export GIT_CLONE_PROTECTION_ACTIVE=false
-
-            git clone http://www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de.git ms
-
-            cd ms
-            rm -fv *.js
-            rm -fv *.data
-
-            git fetch
-            git pull
-            git merge -m "merge remote" --ff origin main
-
-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts/* .
-
-            git status
-            git lfs track "*.data"
-            git lfs track "*.wasm"
-            ls -lh
-
-            git add .
-            git commit -m "update model"
-            git push http://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de.git
-
-      - name: Publish to huggingface
-        env:
-          HF_TOKEN: ${{ secrets.HF_TOKEN }}
-        uses: nick-fields/retry@v2
-        with:
-          max_attempts: 20
-          timeout_seconds: 200
-          shell: bash
-          command: |
-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-            git config --global user.email "csukuangfj@gmail.com"
-            git config --global user.name "Fangjun Kuang"
-
-            rm -rf huggingface
-            export GIT_LFS_SKIP_SMUDGE=1
-            export GIT_CLONE_PROTECTION_ACTIVE=false
-
-            git clone https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de huggingface
-            cd huggingface
-            rm -fv *.js
-            rm -fv *.data
-            git fetch
-            git pull
-            git merge -m "merge remote" --ff origin main
-
-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts/* .
-
-            git status
-            git lfs track "*.data"
-            git lfs track "*.wasm"
-            ls -lh
-
-            git add .
-            git commit -m "update model"
-            git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de main
diff --git a/.github/workflows/wasm-simd-hf-space-en-tts.yaml b/.github/workflows/wasm-simd-hf-space-en-tts.yaml
deleted file mode 100644
index b639706a..00000000
--- a/.github/workflows/wasm-simd-hf-space-en-tts.yaml
+++ /dev/null
@@ -1,165 +0,0 @@
-name: wasm-simd-hf-space-en-tts
-
-on:
-  push:
-    branches:
-      - wasm
-    tags:
-      - 'v[0-9]+.[0-9]+.[0-9]+*'
-
-  workflow_dispatch:
-
-concurrency:
-  group: wasm-simd-hf-space-en-tts-${{ github.ref }}
-  cancel-in-progress: true
-
-jobs:
-  wasm-simd-hf-space-en-tts:
-    runs-on: ${{ matrix.os }}
-    strategy:
-      fail-fast: false
-      matrix:
-        os: [ubuntu-latest]
-
-    steps:
-      - uses: actions/checkout@v4
-        with:
-          fetch-depth: 0
-
-      - name: Update version
-        shell: bash
-        run: |
-          ./new-release.sh
-          git diff .
-
-      - name: Install emsdk
-        uses: mymindstorm/setup-emsdk@v14
-        with:
-          version: 3.1.53
-          actions-cache-folder: 'emsdk-cache'
-
-      - name: View emsdk version
-        shell: bash
-        run: |
-          emcc -v
-          echo "--------------------"
-          emcc --check
-
-      - name: Download model files
-        shell: bash
-        run: |
-          cd wasm/tts/assets
-          ls -lh
-          echo "----------"
-          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-en_US-libritts_r-medium.tar.bz2
-          tar xf vits-piper-en_US-libritts_r-medium.tar.bz2
-          rm vits-piper-en_US-libritts_r-medium.tar.bz2
-          mv vits-piper-en_US-libritts_r-medium/en_US-libritts_r-medium.onnx ./model.onnx
-          mv vits-piper-en_US-libritts_r-medium/tokens.txt ./
-          mv vits-piper-en_US-libritts_r-medium/espeak-ng-data ./
-          rm -rf vits-piper-en_US-libritts_r-medium
-
-          ls -lh
-
-      - name: Build sherpa-onnx for WebAssembly
-        shell: bash
-        run: |
-          ./build-wasm-simd-tts.sh
-
-      - name: collect files
-        shell: bash
-        run: |
-          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-          mv build-wasm-simd-tts/install/bin/wasm/tts sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
-          ls -lh sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
-          tar cjfv sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts.tar.bz2 ./sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
-
-      - name: Upload wasm files
-        uses: actions/upload-artifact@v4
-        with:
-          name: sherpa-onnx-wasm-simd-en-tts
-          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
-
-      - name: Release
-        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
-        uses: svenstaro/upload-release-action@v2
-        with:
-          file_glob: true
-          overwrite: true
-          file: ./*.tar.bz2
-
-      - name: Publish to ModelScope
-        # if: false
-        env:
-          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
-        uses: nick-fields/retry@v2
-        with:
-          max_attempts: 20
-          timeout_seconds: 200
-          shell: bash
-          command: |
-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-            git config --global user.email "csukuangfj@gmail.com"
-            git config --global user.name "Fangjun Kuang"
-
-            rm -rf ms
-            export GIT_LFS_SKIP_SMUDGE=1
-            export GIT_CLONE_PROTECTION_ACTIVE=false
-
-            git clone https://www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en.git ms
-            cd ms
-            rm -fv *.js
-            rm -fv *.data
-            git fetch
-            git pull
-            git merge -m "merge remote" --ff origin main
-
-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts/* .
-
-            git status
-            git lfs track "*.data"
-            git lfs track "*.wasm"
-            ls -lh
-
-            git add .
-            git commit -m "update model"
-            git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en.git
-
-      - name: Publish to huggingface
-        env:
-          HF_TOKEN: ${{ secrets.HF_TOKEN }}
-        uses: nick-fields/retry@v2
-        with:
-          max_attempts: 20
-          timeout_seconds: 200
-          shell: bash
-          command: |
-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
-
-            git config --global user.email "csukuangfj@gmail.com"
-            git config --global user.name "Fangjun Kuang"
-
-            rm -rf huggingface
-            export GIT_LFS_SKIP_SMUDGE=1
-            export GIT_CLONE_PROTECTION_ACTIVE=false
-
-            git clone https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en huggingface
-            cd huggingface
-            rm -fv *.js
-            rm -fv *.data
-            git fetch
-            git pull
-            git merge -m "merge remote" --ff origin main
-
-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts/* .
-
-            git status
-            git lfs track "*.data"
-            git lfs track "*.wasm"
-            ls -lh
-
-            git add .
-            git commit -m "update model"
-            git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en main
diff --git a/.github/workflows/wasm-simd-hf-space-tts.yaml b/.github/workflows/wasm-simd-hf-space-tts.yaml
new file mode 100644
index 00000000..f2522774
--- /dev/null
+++ b/.github/workflows/wasm-simd-hf-space-tts.yaml
@@ -0,0 +1,102 @@
+name: wasm-simd-hf-space-tts
+
+on:
+  push:
+    branches:
+      - wasm
+    tags:
+      - 'v[0-9]+.[0-9]+.[0-9]+*'
+
+  workflow_dispatch:
+
+concurrency:
+  group: wasm-simd-hf-space-tts${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  wasm-simd-hf-space-tts:
+    name: ${{ matrix.index }}/${{ matrix.total }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        total: ["4"]
+        index: ["0", "1", "2", "3"]
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          python3 -m pip install --upgrade pip jinja2
+
+      - name: Install emsdk
+        uses: mymindstorm/setup-emsdk@v14
+        with:
+          version: 3.1.53
+          actions-cache-folder: 'emsdk-cache'
+
+      - name: View emsdk version
+        shell: bash
+        run: |
+          emcc -v
+          echo "--------------------"
+          emcc --check
+
+      - name: Generate build script
+        shell: bash
+        run: |
+          cd scripts/wasm
+
+          total=${{ matrix.total }}
+          index=${{ matrix.index }}
+
+          ./generate-tts.py --total $total --index $index
+
+          chmod +x run-tts.sh
+          mv -v ./run-tts.sh ../..
+
+      - name: Show build scripts
+        shell: bash
+        run: |
+          cat ./run-tts.sh
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: run-tts-${{ matrix.index }}
+          path: ./run-tts.sh
+
+      - name: Build sherpa-onnx for WebAssembly
+        shell: bash
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        run: |
+          ./run-tts.sh
+
+      - name: Release
+        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: ./*.tar.bz2
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.19
+
+      - name: Upload wasm files
+        uses: actions/upload-artifact@v4
+        with:
+          name: sherpa-onnx-wasm-simd-tts-${{ matrix.index }}
+          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
diff --git a/README.md b/README.md
index 72a3079e..6cd6d6f4 100644
--- a/README.md
+++ b/README.md
@@ -139,8 +139,11 @@ We also have spaces built using WebAssembly. They are listed below:
 |VAD + speech recognition (English + Chinese, ) with Paraformer-large          |[Click me][wasm-hf-vad-asr-zh-en-paraformer-large]| [][wasm-ms-vad-asr-zh-en-paraformer-large]|
 |VAD + speech recognition (English + Chinese, ) with Paraformer-small          |[Click me][wasm-hf-vad-asr-zh-en-paraformer-small]| [][wasm-ms-vad-asr-zh-en-paraformer-small]|
 |VAD + speech recognition () with [Dolphin][Dolphin]-base          |[Click me][wasm-hf-vad-asr-multi-lang-dolphin-base]| [][wasm-ms-vad-asr-multi-lang-dolphin-base]|
-|Speech synthesis (English)                                                                  |[Click me][wasm-hf-tts-piper-en]| [][wasm-ms-tts-piper-en]|
-|Speech synthesis (German)                                                                   |[Click me][wasm-hf-tts-piper-de]| [][wasm-ms-tts-piper-de]|
+|Speech synthesis (Piper, English)                                                                  |[Click me][wasm-hf-tts-piper-en]| [][wasm-ms-tts-piper-en]|
+|Speech synthesis (Piper, German)                                                                   |[Click me][wasm-hf-tts-piper-de]| [][wasm-ms-tts-piper-de]|
+|Speech synthesis (Matcha, Chinese)                                                                  |[Click me][wasm-hf-tts-matcha-zh]| [][wasm-ms-tts-matcha-zh]|
+|Speech synthesis (Matcha, English)                                                                  |[Click me][wasm-hf-tts-matcha-en]| [][wasm-ms-tts-matcha-en]|
+|Speech synthesis (Matcha, Chinese+English)                                                          |[Click me][wasm-hf-tts-matcha-zh-en]| [][wasm-ms-tts-matcha-zh-en]|
 |Speaker diarization                                                                         |[Click me][wasm-hf-speaker-diarization]|[][wasm-ms-speaker-diarization]|
 
 </details>
@@ -495,6 +498,12 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
 [wasm-ms-vad-asr-multi-lang-dolphin-base]: https://modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc
 [wasm-hf-vad-asr-multi-lang-dolphin-base]: https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc
 
+[wasm-hf-tts-matcha-zh-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-zh-en-tts-matcha
+[wasm-hf-tts-matcha-zh]: https://huggingface.co/spaces/k2-fsa/web-assembly-zh-tts-matcha
+[wasm-ms-tts-matcha-zh-en]: https://modelscope.cn/studios/csukuangfj/web-assembly-zh-en-tts-matcha
+[wasm-ms-tts-matcha-zh]: https://modelscope.cn/studios/csukuangfj/web-assembly-zh-tts-matcha
+[wasm-hf-tts-matcha-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-en-tts-matcha
+[wasm-ms-tts-matcha-en]: https://modelscope.cn/studios/csukuangfj/web-assembly-en-tts-matcha
 [wasm-hf-tts-piper-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en
 [wasm-ms-tts-piper-en]: https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en
 [wasm-hf-tts-piper-de]: https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de
diff --git a/scripts/wasm/generate-tts.py b/scripts/wasm/generate-tts.py
new file mode 100755
index 00000000..b2b41f03
--- /dev/null
+++ b/scripts/wasm/generate-tts.py
@@ -0,0 +1,189 @@
+#!/usr/bin/env python3
+
+import argparse
+from dataclasses import dataclass
+
+import jinja2
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--total",
+        type=int,
+        default=1,
+        help="Number of runners",
+    )
+    parser.add_argument(
+        "--index",
+        type=int,
+        default=0,
+        help="Index of the current runner",
+    )
+    return parser.parse_args()
+
+
+@dataclass
+class Model:
+    model_name: str
+    hf: str  # huggingface space name
+    ms: str  # modelscope space name
+    cmd: str = ""
+
+
+def get_models():
+    models = [
+        Model(
+            model_name="vits-piper-de_DE-thorsten_emotional-medium",
+            hf="k2-fsa/web-assembly-tts-sherpa-onnx-de",
+            ms="k2-fsa/web-assembly-tts-sherpa-onnx-de",
+            cmd="""
+            pushd $model_name
+
+            mv -v *.onnx ../
+            mv -v tokens.txt ../
+            mv -v espeak-ng-data ../
+            popd
+
+
+            git checkout .
+
+            rm -rf $model_name
+            git diff
+            """,
+        ),
+        Model(
+            model_name="vits-piper-en_US-libritts_r-medium",
+            hf="k2-fsa/web-assembly-tts-sherpa-onnx-en",
+            ms="k2-fsa/web-assembly-tts-sherpa-onnx-en",
+            cmd="""
+            pushd $model_name
+
+            mv -v *.onnx ../
+            mv -v tokens.txt ../
+            mv -v espeak-ng-data ../
+            popd
+
+
+            git checkout .
+
+            rm -rf $model_name
+            git diff
+            """,
+        ),
+        Model(
+            model_name="matcha-icefall-zh-en",
+            hf="k2-fsa/web-assembly-zh-en-tts-matcha",
+            ms="csukuangfj/web-assembly-zh-en-tts-matcha",
+            cmd="""
+            pushd $model_name
+
+            mv -v *.fst ../
+            mv -v *.onnx ../
+            mv -v tokens.txt ../
+            mv -v lexicon.txt ../
+            mv -v espeak-ng-data ../
+            popd
+
+            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
+
+            git checkout .
+            sed -i.bak 's/let type = 0/let type = 1/g' ../sherpa-onnx-tts.js
+
+            rm -rf $model_name
+            git diff
+            """,
+        ),
+        Model(
+            model_name="matcha-icefall-zh-baker",
+            hf="k2-fsa/web-assembly-zh-tts-matcha",
+            ms="csukuangfj/web-assembly-zh-tts-matcha",
+            cmd="""
+            pushd $model_name
+
+            mv -v *.fst ../
+            mv -v *.onnx ../
+            mv -v tokens.txt ../
+            mv -v lexicon.txt ../
+            popd
+
+            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-22khz-univ.onnx
+
+
+            git checkout .
+            sed -i.bak 's/let type = 0/let type = 2/g' ../sherpa-onnx-tts.js
+
+            rm -rf $model_name
+            git diff
+            """,
+        ),
+        Model(
+            model_name="matcha-icefall-en_US-ljspeech",
+            hf="k2-fsa/web-assembly-en-tts-matcha",
+            ms="csukuangfj/web-assembly-en-tts-matcha",
+            cmd="""
+            pushd $model_name
+
+            mv -v *.onnx ../
+            mv -v tokens.txt ../
+            mv -v espeak-ng-data ../
+            popd
+
+            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-22khz-univ.onnx
+
+
+            git checkout .
+            sed -i.bak 's/let type = 0/let type = 3/g' ../sherpa-onnx-tts.js
+
+            rm -rf $model_name
+            git diff
+            """,
+        ),
+    ]
+    return models
+
+
+def main():
+    args = get_args()
+    index = args.index
+    total = args.total
+    assert 0 <= index < total, (index, total)
+
+    all_model_list = get_models()
+
+    num_models = len(all_model_list)
+
+    num_per_runner = num_models // total
+    if num_per_runner <= 0:
+        raise ValueError(f"num_models: {num_models}, num_runners: {total}")
+
+    start = index * num_per_runner
+    end = start + num_per_runner
+
+    remaining = num_models - args.total * num_per_runner
+
+    print(f"{index}/{total}: {start}-{end}/{num_models}")
+
+    d = dict()
+    d["model_list"] = all_model_list[start:end]
+    if index < remaining:
+        s = args.total * num_per_runner + index
+        d["model_list"].append(all_model_list[s])
+        print(f"{s}/{num_models}")
+
+    filename_list = [
+        "./run-tts.sh",
+    ]
+    for filename in filename_list:
+        environment = jinja2.Environment()
+        with open(f"{filename}.in") as f:
+            s = f.read()
+        template = environment.from_string(s)
+
+        s = template.render(**d)
+        with open(filename, "w") as f:
+            print(s, file=f)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/wasm/generate-vad-asr.py b/scripts/wasm/generate-vad-asr.py
index 209ed24d..f20881f5 100755
--- a/scripts/wasm/generate-vad-asr.py
+++ b/scripts/wasm/generate-vad-asr.py
@@ -2,7 +2,6 @@
 
 import argparse
 from dataclasses import dataclass
-from typing import List, Optional
 
 import jinja2
 
diff --git a/scripts/wasm/run-tts.sh.in b/scripts/wasm/run-tts.sh.in
new file mode 100644
index 00000000..7063a37a
--- /dev/null
+++ b/scripts/wasm/run-tts.sh.in
@@ -0,0 +1,90 @@
+#!/usr/bin/env bash
+#
+# Build WebAssembly APPs for huggingface spaces and modelscope spaces
+
+set -ex
+
+log() {
+  # This function is from espnet
+  local fname=${BASH_SOURCE[1]##*/}
+  echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
+}
+
+SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+
+{% for model in model_list %}
+model_name={{ model.model_name }}
+hf_name={{ model.hf }}
+ms_name={{ model.ms }}
+
+pushd wasm/tts
+git checkout .
+rm -rf assets
+mkdir assets
+cd assets
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/${model_name}.tar.bz2
+tar xvf ${model_name}.tar.bz2
+rm ${model_name}.tar.bz2
+
+{{ model.cmd }}
+
+popd
+
+ls -lh wasm/tts/assets
+
+rm -rf build-wasm-simd-tts/install
+rm -rf build-wasm-simd-tts/wasm
+
+./build-wasm-simd-tts.sh
+
+dst=sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-${model_name}
+mv build-wasm-simd-tts/install/bin/wasm/tts $dst
+ls -lh $dst
+tar cjfv $dst.tar.bz2 ./$dst
+ls -lh *.tar.bz2
+
+git config --global user.email "csukuangfj@gmail.com"
+git config --global user.name "Fangjun Kuang"
+
+export GIT_LFS_SKIP_SMUDGE=1
+export GIT_CLONE_PROTECTION_ACTIVE=false
+
+rm -rf ms
+git clone https://www.modelscope.cn/studios/$ms_name.git ms
+
+cd ms
+cp -v ../$dst/* .
+
+git status
+git lfs track "*.data"
+git lfs track "*.wasm"
+ls -lh
+
+git add .
+git commit -m "update model" || true
+git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/$ms_name.git || true
+cd ..
+rm -rf ms
+
+rm -rf huggingface
+
+git clone https://huggingface.co/spaces/$hf_name huggingface
+cd huggingface
+cp -v ../$dst/* .
+
+git status
+git lfs track "*.data"
+git lfs track "*.wasm"
+ls -lh
+
+git add .
+git commit -m "update model" || true
+git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/$hf_name main || true
+cd ..
+rm -rf huggingface
+rm -rf $dst
+
+ls -lh *.tar.bz2
+
+{% endfor %}
diff --git a/wasm/asr/app-asr.js b/wasm/asr/app-asr.js
index 94725057..987449cb 100644
--- a/wasm/asr/app-asr.js
+++ b/wasm/asr/app-asr.js
@@ -15,7 +15,7 @@ let resultList = [];
 clearBtn.onclick = function() {
   resultList = [];
   textArea.value = getDisplayResult();
-  textArea.scrollTop = textArea.scrollHeight; // auto scroll
+  textArea.scrollTop = textArea.scrollHeight;  // auto scroll
 };
 
 function getDisplayResult() {
@@ -48,9 +48,21 @@ Module.locateFile = function(path, scriptDirectory = '') {
 Module.setStatus = function(status) {
   console.log(`status ${status}`);
   const statusElement = document.getElementById('status');
-  if (status == "Running...") {
+  if (status == 'Running...') {
     status = 'Model downloaded. Initializing recongizer...'
   }
+
+  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
+  if (downloadMatch) {
+    const downloaded = BigInt(downloadMatch[1]);
+    const total = BigInt(downloadMatch[2]);
+    const percent =
+        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
+    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
+        downloadMatch[2]})`;
+    console.log(`here ${status}`)
+  }
+
   statusElement.textContent = status;
   if (status === '') {
     statusElement.style.display = 'none';
@@ -80,11 +92,11 @@ let audioCtx;
 let mediaStream;
 
 let expectedSampleRate = 16000;
-let recordSampleRate; // the sampleRate of the microphone
-let recorder = null;  // the microphone
-let leftchannel = []; // TODO: Use a single channel
+let recordSampleRate;  // the sampleRate of the microphone
+let recorder = null;   // the microphone
+let leftchannel = [];  // TODO: Use a single channel
 
-let recordingLength = 0; // number of samples so far
+let recordingLength = 0;  // number of samples so far
 
 let recognizer = null;
 let recognizer_stream = null;
@@ -93,11 +105,11 @@ if (navigator.mediaDevices.getUserMedia) {
   console.log('getUserMedia supported.');
 
   // see https://w3c.github.io/mediacapture-main/#dom-mediadevices-getusermedia
-  const constraints = {audio : true};
+  const constraints = {audio: true};
 
   let onSuccess = function(stream) {
     if (!audioCtx) {
-      audioCtx = new AudioContext({sampleRate : 16000});
+      audioCtx = new AudioContext({sampleRate: 16000});
     }
     console.log(audioCtx);
     recordSampleRate = audioCtx.sampleRate;
@@ -160,7 +172,7 @@ if (navigator.mediaDevices.getUserMedia) {
       }
 
       textArea.value = getDisplayResult();
-      textArea.scrollTop = textArea.scrollHeight; // auto scroll
+      textArea.scrollTop = textArea.scrollHeight;  // auto scroll
 
       let buf = new Int16Array(samples.length);
       for (var i = 0; i < samples.length; ++i) {
@@ -247,8 +259,9 @@ if (navigator.mediaDevices.getUserMedia) {
     };
   };
 
-  let onError = function(
-      err) { console.log('The following error occured: ' + err); };
+  let onError = function(err) {
+    console.log('The following error occured: ' + err);
+  };
 
   navigator.mediaDevices.getUserMedia(constraints).then(onSuccess, onError);
 } else {
@@ -281,22 +294,22 @@ function toWav(samples) {
 
   // http://soundfile.sapp.org/doc/WaveFormat/
   //                   F F I R
-  view.setUint32(0, 0x46464952, true);              // chunkID
-  view.setUint32(4, 36 + samples.length * 2, true); // chunkSize
+  view.setUint32(0, 0x46464952, true);               // chunkID
+  view.setUint32(4, 36 + samples.length * 2, true);  // chunkSize
   //                   E V A W
-  view.setUint32(8, 0x45564157, true); // format
-                                       //
+  view.setUint32(8, 0x45564157, true);  // format
+                                        //
   //                      t m f
-  view.setUint32(12, 0x20746d66, true);             // subchunk1ID
-  view.setUint32(16, 16, true);                     // subchunk1Size, 16 for PCM
-  view.setUint32(20, 1, true);                      // audioFormat, 1 for PCM
-  view.setUint16(22, 1, true);                      // numChannels: 1 channel
-  view.setUint32(24, expectedSampleRate, true);     // sampleRate
-  view.setUint32(28, expectedSampleRate * 2, true); // byteRate
-  view.setUint16(32, 2, true);                      // blockAlign
-  view.setUint16(34, 16, true);                     // bitsPerSample
-  view.setUint32(36, 0x61746164, true);             // Subchunk2ID
-  view.setUint32(40, samples.length * 2, true);     // subchunk2Size
+  view.setUint32(12, 0x20746d66, true);          // subchunk1ID
+  view.setUint32(16, 16, true);                  // subchunk1Size, 16 for PCM
+  view.setUint32(20, 1, true);                   // audioFormat, 1 for PCM
+  view.setUint16(22, 1, true);                   // numChannels: 1 channel
+  view.setUint32(24, expectedSampleRate, true);  // sampleRate
+  view.setUint32(28, expectedSampleRate * 2, true);  // byteRate
+  view.setUint16(32, 2, true);                       // blockAlign
+  view.setUint16(34, 16, true);                      // bitsPerSample
+  view.setUint32(36, 0x61746164, true);              // Subchunk2ID
+  view.setUint32(40, samples.length * 2, true);      // subchunk2Size
 
   let offset = 44;
   for (let i = 0; i < samples.length; ++i) {
@@ -304,7 +317,7 @@ function toWav(samples) {
     offset += 2;
   }
 
-  return new Blob([ view ], {type : 'audio/wav'});
+  return new Blob([view], {type: 'audio/wav'});
 }
 
 // this function is copied from
diff --git a/wasm/tts/CMakeLists.txt b/wasm/tts/CMakeLists.txt
index bc1da0fd..a560289a 100644
--- a/wasm/tts/CMakeLists.txt
+++ b/wasm/tts/CMakeLists.txt
@@ -2,7 +2,7 @@ if(NOT $ENV{SHERPA_ONNX_IS_USING_BUILD_WASM_SH})
   message(FATAL_ERROR "Please use ./build-wasm-simd-tts.sh to build for wasm TTS")
 endif()
 
-if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/assets/model.onnx")
+if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/assets/tokens.txt")
   message(FATAL_ERROR "Please read ${CMAKE_CURRENT_SOURCE_DIR}/assets/README.md before you continue")
 endif()
 
diff --git a/wasm/tts/app-tts.js b/wasm/tts/app-tts.js
index c87e70f9..624c974c 100644
--- a/wasm/tts/app-tts.js
+++ b/wasm/tts/app-tts.js
@@ -26,9 +26,21 @@ Module.locateFile = function(path, scriptDirectory = '') {
 Module.setStatus = function(status) {
   console.log(`status ${status}`);
   const statusElement = document.getElementById('status');
-  if (status == "Running...") {
+  if (status == 'Running...') {
     status = 'Model downloaded. Initializing text to speech model...'
   }
+
+  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
+  if (downloadMatch) {
+    const downloaded = BigInt(downloadMatch[1]);
+    const total = BigInt(downloadMatch[2]);
+    const percent =
+        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
+    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
+        downloadMatch[2]})`;
+    console.log(`here ${status}`)
+  }
+
   statusElement.textContent = status;
   if (status === '') {
     statusElement.style.display = 'none';
@@ -57,7 +69,9 @@ Module.onRuntimeInitialized = function() {
   generateBtn.disabled = false;
 };
 
-speedInput.oninput = function() { speedValue.innerHTML = this.value; };
+speedInput.oninput = function() {
+  speedValue.innerHTML = this.value;
+};
 
 generateBtn.onclick = function() {
   let speakerId = speakerIdInput.value;
@@ -89,12 +103,12 @@ generateBtn.onclick = function() {
   console.log('text', text);
 
   let audio =
-      tts.generate({text : text, sid : speakerId, speed : speedInput.value});
+      tts.generate({text: text, sid: speakerId, speed: speedInput.value});
 
   console.log(audio.samples.length, audio.sampleRate);
 
   if (!audioCtx) {
-    audioCtx = new AudioContext({sampleRate : tts.sampleRate});
+    audioCtx = new AudioContext({sampleRate: tts.sampleRate});
   }
 
   const buffer = audioCtx.createBuffer(1, audio.samples.length, tts.sampleRate);
@@ -175,22 +189,22 @@ function toWav(floatSamples, sampleRate) {
 
   // http://soundfile.sapp.org/doc/WaveFormat/
   //                   F F I R
-  view.setUint32(0, 0x46464952, true);              // chunkID
-  view.setUint32(4, 36 + samples.length * 2, true); // chunkSize
+  view.setUint32(0, 0x46464952, true);               // chunkID
+  view.setUint32(4, 36 + samples.length * 2, true);  // chunkSize
   //                   E V A W
-  view.setUint32(8, 0x45564157, true); // format
-                                       //
+  view.setUint32(8, 0x45564157, true);  // format
+                                        //
   //                      t m f
-  view.setUint32(12, 0x20746d66, true);         // subchunk1ID
-  view.setUint32(16, 16, true);                 // subchunk1Size, 16 for PCM
-  view.setUint32(20, 1, true);                  // audioFormat, 1 for PCM
-  view.setUint16(22, 1, true);                  // numChannels: 1 channel
-  view.setUint32(24, sampleRate, true);         // sampleRate
-  view.setUint32(28, sampleRate * 2, true);     // byteRate
-  view.setUint16(32, 2, true);                  // blockAlign
-  view.setUint16(34, 16, true);                 // bitsPerSample
-  view.setUint32(36, 0x61746164, true);         // Subchunk2ID
-  view.setUint32(40, samples.length * 2, true); // subchunk2Size
+  view.setUint32(12, 0x20746d66, true);          // subchunk1ID
+  view.setUint32(16, 16, true);                  // subchunk1Size, 16 for PCM
+  view.setUint32(20, 1, true);                   // audioFormat, 1 for PCM
+  view.setUint16(22, 1, true);                   // numChannels: 1 channel
+  view.setUint32(24, sampleRate, true);          // sampleRate
+  view.setUint32(28, sampleRate * 2, true);      // byteRate
+  view.setUint16(32, 2, true);                   // blockAlign
+  view.setUint16(34, 16, true);                  // bitsPerSample
+  view.setUint32(36, 0x61746164, true);          // Subchunk2ID
+  view.setUint32(40, samples.length * 2, true);  // subchunk2Size
 
   let offset = 44;
   for (let i = 0; i < samples.length; ++i) {
@@ -198,5 +212,5 @@ function toWav(floatSamples, sampleRate) {
     offset += 2;
   }
 
-  return new Blob([ view ], {type : 'audio/wav'});
+  return new Blob([view], {type: 'audio/wav'});
 }
diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
index a1c4d4bf..e24bbc3f 100644
--- a/wasm/tts/sherpa-onnx-tts.js
+++ b/wasm/tts/sherpa-onnx-tts.js
@@ -543,17 +543,17 @@ class OfflineTts {
 }
 
 function createOfflineTts(Module, myConfig) {
-  const offlineTtsVitsModelConfig = {
-    model: './model.onnx',
+  const vits = {
+    model: '',
     lexicon: '',
-    tokens: './tokens.txt',
-    dataDir: './espeak-ng-data',
+    tokens: '',
+    dataDir: '',
     noiseScale: 0.667,
     noiseScaleW: 0.8,
     lengthScale: 1.0,
   };
 
-  const offlineTtsMatchaModelConfig = {
+  const matcha = {
     acousticModel: '',
     vocoder: '',
     lexicon: '',
@@ -581,9 +581,48 @@ function createOfflineTts(Module, myConfig) {
     lengthScale: 1.0,
   };
 
+  let ruleFsts = '';
+
+  let type = 0;
+  switch (type) {
+    case 0:
+      // vits
+      vits.model = './model.onnx';
+      vits.tokens = './tokens.txt';
+      vits.dataDir = './espeak-ng-data';
+      break;
+    case 1:
+      // matcha zh-en
+      // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
+      matcha.acousticModel = './model-steps-3.onnx';
+      matcha.vocoder = './vocos-16khz-univ.onnx';
+      matcha.lexicon = './lexicon.txt';
+      matcha.tokens = './tokens.txt';
+      matcha.dataDir = './espeak-ng-data';
+      ruleFsts = './phone-zh.fst,./date-zh.fst,./number-zh.fst';
+      break;
+    case 2:
+      // matcha zh
+      // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese/matcha-icefall-zh-baker.html
+      matcha.acousticModel = './model-steps-3.onnx';
+      matcha.vocoder = './vocos-22khz-univ.onnx';
+      matcha.lexicon = './lexicon.txt';
+      matcha.tokens = './tokens.txt';
+      ruleFsts = './phone.fst,./date.fst,./number.fst';
+      break;
+    case 3:
+      // matcha en
+      // https://k2-fsa.github.io/sherpa/onnx/tts/all/English/matcha-icefall-en_US-ljspeech.html
+      matcha.acousticModel = './model-steps-3.onnx';
+      matcha.vocoder = './vocos-22khz-univ.onnx';
+      matcha.tokens = './tokens.txt';
+      matcha.dataDir = './espeak-ng-data';
+      break;
+  }
+
   const offlineTtsModelConfig = {
-    offlineTtsVitsModelConfig: offlineTtsVitsModelConfig,
-    offlineTtsMatchaModelConfig: offlineTtsMatchaModelConfig,
+    offlineTtsVitsModelConfig: vits,
+    offlineTtsMatchaModelConfig: matcha,
     offlineTtsKokoroModelConfig: offlineTtsKokoroModelConfig,
     offlineTtsKittenModelConfig: offlineTtsKittenModelConfig,
     numThreads: 1,
@@ -593,7 +632,7 @@ function createOfflineTts(Module, myConfig) {
 
   let offlineTtsConfig = {
     offlineTtsModelConfig: offlineTtsModelConfig,
-    ruleFsts: '',
+    ruleFsts: ruleFsts,
     ruleFars: '',
     maxNumSentences: 1,
   }
diff --git a/wasm/vad-asr/app-vad-asr.js b/wasm/vad-asr/app-vad-asr.js
index 258753e8..5eaf5d20 100644
--- a/wasm/vad-asr/app-vad-asr.js
+++ b/wasm/vad-asr/app-vad-asr.js
@@ -140,6 +140,18 @@ Module.setStatus = function(status) {
   if (status == 'Running...') {
     status = 'Model downloaded. Initializing recongizer...'
   }
+
+  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
+  if (downloadMatch) {
+    const downloaded = BigInt(downloadMatch[1]);
+    const total = BigInt(downloadMatch[2]);
+    const percent =
+        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
+    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
+        downloadMatch[2]})`;
+    console.log(`here ${status}`)
+  }
+
   statusElement.textContent = status;
   if (status === '') {
     statusElement.style.display = 'none';
diff --git a/wasm/vad/app-vad.js b/wasm/vad/app-vad.js
index 45e8fe4b..e4ec4ba6 100644
--- a/wasm/vad/app-vad.js
+++ b/wasm/vad/app-vad.js
@@ -56,6 +56,18 @@ Module.setStatus = function(status) {
   if (status == 'Running...') {
     status = 'Model downloaded. Initializing vad...'
   }
+
+  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
+  if (downloadMatch) {
+    const downloaded = BigInt(downloadMatch[1]);
+    const total = BigInt(downloadMatch[2]);
+    const percent =
+        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
+    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
+        downloadMatch[2]})`;
+    console.log(`here ${status}`)
+  }
+
   statusElement.textContent = status;
   if (status === '') {
     statusElement.style.display = 'none';

commit afa59281c10fa294e91bfc6b627a402b1ec5a592
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Dec 10 15:57:56 2025 +0800

    Build APKs for MatchaTTS Chinese+English (#2882)

diff --git a/.gitignore b/.gitignore
index 6fdcc69a..612f82b2 100755
--- a/.gitignore
+++ b/.gitignore
@@ -164,3 +164,4 @@ sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
 build-riscv64-linux-gnu-spacemit/
 spacemit-toolchain*
 sherpa-onnx-qnn-*
+matcha-icefall-*
diff --git a/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt b/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
index 07b543e3..1cdb6ab5 100644
--- a/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
+++ b/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
@@ -294,6 +294,15 @@ class MainActivity : AppCompatActivity() {
         // dataDir = "kokoro-multi-lang-v1_0/espeak-ng-data"
         // isKitten = true
 
+        // Example 12
+        // matcha-icefall-zh-en
+        // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
+        // modelDir = "matcha-icefall-zh-en"
+        // acousticModelName = "model-steps-3.onnx"
+        // vocoder = "vocos-16khz-univ.onnx"    // Vocoder should be downloaded separately; place in the **root directory of your resources folder**, not under modelDir.
+        // dataDir = "matcha-icefall-zh-en/espeak-ng-data"
+        // lexicon = "lexicon.txt"
+
         if (dataDir != null) {
             val newDir = copyDataDir(dataDir!!)
             dataDir = "$newDir/$dataDir"
diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
index 38ce8096..f388b5f4 100644
--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
+++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
@@ -44,6 +44,7 @@ import androidx.compose.ui.unit.dp
 import com.k2fsa.sherpa.onnx.tts.engine.ui.theme.SherpaOnnxTtsEngineTheme
 import kotlinx.coroutines.CoroutineScope
 import kotlinx.coroutines.Dispatchers
+import kotlinx.coroutines.SupervisorJob
 import kotlinx.coroutines.channels.Channel
 import kotlinx.coroutines.launch
 import kotlinx.coroutines.withContext
@@ -64,7 +65,9 @@ class MainActivity : ComponentActivity() {
 
     private var stopped: Boolean = false
 
-    private var samplesChannel = Channel<FloatArray>()
+    private var samplesChannel = Channel<FloatArray>(capacity = 128)
+    private val scope = CoroutineScope(Dispatchers.IO + SupervisorJob())
+
 
     override fun onCreate(savedInstanceState: Bundle?) {
         super.onCreate(savedInstanceState)
@@ -177,8 +180,16 @@ class MainActivity : ComponentActivity() {
                                                 rtfText = ""
                                                 Log.i(TAG, "Started with text $testText")
 
-                                                CoroutineScope(Dispatchers.IO).launch {
+                                                scope.launch {
                                                     for (samples in samplesChannel) {
+                                                        if (samples.isEmpty()) {
+                                                            break
+                                                        }
+
+                                                        Log.i(
+                                                            TAG,
+                                                            "Received ${samples.count()} samples"
+                                                        )
                                                         track.write(
                                                             samples,
                                                             0,
@@ -189,10 +200,14 @@ class MainActivity : ComponentActivity() {
                                                             break
                                                         }
                                                     }
+                                                    Log.i(TAG, "Draining the channel")
 
-                                                    for (s in samplesChannel) {
-                                                        // drain the channel
+                                                    // drain remaining
+                                                    while (!samplesChannel.isEmpty) {
+                                                        samplesChannel.tryReceive().getOrNull()
                                                     }
+                                                    Log.i(TAG, "Channel drained")
+
                                                 }
 
                                                 CoroutineScope(Dispatchers.Default).launch {
@@ -222,6 +237,12 @@ class MainActivity : ComponentActivity() {
                                                         elapsed / audioDuration
                                                     )
 
+                                                    scope.launch {
+                                                        Log.i(TAG, "send 0 samples")
+                                                            samplesChannel.send(FloatArray(0))
+                                                        Log.i(TAG, "send 0 samples done")
+                                                    }
+
                                                     val filename =
                                                         application.filesDir.absolutePath + "/generated.wav"
 
@@ -237,8 +258,10 @@ class MainActivity : ComponentActivity() {
                                                             playEnabled = true
                                                             rtfText = RTF
                                                         }
+
+
                                                     }
-                                                }.start()
+                                                }
                                             }
                                         }) {
                                         Text("Start")
@@ -311,8 +334,10 @@ class MainActivity : ComponentActivity() {
     private fun callback(samples: FloatArray): Int {
         if (!stopped) {
             val samplesCopy = samples.copyOf()
-            CoroutineScope(Dispatchers.IO).launch {
-                samplesChannel.send(samplesCopy)
+            scope.launch {
+                Log.i(TAG, "callback called with ${samplesCopy.count()} samples")
+                val ok = samplesChannel.trySend(samplesCopy).isSuccess
+                Log.i(TAG, "callback called with $ok")
             }
             return 1
         } else {
diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
index 078b0948..649b7dce 100644
--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
+++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
@@ -176,6 +176,16 @@ object TtsEngine {
         // dataDir = "kitten-nano-en-v0_1-fp16/espeak-ng-data"
         // lang = "eng"
         // isKitten = true
+
+        // Example 12
+        // matcha-icefall-zh-en
+        // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
+        // modelDir = "matcha-icefall-zh-en"
+        // acousticModelName = "model-steps-3.onnx"
+        // vocoder = "vocos-16khz-univ.onnx"
+        // dataDir = "matcha-icefall-zh-en/espeak-ng-data"
+        // lexicon = "lexicon.txt"
+        // lang = "zho"
     }
 
     fun createTts(context: Context) {
diff --git a/scripts/apk/build-apk-tts-engine.sh.in b/scripts/apk/build-apk-tts-engine.sh.in
index 341bc6f7..b3abfd08 100644
--- a/scripts/apk/build-apk-tts-engine.sh.in
+++ b/scripts/apk/build-apk-tts-engine.sh.in
@@ -113,6 +113,10 @@ if [[ $model_dir == vits-melo-tts-zh_en ]]; then
   lang=zh_en
 fi
 
+if [[ $model_dir == matcha-icefall-zh-en ]]; then
+  lang=zh_en
+fi
+
 if [[ $model_dir == kokoro-multi-lang-v1_0 || $model_dir == kokoro-multi-lang-v1_1 || $model_dir == kokoro-int8-multi-lang-v1_1 ]]; then
   lang=zh_en
 fi
diff --git a/scripts/apk/build-apk-tts.sh.in b/scripts/apk/build-apk-tts.sh.in
index 42e2811b..bdb3b77f 100644
--- a/scripts/apk/build-apk-tts.sh.in
+++ b/scripts/apk/build-apk-tts.sh.in
@@ -107,6 +107,10 @@ if [[ $model_dir == vits-melo-tts-zh_en ]]; then
   lang=zh_en
 fi
 
+if [[ $model_dir == matcha-icefall-zh-en ]]; then
+  lang=zh_en
+fi
+
 if [[ $model_dir == kokoro-multi-lang-v1_0 || $model_dir == kokoro-multi-lang-v1_1 || $model_dir == kokoro-int8-multi-lang-v1_1 ]]; then
   lang=zh_en
 fi
diff --git a/scripts/apk/generate-tts-apk-script.py b/scripts/apk/generate-tts-apk-script.py
index 8ea80d35..756bb8ac 100755
--- a/scripts/apk/generate-tts-apk-script.py
+++ b/scripts/apk/generate-tts-apk-script.py
@@ -411,7 +411,6 @@ def get_vits_models() -> List[TtsModel]:
             or "melo-tts" in m.model_dir
         ):
             s = s[:-1]
-            m.dict_dir = m.model_dir + "/dict"
         else:
             m.rule_fars = f"{m.model_dir}/rule.far"
 
@@ -440,15 +439,30 @@ def get_matcha_models() -> List[TtsModel]:
             model_dir="matcha-icefall-zh-baker",
             acoustic_model_name="model-steps-3.onnx",
             lang="zh",
+            lexicon="lexicon.txt",
         )
     ]
     rule_fsts = ["phone.fst", "date.fst", "number.fst"]
     for m in chinese_models:
         s = [f"{m.model_dir}/{r}" for r in rule_fsts]
         m.rule_fsts = ",".join(s)
-        m.dict_dir = m.model_dir + "/dict"
         m.vocoder = "vocos-22khz-univ.onnx"
 
+    chinese_english_models = [
+        TtsModel(
+            model_dir="matcha-icefall-zh-en",
+            acoustic_model_name="model-steps-3.onnx",
+            lang="zh",
+            lexicon="lexicon.txt",
+        )
+    ]
+    rule_fsts_zh = ["phone-zh.fst", "date-zh.fst", "number-zh.fst"]
+    for m in chinese_english_models:
+        s = [f"{m.model_dir}/{r}" for r in rule_fsts_zh]
+        m.rule_fsts = ",".join(s)
+        m.vocoder = "vocos-16khz-univ.onnx"
+        m.data_dir = f"{m.model_dir}/espeak-ng-data"
+
     english_persian_models = [
         TtsModel(
             model_dir="matcha-icefall-en_US-ljspeech",
@@ -470,7 +484,7 @@ def get_matcha_models() -> List[TtsModel]:
         m.data_dir = f"{m.model_dir}/espeak-ng-data"
         m.vocoder = "vocos-22khz-univ.onnx"
 
-    return chinese_models + english_persian_models
+    return chinese_models + english_persian_models + chinese_english_models
 
 
 def get_kokoro_models() -> List[TtsModel]:
@@ -507,7 +521,6 @@ def get_kokoro_models() -> List[TtsModel]:
     ]
     for m in multi_lingual_models:
         m.data_dir = f"{m.model_dir}/espeak-ng-data"
-        m.dict_dir = f"{m.model_dir}/dict"
         m.voices = "voices.bin"
         m.lexicon = f"{m.model_dir}/lexicon-us-en.txt,{m.model_dir}/lexicon-zh.txt"
         m.rule_fsts = f"{m.model_dir}/phone-zh.fst,{m.model_dir}/date-zh.fst,{m.model_dir}/number-zh.fst"

commit 9fdd5099362a64e9933584a3121544707ed2e08f
Author: zhouyong <769721723@qq.com>
Date:   Tue Dec 9 21:36:06 2025 +0800

    Optimize streaming output results when VAD does not detect human voice for a long time (#2876)
    
    Co-authored-by: zhouyong <yonga.zhou@archermind.com>

diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
index e6d5cb3d..ae5e1bd8 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
@@ -143,6 +143,7 @@ fun HomeScreen() {
                     var startTime = System.currentTimeMillis()
                     var lastText = ""
                     var added = false
+                    var speechStartOffset = 0
 
 
                     while (isStarted) {
@@ -162,6 +163,11 @@ fun HomeScreen() {
                                 offset += windowSize
                                 if (!isSpeechStarted && SimulateStreamingAsr.vad.isSpeechDetected()) {
                                     isSpeechStarted = true
+                                    // offset 0.25s
+                                    speechStartOffset = offset - 6400
+                                    if(speechStartOffset < 0) {
+                                        speechStartOffset = 0
+                                    }
                                     startTime = System.currentTimeMillis()
                                 }
                             }
@@ -172,7 +178,7 @@ fun HomeScreen() {
                                 // You can change it to some other value
                                 val stream = SimulateStreamingAsr.recognizer.createStream()
                                 stream.acceptWaveform(
-                                    buffer.subList(0, offset).toFloatArray(),
+                                    buffer.subList(speechStartOffset, offset).toFloatArray(),
                                     sampleRateInHz
                                 )
                                 SimulateStreamingAsr.recognizer.decode(stream)

commit d5b381ccc4c29446a385458d5a35b14018e3a22e
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Tue Dec 9 21:32:42 2025 +0800

    Export models to Ascend 910B4 (#2878)

diff --git a/.github/scripts/export-ascend/generate_paraformer.py b/.github/scripts/export-ascend/generate_paraformer.py
index fa22d64b..396460bf 100755
--- a/.github/scripts/export-ascend/generate_paraformer.py
+++ b/.github/scripts/export-ascend/generate_paraformer.py
@@ -5,7 +5,7 @@ import itertools
 import json
 from dataclasses import asdict, dataclass
 
-from generate_zipformer_ctc_20250703 import get_image
+from generate_zipformer_ctc_20250703 import get_cann_version, get_image, get_soc_version
 
 
 @dataclass
@@ -26,8 +26,8 @@ class Config:
 
 
 def main():
-    cann_version = ["7.0", "8.0", "8.2"]
-    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    cann_version = get_cann_version()
+    soc_version = get_soc_version()
     framework_list = ["FunASR", "WSChuan-ASR"]
 
     configs = [
diff --git a/.github/scripts/export-ascend/generate_sense_voice.py b/.github/scripts/export-ascend/generate_sense_voice.py
index f1ecbf39..6c54a0a4 100755
--- a/.github/scripts/export-ascend/generate_sense_voice.py
+++ b/.github/scripts/export-ascend/generate_sense_voice.py
@@ -5,7 +5,7 @@ import itertools
 import json
 from dataclasses import asdict, dataclass
 
-from generate_zipformer_ctc_20250703 import get_image
+from generate_zipformer_ctc_20250703 import get_image, get_soc_version, get_cann_version
 
 
 @dataclass
@@ -26,8 +26,8 @@ class Config:
 
 
 def main():
-    cann_version = ["7.0", "8.0", "8.2"]
-    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    cann_version = get_cann_version()
+    soc_version = get_soc_version()
     framework_list = ["FunASR", "WSYue-ASR"]
 
     configs = [
diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
index 511bfee1..f1865d5d 100755
--- a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
@@ -32,6 +32,16 @@ def get_image(cann: str, soc_version: str):
         raise ValueError(f"Unsupported soc_version {soc_version}")
 
 
+def get_soc_version():
+    soc_version = ["910B", "910B2", "910B3", "910B4", "310P3"]
+    return soc_version
+
+
+def get_cann_version():
+    cann_version = ["7.0", "8.0", "8.2"]
+    return cann_version
+
+
 @dataclass
 class Config:
     # 7.0, 8.0, 8.2
@@ -49,8 +59,8 @@ class Config:
 
 
 def main():
-    cann_version = ["7.0", "8.0", "8.2"]
-    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    cann_version = get_cann_version()
+    soc_version = get_soc_version()
     input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
 
     configs = [
diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
index 63587194..9b8d62fc 100644
--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
+++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
 on:
   push:
     branches:
-      - refactor-ascend-export-script
+      - ascend-910b4-2
   workflow_dispatch:
 
 concurrency:
@@ -26,8 +26,8 @@ jobs:
         id: set-matrix
         run: |
           # outputting for debugging purposes
-          python3 .github/scripts/export-ascend/generate_sense_voice.py
-          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
+          python3 .github/scripts/export-ascend/generate_paraformer.py
+          MATRIX=$(python3 .github/scripts/export-ascend/generate_paraformer.py)
 
           # deprecated
           # echo "::set-output name=matrix::${MATRIX}"
@@ -75,6 +75,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           echo "CANN environment:"
           which atc || echo "atc not found"
           atc --help
@@ -129,6 +132,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           soc_version=${{ matrix.soc_version }}
           cann=${{ matrix.cann }}
 
@@ -222,6 +228,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           soc_version=${{ matrix.soc_version }}
           cann=${{ matrix.cann }}
 
diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
index 0be4e277..efc87861 100644
--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
 on:
   push:
     branches:
-      - refactor-ascend-export-script
+      - ascend-910b4-2
   workflow_dispatch:
 
 concurrency:
diff --git a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
index 4169f7c0..af517075 100644
--- a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+++ b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
@@ -3,7 +3,7 @@ name: export-zipformer-ctc-to-ascend-npu-20250703
 on:
   push:
     branches:
-      - export-zipformer-ctc-ascend
+      - ascend-910b4-2
   workflow_dispatch:
 
 concurrency:

commit 8fac37f7d10f74945eda983e019f6deaacbb88b6
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Tue Dec 9 17:55:19 2025 +0800

    Load QNN context binary for faster startup (#2877)

diff --git a/.github/scripts/export-qnn/__init__.py b/.github/scripts/export-qnn/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/.github/scripts/export-qnn/device_info.py b/.github/scripts/export-qnn/device_info.py
new file mode 120000
index 00000000..57618a93
--- /dev/null
+++ b/.github/scripts/export-qnn/device_info.py
@@ -0,0 +1 @@
+../../../scripts/qnn/device_info.py
\ No newline at end of file
diff --git a/.github/scripts/export-qnn/generate_config.py b/.github/scripts/export-qnn/generate_config.py
new file mode 120000
index 00000000..fbdbd361
--- /dev/null
+++ b/.github/scripts/export-qnn/generate_config.py
@@ -0,0 +1 @@
+../../../scripts/qnn/generate_config.py
\ No newline at end of file
diff --git a/.github/scripts/export-qnn/generate_sense_voice.py b/.github/scripts/export-qnn/generate_sense_voice.py
new file mode 100755
index 00000000..8a3e1778
--- /dev/null
+++ b/.github/scripts/export-qnn/generate_sense_voice.py
@@ -0,0 +1,47 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import json
+
+from device_info import soc_info_dict
+from dataclasses import asdict, dataclass
+import itertools
+
+
+@dataclass
+class Config:
+    soc: str  # SM8850
+    soc_id: int  # 87
+    arch: str  # v81
+    input_in_seconds: str
+    framework: str
+
+
+def main():
+
+    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+    framework_list = ["FunASR", "WSYue-ASR"]
+
+    configs = []
+
+    for name, soc in soc_info_dict.items():
+        for num_seconds, framework in itertools.product(
+            input_in_seconds, framework_list
+        ):
+            configs.append(
+                Config(
+                    soc=name,
+                    soc_id=soc.model.value,
+                    arch=soc.info.arch.name,
+                    input_in_seconds=num_seconds,
+                    framework=framework,
+                )
+            )
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/workflows/export-sense-voice-to-qnn.yaml b/.github/workflows/export-sense-voice-to-qnn.yaml
index 73c8a4ec..db88a409 100644
--- a/.github/workflows/export-sense-voice-to-qnn.yaml
+++ b/.github/workflows/export-sense-voice-to-qnn.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-qnn
 on:
   push:
     branches:
-      - export-sense-voice-qnn-2
+      - qnn-binary-2
   workflow_dispatch:
 
 concurrency:
@@ -11,25 +11,45 @@ concurrency:
   cancel-in-progress: true
 
 jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-qnn/generate_sense_voice.py
+          MATRIX=$(python3 .github/scripts/export-qnn/generate_sense_voice.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
   export-sense-voice-to-qnn:
+    needs: generate_build_matrix
     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
-    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }}
-    runs-on: ${{ matrix.os }}
+    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
+    runs-on: ubuntu-22.04
     strategy:
       fail-fast: false
       matrix:
-        os: [ubuntu-22.04]
-        python-version: ["3.10"]
-        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
-        framework: ["FunASR", "WSYue-ASR"]
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
 
     steps:
       - uses: actions/checkout@v4
 
-      - name: Setup Python ${{ matrix.python-version }}
+      - name: Setup Python 3.10
         uses: actions/setup-python@v5
         with:
-          python-version: ${{ matrix.python-version }}
+          python-version: "3.10"
 
       - name: Display NDK HOME
         shell: bash
@@ -37,6 +57,11 @@ jobs:
           echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
           ls -lh ${ANDROID_NDK_LATEST_HOME}
 
+      - name: Create directories
+        shell: bash
+        run: |
+          mkdir so binary
+
       - name: Create Python virtual environment
         shell: bash
         run: |
@@ -54,13 +79,13 @@ jobs:
       - name: Download toolkit
         shell: bash
         run: |
-          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
-          ls -lh v2.33.0.250327.zip
+          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.40.0.251030.zip
+          ls -lh v2.40.0.251030.zip
 
       - name: Unzip toolkit
         shell: bash
         run: |
-          unzip v2.33.0.250327.zip
+          unzip v2.40.0.251030.zip
 
       - name: Show
         shell: bash
@@ -82,7 +107,7 @@ jobs:
 
           ls -lh qairt
 
-          cd qairt/2.33.0.250327/bin
+          cd qairt/2.40.0.251030/bin
           source envsetup.sh
 
           yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
@@ -92,7 +117,7 @@ jobs:
         run: |
           source py310/bin/activate
 
-          cd qairt/2.33.0.250327/bin
+          cd qairt/2.40.0.251030/bin
           source envsetup.sh
 
           python3 -m pip install \
@@ -147,7 +172,7 @@ jobs:
         run: |
           source py310/bin/activate
 
-          pushd qairt/2.33.0.250327/bin
+          pushd qairt/2.40.0.251030/bin
           source envsetup.sh
           popd
 
@@ -158,7 +183,7 @@ jobs:
         run: |
           source py310/bin/activate
 
-          pushd qairt/2.33.0.250327/bin
+          pushd qairt/2.40.0.251030/bin
           source envsetup.sh
           popd
 
@@ -169,7 +194,7 @@ jobs:
         run: |
           source py310/bin/activate
 
-          pushd qairt/2.33.0.250327/bin
+          pushd qairt/2.40.0.251030/bin
           source envsetup.sh
           popd
 
@@ -181,12 +206,13 @@ jobs:
         run: |
           source py310/bin/activate
 
-          pushd qairt/2.33.0.250327/bin
+          pushd qairt/2.40.0.251030/bin
           source envsetup.sh
           popd
 
           export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
           export LDFLAGS="-Wl,-z,max-page-size=16384"
+          dir=$PWD
 
           cd scripts/sense-voice/qnn
 
@@ -269,8 +295,48 @@ jobs:
 
           readelf -lW model_libs/*/lib*.so
 
+          echo "Generate context binary"
+
+          $dir/scripts/qnn/generate_config.py  \
+            --soc ${{ matrix.soc }} \
+            --graph-name "model_${t}_seconds_quantized" \
+            --output-dir ./my-config \
+            --qnn-sdk-root $QNN_SDK_ROOT
+
+          ls -lh my-config
+
+          head -n 1000 my-config/*.json
+
+          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
+            --output_dir ./binary \
+            --binary_file model \
+            --config_file ./my-config/htp_backend_extensions.json
+
+          ls -lh binary/
+
           echo "collect results"
 
+          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v README.md $d
+          cp -v LICENSE $d
+          cp -v binary/model.bin $d/
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+
+          echo "num_frames=$num_frames" > $d/info.txt
+
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+          mv *.tar.bz2 ../../../binary/
+
+
           for p in x86_64-linux-clang aarch64-android; do
             if [[ $p == x86_64-linux-clang ]]; then
               d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-linux-x64
@@ -302,15 +368,17 @@ jobs:
           echo "----show---"
           ls -lh *.tar.bz2
 
-          mv *.tar.bz2 ../../..
+          mv *.tar.bz2 ../../../so/
+
 
       - name: Run SenseVoice from WSYue-ASR
         if: matrix.framework == 'WSYue-ASR'
         shell: bash
         run: |
+          dir=$PWD
           source py310/bin/activate
 
-          pushd qairt/2.33.0.250327/bin
+          pushd qairt/2.40.0.251030/bin
           source envsetup.sh
           popd
 
@@ -416,6 +484,44 @@ jobs:
 
           readelf -lW model_libs/*/lib*.so
 
+          $dir/scripts/qnn/generate_config.py  \
+            --soc ${{ matrix.soc }} \
+            --graph-name "model_${t}_seconds_quantized" \
+            --output-dir ./my-config \
+            --qnn-sdk-root $QNN_SDK_ROOT
+
+          ls -lh my-config
+
+          head -n 1000 my-config/*.json
+
+          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
+            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
+            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
+            --output_dir ./binary \
+            --binary_file model \
+            --config_file ./my-config/htp_backend_extensions.json
+
+          ls -lh binary/
+
+          echo "collect results"
+
+          d=sherpa-onnx-qnn-${{ matrix.soc }}-binary-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v README.md $d
+          cp -v binary/model.bin $d/
+          cp -v tokens.txt $d
+          cp -v *.wav $d/test_wavs
+
+          echo "num_frames=$num_frames" > $d/info.txt
+
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+          mv *.tar.bz2 ../../../binary/
+
           echo "collect results"
           for p in x86_64-linux-clang aarch64-android; do
             if [[ $p == x86_64-linux-clang ]]; then
@@ -447,22 +553,42 @@ jobs:
           echo "----show---"
           ls -lh *.tar.bz2
 
-          mv *.tar.bz2 ../../..
+          mv *.tar.bz2 ../../../so/
 
       - uses: actions/upload-artifact@v4
         with:
-          name: ${{ matrix.framework }}-${{ matrix.input_in_seconds }}-seconds
+          name: ${{ matrix.framework }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
           path: ./scripts/sense-voice/qnn/*.json
 
+      - name: Release
+        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn
+
       - name: Release
         if: github.repository_owner == 'csukuangfj'
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
-          file: ./*.tar.bz2
+          file: ./binary/*.tar.bz2
           overwrite: true
           repo_name: k2-fsa/sherpa-onnx
           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn-binary
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./so/*.tar.bz2
+          overwrite: true
           tag: asr-models-qnn
 
       - name: Release
@@ -470,6 +596,6 @@ jobs:
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
-          file: ./*.tar.bz2
+          file: ./binary/*.tar.bz2
           overwrite: true
-          tag: asr-models-qnn
+          tag: asr-models-qnn-binary
diff --git a/.gitignore b/.gitignore
index 813b50a2..6fdcc69a 100755
--- a/.gitignore
+++ b/.gitignore
@@ -163,3 +163,4 @@ sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
 sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
 build-riscv64-linux-gnu-spacemit/
 spacemit-toolchain*
+sherpa-onnx-qnn-*
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
index 470850c9..8a09d884 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
@@ -132,9 +132,15 @@ object SimulateStreamingAsr {
                 config.modelConfig.tokens =
                     copyAssetToInternalStorage(config.modelConfig.tokens, context)
 
-                if (config.modelConfig.senseVoice.model.isNotEmpty()) {
-                    config.modelConfig.senseVoice.model =
-                        copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
+                if (config.modelConfig.senseVoice.model.isNotEmpty() || assetExists(
+                        context.assets,
+                        path = config.modelConfig.senseVoice.qnnConfig.contextBinary
+                    )
+                ) {
+                    if (config.modelConfig.senseVoice.model.isNotEmpty()) {
+                        config.modelConfig.senseVoice.model =
+                            copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
+                    }
 
                     config.modelConfig.senseVoice.qnnConfig.contextBinary =
                         copyAssetToInternalStorage(
diff --git a/scripts/qnn/__init__.py b/scripts/qnn/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/scripts/qnn/device_info.py b/scripts/qnn/device_info.py
new file mode 100755
index 00000000..03135f2c
--- /dev/null
+++ b/scripts/qnn/device_info.py
@@ -0,0 +1,112 @@
+#!/usr/bin/env python3
+from dataclasses import dataclass
+from enum import IntEnum, unique
+
+"""
+See also
+https://docs.qualcomm.com/doc/80-63442-10/topic/QNN_general_overview.html#supported-snapdragon-devices
+
+SA8255 soc_id    52 dsp_arch     v73 vtcm_size (MB)      8
+SA8295 soc_id    39 dsp_arch     v68 vtcm_size (MB)      8
+SM8350 soc_id    35 dsp_arch     v68 vtcm_size (MB)      4
+SM8450 soc_id    36 dsp_arch     v69 vtcm_size (MB)      8
+SM8475 soc_id    42 dsp_arch     v69 vtcm_size (MB)      8
+SM8550 soc_id    43 dsp_arch     v73 vtcm_size (MB)      8
+SM8650 soc_id    57 dsp_arch     v75 vtcm_size (MB)      8
+SM8750 soc_id    69 dsp_arch     v79 vtcm_size (MB)      8
+SM8850 soc_id    87 dsp_arch     v81 vtcm_size (MB)      8
+SSG2115P soc_id  46 dsp_arch     v73 vtcm_size (MB)      2
+SSG2125P soc_id  58 dsp_arch     v73 vtcm_size (MB)      2
+SXR1230P soc_id  45 dsp_arch     v73 vtcm_size (MB)      2
+SXR2230P soc_id  53 dsp_arch     v69 vtcm_size (MB)      8
+SXR2330P soc_id  75 dsp_arch     v79 vtcm_size (MB)      8
+QCS9100 soc_id   77 dsp_arch     v73 vtcm_size (MB)      8
+SAR2230P soc_id  95 dsp_arch     v81 vtcm_size (MB)      4
+SW6100 soc_id    96 dsp_arch     v81 vtcm_size (MB)      4
+"""
+
+
+@unique
+class Chipset(IntEnum):
+    # see https://github.com/pytorch/executorch/blob/main/backends/qualcomm/serialization/qc_schema.py#L41
+    # SA8255, soc_id 52,  dsp_arch v73
+    SA8255 = 52  # v73
+    SA8295 = 39  # v68
+    SM8350 = 35  # v68
+    SM8450 = 36  # v69
+    SM8475 = 42  # v69
+    SM8550 = 43  # v73
+    SM8650 = 57  # v75
+    SM8750 = 69  # v79
+    SM8850 = 87  # v81
+    #  SSG2115P = 46  # v73
+    #  SSG2125P = 58  # v73
+    #  SXR1230P = 45  # v73
+    #  SXR2230P = 53  # v69
+    #  SXR2330P = 75  # v79
+    QCS9100 = 77  # v73
+    #  SAR2230P = 95  # v81
+    #  SW6100 = 96  # v81
+
+
+@unique
+class HtpArch(IntEnum):
+    v68 = 68
+    v69 = 69
+    v73 = 73
+    v75 = 75
+    v79 = 79
+    v81 = 81
+    v87 = 87
+
+
+@dataclass
+class HtpInfo:
+    arch: HtpArch
+    vtcm_size_in_mb: int
+
+
+@dataclass
+class SocInfo:
+    model: Chipset
+    info: HtpInfo
+
+
+soc_info_list = [
+    SocInfo(Chipset.SA8255, HtpInfo(HtpArch.v73, 8)),
+    SocInfo(Chipset.SA8295, HtpInfo(HtpArch.v68, 8)),
+    SocInfo(Chipset.SM8350, HtpInfo(HtpArch.v68, 4)),
+    SocInfo(Chipset.SM8450, HtpInfo(HtpArch.v69, 8)),
+    SocInfo(Chipset.SM8475, HtpInfo(HtpArch.v69, 8)),
+    SocInfo(Chipset.SM8550, HtpInfo(HtpArch.v73, 8)),
+    SocInfo(Chipset.SM8650, HtpInfo(HtpArch.v75, 8)),
+    SocInfo(Chipset.SM8750, HtpInfo(HtpArch.v79, 8)),
+    SocInfo(Chipset.SM8850, HtpInfo(HtpArch.v81, 8)),
+    #  SocInfo(Chipset.SSG2115P, HtpInfo(HtpArch.v73, 2)),
+    #  SocInfo(Chipset.SSG2125P, HtpInfo(HtpArch.v73, 2)),
+    #  SocInfo(Chipset.SXR1230P, HtpInfo(HtpArch.v73, 2)),
+    #  SocInfo(Chipset.SXR2230P, HtpInfo(HtpArch.v69, 8)),
+    #  SocInfo(Chipset.SXR2330P, HtpInfo(HtpArch.v79, 8)),
+    SocInfo(Chipset.QCS9100, HtpInfo(HtpArch.v73, 8)),
+    #  SocInfo(Chipset.SAR2230P, HtpInfo(HtpArch.v81, 4)),
+    #  SocInfo(Chipset.SW6100, HtpInfo(HtpArch.v81, 4)),
+]
+
+soc_info_dict = {soc.model.name: soc for soc in soc_info_list}
+
+
+def _test():
+    for soc in soc_info_list:
+        print(
+            soc.model.name,
+            "soc_id\t",
+            soc.model.value,
+            "dsp_arch\t",
+            soc.info.arch.name,
+            "vtcm_size (MB)\t",
+            soc.info.vtcm_size_in_mb,
+        )
+
+
+if __name__ == "__main__":
+    _test()
diff --git a/scripts/qnn/generate_config.py b/scripts/qnn/generate_config.py
new file mode 100755
index 00000000..cb985c85
--- /dev/null
+++ b/scripts/qnn/generate_config.py
@@ -0,0 +1,122 @@
+#!/usr/bin/env python3
+
+# see
+# https://github.com/MollySophia/rwkv-qualcomm/blob/2a82c641c90ee130cbd7038ca7449b2fa818de71/utils/htp_devices_config.py
+# https://docs.qualcomm.com/bundle/publicresource/topics/80-64748-1/model_prep_linux.html#QNN-HTP-context-binary
+
+import argparse
+import json
+from pathlib import Path
+
+from device_info import soc_info_dict
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--soc",
+        type=str,
+        required=True,
+        help="SM8850, SA8295, etc",
+    )
+
+    parser.add_argument(
+        "--graph-name",
+        type=str,
+        required=True,
+        help="Graph name",
+    )
+
+    parser.add_argument(
+        "--output-dir",
+        type=str,
+        required=True,
+        help="Output directory to save the generated json files",
+    )
+
+    parser.add_argument(
+        "--qnn-sdk-root",
+        type=str,
+        required=True,
+        help="Path to qnn sdk",
+    )
+
+    return parser.parse_args()
+
+
+def generate_config(
+    soc_name: str,
+    graph_name: str,
+    output_dir: str,
+    qnn_sdk_root: str,
+):
+    if soc_name not in soc_info_dict:
+        raise ValueError(
+            f"Unsupported SOC {soc_name}. Supported: - {sorted(list(soc_info_dict.keys()))}"
+        )
+    soc = soc_info_dict[soc_name]
+
+    output_dir = Path(output_dir).absolute()
+    output_dir.mkdir(parents=True, exist_ok=True)
+
+    htp_backend_extensions_data = {
+        "backend_extensions": {
+            "shared_library_path": f"{qnn_sdk_root}/lib/x86_64-linux-clang/libQnnHtpNetRunExtensions.so",
+            "config_file_path": f"{output_dir}/htp_config.json",
+        }
+    }
+
+    htp_backend_config_data = {
+        "graphs": [
+            {
+                "vtcm_mb": soc.info.vtcm_size_in_mb,
+                "O": 3,
+                "graph_names": [graph_name],
+            }
+        ],
+        "devices": [
+            {
+                "device_id": 0,
+                "soc_id": soc.model.value,
+                "dsp_arch": soc.info.arch.name,
+                "cores": [
+                    {
+                        "core_id": 0,
+                        "perf_profile": "burst",
+                        "rpc_control_latency": 200,
+                    }
+                ],
+            }
+        ],
+    }
+
+    with open(str(output_dir / "htp_backend_extensions.json"), "w") as f:
+        json.dump(htp_backend_extensions_data, f, indent=4)
+
+    with open(str(output_dir / "htp_config.json"), "w") as f:
+        json.dump(htp_backend_config_data, f, indent=4)
+
+
+def _test():
+    qnn_sdk_root = "/home/fangjun/open-source/qairt/2.40.0.251030"
+    generate_config(
+        soc_name="SM8850",
+        graph_name="model_10_seconds_quantized",
+        output_dir="./tmp",
+        qnn_sdk_root=qnn_sdk_root,
+    )
+
+
+if __name__ == "__main__":
+    #  _test()
+
+    args = get_args()
+    print(vars(args))
+    generate_config(
+        soc_name=args.soc,
+        graph_name=args.graph_name,
+        output_dir=args.output_dir,
+        qnn_sdk_root=args.qnn_sdk_root,
+    )
+
+# ./generate_config.py  --soc SM8850 --graph-name abc --output-dir ./tmp2 --qnn-sdk-root $QNN_SDK_ROOT
diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
index 7d536750..49189f9f 100644
--- a/sherpa-onnx/csrc/offline-model-config.cc
+++ b/sherpa-onnx/csrc/offline-model-config.cc
@@ -135,7 +135,8 @@ bool OfflineModelConfig::Validate() const {
     return wenet_ctc.Validate();
   }
 
-  if (!sense_voice.model.empty()) {
+  if (!sense_voice.model.empty() ||
+      !sense_voice.qnn_config.context_binary.empty()) {
     return sense_voice.Validate();
   }
 
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index a473a717..a58b34ef 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -171,7 +171,8 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 
   if (config.model_config.provider == "qnn") {
 #if SHERPA_ONNX_ENABLE_QNN
-    if (!config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty() ||
+        !config.model_config.sense_voice.qnn_config.context_binary.empty()) {
       return std::make_unique<
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
           config);
@@ -491,7 +492,8 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 
   if (config.model_config.provider == "qnn") {
 #if SHERPA_ONNX_ENABLE_QNN
-    if (!config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty() ||
+        !config.model_config.sense_voice.qnn_config.context_binary.empty()) {
       return std::make_unique<
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
           mgr, config);
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
index ddadbfb9..ae5482fc 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
@@ -29,9 +29,16 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
 }
 
 bool OfflineSenseVoiceModelConfig::Validate() const {
-  if (!FileExists(model)) {
-    SHERPA_ONNX_LOGE("SenseVoice model '%s' does not exist", model.c_str());
-    return false;
+  if (qnn_config.context_binary.empty()) {
+    if (model.empty()) {
+      SHERPA_ONNX_LOGE("Please provide a senseVoice model");
+      return false;
+    }
+
+    if (!FileExists(model)) {
+      SHERPA_ONNX_LOGE("SenseVoice model '%s' does not exist", model.c_str());
+      return false;
+    }
   }
 
   if (!language.empty()) {
@@ -46,7 +53,18 @@ bool OfflineSenseVoiceModelConfig::Validate() const {
     }
   }
 
-  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
+  if (model.empty() && !qnn_config.context_binary.empty()) {
+    // we require that the context_binary exists
+    if (!FileExists(qnn_config.context_binary)) {
+      SHERPA_ONNX_LOGE(
+          "Model is empty, but you provide a context binary that does not "
+          "exist");
+      return false;
+    }
+  }
+
+  if (EndsWith(model, ".so") || EndsWith(model, ".bin") ||
+      (model.empty() && !qnn_config.context_binary.empty())) {
     return qnn_config.Validate();
   }
 
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index b1b4bd8e..f27dfbc5 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -1113,6 +1113,24 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                 tokens = "$modelDir/tokens.txt",
             )
         }
+
+        9022 -> {
+            // for my Xiaomi 17 Pro
+            val modelDir = "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    qnnConfig = QnnConfig(
+                        // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
     }
     return null
 }

commit b229e031c54f3c25f3677003fe01c8a83cf77c16
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Dec 8 14:48:42 2025 +0800

    Remove unused lock file (#2875)

diff --git a/dart-api-examples/streaming-asr/pubspec.lock b/dart-api-examples/streaming-asr/pubspec.lock
deleted file mode 100644
index 349b3b46..00000000
--- a/dart-api-examples/streaming-asr/pubspec.lock
+++ /dev/null
@@ -1,432 +0,0 @@
-# Generated by pub
-# See https://dart.dev/tools/pub/glossary#lockfile
-packages:
-  _fe_analyzer_shared:
-    dependency: transitive
-    description:
-      name: _fe_analyzer_shared
-      sha256: "0b2f2bd91ba804e53a61d757b986f89f1f9eaed5b11e4b2f5a2468d86d6c9fc7"
-      url: "https://pub.dev"
-    source: hosted
-    version: "67.0.0"
-  analyzer:
-    dependency: transitive
-    description:
-      name: analyzer
-      sha256: "37577842a27e4338429a1cbc32679d508836510b056f1eedf0c8d20e39c1383d"
-      url: "https://pub.dev"
-    source: hosted
-    version: "6.4.1"
-  args:
-    dependency: "direct main"
-    description:
-      name: args
-      sha256: "7cf60b9f0cc88203c5a190b4cd62a99feea42759a7fa695010eb5de1c0b2252a"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.5.0"
-  async:
-    dependency: transitive
-    description:
-      name: async
-      sha256: "947bfcf187f74dbc5e146c9eb9c0f10c9f8b30743e341481c1e2ed3ecc18c20c"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.11.0"
-  boolean_selector:
-    dependency: transitive
-    description:
-      name: boolean_selector
-      sha256: "6cfb5af12253eaf2b368f07bacc5a80d1301a071c73360d746b7f2e32d762c66"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.1"
-  characters:
-    dependency: transitive
-    description:
-      name: characters
-      sha256: "04a925763edad70e8443c99234dc3328f442e811f1d8fd1a72f1c8ad0f69a605"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.3.0"
-  collection:
-    dependency: transitive
-    description:
-      name: collection
-      sha256: ee67cb0715911d28db6bf4af1026078bd6f0128b07a5f66fb2ed94ec6783c09a
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.18.0"
-  convert:
-    dependency: transitive
-    description:
-      name: convert
-      sha256: "0f08b14755d163f6e2134cb58222dd25ea2a2ee8a195e53983d57c075324d592"
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.1.1"
-  coverage:
-    dependency: transitive
-    description:
-      name: coverage
-      sha256: "3945034e86ea203af7a056d98e98e42a5518fff200d6e8e6647e1886b07e936e"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.8.0"
-  crypto:
-    dependency: transitive
-    description:
-      name: crypto
-      sha256: ff625774173754681d66daaf4a448684fb04b78f902da9cb3d308c19cc5e8bab
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.0.3"
-  ffi:
-    dependency: transitive
-    description:
-      name: ffi
-      sha256: "493f37e7df1804778ff3a53bd691d8692ddf69702cf4c1c1096a2e41b4779e21"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.2"
-  file:
-    dependency: transitive
-    description:
-      name: file
-      sha256: "5fc22d7c25582e38ad9a8515372cd9a93834027aacf1801cf01164dac0ffa08c"
-      url: "https://pub.dev"
-    source: hosted
-    version: "7.0.0"
-  flutter:
-    dependency: transitive
-    description: flutter
-    source: sdk
-    version: "0.0.0"
-  frontend_server_client:
-    dependency: transitive
-    description:
-      name: frontend_server_client
-      sha256: f64a0333a82f30b0cca061bc3d143813a486dc086b574bfb233b7c1372427694
-      url: "https://pub.dev"
-    source: hosted
-    version: "4.0.0"
-  glob:
-    dependency: transitive
-    description:
-      name: glob
-      sha256: "0e7014b3b7d4dac1ca4d6114f82bf1782ee86745b9b42a92c9289c23d8a0ab63"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.2"
-  http_multi_server:
-    dependency: transitive
-    description:
-      name: http_multi_server
-      sha256: "97486f20f9c2f7be8f514851703d0119c3596d14ea63227af6f7a481ef2b2f8b"
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.2.1"
-  http_parser:
-    dependency: transitive
-    description:
-      name: http_parser
-      sha256: "2aa08ce0341cc9b354a498388e30986515406668dbcc4f7c950c3e715496693b"
-      url: "https://pub.dev"
-    source: hosted
-    version: "4.0.2"
-  io:
-    dependency: transitive
-    description:
-      name: io
-      sha256: "2ec25704aba361659e10e3e5f5d672068d332fc8ac516421d483a11e5cbd061e"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.0.4"
-  js:
-    dependency: transitive
-    description:
-      name: js
-      sha256: c1b2e9b5ea78c45e1a0788d29606ba27dc5f71f019f32ca5140f61ef071838cf
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.7.1"
-  lints:
-    dependency: "direct dev"
-    description:
-      name: lints
-      sha256: cbf8d4b858bb0134ef3ef87841abdf8d63bfc255c266b7bf6b39daa1085c4290
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.0.0"
-  logging:
-    dependency: transitive
-    description:
-      name: logging
-      sha256: "623a88c9594aa774443aa3eb2d41807a48486b5613e67599fb4c41c0ad47c340"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.2.0"
-  matcher:
-    dependency: transitive
-    description:
-      name: matcher
-      sha256: d2323aa2060500f906aa31a895b4030b6da3ebdcc5619d14ce1aada65cd161cb
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.12.16+1"
-  material_color_utilities:
-    dependency: transitive
-    description:
-      name: material_color_utilities
-      sha256: "0e0a020085b65b6083975e499759762399b4475f766c21668c4ecca34ea74e5a"
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.8.0"
-  meta:
-    dependency: transitive
-    description:
-      name: meta
-      sha256: "7687075e408b093f36e6bbf6c91878cc0d4cd10f409506f7bc996f68220b9136"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.12.0"
-  mime:
-    dependency: transitive
-    description:
-      name: mime
-      sha256: "2e123074287cc9fd6c09de8336dae606d1ddb88d9ac47358826db698c176a1f2"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.0.5"
-  node_preamble:
-    dependency: transitive
-    description:
-      name: node_preamble
-      sha256: "6e7eac89047ab8a8d26cf16127b5ed26de65209847630400f9aefd7cd5c730db"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.0.2"
-  package_config:
-    dependency: transitive
-    description:
-      name: package_config
-      sha256: "1c5b77ccc91e4823a5af61ee74e6b972db1ef98c2ff5a18d3161c982a55448bd"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.0"
-  path:
-    dependency: "direct main"
-    description:
-      name: path
-      sha256: "087ce49c3f0dc39180befefc60fdb4acd8f8620e5682fe2476afd0b3688bb4af"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.9.0"
-  pool:
-    dependency: transitive
-    description:
-      name: pool
-      sha256: "20fe868b6314b322ea036ba325e6fc0711a22948856475e2c2b6306e8ab39c2a"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.5.1"
-  pub_semver:
-    dependency: transitive
-    description:
-      name: pub_semver
-      sha256: "40d3ab1bbd474c4c2328c91e3a7df8c6dd629b79ece4c4bd04bee496a224fb0c"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.4"
-  shelf:
-    dependency: transitive
-    description:
-      name: shelf
-      sha256: ad29c505aee705f41a4d8963641f91ac4cee3c8fad5947e033390a7bd8180fa4
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.4.1"
-  shelf_packages_handler:
-    dependency: transitive
-    description:
-      name: shelf_packages_handler
-      sha256: "89f967eca29607c933ba9571d838be31d67f53f6e4ee15147d5dc2934fee1b1e"
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.0.2"
-  shelf_static:
-    dependency: transitive
-    description:
-      name: shelf_static
-      sha256: a41d3f53c4adf0f57480578c1d61d90342cd617de7fc8077b1304643c2d85c1e
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.1.2"
-  shelf_web_socket:
-    dependency: transitive
-    description:
-      name: shelf_web_socket
-      sha256: "9ca081be41c60190ebcb4766b2486a7d50261db7bd0f5d9615f2d653637a84c1"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.0.4"
-  sherpa_onnx:
-    dependency: "direct main"
-    description:
-      name: sherpa_onnx
-      sha256: e45894f81e7c854ca96d678bcab5303036e884a7c90e9a6c4ec04c7b1ee215a8
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.9.29"
-  sky_engine:
-    dependency: transitive
-    description: flutter
-    source: sdk
-    version: "0.0.99"
-  source_map_stack_trace:
-    dependency: transitive
-    description:
-      name: source_map_stack_trace
-      sha256: "84cf769ad83aa6bb61e0aa5a18e53aea683395f196a6f39c4c881fb90ed4f7ae"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.1"
-  source_maps:
-    dependency: transitive
-    description:
-      name: source_maps
-      sha256: "708b3f6b97248e5781f493b765c3337db11c5d2c81c3094f10904bfa8004c703"
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.10.12"
-  source_span:
-    dependency: transitive
-    description:
-      name: source_span
-      sha256: "53e943d4206a5e30df338fd4c6e7a077e02254531b138a15aec3bd143c1a8b3c"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.10.0"
-  stack_trace:
-    dependency: transitive
-    description:
-      name: stack_trace
-      sha256: "73713990125a6d93122541237550ee3352a2d84baad52d375a4cad2eb9b7ce0b"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.11.1"
-  stream_channel:
-    dependency: transitive
-    description:
-      name: stream_channel
-      sha256: ba2aa5d8cc609d96bbb2899c28934f9e1af5cddbd60a827822ea467161eb54e7
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.2"
-  string_scanner:
-    dependency: transitive
-    description:
-      name: string_scanner
-      sha256: "556692adab6cfa87322a115640c11f13cb77b3f076ddcc5d6ae3c20242bedcde"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.2.0"
-  term_glyph:
-    dependency: transitive
-    description:
-      name: term_glyph
-      sha256: a29248a84fbb7c79282b40b8c72a1209db169a2e0542bce341da992fe1bc7e84
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.2.1"
-  test:
-    dependency: "direct dev"
-    description:
-      name: test
-      sha256: "7ee446762c2c50b3bd4ea96fe13ffac69919352bd3b4b17bac3f3465edc58073"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.25.2"
-  test_api:
-    dependency: transitive
-    description:
-      name: test_api
-      sha256: "9955ae474176f7ac8ee4e989dadfb411a58c30415bcfb648fa04b2b8a03afa7f"
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.7.0"
-  test_core:
-    dependency: transitive
-    description:
-      name: test_core
-      sha256: "2bc4b4ecddd75309300d8096f781c0e3280ca1ef85beda558d33fcbedc2eead4"
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.6.0"
-  typed_data:
-    dependency: transitive
-    description:
-      name: typed_data
-      sha256: facc8d6582f16042dd49f2463ff1bd6e2c9ef9f3d5da3d9b087e244a7b564b3c
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.3.2"
-  vector_math:
-    dependency: transitive
-    description:
-      name: vector_math
-      sha256: "80b3257d1492ce4d091729e3a67a60407d227c27241d6927be0130c98e741803"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.1.4"
-  vm_service:
-    dependency: transitive
-    description:
-      name: vm_service
-      sha256: f652077d0bdf60abe4c1f6377448e8655008eef28f128bc023f7b5e8dfeb48fc
-      url: "https://pub.dev"
-    source: hosted
-    version: "14.2.4"
-  watcher:
-    dependency: transitive
-    description:
-      name: watcher
-      sha256: "3d2ad6751b3c16cf07c7fca317a1413b3f26530319181b37e3b9039b84fc01d8"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.1.0"
-  web:
-    dependency: transitive
-    description:
-      name: web
-      sha256: "97da13628db363c635202ad97068d47c5b8aa555808e7a9411963c533b449b27"
-      url: "https://pub.dev"
-    source: hosted
-    version: "0.5.1"
-  web_socket_channel:
-    dependency: transitive
-    description:
-      name: web_socket_channel
-      sha256: "58c6666b342a38816b2e7e50ed0f1e261959630becd4c879c4f26bfa14aa5a42"
-      url: "https://pub.dev"
-    source: hosted
-    version: "2.4.5"
-  webkit_inspection_protocol:
-    dependency: transitive
-    description:
-      name: webkit_inspection_protocol
-      sha256: "87d3f2333bb240704cd3f1c6b5b7acd8a10e7f0bc28c28dcf14e782014f4a572"
-      url: "https://pub.dev"
-    source: hosted
-    version: "1.2.1"
-  yaml:
-    dependency: transitive
-    description:
-      name: yaml
-      sha256: "75769501ea3489fca56601ff33454fe45507ea3bfb014161abc3b43ae25989d5"
-      url: "https://pub.dev"
-    source: hosted
-    version: "3.1.2"
-sdks:
-  dart: ">=3.4.0 <4.0.0"
-  flutter: ">=3.3.0"

commit 132c91d7dcf0763eb0c3a518ad968b784682c568
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Dec 8 13:32:18 2025 +0800

    Refactor Paraformer Impl (#2874)
    
    This pull request refactors the Paraformer offline speech recognizer by introducing a generic templated implementation. This change consolidates the code for different hardware backends like RKNN and Ascend, promoting code reuse and simplifying future maintenance and extension of Paraformer support across various platforms. The previous platform-specific implementation files have been either removed or converted into this new templated structure.

diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
deleted file mode 100644
index 2e1bf68a..00000000
--- a/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
+++ /dev/null
@@ -1,121 +0,0 @@
-// sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
-#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
-
-#include <memory>
-#include <utility>
-#include <vector>
-
-#include "sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.h"
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-model-config.h"
-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
-#include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/symbol-table.h"
-
-namespace sherpa_onnx {
-
-// defined in ../offline-recognizer-paraformer-impl.h
-OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
-                                 const SymbolTable &sym_table);
-
-class OfflineRecognizerParaformerAscendImpl : public OfflineRecognizerImpl {
- public:
-  explicit OfflineRecognizerParaformerAscendImpl(
-      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(config),
-        config_(config),
-        symbol_table_(config_.model_config.tokens),
-        model_(std::make_unique<OfflineParaformerModelAscend>(
-            config.model_config)) {
-    if (config.decoding_method != "greedy_search") {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  template <typename Manager>
-  OfflineRecognizerParaformerAscendImpl(Manager *mgr,
-                                        const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(mgr, config),
-        config_(config),
-        symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineParaformerModelAscend>(
-            mgr, config.model_config)) {
-    if (config.decoding_method != "greedy_search") {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
-  }
-
-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    for (int32_t i = 0; i < n; ++i) {
-      DecodeOneStream(ss[i]);
-    }
-  }
-
-  OfflineRecognizerConfig GetConfig() const override { return config_; }
-
- private:
-  void InitFeatConfig() {
-    config_.feat_config.normalize_samples = false;
-    config_.feat_config.window_type = "hamming";
-    config_.feat_config.high_freq = 0;
-    config_.feat_config.snip_edges = true;
-  }
-
-  void DecodeOneStream(OfflineStream *s) const {
-    std::vector<float> f = s->GetFrames();
-
-    std::vector<float> logits = model_->Run(std::move(f));
-    if (logits.empty()) {
-      SHERPA_ONNX_LOGE("No speech detected");
-      return;
-    }
-
-    int32_t vocab_size = model_->VocabSize();
-    int32_t num_tokens = logits.size() / vocab_size;
-
-    int32_t eos_id = symbol_table_["</s>"];
-
-    OfflineParaformerDecoderResult r;
-    const float *p = logits.data();
-    for (int32_t i = 0; i < num_tokens; ++i) {
-      auto max_idx = static_cast<int64_t>(
-          std::distance(p, std::max_element(p, p + vocab_size)));
-
-      if (max_idx == eos_id) {
-        break;
-      }
-      r.tokens.push_back(max_idx);
-      p += vocab_size;
-    }
-
-    auto result = Convert(r, symbol_table_);
-    result.text = ApplyInverseTextNormalization(std::move(result.text));
-    result.text = ApplyHomophoneReplacer(std::move(result.text));
-    s->SetResult(result);
-  }
-
- private:
-  OfflineRecognizerConfig config_;
-  SymbolTable symbol_table_;
-  std::unique_ptr<OfflineParaformerModelAscend> model_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index bde04386..a473a717 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -30,6 +30,7 @@
 #include "sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-moonshine-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-transducer-impl.h"
@@ -38,7 +39,7 @@
 #include "sherpa-onnx/csrc/text-utils.h"
 
 #if SHERPA_ONNX_ENABLE_RKNN
-#include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
+#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
 #include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
 #endif
 
@@ -51,7 +52,7 @@
 #endif
 
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
-#include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
+#include "sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.h"
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
 #include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
 #endif
@@ -72,7 +73,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
           config);
     } else if (!config.model_config.paraformer.model.empty()) {
-      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelRknn>>(
+          config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice and Paraformer models are currently supported "
@@ -144,7 +147,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
           config);
     } else if (!config.model_config.paraformer.model.empty()) {
-      return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelAscend>>(
+          config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(config);
     } else {
@@ -387,7 +392,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
           mgr, config);
     } else if (!config.model_config.paraformer.model.empty()) {
-      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelRknn>>(
+          mgr, config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice and Paraformer models are currently supported "
@@ -459,8 +466,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
           mgr, config);
     } else if (!config.model_config.paraformer.model.empty()) {
-      return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
-                                                                     config);
+      return std::make_unique<
+          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelAscend>>(
+          mgr, config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(mgr,
                                                                        config);
diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h b/sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
similarity index 77%
rename from sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
rename to sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
index 28fc17a6..a6b264cb 100644
--- a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
@@ -1,9 +1,9 @@
-// sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+// sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
 //
 // Copyright (c)  2025  Xiaomi Corporation
 
-#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
-#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_
 
 #include <memory>
 #include <utility>
@@ -13,7 +13,6 @@
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
 #include "sherpa-onnx/csrc/symbol-table.h"
 
 namespace sherpa_onnx {
@@ -22,15 +21,15 @@ namespace sherpa_onnx {
 OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
                                  const SymbolTable &sym_table);
 
-class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
+template <typename ParaformerModel>
+class OfflineRecognizerParaformerTplImpl : public OfflineRecognizerImpl {
  public:
-  explicit OfflineRecognizerParaformerRknnImpl(
+  explicit OfflineRecognizerParaformerTplImpl(
       const OfflineRecognizerConfig &config)
       : OfflineRecognizerImpl(config),
         config_(config),
         symbol_table_(config_.model_config.tokens),
-        model_(
-            std::make_unique<OfflineParaformerModelRknn>(config.model_config)) {
+        model_(std::make_unique<ParaformerModel>(config.model_config)) {
     if (config.decoding_method != "greedy_search") {
       SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
                        config.decoding_method.c_str());
@@ -41,13 +40,12 @@ class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
   }
 
   template <typename Manager>
-  OfflineRecognizerParaformerRknnImpl(Manager *mgr,
-                                      const OfflineRecognizerConfig &config)
+  OfflineRecognizerParaformerTplImpl(Manager *mgr,
+                                     const OfflineRecognizerConfig &config)
       : OfflineRecognizerImpl(mgr, config),
         config_(config),
         symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineParaformerModelRknn>(
-            mgr, config.model_config)) {
+        model_(std::make_unique<ParaformerModel>(mgr, config.model_config)) {
     if (config.decoding_method != "greedy_search") {
       SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
                        config.decoding_method.c_str());
@@ -113,9 +111,9 @@ class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
  private:
   OfflineRecognizerConfig config_;
   SymbolTable symbol_table_;
-  std::unique_ptr<OfflineParaformerModelRknn> model_;
+  std::unique_ptr<ParaformerModel> model_;
 };
 
 }  // namespace sherpa_onnx
 
-#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_

commit f201a29fa53354b52d5bfc56f70ae697a607640c
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Dec 8 12:54:12 2025 +0800

    Refactor sense voice impl (#2873)
    
    This PR refactors the SenseVoice offline recognizer implementations to eliminate code duplication across multiple hardware backends. Previously, each hardware provider (RKNN, Axera, AXCL, Ascend, QNN) had its own nearly identical implementation file. The refactoring consolidates all these implementations into a single template class OfflineRecognizerSenseVoiceTplImpl parameterized by the model type.

diff --git a/build-axcl-linux-aarch64.sh b/build-axcl-linux-aarch64.sh
index 7b767cae..e3e7e563 100755
--- a/build-axcl-linux-aarch64.sh
+++ b/build-axcl-linux-aarch64.sh
@@ -21,7 +21,11 @@ set -ex
 
 # Before you run this file, make sure you have first cloned
 # https://github.com/Abandon-ht/axcl_bsp_sdk
-# and set the environment variable SHERPA_ONNX_AXERA_PATH
+# and set the environment variable SHERPA_ONNX_AXCL_SDK_ROOT
+
+if [ -d ./axcl_bsp_sdk ]; then
+  AXCL_SDK_ROOT=/star-fj/fangjun/open-source/sherpa-onnx/axcl_bsp_sdk/out
+fi
 
 if [ -z "$AXCL_SDK_ROOT" ]; then
   AXCL_SDK_ROOT=/home/m5stack/Workspace/kaldi/sherpa-onnx/axcl_bsp_sdk/out
@@ -114,7 +118,7 @@ cmake \
   -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
   ..
 
-make VERBOSE=1 -j22
+make VERBOSE=1 -j2
 make install/strip
 
 # Enable it if only needed
diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
deleted file mode 100644
index 53da0300..00000000
--- a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+++ /dev/null
@@ -1,142 +0,0 @@
-// sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
-#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
-
-#include <memory>
-#include <utility>
-#include <vector>
-
-#include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-model-config.h"
-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
-#include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
-#include "sherpa-onnx/csrc/symbol-table.h"
-
-namespace sherpa_onnx {
-
-// defined in ../offline-recognizer-sense-voice-impl.h
-OfflineRecognitionResult ConvertSenseVoiceResult(
-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor);
-
-class OfflineRecognizerSenseVoiceAscendImpl : public OfflineRecognizerImpl {
- public:
-  explicit OfflineRecognizerSenseVoiceAscendImpl(
-      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(config),
-        config_(config),
-        symbol_table_(config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelAscend>(
-            config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  template <typename Manager>
-  OfflineRecognizerSenseVoiceAscendImpl(Manager *mgr,
-                                        const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(mgr, config),
-        config_(config),
-        symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelAscend>(
-            mgr, config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
-  }
-
-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    for (int32_t i = 0; i < n; ++i) {
-      DecodeOneStream(ss[i]);
-    }
-  }
-
-  OfflineRecognizerConfig GetConfig() const override { return config_; }
-
- private:
-  void InitFeatConfig() {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
-    config_.feat_config.window_type = "hamming";
-    config_.feat_config.high_freq = 0;
-    config_.feat_config.snip_edges = true;
-  }
-
-  void DecodeOneStream(OfflineStream *s) const {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    std::vector<float> f = s->GetFrames();
-
-    int32_t language = 0;
-    if (config_.model_config.sense_voice.language.empty()) {
-      language = 0;
-    } else if (meta_data.lang2id.count(
-                   config_.model_config.sense_voice.language)) {
-      language =
-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
-    } else {
-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
-                       config_.model_config.sense_voice.language.c_str());
-    }
-
-    int32_t text_norm = config_.model_config.sense_voice.use_itn
-                            ? meta_data.with_itn_id
-                            : meta_data.without_itn_id;
-
-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
-    if (logits.empty()) {
-      return;
-    }
-
-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
-
-    auto result =
-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
-
-    int32_t frame_shift_ms = 10;
-    int32_t subsampling_factor = meta_data.window_shift;
-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
-                                     subsampling_factor);
-
-    r.text = ApplyInverseTextNormalization(std::move(r.text));
-    r.text = ApplyHomophoneReplacer(std::move(r.text));
-    s->SetResult(r);
-  }
-
- private:
-  OfflineRecognizerConfig config_;
-  SymbolTable symbol_table_;
-  std::unique_ptr<OfflineSenseVoiceModelAscend> model_;
-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
diff --git a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
deleted file mode 100644
index 59062ac4..00000000
--- a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+++ /dev/null
@@ -1,138 +0,0 @@
-// sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
-#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
-
-#include <memory>
-#include <utility>
-#include <vector>
-
-#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-model-config.h"
-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
-#include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
-#include "sherpa-onnx/csrc/symbol-table.h"
-
-namespace sherpa_onnx {
-
-// defined in ../online-recognizer-sense-voice-impl.h
-OfflineRecognitionResult ConvertSenseVoiceResult(
-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor);
-
-class OfflineRecognizerSenseVoiceAxclImpl : public OfflineRecognizerImpl {
- public:
-  explicit OfflineRecognizerSenseVoiceAxclImpl(
-      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(config),
-        config_(config),
-        symbol_table_(config_.model_config.tokens),
-        model_(
-            std::make_unique<OfflineSenseVoiceModelAxcl>(config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  template <typename Manager>
-  OfflineRecognizerSenseVoiceAxclImpl(Manager *mgr,
-                                      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(mgr, config),
-        config_(config),
-        symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelAxcl>(
-            mgr, config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
-  }
-
-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    for (int32_t i = 0; i < n; ++i) {
-      DecodeOneStream(ss[i]);
-    }
-  }
-
-  OfflineRecognizerConfig GetConfig() const override { return config_; }
-
- private:
-  void InitFeatConfig() {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
-    config_.feat_config.window_type = "hamming";
-    config_.feat_config.high_freq = 0;
-    config_.feat_config.snip_edges = true;
-  }
-
-  void DecodeOneStream(OfflineStream *s) const {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    std::vector<float> f = s->GetFrames();
-
-    int32_t language = 0;
-    if (config_.model_config.sense_voice.language.empty()) {
-      language = 0;
-    } else if (meta_data.lang2id.count(
-                   config_.model_config.sense_voice.language)) {
-      language =
-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
-    } else {
-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
-                       config_.model_config.sense_voice.language.c_str());
-    }
-
-    int32_t text_norm = config_.model_config.sense_voice.use_itn
-                            ? meta_data.with_itn_id
-                            : meta_data.without_itn_id;
-
-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
-
-    auto result =
-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
-
-    int32_t frame_shift_ms = 10;
-    int32_t subsampling_factor = meta_data.window_shift;
-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
-                                     subsampling_factor);
-
-    r.text = ApplyInverseTextNormalization(std::move(r.text));
-    r.text = ApplyHomophoneReplacer(std::move(r.text));
-    s->SetResult(r);
-  }
-
- private:
-  OfflineRecognizerConfig config_;
-  SymbolTable symbol_table_;
-  std::unique_ptr<OfflineSenseVoiceModelAxcl> model_;
-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
deleted file mode 100644
index 37d9ad2c..00000000
--- a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+++ /dev/null
@@ -1,138 +0,0 @@
-// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-axera-impl.h
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
-#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
-
-#include <memory>
-#include <utility>
-#include <vector>
-
-#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-model-config.h"
-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
-#include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
-#include "sherpa-onnx/csrc/symbol-table.h"
-
-namespace sherpa_onnx {
-
-// defined in ../offline-recognizer-sense-voice-impl.h
-OfflineRecognitionResult ConvertSenseVoiceResult(
-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor);
-
-class OfflineRecognizerSenseVoiceAxeraImpl : public OfflineRecognizerImpl {
- public:
-  explicit OfflineRecognizerSenseVoiceAxeraImpl(
-      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(config),
-        config_(config),
-        symbol_table_(config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
-            config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  template <typename Manager>
-  OfflineRecognizerSenseVoiceAxeraImpl(Manager *mgr,
-                                       const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(mgr, config),
-        config_(config),
-        symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
-            mgr, config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
-  }
-
-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    for (int32_t i = 0; i < n; ++i) {
-      DecodeOneStream(ss[i]);
-    }
-  }
-
-  OfflineRecognizerConfig GetConfig() const override { return config_; }
-
- private:
-  void InitFeatConfig() {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
-    config_.feat_config.window_type = "hamming";
-    config_.feat_config.high_freq = 0;
-    config_.feat_config.snip_edges = true;
-  }
-
-  void DecodeOneStream(OfflineStream *s) const {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    std::vector<float> f = s->GetFrames();
-
-    int32_t language = 0;
-    if (config_.model_config.sense_voice.language.empty()) {
-      language = 0;
-    } else if (meta_data.lang2id.count(
-                   config_.model_config.sense_voice.language)) {
-      language =
-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
-    } else {
-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
-                       config_.model_config.sense_voice.language.c_str());
-    }
-
-    int32_t text_norm = config_.model_config.sense_voice.use_itn
-                            ? meta_data.with_itn_id
-                            : meta_data.without_itn_id;
-
-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
-
-    auto result =
-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
-
-    int32_t frame_shift_ms = 10;
-    int32_t subsampling_factor = meta_data.window_shift;
-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
-                                     subsampling_factor);
-
-    r.text = ApplyInverseTextNormalization(std::move(r.text));
-    r.text = ApplyHomophoneReplacer(std::move(r.text));
-    s->SetResult(r);
-  }
-
- private:
-  OfflineRecognizerConfig config_;
-  SymbolTable symbol_table_;
-  std::unique_ptr<OfflineSenseVoiceModelAxera> model_;
-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 683fbddc..bde04386 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -31,6 +31,7 @@
 #include "sherpa-onnx/csrc/offline-recognizer-moonshine-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-transducer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-transducer-nemo-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-whisper-impl.h"
@@ -38,26 +39,26 @@
 
 #if SHERPA_ONNX_ENABLE_RKNN
 #include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
-#include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
+#include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_AXERA
-#include "sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h"
+#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_AXCL
-#include "sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h"
+#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
-#include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
+#include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_QNN
-#include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
 #include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
+#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
 #endif
 
 namespace sherpa_onnx {
@@ -67,7 +68,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "rknn") {
 #if SHERPA_ONNX_ENABLE_RKNN
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
+          config);
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
     } else {
@@ -90,7 +93,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "axera") {
 #if SHERPA_ONNX_ENABLE_AXERA
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxera>>(
+          config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice models are currently supported by Axera NPU for "
@@ -111,7 +116,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "axcl") {
 #if SHERPA_ONNX_ENABLE_AXCL
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxcl>>(
+          config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice models are currently supported by axcl for "
@@ -133,7 +140,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "ascend") {
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
+          config);
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
@@ -158,7 +167,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "qnn") {
 #if SHERPA_ONNX_ENABLE_QNN
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+          config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
     } else {
@@ -372,7 +383,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "rknn") {
 #if SHERPA_ONNX_ENABLE_RKNN
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(mgr, config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
+          mgr, config);
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
     } else {
@@ -395,8 +408,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "axera") {
 #if SHERPA_ONNX_ENABLE_AXERA
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(mgr,
-                                                                    config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxera>>(
+          mgr, config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice models are currently supported by Axera NPU for "
@@ -417,7 +431,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "axcl") {
 #if SHERPA_ONNX_ENABLE_AXCL
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(mgr, config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxcl>>(
+          mgr, config);
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice models are currently supported by axcl for "
@@ -439,8 +455,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "ascend") {
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(mgr,
-                                                                     config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
+          mgr, config);
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
                                                                      config);
@@ -467,7 +484,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
   if (config.model_config.provider == "qnn") {
 #if SHERPA_ONNX_ENABLE_QNN
     if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
+      return std::make_unique<
+          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+          mgr, config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
                                                                     config);
@@ -770,8 +789,8 @@ OfflineRecognizerImpl::OfflineRecognizerImpl(
         itn_list_.push_back(
             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
       }  // for (; !reader->Done(); reader->Next())
-    }    // for (const auto &f : files)
-  }      // if (!config.rule_fars.empty())
+    }  // for (const auto &f : files)
+  }  // if (!config.rule_fars.empty())
 
   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
     auto hr_config = config.hr;
diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
similarity index 84%
rename from sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
rename to sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
index 7c793ed9..bdaeb855 100644
--- a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
@@ -1,9 +1,9 @@
-// sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+// sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
 //
 // Copyright (c)  2025  Xiaomi Corporation
 
-#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
-#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
 
 #include <memory>
 #include <utility>
@@ -13,7 +13,6 @@
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
 #include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
 #include "sherpa-onnx/csrc/symbol-table.h"
 
@@ -24,15 +23,15 @@ OfflineRecognitionResult ConvertSenseVoiceResult(
     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
     int32_t frame_shift_ms, int32_t subsampling_factor);
 
-class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
+template <typename SenseVoiceModel>
+class OfflineRecognizerSenseVoiceTplImpl : public OfflineRecognizerImpl {
  public:
-  explicit OfflineRecognizerSenseVoiceQnnImpl(
+  explicit OfflineRecognizerSenseVoiceTplImpl(
       const OfflineRecognizerConfig &config)
       : OfflineRecognizerImpl(config),
         config_(config),
         symbol_table_(config_.model_config.tokens),
-        model_(
-            std::make_unique<OfflineSenseVoiceModelQnn>(config.model_config)) {
+        model_(std::make_unique<SenseVoiceModel>(config.model_config)) {
     const auto &meta_data = model_->GetModelMetadata();
     if (config.decoding_method == "greedy_search") {
       decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
@@ -47,13 +46,12 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
   }
 
   template <typename Manager>
-  OfflineRecognizerSenseVoiceQnnImpl(Manager *mgr,
+  OfflineRecognizerSenseVoiceTplImpl(Manager *mgr,
                                      const OfflineRecognizerConfig &config)
       : OfflineRecognizerImpl(mgr, config),
         config_(config),
         symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelQnn>(
-            mgr, config.model_config)) {
+        model_(std::make_unique<SenseVoiceModel>(mgr, config.model_config)) {
     const auto &meta_data = model_->GetModelMetadata();
     if (config.decoding_method == "greedy_search") {
       decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
@@ -111,6 +109,10 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
                             : meta_data.without_itn_id;
 
     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    if (logits.empty()) {
+      return;
+    }
+
     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
 
     auto result =
@@ -129,10 +131,10 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
  private:
   OfflineRecognizerConfig config_;
   SymbolTable symbol_table_;
-  std::unique_ptr<OfflineSenseVoiceModelQnn> model_;
+  std::unique_ptr<SenseVoiceModel> model_;
   std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
 };
 
 }  // namespace sherpa_onnx
 
-#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
deleted file mode 100644
index 8daccec0..00000000
--- a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+++ /dev/null
@@ -1,142 +0,0 @@
-// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
-//
-// Copyright (c)  2025  Xiaomi Corporation
-
-#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_
-#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_
-
-#include <memory>
-#include <utility>
-#include <vector>
-
-#include "sherpa-onnx/csrc/macros.h"
-#include "sherpa-onnx/csrc/offline-model-config.h"
-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
-#include "sherpa-onnx/csrc/offline-recognizer.h"
-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
-#include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
-#include "sherpa-onnx/csrc/symbol-table.h"
-
-namespace sherpa_onnx {
-
-// defined in ../online-recognizer-sense-voice-impl.h
-OfflineRecognitionResult ConvertSenseVoiceResult(
-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
-    int32_t frame_shift_ms, int32_t subsampling_factor);
-
-class OfflineRecognizerSenseVoiceRknnImpl : public OfflineRecognizerImpl {
- public:
-  explicit OfflineRecognizerSenseVoiceRknnImpl(
-      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(config),
-        config_(config),
-        symbol_table_(config_.model_config.tokens),
-        model_(
-            std::make_unique<OfflineSenseVoiceModelRknn>(config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  template <typename Manager>
-  OfflineRecognizerSenseVoiceRknnImpl(Manager *mgr,
-                                      const OfflineRecognizerConfig &config)
-      : OfflineRecognizerImpl(mgr, config),
-        config_(config),
-        symbol_table_(mgr, config_.model_config.tokens),
-        model_(std::make_unique<OfflineSenseVoiceModelRknn>(
-            mgr, config.model_config)) {
-    const auto &meta_data = model_->GetModelMetadata();
-    if (config.decoding_method == "greedy_search") {
-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
-          meta_data.blank_id);
-    } else {
-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
-                       config.decoding_method.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    InitFeatConfig();
-  }
-
-  std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
-  }
-
-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    for (int32_t i = 0; i < n; ++i) {
-      DecodeOneStream(ss[i]);
-    }
-  }
-
-  OfflineRecognizerConfig GetConfig() const override { return config_; }
-
- private:
-  void InitFeatConfig() {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
-    config_.feat_config.window_type = "hamming";
-    config_.feat_config.high_freq = 0;
-    config_.feat_config.snip_edges = true;
-  }
-
-  void DecodeOneStream(OfflineStream *s) const {
-    const auto &meta_data = model_->GetModelMetadata();
-
-    std::vector<float> f = s->GetFrames();
-
-    int32_t language = 0;
-    if (config_.model_config.sense_voice.language.empty()) {
-      language = 0;
-    } else if (meta_data.lang2id.count(
-                   config_.model_config.sense_voice.language)) {
-      language =
-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
-    } else {
-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
-                       config_.model_config.sense_voice.language.c_str());
-    }
-
-    int32_t text_norm = config_.model_config.sense_voice.use_itn
-                            ? meta_data.with_itn_id
-                            : meta_data.without_itn_id;
-
-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
-    if (logits.empty()) {
-      return;
-    }
-
-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
-
-    auto result =
-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
-
-    int32_t frame_shift_ms = 10;
-    int32_t subsampling_factor = meta_data.window_shift;
-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
-                                     subsampling_factor);
-
-    r.text = ApplyInverseTextNormalization(std::move(r.text));
-    r.text = ApplyHomophoneReplacer(std::move(r.text));
-    s->SetResult(r);
-  }
-
- private:
-  OfflineRecognizerConfig config_;
-  SymbolTable symbol_table_;
-  std::unique_ptr<OfflineSenseVoiceModelRknn> model_;
-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
-};
-
-}  // namespace sherpa_onnx
-
-#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_

commit 5af7603e1a2c19ba03a492e4f9f6d9b491bce1d2
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Dec 8 12:17:31 2025 +0800

    Add CI for Axera NPU (#2872)

diff --git a/.github/workflows/axcl-linux-aarch64.yaml b/.github/workflows/axcl-linux-aarch64.yaml
new file mode 100644
index 00000000..f47419f5
--- /dev/null
+++ b/.github/workflows/axcl-linux-aarch64.yaml
@@ -0,0 +1,238 @@
+name: axcl-linux-aarch64
+
+on:
+  push:
+    branches:
+      - master
+    tags:
+      - 'v[0-9]+.[0-9]+.[0-9]+*'
+    paths:
+      - '.github/workflows/axcl-linux-aarch64.yaml'
+      - 'cmake/**'
+      - 'sherpa-onnx/csrc/*'
+      - 'sherpa-onnx/csrc/axcl/*'
+      - 'sherpa-onnx/c-api/*'
+      - 'toolchains/aarch64-linux-gnu.toolchain.cmake'
+
+  workflow_dispatch:
+
+concurrency:
+  group: axcl-linux-aarch64-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  axcl_linux_aarch64:
+    runs-on: ubuntu-22.04-arm
+    name: axcl npu
+    strategy:
+      fail-fast: false
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      - name: Download SDK
+        shell: bash
+        run: |
+          git clone --depth 1 https://github.com/Abandon-ht/axcl_bsp_sdk
+          mv axcl_bsp_sdk/out sdk_dir
+
+          ls -lh sdk_dir/include
+          echo "---"
+          ls -lh sdk_dir/bsp
+          echo "---"
+          ls -lh sdk_dir/lib
+
+      - name: ccache
+        uses: hendrikmuhs/ccache-action@v1.2
+        with:
+          key: axcl-linux-aarch64
+
+      - name: Build sherpa-onnx
+        uses: addnab/docker-run-action@v3
+        with:
+            image: quay.io/pypa/manylinux_2_28_aarch64
+            # image: quay.io/pypa/manylinux2014_aarch64 # won't work
+            options: |
+              --volume ${{ github.workspace }}/:/k2-fsa/sherpa-onnx
+            shell: bash
+            run: |
+              uname -a
+              which gcc
+
+              gcc --version
+              g++ --version
+
+              cmake --version
+
+
+              cd /k2-fsa/sherpa-onnx/
+
+              export AXCL_SDK_ROOT=$PWD/sdk_dir
+              echo "AXCL_SDK_ROOT: $AXCL_SDK_ROOT"
+              export CPLUS_INCLUDE_PATH="$AXCL_SDK_ROOT/include:$AXCL_SDK_ROOT/bsp:$CPLUS_INCLUDE_PATH"
+              export SHERPA_ONNX_AXCL_LIB_DIR="$AXCL_SDK_ROOT/lib"
+
+              echo "pwd"
+
+              ls -lh
+
+              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+              pushd alsa-lib
+              ./gitcompile
+              popd
+
+              ls -lh $PWD/alsa-lib/src/.libs
+
+              strings $PWD/alsa-lib/src/.libs/libasound.so.2.0.0 | grep "^GLIBC"
+
+              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+              export C_INCLUDE_PATH=$PWD/alsa-lib/include:$C_INCLUDE_PATH
+              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+              p=$PWD
+
+              export SHERPA_ONNX_ENABLE_ALSA=1
+
+              mkdir build
+              cd build
+
+              cmake \
+                -DALSA_INCLUDE_DIR=$p/alsa-lib/include \
+                -DALSA_LIBRARY=$p/alsa-lib/src/.libs/libasound.so \
+                -DBUILD_SHARED_LIBS=ON \
+                -DCMAKE_INSTALL_PREFIX=./install \
+                -DSHERPA_ONNX_ENABLE_AXCL=ON \
+                ..
+
+              make -j4 install
+
+              rm -rf install/lib/pkgconfig
+              rm -fv install/lib/cargs.h
+              rm -fv install/lib/libcargs.so
+
+      - name: Display system info
+        shell: bash
+        run: |
+          uname -a
+          gcc --version
+          g++ --version
+
+      - name: Display generated files
+        shell: bash
+        run: |
+          export AXCL_SDK_ROOT=$PWD/sdk_dir
+          export LD_LIBRARY_PATH=$AXCL_SDK_ROOT/lib:$LD_LIBRARY_PATH
+
+          ls -lh $AXCL_SDK_ROOT/lib/
+
+          cd build/install
+
+          ls -lh bin
+
+          echo "---"
+
+          ls -lh lib
+
+          file bin/sherpa-onnx
+
+          readelf -d bin/sherpa-onnx
+
+          ldd bin/sherpa-onnx
+
+          echo "---"
+          strings bin/sherpa-onnx | grep "^GLIBC"
+
+      - name: Copy files
+        shell: bash
+        run: |
+          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+          suffix=shared
+
+          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-axcl-linux-aarch64-$suffix
+          mkdir $dst
+
+          cp -a build/install/bin $dst/
+
+          mkdir -p $dst/lib
+          cp -v build/install/lib/lib*.so $dst/lib/
+
+          ls -lh build/install/lib
+          ls -lh build/install/bin
+
+          ls -lh $dst/bin/
+          echo "strip"
+          strip $dst/bin/*
+
+          echo "after strip"
+          ls -lh $dst/bin/
+
+          tree $dst
+
+          tar cjvf ${dst}.tar.bz2 $dst
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: sherpa-onnx-axcl-linux-aarch64-shared
+          path: sherpa-onnx-*linux-aarch64*.tar.bz2
+
+      # https://huggingface.co/docs/hub/spaces-github-actions
+      - name: Publish to huggingface
+        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+            GIT_LFS_SKIP_SMUDGE=1 git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
+
+            cd huggingface
+            dst=axcl-linux-aarch64/$SHERPA_ONNX_VERSION
+            mkdir -p $dst
+
+            cp -v ../sherpa-onnx-*axcl*-*.tar.bz2 $dst
+
+            git status
+            git lfs track "*.bz2"
+
+            git add .
+
+            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-axcl-linux-aarch64.tar.bz2"
+
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
+
+      - name: Release pre-compiled binaries and libs for linux aarch64
+        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-aarch64*.tar.bz2
+
+      - name: Release pre-compiled binaries and libs for linux aarch64
+        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-aarch64*.tar.bz2
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: v1.12.19
diff --git a/.github/workflows/axera-linux-aarch64.yaml b/.github/workflows/axera-linux-aarch64.yaml
new file mode 100644
index 00000000..a60780bd
--- /dev/null
+++ b/.github/workflows/axera-linux-aarch64.yaml
@@ -0,0 +1,251 @@
+name: axera-linux-aarch64
+
+on:
+  push:
+    branches:
+      - master
+    tags:
+      - 'v[0-9]+.[0-9]+.[0-9]+*'
+    paths:
+      - '.github/workflows/axera-linux-aarch64.yaml'
+      - 'cmake/**'
+      - 'sherpa-onnx/csrc/*'
+      - 'sherpa-onnx/csrc/axera/*'
+      - 'sherpa-onnx/c-api/*'
+      - 'toolchains/aarch64-linux-gnu.toolchain.cmake'
+
+  workflow_dispatch:
+
+concurrency:
+  group: axera-linux-aarch64-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  axera_linux_aarch64:
+    runs-on: ubuntu-22.04-arm
+    name: axera npu
+    strategy:
+      fail-fast: false
+      matrix:
+        include:
+          - soc: ax650
+          - soc: ax630c
+          - soc: ax620q
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      - name: Download SDK
+        shell: bash
+        run: |
+          soc=${{ matrix.soc }}
+          if [[ $soc == ax650 ]]; then
+            version=1.45.0_p39
+            curl -SL -O https://github.com/AXERA-TECH/ax650n_bsp_sdk/archive/refs/tags/v$version.zip
+            unzip -qq v$version.zip
+
+            mv $PWD/ax650n_bsp_sdk-$version/msp/out sdk_dir
+          elif [[ $soc == ax630c || $soc == ax620q ]]; then
+            version=2.0.0_P7
+            curl -SL -O https://github.com/AXERA-TECH/ax620e_bsp_sdk/archive/refs/tags/v2.0.0_P7.zip
+            unzip -qq v$version.zip
+            mv $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc sdk_dir
+
+          fi
+
+      - name: ccache
+        uses: hendrikmuhs/ccache-action@v1.2
+        with:
+          key: axera-${{ matrix.soc }}-linux-aarch64
+
+      - name: Build sherpa-onnx
+        uses: addnab/docker-run-action@v3
+        with:
+            image: quay.io/pypa/manylinux_2_28_aarch64
+            # image: quay.io/pypa/manylinux2014_aarch64 # won't work
+            options: |
+              --volume ${{ github.workspace }}/:/k2-fsa/sherpa-onnx
+            shell: bash
+            run: |
+              uname -a
+              which gcc
+
+              gcc --version
+              g++ --version
+
+              cmake --version
+
+
+              cd /k2-fsa/sherpa-onnx/
+
+              export AXERA_SDK_ROOT=$PWD/sdk_dir
+              echo "AXERA_SDK_ROOT: $AXERA_SDK_ROOT"
+              export CPLUS_INCLUDE_PATH="$AXERA_SDK_ROOT/include:$CPLUS_INCLUDE_PATH"
+              export SHERPA_ONNX_AXERA_LIB_DIR="$AXERA_SDK_ROOT/lib"
+
+              echo "pwd"
+
+              ls -lh
+
+              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+              pushd alsa-lib
+              ./gitcompile
+              popd
+
+              ls -lh $PWD/alsa-lib/src/.libs
+
+              strings $PWD/alsa-lib/src/.libs/libasound.so.2.0.0 | grep "^GLIBC"
+
+              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+              export C_INCLUDE_PATH=$PWD/alsa-lib/include:$C_INCLUDE_PATH
+              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+              p=$PWD
+
+              export SHERPA_ONNX_ENABLE_ALSA=1
+
+              mkdir build
+              cd build
+
+              cmake \
+                -DALSA_INCLUDE_DIR=$p/alsa-lib/include \
+                -DALSA_LIBRARY=$p/alsa-lib/src/.libs/libasound.so \
+                -DBUILD_SHARED_LIBS=ON \
+                -DCMAKE_INSTALL_PREFIX=./install \
+                -DSHERPA_ONNX_ENABLE_AXERA=ON \
+                ..
+
+              make -j4 install
+
+              rm -rf install/lib/pkgconfig
+              rm -fv install/lib/cargs.h
+              rm -fv install/lib/libcargs.so
+
+      - name: Display system info
+        shell: bash
+        run: |
+          uname -a
+          gcc --version
+          g++ --version
+
+      - name: Display generated files
+        shell: bash
+        run: |
+          export AXERA_SDK_ROOT=$PWD/sdk_dir
+          export LD_LIBRARY_PATH=$AXERA_SDK_ROOT/lib:$LD_LIBRARY_PATH
+
+          ls -lh $AXERA_SDK_ROOT/lib/
+
+          cd build/install
+
+          ls -lh bin
+
+          echo "---"
+
+          ls -lh lib
+
+          file bin/sherpa-onnx
+
+          readelf -d bin/sherpa-onnx
+
+          ldd bin/sherpa-onnx
+
+          echo "---"
+          strings bin/sherpa-onnx | grep "^GLIBC"
+
+      - name: Copy files
+        shell: bash
+        run: |
+          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+          suffix=shared
+
+          soc=${{ matrix.soc }}
+
+          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-axera-$soc-linux-aarch64-$suffix
+          mkdir $dst
+
+          cp -a build/install/bin $dst/
+
+          mkdir -p $dst/lib
+          cp -v build/install/lib/lib*.so $dst/lib/
+
+          ls -lh build/install/lib
+          ls -lh build/install/bin
+
+          ls -lh $dst/bin/
+          echo "strip"
+          strip $dst/bin/*
+
+          echo "after strip"
+          ls -lh $dst/bin/
+
+          tree $dst
+
+          tar cjvf ${dst}.tar.bz2 $dst
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: sherpa-onnx-axera-${{ matrix.soc }}-linux-aarch64-shared
+          path: sherpa-onnx-*linux-aarch64*.tar.bz2
+
+      # https://huggingface.co/docs/hub/spaces-github-actions
+      - name: Publish to huggingface
+        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+            GIT_LFS_SKIP_SMUDGE=1 git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
+
+            cd huggingface
+            dst=axera-linux-aarch64/$SHERPA_ONNX_VERSION/${{ matrix.soc }}
+            mkdir -p $dst
+
+            cp -v ../sherpa-onnx-*axera*-*.tar.bz2 $dst
+
+            git status
+            git lfs track "*.bz2"
+
+            git add .
+
+            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-axera-${{ matrix.soc }}-linux-aarch64.tar.bz2"
+
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
+
+      - name: Release pre-compiled binaries and libs for linux aarch64
+        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-aarch64*.tar.bz2
+
+      - name: Release pre-compiled binaries and libs for linux aarch64
+        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-aarch64*.tar.bz2
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: v1.12.19

commit 6c9d1940224d712a2db927500b3822aee0cc1aa3
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Sun Dec 7 14:02:22 2025 +0800

    Update README to include Axera NPU (#2870)

diff --git a/README.md b/README.md
index 6eb4cd4e..72a3079e 100644
--- a/README.md
+++ b/README.md
@@ -51,6 +51,10 @@ It also supports WebAssembly.
 |-------------------------------------|-----------------------------------|-----------------------------|
 |                                   |                                 |                           |
 
+| [4. Axera NPU][axera-npu] |
+|---------------------------|
+|                         |
+
 [Join our discord](https://discord.gg/fJdxzg2VbG)
 
 
@@ -582,3 +586,4 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
 [rknpu-doc]: https://k2-fsa.github.io/sherpa/onnx/rknn/index.html
 [qnn-doc]: https://k2-fsa.github.io/sherpa/onnx/qnn/index.html
 [ascend-doc]: https://k2-fsa.github.io/sherpa/onnx/ascend/index.html
+[axera-npu]: https://axera-tech.com/Skill/166.html

commit 1e742cee3089febbbdc7b1792abe6b530eed4b9f
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Sun Dec 7 13:08:31 2025 +0800

    Refactor axcl examples. (#2867)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index ceedb01c..2a7e7433 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -206,8 +206,13 @@ endif()
 
 if(SHERPA_ONNX_ENABLE_AXCL)
   list(APPEND sources
+    ./axcl/axcl-engine-guard.cc
+    ./axcl/axcl-engine-io-guard.cc
+    ./axcl/axcl-engine-io-info-guard.cc
+    ./axcl/axcl-manager.cc
+    ./axcl/axcl-model.cc
     ./axcl/offline-sense-voice-model-axcl.cc
-    ./axcl/ax_model_runner_axcl.cc
+    ./axcl/utils.cc
   )
 endif()
 
diff --git a/sherpa-onnx/csrc/ascend/utils.h b/sherpa-onnx/csrc/ascend/utils.h
index 2aaadf27..0a3aaf7c 100644
--- a/sherpa-onnx/csrc/ascend/utils.h
+++ b/sherpa-onnx/csrc/ascend/utils.h
@@ -19,7 +19,10 @@ class Acl {
   ~Acl();
 
   Acl(const Acl &) = delete;
-  const Acl &operator=(const Acl &) = delete;
+  Acl &operator=(const Acl &) = delete;
+
+  Acl(Acl &&) = delete;
+  Acl &operator=(Acl &&) = delete;
 
  private:
   bool initialized_ = false;
@@ -32,7 +35,10 @@ class AclContext {
   ~AclContext();
 
   AclContext(const AclContext &) = delete;
-  const AclContext &operator=(const AclContext &) = delete;
+  AclContext &operator=(const AclContext &) = delete;
+
+  AclContext(AclContext &&) = delete;
+  AclContext &operator=(AclContext &&) = delete;
 
   aclrtContext Get() const;
   operator aclrtContext() { return context_; }
@@ -49,7 +55,10 @@ class AclDevicePtr {
   ~AclDevicePtr();
 
   AclDevicePtr(const AclDevicePtr &) = delete;
-  const AclDevicePtr &operator=(const AclDevicePtr &) = delete;
+  AclDevicePtr &operator=(const AclDevicePtr &) = delete;
+
+  AclDevicePtr(AclDevicePtr &&) = delete;
+  AclDevicePtr &operator=(AclDevicePtr &&) = delete;
 
   void *Get() const { return p_; }
   operator void *() { return p_; }
@@ -68,7 +77,10 @@ class AclModelDesc {
   ~AclModelDesc();
 
   AclModelDesc(const AclModelDesc &) = delete;
-  const AclModelDesc &operator=(const AclModelDesc &) = delete;
+  AclModelDesc &operator=(const AclModelDesc &) = delete;
+
+  AclModelDesc(AclModelDesc &&) = delete;
+  AclModelDesc &operator=(AclModelDesc &&) = delete;
 
   aclmdlDesc *Get() const { return p_; }
   operator aclmdlDesc *() const { return p_; }
@@ -90,7 +102,10 @@ class AclModel {
   operator uint32_t() const { return model_id_; }
 
   AclModel(const AclModel &) = delete;
-  const AclModel &operator=(const AclModel &) = delete;
+  AclModel &operator=(const AclModel &) = delete;
+
+  AclModel(AclModel &&) = delete;
+  AclModel &operator=(AclModel &&) = delete;
 
   std::string GetInfo() const;
 
@@ -135,6 +150,9 @@ class AclMdlDataset {
   AclMdlDataset(const AclMdlDataset &) = delete;
   AclMdlDataset &operator=(const AclMdlDataset &) = delete;
 
+  AclMdlDataset(AclMdlDataset &&) = delete;
+  AclMdlDataset &operator=(AclMdlDataset &&) = delete;
+
   void AddBuffer(aclDataBuffer *buffer) const;
   void SetTensorDesc(aclTensorDesc *tensor_desc, size_t index) const;
 
@@ -153,6 +171,9 @@ class AclDataBuffer {
   AclDataBuffer(const AclDataBuffer &) = delete;
   AclDataBuffer &operator=(const AclDataBuffer &) = delete;
 
+  AclDataBuffer(AclDataBuffer &&) = delete;
+  AclDataBuffer &operator=(AclDataBuffer &&) = delete;
+
   aclDataBuffer *Get() const { return p_; }
   operator aclDataBuffer *() const { return p_; }
 
@@ -169,6 +190,9 @@ class AclTensorDesc {
   AclTensorDesc(const AclTensorDesc &) = delete;
   AclTensorDesc &operator=(const AclTensorDesc &) = delete;
 
+  AclTensorDesc(AclTensorDesc &&) = delete;
+  AclTensorDesc &operator=(AclTensorDesc &&) = delete;
+
   aclTensorDesc *Get() const { return p_; }
   operator aclTensorDesc *() const { return p_; }
 
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
deleted file mode 100644
index 42baad0d..00000000
--- a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+++ /dev/null
@@ -1,148 +0,0 @@
-// sherpa-onnx/csrc/axcl/ax_model_runner.hpp
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-#pragma once
-#include <map>
-#include <stdexcept>
-#include <string>
-#include <vector>
-
-typedef enum _color_space_e {
-  ax_color_space_unknown,
-  ax_color_space_nv12,
-  ax_color_space_nv21,
-  ax_color_space_bgr,
-  ax_color_space_rgb,
-} ax_color_space_e;
-
-typedef struct {
-  std::string sName;
-  unsigned int nIdx;
-  std::vector<unsigned int> vShape;
-  int nSize;
-  unsigned long long phyAddr;
-  void *pVirAddr;
-} ax_runner_tensor_t;
-
-class ax_runner_base {
- public:
-  std::vector<ax_runner_tensor_t> moutput_tensors;
-  std::vector<ax_runner_tensor_t> minput_tensors;
-
-  std::vector<std::vector<ax_runner_tensor_t>> mgroup_output_tensors;
-  std::vector<std::vector<ax_runner_tensor_t>> mgroup_input_tensors;
-
-  std::map<std::string, ax_runner_tensor_t> map_output_tensors;
-  std::map<std::string, ax_runner_tensor_t> map_input_tensors;
-
-  std::map<std::string, std::vector<ax_runner_tensor_t>>
-      map_group_output_tensors;
-  std::map<std::string, std::vector<ax_runner_tensor_t>>
-      map_group_input_tensors;
-
-  bool _auto_sync_before_inference = true;
-  bool _auto_sync_after_inference = true;
-
-  float cost_host_to_device = 0;
-  float cost_inference = 0;
-  float cost_device_to_host = 0;
-
- public:
-  virtual int init(const char *model_file) = 0;
-  virtual int init(char *model_buffer, size_t model_size) = 0;
-
-  virtual void deinit() = 0;
-
-  float get_inference_time() { return cost_inference; }
-
-  int get_num_inputs() { return minput_tensors.size(); };
-  int get_num_outputs() { return moutput_tensors.size(); };
-
-  const ax_runner_tensor_t &get_input(int idx) { return minput_tensors[idx]; }
-  const ax_runner_tensor_t *get_inputs_ptr() { return minput_tensors.data(); }
-  const ax_runner_tensor_t &get_input(std::string name) {
-    if (map_input_tensors.size() == 0) {
-      for (size_t i = 0; i < minput_tensors.size(); i++) {
-        map_input_tensors[minput_tensors[i].sName] = minput_tensors[i];
-      }
-    }
-    if (map_input_tensors.find(name) == map_input_tensors.end()) {
-      throw std::runtime_error("input tensor not found: " + name);
-    }
-
-    return map_input_tensors[name];
-  }
-
-  const ax_runner_tensor_t &get_input(int grpid, int idx) {
-    return mgroup_input_tensors[grpid][idx];
-  }
-  const ax_runner_tensor_t *get_inputs_ptr(int grpid) {
-    return mgroup_input_tensors[grpid].data();
-  }
-  const ax_runner_tensor_t &get_input(int grpid, std::string name) {
-    if (map_group_input_tensors.size() == 0) {
-      for (size_t i = 0; i < mgroup_input_tensors.size(); i++) {
-        for (size_t j = 0; j < mgroup_input_tensors[i].size(); j++) {
-          map_group_input_tensors[mgroup_input_tensors[i][j].sName].push_back(
-              mgroup_input_tensors[i][j]);
-        }
-      }
-    }
-    if (map_group_input_tensors.find(name) == map_group_input_tensors.end()) {
-      throw std::runtime_error("input tensor not found: " + name);
-    }
-    return map_group_input_tensors[name][grpid];
-    // return map_input_tensors[name];
-  }
-
-  const ax_runner_tensor_t &get_output(int idx) { return moutput_tensors[idx]; }
-  const ax_runner_tensor_t *get_outputs_ptr() { return moutput_tensors.data(); }
-  const ax_runner_tensor_t &get_output(std::string name) {
-    if (map_output_tensors.size() == 0) {
-      for (size_t i = 0; i < moutput_tensors.size(); i++) {
-        map_output_tensors[moutput_tensors[i].sName] = moutput_tensors[i];
-      }
-    }
-    if (map_output_tensors.find(name) == map_output_tensors.end()) {
-      throw std::runtime_error("output tensor not found: " + name);
-    }
-
-    return map_output_tensors[name];
-  }
-
-  const ax_runner_tensor_t &get_output(int grpid, int idx) {
-    return mgroup_output_tensors[grpid][idx];
-  }
-  const ax_runner_tensor_t *get_outputs_ptr(int grpid) {
-    return mgroup_output_tensors[grpid].data();
-  }
-  const ax_runner_tensor_t &get_output(int grpid, std::string name) {
-    if (map_group_output_tensors.size() == 0) {
-      for (size_t i = 0; i < mgroup_output_tensors.size(); i++) {
-        for (size_t j = 0; j < mgroup_output_tensors[i].size(); j++) {
-          map_group_output_tensors[mgroup_output_tensors[i][j].sName].push_back(
-              mgroup_output_tensors[i][j]);
-        }
-      }
-    }
-    if (map_group_output_tensors.find(name) == map_group_output_tensors.end()) {
-      throw std::runtime_error("input tensor not found: " + name);
-    }
-    return map_group_output_tensors[name][grpid];
-  }
-
-  virtual int get_algo_width() = 0;
-  virtual int get_algo_height() = 0;
-  virtual ax_color_space_e get_color_space() = 0;
-
-  void set_auto_sync_before_inference(bool sync) {
-    _auto_sync_before_inference = sync;
-  }
-  void set_auto_sync_after_inference(bool sync) {
-    _auto_sync_after_inference = sync;
-  }
-
-  virtual int inference() = 0;
-  virtual int inference(int grpid) = 0;
-};
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
deleted file mode 100644
index 7f3733a9..00000000
--- a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+++ /dev/null
@@ -1,470 +0,0 @@
-// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-#include "ax_model_runner_axcl.hpp"
-
-#include <axcl.h>
-#include <fcntl.h>
-#include <string.h>
-
-#include <fstream>
-#include <memory>
-
-typedef enum {
-  AX_ENGINE_ABST_DEFAULT = 0,
-  AX_ENGINE_ABST_CACHED = 1,
-} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
-
-typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
-                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
-    INPUT_OUTPUT_ALLOC_STRATEGY;
-
-static void print_io_info(std::vector<ax_runner_tensor_t> &input,
-                          std::vector<ax_runner_tensor_t> &output) {
-  printf("\ninput size: %ld\n", input.size());
-  for (size_t i = 0; i < input.size(); ++i) {
-    // print shape info,like [batchsize x channel x height x width]
-    auto &info = input[i];
-    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
-    for (size_t s = 0; s < info.vShape.size(); s++) {
-      printf("%d", info.vShape[s]);
-      if (s != info.vShape.size() - 1) {
-        printf(" x ");
-      }
-    }
-    printf("\e[0m\n\n");
-  }
-
-  printf("\noutput size: %ld\n", output.size());
-  for (size_t i = 0; i < output.size(); ++i) {
-    // print shape info,like [batchsize x channel x height x width]
-    auto &info = output[i];
-    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
-    for (size_t s = 0; s < info.vShape.size(); s++) {
-      printf("%d", info.vShape[s]);
-      if (s != info.vShape.size() - 1) {
-        printf(" x ");
-      }
-    }
-    printf("\e[0m\n\n");
-  }
-}
-
-static bool read_file(const char *fn, std::vector<unsigned char> &data) {
-  FILE *fp = fopen(fn, "r");
-  if (fp != nullptr) {
-    fseek(fp, 0L, SEEK_END);
-    auto len = ftell(fp);
-    fseek(fp, 0, SEEK_SET);
-    data.clear();
-    size_t read_size = 0;
-    if (len > 0) {
-      data.resize(len);
-      read_size = fread(data.data(), 1, len, fp);
-    }
-    fclose(fp);
-    return read_size == (size_t)len;
-  }
-  return false;
-}
-
-typedef struct {
-  int nIndex;
-  int nSize;
-  void *pBuf;
-  void *pVirAddr;
-
-  std::string Name;
-
-  axclrtEngineIODims dims;
-} AXCL_IO_BUF_T;
-
-typedef struct {
-  uint32_t nInputSize;
-  uint32_t nOutputSize;
-  AXCL_IO_BUF_T *pInputs;
-  AXCL_IO_BUF_T *pOutputs;
-} AXCL_IO_DATA_T;
-
-static void free_io_index(AXCL_IO_BUF_T *pBuf, size_t index) {
-  for (size_t i = 0; i < index; ++i) {
-    axclrtFree(pBuf[i].pBuf);
-  }
-}
-
-static void free_io(AXCL_IO_DATA_T *io_data) {
-  for (size_t j = 0; j < io_data->nInputSize; ++j) {
-    axclrtFree(io_data->pInputs[j].pBuf);
-    free(io_data->pInputs[j].pVirAddr);
-  }
-  for (size_t j = 0; j < io_data->nOutputSize; ++j) {
-    axclrtFree(io_data->pOutputs[j].pBuf);
-    free(io_data->pOutputs[j].pVirAddr);
-  }
-  delete[] io_data->pInputs;
-  delete[] io_data->pOutputs;
-}
-
-static inline int prepare_io(int grpid, axclrtEngineIOInfo io_info,
-                             axclrtEngineIO io, AXCL_IO_DATA_T *io_data,
-                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
-  memset(io_data, 0, sizeof(AXCL_IO_DATA_T));
-
-  auto inputNum = axclrtEngineGetNumInputs(io_info);
-  auto outputNum = axclrtEngineGetNumOutputs(io_info);
-  io_data->nInputSize = inputNum;
-  io_data->nOutputSize = outputNum;
-  io_data->pInputs = new AXCL_IO_BUF_T[inputNum];
-  io_data->pOutputs = new AXCL_IO_BUF_T[outputNum];
-
-  // 1. alloc inputs
-  for (uint32_t i = 0; i < inputNum; i++) {
-    auto bufSize = axclrtEngineGetInputSizeByIndex(io_info, grpid, i);
-    void *devPtr = nullptr;
-    axclError ret = 0;
-    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
-      ret = axclrtMalloc(&devPtr, bufSize,
-                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
-    } else {
-      ret = axclrtMallocCached(
-          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
-    }
-
-    if (ret != 0) {
-      free_io_index(io_data->pInputs, i);
-      fprintf(stderr, "Malloc input(index: %d, size: %ld) failed! ret=0x%x\n",
-              i, bufSize, ret);
-      return -1;
-    }
-    std::vector<char> tmp(bufSize, 0);
-    axclrtMemcpy(devPtr, tmp.data(), bufSize,
-                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
-    // axclrtMemset(devPtr, 0, bufSize);
-
-    axclrtEngineIODims dims;
-    ret = axclrtEngineGetInputDims(io_info, grpid, i, &dims);
-    if (ret != 0) {
-      free_io_index(io_data->pInputs, i);
-      fprintf(stderr, "Get input dims(index: %d) failed! ret=0x%x\n", i, ret);
-      return -1;
-    }
-
-    io_data->pInputs[i].nIndex = i;
-    io_data->pInputs[i].nSize = bufSize;
-    io_data->pInputs[i].pBuf = devPtr;
-    io_data->pInputs[i].dims = dims;
-    io_data->pInputs[i].Name = axclrtEngineGetInputNameByIndex(io_info, i);
-    io_data->pInputs[i].pVirAddr = malloc(bufSize);
-    memset(io_data->pInputs[i].pVirAddr, 0, bufSize);
-    ret = axclrtEngineSetInputBufferByIndex(io, i, devPtr, bufSize);
-    if (ret != 0) {
-      free_io_index(io_data->pInputs, i);
-      fprintf(stderr,
-              "Set input buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
-              bufSize, ret);
-      return -1;
-    }
-  }
-
-  // 2. alloc outputs
-  for (uint32_t i = 0; i < outputNum; i++) {
-    auto bufSize = axclrtEngineGetOutputSizeByIndex(io_info, grpid, i);
-    void *devPtr = NULL;
-    axclError ret = 0;
-    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
-      ret = axclrtMalloc(&devPtr, bufSize,
-                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
-    } else {
-      ret = axclrtMallocCached(
-          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
-    }
-
-    if (ret != 0) {
-      free_io_index(io_data->pOutputs, i);
-      fprintf(stderr, "Malloc output(index: %d, size: %ld) failed! ret=0x%x\n",
-              i, bufSize, ret);
-      return -1;
-    }
-    std::vector<char> tmp(bufSize, 0);
-    axclrtMemcpy(devPtr, tmp.data(), bufSize,
-                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
-    axclrtEngineIODims dims;
-    ret = axclrtEngineGetOutputDims(io_info, grpid, i, &dims);
-    if (ret != 0) {
-      free_io_index(io_data->pOutputs, i);
-      fprintf(stderr, "Get output dims(index: %d) failed! ret=0x%x\n", i, ret);
-      return -1;
-    }
-
-    io_data->pOutputs[i].nIndex = i;
-    io_data->pOutputs[i].nSize = bufSize;
-    io_data->pOutputs[i].pBuf = devPtr;
-    io_data->pOutputs[i].dims = dims;
-    io_data->pOutputs[i].Name = axclrtEngineGetOutputNameByIndex(io_info, i);
-    io_data->pOutputs[i].pVirAddr = malloc(bufSize);
-    memset(io_data->pOutputs[i].pVirAddr, 0, bufSize);
-    ret = axclrtEngineSetOutputBufferByIndex(io, i, devPtr, bufSize);
-    if (ret != 0) {
-      free_io_index(io_data->pOutputs, i);
-      fprintf(stderr,
-              "Set output buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
-              bufSize, ret);
-      return -1;
-    }
-  }
-
-  return 0;
-}
-
-struct ax_joint_runner_axcl_handle_t {
-  uint64_t handle = 0;
-  uint64_t context = 0;
-  axclrtEngineIOInfo io_info = 0;
-  std::vector<axclrtEngineIO> ios;
-  std::vector<AXCL_IO_DATA_T> io_datas;
-
-  // int algo_width, algo_height;
-  // int algo_colorformat;
-};
-
-int ax_runner_axcl::sub_init() {
-  // 4. create context
-  int ret = axclrtEngineCreateContext(m_handle->handle, &m_handle->context);
-  if (0 != ret) {
-    fprintf(stderr, "axclrtEngineCreateContext failed.\n");
-    return ret;
-  }
-  fprintf(stdout, "axclrtEngineCreateContextt is done. \n");
-
-  // 5. set io
-
-  ret = axclrtEngineGetIOInfo(m_handle->handle, &m_handle->io_info);
-  if (0 != ret) {
-    fprintf(stderr, "axclrtEngineGetIOInfo failed.\n");
-    return ret;
-  }
-  fprintf(stdout, "axclrtEngineGetIOInfo is done. \n");
-
-  ret = axclrtEngineGetShapeGroupsCount(m_handle->io_info, &group_count);
-  if (ret != 0) {
-    axclrtEngineUnload(m_handle->handle);
-    return ret;
-  }
-
-  // 6. alloc io
-  if (!_parepare_io) {
-    m_handle->ios.resize(group_count);
-    m_handle->io_datas.resize(group_count);
-    mgroup_input_tensors.resize(group_count);
-    mgroup_output_tensors.resize(group_count);
-
-    memset(&m_handle->io_datas[0], 0, sizeof(AXCL_IO_DATA_T) * group_count);
-
-    auto malloc_strategy =
-        std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_DEFAULT);
-
-    for (int grpid = 0; grpid < group_count; grpid++) {
-      ret = axclrtEngineCreateIO(m_handle->io_info, &m_handle->ios[grpid]);
-      if (ret != 0) {
-        axclrtEngineUnload(m_handle->handle);
-        fprintf(stderr, "Create io failed. ret=0x%x\n", ret);
-        return -1;
-      }
-
-      ret = prepare_io(grpid, m_handle->io_info, m_handle->ios[grpid],
-                       &m_handle->io_datas[grpid], malloc_strategy);
-      if (ret != 0) {
-        free_io(&m_handle->io_datas[grpid]);
-        axclrtEngineDestroyIO(m_handle->ios[grpid]);
-        axclrtEngineUnload(m_handle->handle);
-
-        fprintf(stderr, "prepare_io failed.\n");
-        return ret;
-      }
-    }
-
-    for (int grpid = 0; grpid < group_count; grpid++) {
-      // auto &io_info = m_handle->io_info[grpid];
-      auto &io_data = m_handle->io_datas[grpid];
-      for (uint32_t i = 0; i < io_data.nOutputSize; i++) {
-        ax_runner_tensor_t tensor;
-        tensor.nIdx = i;
-        tensor.sName = std::string(io_data.pOutputs[i].Name);
-        tensor.nSize = io_data.pOutputs[i].nSize;
-        for (int32_t j = 0; j < io_data.pOutputs[i].dims.dimCount; j++) {
-          tensor.vShape.push_back(io_data.pOutputs[i].dims.dims[j]);
-        }
-        tensor.phyAddr = (unsigned long long)io_data.pOutputs[i].pBuf;
-        tensor.pVirAddr = io_data.pOutputs[i].pVirAddr;
-        mgroup_output_tensors[grpid].push_back(tensor);
-      }
-
-      for (size_t i = 0; i < io_data.nInputSize; i++) {
-        ax_runner_tensor_t tensor;
-        tensor.nIdx = i;
-        tensor.sName = std::string(io_data.pInputs[i].Name);
-        tensor.nSize = io_data.pInputs[i].nSize;
-        for (int32_t j = 0; j < io_data.pInputs[i].dims.dimCount; j++) {
-          tensor.vShape.push_back(io_data.pInputs[i].dims.dims[j]);
-        }
-        tensor.phyAddr = (unsigned long long)io_data.pInputs[i].pBuf;
-        tensor.pVirAddr = io_data.pInputs[i].pVirAddr;
-        mgroup_input_tensors[grpid].push_back(tensor);
-      }
-    }
-
-    moutput_tensors = mgroup_output_tensors[0];
-    minput_tensors = mgroup_input_tensors[0];
-    _parepare_io = true;
-  } else {
-  }
-  // for (int grpid = 0; grpid < group_count; grpid++) {
-  //   printf("\ngrpid: %d\n", grpid);
-  //   print_io_info(mgroup_input_tensors[grpid], mgroup_output_tensors[grpid]);
-  //   printf("==================================================\n\n");
-  // }
-
-  return ret;
-}
-
-int ax_runner_axcl::init(const char *model_file) {
-  std::vector<unsigned char> model_buffer;
-  if (!read_file(model_file, model_buffer)) {
-    fprintf(stderr, "read_file failed.\n");
-    return -1;
-  }
-  auto ret = init((char *)model_buffer.data(), model_buffer.size());
-  return ret;
-}
-
-int ax_runner_axcl::init(char *model_buffer, size_t model_size) {
-  if (!m_handle) {
-    m_handle = new ax_joint_runner_axcl_handle_t;
-  }
-  memset((void *)m_handle, 0, sizeof(ax_joint_runner_axcl_handle_t));
-
-  // 3. create handle
-  void *devMem = nullptr;
-  axclrtMalloc(&devMem, model_size, AXCL_MEM_MALLOC_NORMAL_ONLY);
-
-  // 4. copy model to device
-  axclrtMemcpy(devMem, model_buffer, model_size, AXCL_MEMCPY_HOST_TO_DEVICE);
-
-  int ret = axclrtEngineLoadFromMem(devMem, model_size, &m_handle->handle);
-  if (0 != ret) {
-    fprintf(stderr, "AX_ENGINE_CreateHandle");
-    return ret;
-  }
-  axclrtFree(devMem);
-
-  return sub_init();
-}
-
-void ax_runner_axcl::release() {
-  if (m_handle && m_handle->handle) {
-    for (int grpid = 0; grpid < group_count; grpid++) {
-      free_io(&m_handle->io_datas[grpid]);
-      axclrtEngineDestroyIO(m_handle->ios[grpid]);
-    }
-
-    axclrtEngineUnload(m_handle->handle);
-    m_handle->handle = 0;
-  }
-
-  if (m_handle) {
-    delete m_handle;
-    m_handle = nullptr;
-  }
-
-  minput_tensors.clear();
-  moutput_tensors.clear();
-
-  map_input_tensors.clear();
-  map_output_tensors.clear();
-
-  mgroup_input_tensors.clear();
-  mgroup_output_tensors.clear();
-
-  map_group_input_tensors.clear();
-  map_group_output_tensors.clear();
-}
-
-void ax_runner_axcl::deinit() {
-  if (m_handle && m_handle->handle) {
-    axclrtEngineUnload(m_handle->handle);
-    m_handle->handle = 0;
-  }
-}
-
-int ax_runner_axcl::get_algo_width() {
-  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
-    return minput_tensors[0].vShape[2];
-  }
-  return -1;
-}
-int ax_runner_axcl::get_algo_height() {
-  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
-    return minput_tensors[0].vShape[1];
-  }
-  return -1;
-}
-
-int ax_runner_axcl::set_input(int grpid, int idx,
-                              unsigned long long int phy_addr,
-                              unsigned long size) {
-  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid], idx,
-                                           (void *)phy_addr, size);
-}
-int ax_runner_axcl::set_output(int grpid, int idx,
-                               unsigned long long int phy_addr,
-                               unsigned long size) {
-  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid], idx,
-                                            (void *)phy_addr, size);
-}
-
-int ax_runner_axcl::set_input(int grpid, std::string name,
-                              unsigned long long int phy_addr,
-                              unsigned long size) {
-  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid],
-                                           get_input(grpid, name).nIdx,
-                                           (void *)phy_addr, size);
-}
-
-int ax_runner_axcl::set_output(int grpid, std::string name,
-                               unsigned long long int phy_addr,
-                               unsigned long size) {
-  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid],
-                                            get_output(grpid, name).nIdx,
-                                            (void *)phy_addr, size);
-}
-
-ax_color_space_e ax_runner_axcl::get_color_space() {
-  return ax_color_space_unknown;
-}
-
-int ax_runner_axcl::inference() { return inference(0); }
-
-int ax_runner_axcl::inference(int grpid) {
-  if (_auto_sync_before_inference)
-    for (size_t i = 0; i < mgroup_input_tensors[grpid].size(); i++)
-      axclrtMemcpy((void *)mgroup_input_tensors[grpid][i].phyAddr,
-                   mgroup_input_tensors[grpid][i].pVirAddr,
-                   mgroup_input_tensors[grpid][i].nSize,
-                   AXCL_MEMCPY_HOST_TO_DEVICE);
-
-  auto ret = axclrtEngineExecute(m_handle->handle, m_handle->context, grpid,
-                                 m_handle->ios[grpid]);
-  if (ret != 0) {
-    fprintf(stderr, "axclrtEngineExecute failed. ret=0x%x\n", ret);
-    return ret;
-  }
-
-  if (_auto_sync_after_inference)
-    for (size_t i = 0; i < mgroup_output_tensors[grpid].size(); i++)
-      axclrtMemcpy(mgroup_output_tensors[grpid][i].pVirAddr,
-                   (void *)mgroup_output_tensors[grpid][i].phyAddr,
-                   mgroup_output_tensors[grpid][i].nSize,
-                   AXCL_MEMCPY_DEVICE_TO_HOST);
-  return 0;
-}
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
deleted file mode 100644
index 4aba11b0..00000000
--- a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+++ /dev/null
@@ -1,40 +0,0 @@
-// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-#pragma once
-#include "ax_model_runner.hpp"
-
-class ax_runner_axcl : public ax_runner_base {
- protected:
-  struct ax_joint_runner_axcl_handle_t *m_handle = nullptr;
-  int group_count = 0;
-  bool _parepare_io = false;
-
-  int sub_init();
-
- public:
-  int init(const char *model_file) override;
-  int init(char *model_buffer, size_t model_size) override;
-
-  void release();
-  void deinit() override;
-
-  int get_algo_width() override;
-  int get_algo_height() override;
-  ax_color_space_e get_color_space() override;
-
-  int set_input(int grpid, int idx, unsigned long long int phy_addr,
-                unsigned long size);
-  int set_output(int grpid, int idx, unsigned long long int phy_addr,
-                 unsigned long size);
-
-  int set_input(int grpid, std::string name, unsigned long long int phy_addr,
-                unsigned long size);
-  int set_output(int grpid, std::string name, unsigned long long int phy_addr,
-                 unsigned long size);
-
-  // int inference(ax_image_t *pstFrame) override;
-  int inference() override;
-  int inference(int grpid) override;
-};
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
new file mode 100644
index 00000000..706095b6
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/axcl-engine-guard.h"
+
+#include <cstdint>
+
+#include "axcl.h"  // NOLINT
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+AxclEngineGuard::AxclEngineGuard(
+    axclrtEngineVNpuKind npuKind /*= AXCL_VNPU_DISABLE*/) {
+  axclError ret = axclrtEngineInit(npuKind);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("Failed to call axclrtEngineInit(). Return code is: %d",
+                     static_cast<int32_t>(ret));
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  initialized_ = true;
+}
+
+AxclEngineGuard::~AxclEngineGuard() {
+  if (initialized_) {
+    auto ret = axclrtEngineFinalize();
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineFinalize(). Return code is: %d",
+          static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-guard.h
new file mode 100644
index 00000000..15fed786
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-guard.h
@@ -0,0 +1,27 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-guard.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
+#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
+#include "axcl.h"  // NOLINT
+
+namespace sherpa_onnx {
+
+class AxclEngineGuard {
+ public:
+  explicit AxclEngineGuard(axclrtEngineVNpuKind npuKind = AXCL_VNPU_DISABLE);
+  ~AxclEngineGuard();
+
+  AxclEngineGuard(const AxclEngineGuard &) = delete;
+  AxclEngineGuard &operator=(const AxclEngineGuard &) = delete;
+  AxclEngineGuard(AxclEngineGuard &&) = delete;
+  AxclEngineGuard &operator=(AxclEngineGuard &&) = delete;
+
+ private:
+  bool initialized_ = false;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
new file mode 100644
index 00000000..f05b0131
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h"
+
+#include <cstdint>
+
+#include "axcl.h"  // NOLINT
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+AxclEngineIOGuard::AxclEngineIOGuard(axclrtEngineIOInfo io_info) {
+  axclError ret = axclrtEngineCreateIO(io_info, &io_);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE(
+        "Failed to call axclrtEngineCreateIO(). Return code is: %d",
+        static_cast<int32_t>(ret));
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  initialized_ = true;
+}
+
+AxclEngineIOGuard::~AxclEngineIOGuard() {
+  if (initialized_) {
+    auto ret = axclrtEngineDestroyIO(io_);
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineDestroyIO(). Return code is: %d",
+          static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
new file mode 100644
index 00000000..6401f0a8
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
@@ -0,0 +1,30 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
+#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
+#include "axcl.h"  // NOLINT
+
+namespace sherpa_onnx {
+
+class AxclEngineIOGuard {
+ public:
+  explicit AxclEngineIOGuard(axclrtEngineIOInfo io_info);
+  ~AxclEngineIOGuard();
+
+  AxclEngineIOGuard(const AxclEngineIOGuard &) = delete;
+  AxclEngineIOGuard &operator=(const AxclEngineIOGuard &) = delete;
+  AxclEngineIOGuard(AxclEngineIOGuard &&) = delete;
+  AxclEngineIOGuard &operator=(AxclEngineIOGuard &&) = delete;
+
+  operator axclrtEngineIO() { return io_; }
+
+ private:
+  bool initialized_ = false;
+  axclrtEngineIO io_ = nullptr;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
new file mode 100644
index 00000000..e2e7a124
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h"
+
+#include <cstdint>
+
+#include "axcl.h"  // NOLINT
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+AxclEngineIOInfoGuard::AxclEngineIOInfoGuard(uint64_t model_id) {
+  axclError ret = axclrtEngineGetIOInfo(model_id, &io_info_);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE(
+        "Failed to call axclrtEngineGetIOInfo(). Return code is: %d",
+        static_cast<int32_t>(ret));
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  initialized_ = true;
+}
+
+AxclEngineIOInfoGuard::~AxclEngineIOInfoGuard() {
+  if (initialized_) {
+    auto ret = axclrtEngineDestroyIOInfo(io_info_);
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineDestroyIOInfo(). Return code is: %d",
+          static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
new file mode 100644
index 00000000..0926b1b1
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
@@ -0,0 +1,32 @@
+// sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
+#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
+#include <cstdint>
+
+#include "axcl.h"  // NOLINT
+
+namespace sherpa_onnx {
+
+class AxclEngineIOInfoGuard {
+ public:
+  explicit AxclEngineIOInfoGuard(uint64_t model_id);
+  ~AxclEngineIOInfoGuard();
+
+  AxclEngineIOInfoGuard(const AxclEngineIOInfoGuard &) = delete;
+  AxclEngineIOInfoGuard &operator=(const AxclEngineIOInfoGuard &) = delete;
+  AxclEngineIOInfoGuard(AxclEngineIOInfoGuard &&) = delete;
+  AxclEngineIOInfoGuard &operator=(AxclEngineIOInfoGuard &&) = delete;
+
+  operator axclrtEngineIOInfo() { return io_info_; }
+
+ private:
+  bool initialized_ = false;
+  axclrtEngineIOInfo io_info_ = nullptr;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
diff --git a/sherpa-onnx/csrc/axcl/axcl-manager.cc b/sherpa-onnx/csrc/axcl/axcl-manager.cc
new file mode 100644
index 00000000..2166fb2f
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-manager.cc
@@ -0,0 +1,45 @@
+// sherpa-onnx/csrc/axcl/axcl-manager.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/axcl-manager.h"
+
+#include <cstdint>
+
+#include "axcl.h"  // NOLINT
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+std::mutex AxclManager::mutex_;
+
+int32_t AxclManager::count_{0};
+
+AxclManager::AxclManager(const char *config /*= nullptr*/) {
+  std::lock_guard<std::mutex> lock(mutex_);
+  if (count_ == 0) {
+    auto ret = axclInit(config);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call axclInit(). Return code: %d",
+                       static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  ++count_;
+}
+
+AxclManager::~AxclManager() {
+  std::lock_guard<std::mutex> lock(mutex_);
+  if (--count_ == 0) {
+    auto ret = axclFinalize();
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call axclFinalize(). Return code: %d",
+                       static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/axcl-manager.h b/sherpa-onnx/csrc/axcl/axcl-manager.h
new file mode 100644
index 00000000..ce349d93
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-manager.h
@@ -0,0 +1,29 @@
+// sherpa-onnx/csrc/axcl/axcl-manager.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
+#define SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
+
+#include <cstdint>
+#include <mutex>
+
+namespace sherpa_onnx {
+
+class AxclManager {
+ public:
+  explicit AxclManager(const char *config = nullptr);
+  ~AxclManager();
+
+  AxclManager(const AxclManager &) = delete;
+  AxclManager &operator=(const AxclManager &) = delete;
+
+  AxclManager(AxclManager &&) = delete;
+  AxclManager &operator=(AxclManager &&) = delete;
+
+ private:
+  static std::mutex mutex_;
+  static int32_t count_;
+};
+}  // namespace sherpa_onnx
+#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
diff --git a/sherpa-onnx/csrc/axcl/axcl-model.cc b/sherpa-onnx/csrc/axcl/axcl-model.cc
new file mode 100644
index 00000000..85a3b8a8
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-model.cc
@@ -0,0 +1,441 @@
+// sherpa-onnx/csrc/axcl/axcl-model.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/axcl-model.h"
+
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "axcl.h"  // NOLINT
+#include "sherpa-onnx/csrc/axcl/axcl-engine-guard.h"
+#include "sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h"
+#include "sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h"
+#include "sherpa-onnx/csrc/axcl/axcl-manager.h"
+#include "sherpa-onnx/csrc/axcl/utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+/*
+Initialization step:
+
+1. AxclInit()
+2. set device
+3. init engine
+4. axclrtEngineLoadFromMem or axclrtEngineLoadFromFile
+5. axclrtEngineCreateContext
+ */
+
+class AxclModel::Impl {
+ public:
+  Impl(const std::string &filename, int32_t device_id) {
+    if (!SetDevice(device_id)) {
+      return;
+    }
+
+    InitEngine();
+
+    axclError ret = axclrtEngineLoadFromFile(filename.c_str(), &model_id_);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineLoadFromFile() with file: %s. Return "
+          "code is: %d",
+          filename.c_str(), static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    model_loaded_ = true;
+
+    PostInit();
+  }
+
+  Impl(const void *cpu_buf, size_t buf_len_in_bytes, int32_t device_id) {
+    if (!SetDevice(device_id)) {
+      return;
+    }
+
+    InitEngine();
+
+    {
+      AxclDevicePtr device_ptr(buf_len_in_bytes, AXCL_MEM_MALLOC_NORMAL_ONLY);
+      auto ret = axclrtMemcpy(device_ptr, cpu_buf, buf_len_in_bytes,
+                              AXCL_MEMCPY_HOST_TO_DEVICE);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE("Failed to call axclrtMemcpy(). Return code is: %d",
+                         static_cast<int32_t>(ret));
+        return;
+      }
+
+      ret = axclrtEngineLoadFromMem(device_ptr, buf_len_in_bytes, &model_id_);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineLoadFromMem(). Return code is: %d",
+            static_cast<int32_t>(ret));
+        return;
+      }
+    }
+
+    model_loaded_ = true;
+
+    PostInit();
+  }
+
+  ~Impl() {
+    if (model_loaded_) {
+      axclError ret = axclrtEngineUnload(model_id_);
+
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineUnload(). Return code is: %d",
+            static_cast<int32_t>(ret));
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
+  const std::vector<std::string> &InputTensorNames() const {
+    return input_tensor_names_;
+  }
+  const std::vector<std::string> &OutputTensorNames() const {
+    return output_tensor_names_;
+  }
+
+  std::vector<int32_t> TensorShape(const std::string &name) const {
+    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
+      if (input_tensor_names_[i] == name) {
+        return input_tensor_shapes_[i];
+      }
+    }
+
+    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
+      if (output_tensor_names_[i] == name) {
+        return output_tensor_shapes_[i];
+      }
+    }
+
+    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
+    return {};
+  }
+
+  int32_t TensorSizeInBytes(const std::string &name) const {
+    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
+      if (input_tensor_names_[i] == name) {
+        return input_tensors_[i].Size();
+      }
+    }
+
+    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
+      if (output_tensor_names_[i] == name) {
+        return output_tensors_[i].Size();
+      }
+    }
+
+    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
+    return 0;
+  }
+
+  bool HasTensor(const std::string &name) const {
+    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
+      if (input_tensor_names_[i] == name) {
+        return true;
+      }
+    }
+
+    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
+      if (output_tensor_names_[i] == name) {
+        return true;
+      }
+    }
+
+    return false;
+  }
+
+  template <typename T>
+  bool SetInputTensorData(const std::string &name, const T *p,
+                          int32_t n) const {
+    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
+      if (input_tensor_names_[i] == name) {
+        if (n * sizeof(T) != input_tensors_[i].Size()) {
+          SHERPA_ONNX_LOGE("Expected size: %zu, given: %zu",
+                           input_tensors_[i].Size(), n * sizeof(T));
+          return false;
+        }
+
+        auto ret =
+            axclrtMemcpy(input_tensors_[i].Get(), p, input_tensors_[i].Size(),
+                         AXCL_MEMCPY_HOST_TO_DEVICE);
+        if (ret != 0) {
+          SHERPA_ONNX_LOGE(
+              "Failed to call axclrtMemcpy(). tensor name: '%s', return code: "
+              "%d",
+              name.c_str(), static_cast<int32_t>(ret));
+          return false;
+        }
+
+        return true;
+      }
+    }
+
+    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
+
+    return false;
+  }
+
+  std::vector<float> GetOutputTensorData(const std::string &name) const {
+    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
+      if (output_tensor_names_[i] == name) {
+        size_t bytes = output_tensors_[i].Size();
+        std::vector<float> out(bytes / sizeof(float));
+
+        auto ret = axclrtMemcpy(out.data(), output_tensors_[i].Get(), bytes,
+                                AXCL_MEMCPY_DEVICE_TO_HOST);
+        if (ret != 0) {
+          SHERPA_ONNX_LOGE(
+              "Failed to call axclrtMemcpy(). tensor name: '%s', return code: "
+              "%d",
+              name.c_str(), static_cast<int32_t>(ret));
+          return {};
+        }
+
+        return out;
+      }
+    }
+
+    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
+
+    return {};
+  }
+
+  bool Run() const {
+    uint32_t group = 0;
+    auto ret =
+        axclrtEngineExecute(model_id_, context_id_, group, *engine_io_guard_);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call axclrtEngineExecute(), return code: %d",
+                       static_cast<int32_t>(ret));
+      return false;
+    }
+    return true;
+  }
+
+  bool IsInitialized() const { return model_loaded_; }
+
+ private:
+  bool SetDevice(int32_t device_id) {
+    axclrtDeviceList lst;
+    auto ret = axclrtGetDeviceList(&lst);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtGetDeviceList(). Return code is: %d",
+          static_cast<int32_t>(ret));
+      return false;
+    }
+
+    if (lst.num == 0) {
+      SHERPA_ONNX_LOGE("Found 0 device.");
+      return false;
+    }
+
+    // device_id counts from 0
+    if (device_id < 0 || device_id >= lst.num) {
+      SHERPA_ONNX_LOGE("Invalid device_id: %d. Valid range: 0-%d", device_id,
+                       lst.num - 1);
+      return false;
+    }
+
+    ret = axclrtSetDevice(lst.devices[device_id]);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call axclrtSetDevice(). Return code is: %d",
+                       static_cast<int32_t>(ret));
+      return false;
+    }
+
+    return true;
+  }
+
+  void InitEngine() { engine_guard_ = std::make_unique<AxclEngineGuard>(); }
+
+  void PostInit() {
+    InitContext();
+
+    io_info_guard_ = std::make_unique<AxclEngineIOInfoGuard>(model_id_);
+
+    int32_t count = 0;
+    auto ret = axclrtEngineGetShapeGroupsCount(*io_info_guard_, &count);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineGetShapeGroupsCount(). Return code is: "
+          "%d",
+          static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (count != 1) {
+      SHERPA_ONNX_LOGE("Only support 1 group at present. Given: %d", count);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    engine_io_guard_ = std::make_unique<AxclEngineIOGuard>(*io_info_guard_);
+
+    InitInput();
+    InitOutput();
+  }
+
+  void InitContext() {
+    // Note(fangjun): No need to destroy context_id_
+    auto ret = axclrtEngineCreateContext(model_id_, &context_id_);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE(
+          "Failed to call axclrtEngineCreateContext(). Return code is: %d",
+          static_cast<int32_t>(ret));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  void InitInput() {
+    uint32_t group = 0;
+
+    int32_t num_inputs = axclrtEngineGetNumInputs(*io_info_guard_);
+
+    input_tensor_names_.resize(num_inputs);
+    input_tensor_shapes_.reserve(num_inputs);
+
+    for (int32_t i = 0; i < num_inputs; ++i) {
+      size_t size_in_bytes =
+          axclrtEngineGetInputSizeByIndex(*io_info_guard_, group, i);
+      input_tensors_.emplace_back(size_in_bytes, AXCL_MEM_MALLOC_HUGE_FIRST);
+
+      axclrtEngineIODims dims;
+      auto ret = axclrtEngineGetInputDims(*io_info_guard_, group, i, &dims);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineGetInputDims(). Return code is: %d",
+            static_cast<int32_t>(ret));
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      input_tensor_shapes_.emplace_back(dims.dims, dims.dims + dims.dimCount);
+
+      input_tensor_names_[i] =
+          axclrtEngineGetInputNameByIndex(*io_info_guard_, i);
+
+      ret = axclrtEngineSetInputBufferByIndex(*engine_io_guard_, i,
+                                              input_tensors_[i], size_in_bytes);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineSetInputBufferByIndex(). Return code "
+            "is: %d",
+            static_cast<int32_t>(ret));
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
+  void InitOutput() {
+    uint32_t group = 0;
+
+    int32_t num_outputs = axclrtEngineGetNumOutputs(*io_info_guard_);
+
+    output_tensor_names_.resize(num_outputs);
+    output_tensor_shapes_.reserve(num_outputs);
+
+    for (int32_t i = 0; i < num_outputs; ++i) {
+      auto size_in_bytes =
+          axclrtEngineGetOutputSizeByIndex(*io_info_guard_, group, i);
+      output_tensors_.emplace_back(size_in_bytes, AXCL_MEM_MALLOC_HUGE_FIRST);
+
+      axclrtEngineIODims dims;
+      auto ret = axclrtEngineGetOutputDims(*io_info_guard_, group, i, &dims);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineGetOutputDims(). Return code is: %d",
+            static_cast<int32_t>(ret));
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      output_tensor_shapes_.emplace_back(dims.dims, dims.dims + dims.dimCount);
+      output_tensor_names_[i] =
+          axclrtEngineGetOutputNameByIndex(*io_info_guard_, i);
+
+      ret = axclrtEngineSetOutputBufferByIndex(
+          *engine_io_guard_, i, output_tensors_[i], size_in_bytes);
+      if (ret != 0) {
+        SHERPA_ONNX_LOGE(
+            "Failed to call axclrtEngineSetOutputBufferByIndex(). Return code "
+            "is: %d",
+            static_cast<int32_t>(ret));
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
+ private:
+  AxclManager manager_;
+  std::unique_ptr<AxclEngineGuard> engine_guard_;
+  std::unique_ptr<AxclEngineIOGuard> engine_io_guard_;
+  std::unique_ptr<AxclEngineIOInfoGuard> io_info_guard_;
+
+  bool model_loaded_ = false;
+  uint64_t model_id_ = 0;
+  uint64_t context_id_ = 0;
+
+  std::vector<std::string> input_tensor_names_;
+  std::vector<std::string> output_tensor_names_;
+
+  std::vector<AxclDevicePtr> input_tensors_;
+  std::vector<AxclDevicePtr> output_tensors_;
+
+  std::vector<std::vector<int32_t>> input_tensor_shapes_;
+  std::vector<std::vector<int32_t>> output_tensor_shapes_;
+};
+
+AxclModel::AxclModel(const std::string &filename, int32_t device_id /*= 0*/)
+    : impl_(std::make_unique<Impl>(filename, device_id)) {}
+
+AxclModel::AxclModel(const void *cpu_buf, size_t buf_len_in_bytes,
+                     int32_t device_id /*= 0*/)
+    : impl_(std::make_unique<Impl>(cpu_buf, buf_len_in_bytes, device_id)) {}
+
+AxclModel::~AxclModel() = default;
+
+const std::vector<std::string> &AxclModel::InputTensorNames() const {
+  return impl_->InputTensorNames();
+}
+const std::vector<std::string> &AxclModel::OutputTensorNames() const {
+  return impl_->OutputTensorNames();
+}
+
+std::vector<int32_t> AxclModel::TensorShape(const std::string &name) const {
+  return impl_->TensorShape(name);
+}
+
+int32_t AxclModel::TensorSizeInBytes(const std::string &name) const {
+  return impl_->TensorSizeInBytes(name);
+}
+
+bool AxclModel::HasTensor(const std::string &name) const {
+  return impl_->HasTensor(name);
+}
+
+bool AxclModel::SetInputTensorData(const std::string &name, const float *p,
+                                   int32_t n) const {
+  return impl_->SetInputTensorData(name, p, n);
+}
+
+bool AxclModel::SetInputTensorData(const std::string &name, const int32_t *p,
+                                   int32_t n) const {
+  return impl_->SetInputTensorData(name, p, n);
+}
+
+std::vector<float> AxclModel::GetOutputTensorData(
+    const std::string &name) const {
+  return impl_->GetOutputTensorData(name);
+}
+
+bool AxclModel::Run() const { return impl_->Run(); }
+
+bool AxclModel::IsInitialized() const { return impl_->IsInitialized(); }
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/axcl-model.h b/sherpa-onnx/csrc/axcl/axcl-model.h
new file mode 100644
index 00000000..e6685bae
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/axcl-model.h
@@ -0,0 +1,49 @@
+// sherpa-onnx/csrc/axcl/axcl-model.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
+#define SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <vector>
+
+namespace sherpa_onnx {
+
+class AxclModel {
+ public:
+  explicit AxclModel(const std::string &filename, int32_t device_id = 0);
+
+  AxclModel(const void *cpu_buf, size_t buf_len_in_bytes,
+            int32_t device_id = 0);
+  ~AxclModel();
+
+  const std::vector<std::string> &InputTensorNames() const;
+  const std::vector<std::string> &OutputTensorNames() const;
+
+  std::vector<int32_t> TensorShape(const std::string &name) const;
+  int32_t TensorSizeInBytes(const std::string &name) const;
+
+  bool HasTensor(const std::string &name) const;
+
+  bool SetInputTensorData(const std::string &name, const float *p,
+                          int32_t n) const;
+
+  bool SetInputTensorData(const std::string &name, const int32_t *p,
+                          int32_t n) const;
+
+  std::vector<float> GetOutputTensorData(const std::string &name) const;
+
+  bool Run() const;
+  bool IsInitialized() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
index 54461590..db8dd643 100644
--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
@@ -7,10 +7,11 @@
 #include <algorithm>
 #include <array>
 #include <cstring>
+#include <memory>
 #include <utility>
 #include <vector>
 
-#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
+#include "sherpa-onnx/csrc/axcl/axcl-model.h"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
@@ -18,17 +19,18 @@ namespace sherpa_onnx {
 
 class OfflineSenseVoiceModelAxcl::Impl {
  public:
-  ~Impl() { runner_.release(); }
-
   explicit Impl(const OfflineModelConfig &config) : config_(config) {
-    auto buf = ReadFile(config_.sense_voice.model);
-    Init(buf.data(), buf.size());
+    model_ = std::make_unique<AxclModel>(config_.sense_voice.model);
+
+    PostInit();
   }
 
   template <typename Manager>
   Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
     auto buf = ReadFile(mgr, config_.sense_voice.model);
-    Init(buf.data(), buf.size());
+    model_ = std::make_unique<AxclModel>(buf.data(), buf.size());
+
+    PostInit();
   }
 
   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
@@ -40,86 +42,23 @@ class OfflineSenseVoiceModelAxcl::Impl {
     features = ApplyLFR(std::move(features));
     std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
 
-    // input 0: features
-    auto &in0 = runner_.get_input(0);
-    size_t bytes0 = in0.nSize;
-    if (bytes0 != features.size() * sizeof(float)) {
-      SHERPA_ONNX_LOGE(
-          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
-          in0.nSize, features.size() * sizeof(float));
-      SHERPA_ONNX_EXIT(-1);
-    }
-    std::memcpy(in0.pVirAddr, features.data(), bytes0);
-
-    auto &in1 = runner_.get_input(1);
-    size_t bytes1 = in1.nSize;
-    if (bytes1 != prompt.size() * sizeof(int32_t)) {
-      SHERPA_ONNX_LOGE(
-          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
-          in1.nSize, prompt.size() * sizeof(int32_t));
-      SHERPA_ONNX_EXIT(-1);
-    }
-    std::memcpy(in1.pVirAddr, prompt.data(), bytes1);
-
-    int ret = runner_.inference();
-    if (ret != 0) {
-      SHERPA_ONNX_LOGE("ax_runner_axcl inference failed, ret = %d", ret);
-      SHERPA_ONNX_EXIT(-1);
-    }
-
-    // output 0
-    auto &out0 = runner_.get_output(0);
-    size_t out_elems = out0.nSize / sizeof(float);
-    std::vector<float> out(out_elems);
-    std::memcpy(out.data(), out0.pVirAddr, out0.nSize);
-    return out;
+    model_->SetInputTensorData("x", features.data(), features.size());
+    model_->SetInputTensorData("prompt", prompt.data(), prompt.size());
+    model_->Run();
+    return model_->GetOutputTensorData("logits");
   }
 
  private:
-  void Init(void *model_data, size_t model_data_length) {
-    {
-      if (auto ret = axclInit(0); 0 != ret) {
-        fprintf(stderr, "Init AXCL failed{0x%8x}.\n", ret);
-        return;
-      }
-      axclrtDeviceList lst;
-      if (const auto ret = axclrtGetDeviceList(&lst);
-          0 != ret || 0 == lst.num) {
-        fprintf(stderr,
-                "Get AXCL device failed{0x%8x}, find total %d device.\n", ret,
-                lst.num);
-        return;
-      }
-      if (const auto ret = axclrtSetDevice(lst.devices[0]); 0 != ret) {
-        fprintf(stderr, "Set AXCL device failed{0x%8x}.\n", ret);
-        return;
-      }
-      int ret = axclrtEngineInit(AXCL_VNPU_DISABLE);
-      if (0 != ret) {
-        fprintf(stderr, "axclrtEngineInit %d\n", ret);
-        return;
-      }
-    }
-
-    int ret =
-        runner_.init(reinterpret_cast<char *>(model_data), model_data_length);
-    if (ret != 0) {
-      SHERPA_ONNX_LOGE("Init ax_runner_axcl failed, ret = %d", ret);
+  void PostInit() {
+    if (!model_->IsInitialized()) {
+      SHERPA_ONNX_LOGE("Failed to initialize the model with '%s'",
+                       config_.sense_voice.model.c_str());
       SHERPA_ONNX_EXIT(-1);
     }
 
-    auto &in0 = runner_.get_input(0);
-    if (in0.vShape.size() < 2) {
-      SHERPA_ONNX_LOGE(
-          "Input tensor rank is too small (rank = %zu). Shape vector is empty "
-          "or has only 1 dim.",
-          in0.vShape.size());
-      SHERPA_ONNX_EXIT(-1);
-    }
-    num_input_frames_ = in0.vShape[1];
+    num_input_frames_ = model_->TensorShape("x")[1];
 
     if (config_.debug) {
-      SHERPA_ONNX_LOGE("Axcl SenseVoice model init done with ax_runner_axcl.");
       SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
     }
   }
@@ -156,7 +95,7 @@ class OfflineSenseVoiceModelAxcl::Impl {
 
  private:
   OfflineModelConfig config_;
-  ax_runner_axcl runner_;
+  std::unique_ptr<AxclModel> model_;
   OfflineSenseVoiceModelMetaData meta_data_;
   int32_t num_input_frames_ = -1;
 };
@@ -193,4 +132,4 @@ template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
     NativeResourceManager *mgr, const OfflineModelConfig &config);
 #endif
 
-}  // namespace sherpa_onnx
\ No newline at end of file
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
index 11a7503b..2ef47ee8 100644
--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
@@ -8,8 +8,6 @@
 #include <memory>
 #include <vector>
 
-#include "axcl.h"  // NOLINT
-#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
 
diff --git a/sherpa-onnx/csrc/axcl/utils.cc b/sherpa-onnx/csrc/axcl/utils.cc
new file mode 100644
index 00000000..0dec25bc
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/utils.cc
@@ -0,0 +1,41 @@
+// sherpa-onnx/csrc/axcl/utils.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axcl/utils.h"
+
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+AxclDevicePtr::AxclDevicePtr(
+    size_t size,
+    axclrtMemMallocPolicy policy /*= AXCL_MEM_MALLOC_HUGE_FIRST*/) {
+  auto ret = axclrtMalloc(&p_, size, policy);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("Failed to call axclrtMalloc(). Return code: %d",
+                     static_cast<int32_t>(ret));
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  size_ = size;
+}
+
+void AxclDevicePtr::Release() {
+  if (!p_) {
+    return;
+  }
+
+  auto ret = axclrtFree(p_);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("Failed to call axclrtFree(). Return code: %d",
+                     static_cast<int32_t>(ret));
+    SHERPA_ONNX_EXIT(-1);
+  }
+  p_ = nullptr;
+  size_ = 0;
+}
+
+AxclDevicePtr::~AxclDevicePtr() { Release(); }
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axcl/utils.h b/sherpa-onnx/csrc/axcl/utils.h
new file mode 100644
index 00000000..e3ea50f8
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/utils.h
@@ -0,0 +1,57 @@
+// sherpa-onnx/csrc/axcl/utils.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_UTILS_H_
+#define SHERPA_ONNX_CSRC_AXCL_UTILS_H_
+
+#include "axcl.h"  // NOLINT
+
+namespace sherpa_onnx {
+
+class AxclDevicePtr {
+ public:
+  explicit AxclDevicePtr(
+      size_t size, axclrtMemMallocPolicy policy = AXCL_MEM_MALLOC_HUGE_FIRST);
+
+  ~AxclDevicePtr();
+
+  AxclDevicePtr(const AxclDevicePtr &) = delete;
+  AxclDevicePtr &operator=(const AxclDevicePtr &) = delete;
+
+  AxclDevicePtr(AxclDevicePtr &&other) {
+    p_ = other.p_;
+    size_ = other.size_;
+
+    other.p_ = nullptr;
+    other.size_ = 0;
+  }
+  AxclDevicePtr &operator=(AxclDevicePtr &&other) {
+    if (this == &other) {
+      return *this;
+    }
+    Release();
+    p_ = other.p_;
+    size_ = other.size_;
+
+    other.p_ = nullptr;
+    other.size_ = 0;
+
+    return *this;
+  }
+
+  void Release();
+
+  void *Get() const { return p_; }
+  operator void *() { return p_; }
+
+  size_t Size() const { return size_; }
+
+ private:
+  void *p_ = nullptr;
+  size_t size_ = 0;  // in bytes
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_UTILS_H_

commit 2202e2a59ba7687dd007afcd0b3050a3af7e5d6c
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Fri Dec 5 20:44:58 2025 +0800

    Release v1.12.19 (#2868)

diff --git a/.github/workflows/windows-x64-jni.yaml b/.github/workflows/windows-x64-jni.yaml
index 2f9682f5..bf1e5468 100644
--- a/.github/workflows/windows-x64-jni.yaml
+++ b/.github/workflows/windows-x64-jni.yaml
@@ -81,9 +81,7 @@ jobs:
           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-win-x64-jni
           mkdir -p $dst
 
-          cp -a build/install/bin $dst/ || true
           cp -a build/install/lib $dst/ || true
-          cp -a build/install/include $dst/ || true
 
           tar cjvf ${dst}.tar.bz2 $dst
 
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 78bf0cb7..439b0db9 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,24 @@
+## 1.12.19
+
+* Fix building without TTS for C API (#2838)
+* [ZipVoice] Fix english tokenization error (#2834)
+* Add simulate streaming ASR Python example for Paraformer (#2839)
+* Fix building JNI for Windows (#2840)
+* Avoid NaN in NeMo speaker embedding models. (#2844)
+* Add spacemit ort ep for spacemit riscv cpus (#2837)
+* Add token-level confidence scores (ys_probs) for offline transducer models (#2843)
+* Fix token log probabilities in offline transducer modified beam search decoder (#2846)
+* Support AXERA ax630, ax650, and axcl backends. (#2849)
+* Refactor axera npu examples (#2850)
+* Fix matcha tts zh-en model (#2851)
+* Fix the English part for Matcha TTS. (#2853)
+* Refactor text-utils (#2855)
+* Fix matcha tts (#2856)
+* Add a space between English words for Matcha zh-en TTS (#2858)
+* Fix punctuations in matcha zh-en tts (#2859)
+* Upload matcha tts zh-en model (#2865)
+* Fix the discrepancy with the Silero VAD isSpeech logic (#2863)
+
 ## 1.12.18
 
 * Fix building wheels (#2786)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 20f63f71..84be55ea 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.18")
+set(SHERPA_ONNX_VERSION "1.12.19")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index 214a87cd..2ac752f2 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index 214a87cd..2ac752f2 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index 7d1505bd..d7942a39 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-v1.12.18-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.18-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-v1.12.19-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.19-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.18.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.19.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index cf40dd2a..dafdd504 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index 27ad7ac2..29f894c1 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index f9d0c62b..c0b95fdf 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.18'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.19'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index 214a87cd..2ac752f2 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index 43b998bf..571a805f 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index 3a5e95cc..ac309729 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.18")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.19")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index 34f1fcd5..fe528faa 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index 1af89d4a..12e210ee 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index 41a2bdf9..fc0c03e1 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index b1151b94..0b6ba6c0 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 53ad4569..6a8978b9 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251127
-        versionName = "1.12.18"
+        versionCode = 20251205
+        versionName = "1.12.19"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index b65311a6..66bf1f41 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index b65311a6..66bf1f41 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 45e27edd..3a29c0b1 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251127
-        versionName "1.12.18"
+        versionCode 20251205
+        versionName "1.12.19"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index a52287f5..df5c6037 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.18</string>
+	<string>1.12.19</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index 9eec66cc..b77879a4 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index c48cb85a..396260c2 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index 4b90ab48..04eeb417 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index 4f89b81f..fcae6b01 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index 1850039f..8a1fadb3 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index e5122ffa..ede5c6e4 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index 0292fd08..f42abe91 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index 90ff0b20..0564cda2 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index f6abcf2f..377dd7bb 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index d551987f..cb072fd3 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index de635d33..d581ebee 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 83a6e030..403fd353 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index c3588163..70fbb929 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.18
+version: 1.12.19
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index 0a259b1d..e3527e8a 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.18
+version: 1.12.19
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index 21d23eb4..d98a541b 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.18
+version: 1.12.19
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.18
+  sherpa_onnx: ^1.12.19
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index 3491fead..b65ccfeb 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.18
+version: 1.12.19
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.18
+  sherpa_onnx_android: ^1.12.19
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.18
+  sherpa_onnx_macos: ^1.12.19
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.18
+  sherpa_onnx_linux: ^1.12.19
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.18
+  sherpa_onnx_windows: ^1.12.19
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.18
+  sherpa_onnx_ios: ^1.12.19
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index a4b7b606..ffd952bf 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.18'
+  s.version          = '1.12.19'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index dcd8f6b5..ec1ea5fd 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.18'
+  s.version          = '1.12.19'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index b04e4790..2a5e5dbf 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.18';
+export const HAR_VERSION = '1.12.19';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index 9467e26d..dde223ef 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.18",
+    "sherpa_onnx": "1.12.19",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index 7438ad28..f3e43a76 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.18",
+  "version": "1.12.19",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index 4fc965d4..c703cba9 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.18"
+    "sherpa_onnx": "1.12.19"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 88735c1c..21ccbcef 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.18",
+    "sherpa_onnx": "1.12.19",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 88735c1c..21ccbcef 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.18",
+    "sherpa_onnx": "1.12.19",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 88735c1c..21ccbcef 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.18",
+    "sherpa_onnx": "1.12.19",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index dff1eb6b..af0510b9 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.18.har
+Please download ./sherpa_onnx-v1.12.19.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index cf0d6dee..c24b1baf 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.18",
+    "sherpa_onnx": "1.12.19",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index bd8c3c5d..5591d3ac 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-1.12.18.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-1.12.19.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.18.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.18 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.19.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.19 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index 683034ac..970c9cc3 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x86-v1.12.18.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x86-v1.12.18.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x86-v1.12.18.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x86-v1.12.19.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x86-v1.12.19.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x86-v1.12.19.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 5d4a89c5..4d2d2280 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20251113
-new_version_code=20251127
+old_version_code=20251127
+new_version_code=20251205
 
-old_version="1\.12\.17"
-new_version="1\.12\.18"
+old_version="1\.12\.18"
+new_version="1\.12\.19"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index 3555e6ed..d24f1105 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.18"
+    "sherpa-onnx-node": "^1.12.19"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index db474390..af3538c6 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.18",
+    "sherpa-onnx": "^1.12.19",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index f54d3c84..0d84b015 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.18</version>
+    <version>1.12.19</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index c86f3727..e9a561da 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.18",
+    version="1.12.19",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.18",
+        "sherpa-onnx-core==1.12.19",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index 456e2b7f..d053d293 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.18",
+    version="1.12.19",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 329bddd4..63991827 100644
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.18"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.19"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index e118dcc5..315d3d2e 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Thu Nov 27 15:24:39 2025";
+  static const char *date = "Fri Dec 5 11:45:41 2025";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "7d1d2270";
+  static const char *sha1 = "e6a6599f";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.18";
+  static const char *version = "1.12.19";
   return version;
 }
 

commit 32fbd3a6d68e8859348747e77e8b6e8a985119a1
Author: ming030890 <67713085+ming030890@users.noreply.github.com>
Date:   Fri Dec 5 04:00:29 2025 +0000

    Fix the discrepancy with the Silero VAD isSpeech logic (#2863)

diff --git a/sherpa-onnx/csrc/silero-vad-model-config.cc b/sherpa-onnx/csrc/silero-vad-model-config.cc
index 2623a179..6f015e55 100644
--- a/sherpa-onnx/csrc/silero-vad-model-config.cc
+++ b/sherpa-onnx/csrc/silero-vad-model-config.cc
@@ -43,6 +43,12 @@ void SileroVadModelConfig::Register(ParseOptions *po) {
       "512, 1024, 1536 samples for 16000 sample rate and 256, 512, 768 samples "
       "for 8000 sample rate. Values other than these may affect model "
       "performance!");
+
+  po->Register(
+    "silero-vad-neg-threshold", &neg_threshold,
+    "Negative threshold (noise threshold). If < 0, defaults to "
+    "(threshold - 0.15) with lower bound 0.01."
+  );
 }
 
 bool SileroVadModelConfig::Validate() const {
@@ -110,7 +116,8 @@ std::string SileroVadModelConfig::ToString() const {
   os << "min_silence_duration=" << min_silence_duration << ", ";
   os << "min_speech_duration=" << min_speech_duration << ", ";
   os << "max_speech_duration=" << max_speech_duration << ", ";
-  os << "window_size=" << window_size << ")";
+  os << "window_size=" << window_size << ", ";
+  os << "neg_threshold=" << neg_threshold << ")";
 
   return os.str();
 }
diff --git a/sherpa-onnx/csrc/silero-vad-model-config.h b/sherpa-onnx/csrc/silero-vad-model-config.h
index 95b32e6b..e99c8c78 100644
--- a/sherpa-onnx/csrc/silero-vad-model-config.h
+++ b/sherpa-onnx/csrc/silero-vad-model-config.h
@@ -31,6 +31,12 @@ struct SileroVadModelConfig {
   // the threshold value is reset to its original value.
   float max_speech_duration = 20;  // in seconds
 
+  // Negative (exit) threshold for transitioning from speech  silence.
+  // If left as a negative value, the default Silero rule applies:
+  //     neg_threshold = max(threshold - 0.15f, 0.01f)
+  // This prevents the exit threshold from becoming negative when threshold < 0.15.
+  float neg_threshold = -1;
+
   SileroVadModelConfig() = default;
 
   void Register(ParseOptions *po);
diff --git a/sherpa-onnx/csrc/silero-vad-model.cc b/sherpa-onnx/csrc/silero-vad-model.cc
index 00a0f399..5b1bb27a 100644
--- a/sherpa-onnx/csrc/silero-vad-model.cc
+++ b/sherpa-onnx/csrc/silero-vad-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/silero-vad-model.h"
 
+#include <algorithm>
 #include <memory>
 #include <string>
 #include <utility>
@@ -131,7 +132,13 @@ class SileroVadModel::Impl {
       return false;
     }
 
-    if ((prob > threshold - 0.15) && triggered_) {
+    float neg_threshold;
+    if (config_.silero_vad.neg_threshold < 0) {
+        neg_threshold = std::max(threshold - 0.15f, 0.01f);
+    } else {
+        neg_threshold = std::max(config_.silero_vad.neg_threshold, 0.01f);
+    }
+    if ((prob > neg_threshold) && triggered_) {
       // speaking
       return true;
     }

commit e6a6599f59ca2dc8ef2e818995d741a4121c4757
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Fri Dec 5 11:45:41 2025 +0800

    Upload matcha tts zh-en model (#2865)

diff --git a/.github/workflows/export-matcha-zh-en.yaml b/.github/workflows/export-matcha-zh-en.yaml
new file mode 100644
index 00000000..154be1b4
--- /dev/null
+++ b/.github/workflows/export-matcha-zh-en.yaml
@@ -0,0 +1,163 @@
+name: export-matcha-zh-en-to-onnx
+
+on:
+  push:
+    branches:
+      - matcha-zh-en
+
+  workflow_dispatch:
+
+concurrency:
+  group: export-matcha-zh-en-to-onnx-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-matcha-zh-en-to-onnx:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: export matcha zh-en ${{ matrix.version }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        python-version: ["3.10"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          pip install "numpy<=1.26.4" pypinyin soundfile \
+            sherpa-onnx -f https://k2-fsa.github.io/sherpa/onnx/cpu.html
+
+      - name: Generate samples
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        shell: bash
+        run: |
+          cd scripts/matcha-tts/zh-en
+
+          git config --global user.email "csukuangfj@gmail.com"
+          git config --global user.name "Fangjun Kuang"
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/matcha-icefall-zh-en.tar.bz2
+          tar xvf matcha-icefall-zh-en.tar.bz2
+          rm matcha-icefall-zh-en.tar.bz2
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
+
+          git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-tts-samples hf
+          mkdir -p ./hf/matcha/icefall-zh-en/mp3
+
+          ./generate_samples.py
+
+          pushd hf
+          git pull
+          git add .
+          git commit -m 'add samples for matcha tts zh en'
+          git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-tts-samples main
+          popd
+          rm -rf hf
+
+          ls -lh
+
+      - name: Run
+        shell: bash
+        run: |
+          cd scripts/matcha-tts/zh-en
+          curl -SL -O https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/model-steps-3.onnx
+          curl -SL -O https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocab_tts.txt
+
+          ./generate_tokens.py
+          ./generate_lexicon.py
+
+          curl -SL -o date-zh.fst https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/date.fst
+          curl -SL -o number-zh.fst  https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/number.fst
+          curl -SL -o phone-zh.fst https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/phone.fst
+
+      - name: Collect results ${{ matrix.version }}
+        shell: bash
+        run: |
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
+          tar xf espeak-ng-data.tar.bz2
+          rm espeak-ng-data.tar.bz2
+
+          src=scripts/matcha-tts/zh-en
+          dst=matcha-icefall-zh-en
+
+          mkdir $dst
+
+          cp -a espeak-ng-data $dst/
+
+          cp -v $src/tokens.txt $dst
+          cp -v $src/lexicon.txt $dst
+          cp -v $src/model-steps-3.onnx $dst
+          cp -v $src/README.md $dst
+          cp -v $src/*.fst $dst
+
+          tar cjfv $dst.tar.bz2 $dst
+
+          ls -lh $dst.tar.bz2
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_LFS_SKIP_SMUDGE=1
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+
+            git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/matcha-icefall-zh-en huggingface
+            cd huggingface
+            rm -rf ./*
+            git fetch
+            git pull
+
+            git lfs track "cmn_dict"
+            git lfs track "ru_dict" af_dict ar_dict da_dict en_dict fa_dict hu_dict ia_dict it_dict lb_dict phondata ta_dict ur_dict yue_dict
+
+            cp -a ../matcha-icefall-zh-en/* ./
+
+            git lfs track "*.onnx"
+            git add .
+
+            ls -lh
+
+            git status
+
+            git commit -m "add models"
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/matcha-icefall-zh-en main || true
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: tts-models
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          tag: tts-models
diff --git a/scripts/matcha-tts/zh-en/README.md b/scripts/matcha-tts/zh-en/README.md
index 0fba2f75..e46b40e4 100644
--- a/scripts/matcha-tts/zh-en/README.md
+++ b/scripts/matcha-tts/zh-en/README.md
@@ -6,9 +6,10 @@ https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/summary
 Note that you have to use
 vocos-16khz-univ.onnx
 
-You can download it from 
+You can download it from
  https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocos-16khz-univ.onnx
 or
+ https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
 
 ```
 {'am': './model-steps-3.onnx', 'vocoder': './vocos-16khz-univ.onnx', 'tokens': './tokens.txt', 'lexicon': './lexicon.txt', 'text': '. It supports both English ', 'out_wav': 'generated.wav'}
diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
index 4722dc58..21392359 100755
--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
+++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
@@ -45,12 +45,13 @@ def main():
             if key in user_defined:
                 continue
             tokens = pinyin(key, style=Style.TONE3, neutral_tone_with_five=True)
+
             for i in range(len(tokens)):
-                if tokens[i] == "shei2":
-                    tokens[i] = "shui2"
+                if tokens[i][0] == "shei2":
+                    tokens[i][0] = "shui2"
 
-                if tokens[i][-1] not in ("1", "2", "3", "4", "5"):
-                    tokens[i] += "1"
+                if tokens[i][0][-1] not in ("1", "2", "3", "4", "5"):
+                    tokens[i][0] += "1"
 
             flatten = [t[0] for t in tokens]
 
diff --git a/scripts/matcha-tts/zh-en/generate_samples.py b/scripts/matcha-tts/zh-en/generate_samples.py
new file mode 100755
index 00000000..81df8d9c
--- /dev/null
+++ b/scripts/matcha-tts/zh-en/generate_samples.py
@@ -0,0 +1,40 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+"""
+Generate samples for
+https://k2-fsa.github.io/sherpa/onnx/tts/all/
+"""
+
+
+import sherpa_onnx
+import soundfile as sf
+
+config = sherpa_onnx.OfflineTtsConfig(
+    model=sherpa_onnx.OfflineTtsModelConfig(
+        matcha=sherpa_onnx.OfflineTtsMatchaModelConfig(
+            acoustic_model="matcha-icefall-zh-en/model-steps-3.onnx",
+            vocoder="vocos-16khz-univ.onnx",
+            lexicon="matcha-icefall-zh-en/lexicon.txt",
+            tokens="matcha-icefall-zh-en/tokens.txt",
+            data_dir="matcha-icefall-zh-en/espeak-ng-data",
+        ),
+        num_threads=2,
+    ),
+    max_num_sentences=1,
+    rule_fsts="./matcha-icefall-zh-en/phone-zh.fst,./matcha-icefall-zh-en/date-zh.fst,./matcha-icefall-zh-en/number-zh.fst",
+)
+
+if not config.validate():
+    raise ValueError("Please check your config")
+
+tts = sherpa_onnx.OfflineTts(config)
+text = "machine learningartificial intelligencevocationParis; 2025124110189202512043123456"
+
+
+audio = tts.generate(text, sid=0, speed=1.0)
+
+sf.write(
+    "./hf/matcha/icefall-zh-en/mp3/0.mp3",
+    audio.samples,
+    samplerate=audio.sample_rate,
+)

commit ab479d8e01d580d86f666d5d5fac278e2a86b6fd
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Dec 4 16:17:01 2025 +0800

    Fix punctuations in matcha zh-en tts (#2859)

diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index e7059c99..873918d0 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -197,7 +197,7 @@ class MatchaTtsLexicon::Impl {
   std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
     std::string text = _text;
     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
-        {"", ","}, {"", ","}, {"", ";"}, {"", ":"},
+        {"", ","}, {"", ","}, {"", ";"}, {"", ","},   {":", ","},
         {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
     };
     for (const auto &p : replace_str_pairs) {

commit 5ea95ee047cc7d337ed1802ce9e7f6cf0c82f5b6
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Dec 4 16:07:08 2025 +0800

    Add a space between English words for Matcha zh-en TTS (#2858)

diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 1e08d478..e7059c99 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
 
+#include <ctype.h>
+
 #include <algorithm>
 #include <fstream>
 #include <memory>
@@ -281,7 +283,10 @@ class MatchaTtsLexicon::Impl {
 
     PhraseMatcher matcher(&all_words_, words, debug_);
 
+    int32_t blank = token2id_.at(" ");
+
     std::vector<int32_t> ids;
+    std::string last_word;
     for (const std::string &w : matcher) {
       ids = ConvertWordToIds(w);
 
@@ -291,9 +296,15 @@ class MatchaTtsLexicon::Impl {
 #else
         SHERPA_ONNX_LOGE("Ignore OOV '%s'", w.c_str());
 #endif
+
+        last_word = w;
         continue;
       }
 
+      if (!last_word.empty() && isalpha(last_word[0])) {
+        this_sentence.push_back(blank);
+      }
+
       this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
 
       if (IsPunct(w)) {
@@ -312,6 +323,8 @@ class MatchaTtsLexicon::Impl {
         ans.emplace_back(std::move(this_sentence));
         this_sentence = {};
       }
+
+      last_word = w;
     }  // for (const std::string &w : matcher)
 
     if (!this_sentence.empty()) {

commit 7cfded8fafbc259fd138434caa183d50397d6ccc
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Dec 4 13:50:26 2025 +0800

    Fix matcha tts (#2856)

diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
index 2d1255ce..92d7efff 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
@@ -431,10 +431,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
 
     std::vector<int64_t> x;
     x.reserve(num_tokens);
+    for (const auto &k : tokens) {
+      x.insert(x.end(), k.begin(), k.end());
+    }
+
     if (config_.model.debug) {
-      for (const auto &k : tokens) {
-        x.insert(x.end(), k.begin(), k.end());
-      }
       std::ostringstream oss;
       for (int32_t i : x) {
         oss << i << ", ";

commit 5e7e2525661ddc46dc48c096777cd43cc6b69f4d
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Dec 4 12:06:52 2025 +0800

    Refactor text-utils (#2855)
    
    This pull request refactors text utility functions by introducing a generic Join function in text-utils and removing a specialized JoinTokensNoSpace function.

diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 6248cb64..1e08d478 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -92,14 +92,6 @@ std::vector<std::string> ConvertPhonemesToUTF8(
   return out;
 }
 
-std::string JoinTokensNoSpace(const std::vector<std::string> &tokens) {
-  std::string out;
-  for (const auto &t : tokens) {
-    out += t;
-  }
-  return out;
-}
-
 std::string ApplyReplacements(std::string s) {
   for (const auto &p : kReplacements) {
     const std::string &from = p.first;
@@ -131,7 +123,7 @@ std::vector<std::string> SplitTokensUTF8(const std::string &s) {
 std::vector<std::string> ProcessPhonemes(
     const std::vector<std::vector<char32_t>> &phonemes) {
   auto tokens = ConvertPhonemesToUTF8(phonemes);
-  std::string joined = JoinTokensNoSpace(tokens);
+  std::string joined = Join(tokens);
   std::string replaced = ApplyReplacements(joined);
   return SplitTokensUTF8(replaced);
 }
diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
index 993f2bb0..3c759abe 100644
--- a/sherpa-onnx/csrc/text-utils.cc
+++ b/sherpa-onnx/csrc/text-utils.cc
@@ -733,6 +733,17 @@ std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size) {
   return ans;
 }
 
+std::string Join(const std::vector<std::string> &ss, const std::string &delim) {
+  std::ostringstream oss;
+  if (!ss.empty()) {
+    oss << ss[0];
+    for (size_t i = 1; i < ss.size(); ++i) {
+      oss << delim << ss[i];
+    }
+  }
+  return oss.str();
+}
+
 std::u32string Utf8ToUtf32(const std::string &str) {
   std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
   return conv.from_bytes(str);
diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
index 3f0f80d5..d1f5ce39 100644
--- a/sherpa-onnx/csrc/text-utils.h
+++ b/sherpa-onnx/csrc/text-utils.h
@@ -151,6 +151,9 @@ bool Contains(const std::string &haystack, const std::string &needle);
 
 std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size);
 
+std::string Join(const std::vector<std::string> &ss,
+                 const std::string &delim = "");
+
 // Converts a UTF-8 std::string to a UTF-32 std::u32string
 std::u32string Utf8ToUtf32(const std::string &str);
 

commit 2493d1f9be7cddb397b7aa4e340b9072a0ac0618
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Dec 4 11:18:15 2025 +0800

    Fix the English part for Matcha TTS. (#2853)

diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 21726791..6248cb64 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -5,7 +5,6 @@
 #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
 
 #include <algorithm>
-#include <codecvt>
 #include <fstream>
 #include <memory>
 #include <regex>  // NOLINT
@@ -38,6 +37,107 @@
 
 namespace sherpa_onnx {
 
+namespace {
+// code in this anonymous namespace is written by ChatGPT
+//
+// Please see https://github.com/k2-fsa/sherpa-onnx/pull/2853
+// for why we need to do the replacement
+static const std::vector<std::pair<std::string, std::string>> kReplacements = {
+    {"", ""}, {"", ""},
+
+    {"e", "A"}, {"a", "I"}, {"", "Y"},
+    {"o", "O"}, {"", "O"}, {"a", "W"},
+
+    {"t", ""}, {"d", ""},
+
+    {"", ""},
+
+    {"g", ""},  {"r", ""},
+
+    {"e", ""},
+};
+
+std::string Utf32ToUtf8(char32_t cp) {
+  std::string out;
+
+  if (cp <= 0x7F) {
+    out.push_back(static_cast<char>(cp));
+  } else if (cp <= 0x7FF) {
+    out.push_back(static_cast<char>(0xC0 | (cp >> 6)));
+    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
+  } else if (cp <= 0xFFFF) {
+    out.push_back(static_cast<char>(0xE0 | (cp >> 12)));
+    out.push_back(static_cast<char>(0x80 | ((cp >> 6) & 0x3F)));
+    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
+  } else {
+    out.push_back(static_cast<char>(0xF0 | (cp >> 18)));
+    out.push_back(static_cast<char>(0x80 | ((cp >> 12) & 0x3F)));
+    out.push_back(static_cast<char>(0x80 | ((cp >> 6) & 0x3F)));
+    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
+  }
+
+  return out;
+}
+
+std::vector<std::string> ConvertPhonemesToUTF8(
+    const std::vector<std::vector<char32_t>> &phonemes) {
+  std::vector<std::string> out;
+
+  for (const auto &word : phonemes) {
+    for (char32_t cp : word) {
+      out.push_back(Utf32ToUtf8(cp));
+    }
+  }
+
+  return out;
+}
+
+std::string JoinTokensNoSpace(const std::vector<std::string> &tokens) {
+  std::string out;
+  for (const auto &t : tokens) {
+    out += t;
+  }
+  return out;
+}
+
+std::string ApplyReplacements(std::string s) {
+  for (const auto &p : kReplacements) {
+    const std::string &from = p.first;
+    const std::string &to = p.second;
+
+    size_t pos = 0;
+    while ((pos = s.find(from, pos)) != std::string::npos) {
+      s.replace(pos, from.size(), to);
+      pos += to.size();
+    }
+  }
+  return s;
+}
+
+std::vector<std::string> SplitTokensUTF8(const std::string &s) {
+  std::vector<std::string> out;
+
+  for (size_t i = 0; i < s.size();) {
+    unsigned char c = s[i];
+    size_t len = (c < 0x80) ? 1 : (c < 0xE0) ? 2 : (c < 0xF0) ? 3 : 4;
+
+    out.push_back(s.substr(i, len));
+    i += len;
+  }
+
+  return out;
+}
+
+std::vector<std::string> ProcessPhonemes(
+    const std::vector<std::vector<char32_t>> &phonemes) {
+  auto tokens = ConvertPhonemesToUTF8(phonemes);
+  std::string joined = JoinTokensNoSpace(tokens);
+  std::string replaced = ApplyReplacements(joined);
+  return SplitTokensUTF8(replaced);
+}
+
+}  // namespace
+
 void CallPhonemizeEspeak(const std::string &text,
                          piper::eSpeakPhonemeConfig &config,  // NOLINT
                          std::vector<std::vector<piper::Phoneme>> *phonemes);
@@ -246,21 +346,22 @@ class MatchaTtsLexicon::Impl {
           }
         }
       } else {
-        SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
+        if (debug_) {
+          SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
+        }
         // use espeak
         piper::eSpeakPhonemeConfig config;
         config.voice = "en-us";
         std::vector<std::vector<piper::Phoneme>> phonemes;
         CallPhonemizeEspeak(w, config, &phonemes);
-        for (const auto &ps : phonemes) {
-          for (const auto &p : ps) {
-            if (phoneme2id_.count(p)) {
-              ans.push_back(phoneme2id_.at(p));
-            } else {
-              SHERPA_ONNX_LOGE(
-                  "Skip unknown phonemes. Unicode codepoint: \\U+%04x. for %s",
-                  static_cast<uint32_t>(p), w.c_str());
-            }
+
+        auto pp = ProcessPhonemes(phonemes);
+
+        for (const auto &p : pp) {
+          if (token2id_.count(p)) {
+            ans.push_back(token2id_.at(p));
+          } else {
+            SHERPA_ONNX_LOGE("Skip token: %s", p.c_str());
           }
         }
       }
@@ -290,29 +391,6 @@ class MatchaTtsLexicon::Impl {
         id2token_[p.second] = p.first;
       }
     }
-
-    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
-    std::u32string s;
-    for (const auto &p : token2id_) {
-      if ((p.first.front() == '<' && p.first.back() == '>') ||
-          p.first.back() == '1' || p.first.back() == '2' ||
-          p.first.back() == '3' || p.first.back() == '4' ||
-          p.first.back() == '5') {
-        continue;
-      }
-      s = conv.from_bytes(p.first);
-
-      if (s.size() != 1) {
-        SHERPA_ONNX_LOGE("Error for token %s with id %d", p.first.c_str(),
-                         p.second);
-        SHERPA_ONNX_EXIT(-1);
-      }
-
-      char32_t c = s[0];
-      if (!phoneme2id_.count(c)) {
-        phoneme2id_.insert({c, p.second});
-      }
-    }
   }
 
   void InitLexicon(const std::string &lexicon) {
@@ -390,7 +468,6 @@ class MatchaTtsLexicon::Impl {
 
   // tokens.txt is saved in token2id_
   std::unordered_map<std::string, int32_t> token2id_;
-  std::unordered_map<char32_t, int32_t> phoneme2id_;
 
   std::unordered_map<int32_t, std::string> id2token_;
 

commit a60d6d0b31e2b17d6e2490ac1311e18a4cd3f3ae
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Dec 3 19:20:12 2025 +0800

    Fix matcha tts zh-en model (#2851)

diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
index 15bc5f44..4722dc58 100755
--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
+++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python3
 
-from pypinyin import Style, lazy_pinyin, load_phrases_dict, phrases_dict, pinyin_dict
+from pypinyin import Style, pinyin, load_phrases_dict, phrases_dict, pinyin_dict
 
 load_phrases_dict(
     {
@@ -28,7 +28,8 @@ def main():
                 continue
 
             w = chr(key)
-            tokens = lazy_pinyin(w, style=Style.TONE3, tone_sandhi=True)[0]
+            tokens = pinyin(w, style=Style.TONE3, neutral_tone_with_five=True)[0][0]
+
             if tokens == "shei2":
                 tokens = "shui2"
 
@@ -43,7 +44,7 @@ def main():
         for key in phrases:
             if key in user_defined:
                 continue
-            tokens = lazy_pinyin(key, style=Style.TONE3, tone_sandhi=True)
+            tokens = pinyin(key, style=Style.TONE3, neutral_tone_with_five=True)
             for i in range(len(tokens)):
                 if tokens[i] == "shei2":
                     tokens[i] = "shui2"
@@ -51,7 +52,9 @@ def main():
                 if tokens[i][-1] not in ("1", "2", "3", "4", "5"):
                     tokens[i] += "1"
 
-            tokens = " ".join(tokens)
+            flatten = [t[0] for t in tokens]
+
+            tokens = " ".join(flatten)
 
             f.write(f"{key} {tokens}\n")
 
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index b5247d6e..21726791 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -266,10 +266,6 @@ class MatchaTtsLexicon::Impl {
       }
     }
 
-    if (IsAlphaOrPunct(w.front())) {
-      ans.push_back(token2id_.at(" "));
-    }
-
     if (debug_) {
       std::ostringstream os;
       os << w << ": ";
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
index 3588b2d2..2d1255ce 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
@@ -431,8 +431,16 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
 
     std::vector<int64_t> x;
     x.reserve(num_tokens);
-    for (const auto &k : tokens) {
-      x.insert(x.end(), k.begin(), k.end());
+    if (config_.model.debug) {
+      for (const auto &k : tokens) {
+        x.insert(x.end(), k.begin(), k.end());
+      }
+      std::ostringstream oss;
+      for (int32_t i : x) {
+        oss << i << ", ";
+      }
+      oss << "\n";
+      SHERPA_ONNX_LOGE("%s\n", oss.str().c_str());
     }
 
     auto memory_info =

commit 172c906aee1086d4da1cd5cba08255119457406c
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Dec 3 17:06:55 2025 +0800

    Refactor axera npu examples (#2850)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index a9fd3c06..ceedb01c 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -198,10 +198,10 @@ endif()
 
 if(SHERPA_ONNX_ENABLE_AXERA)
   list(APPEND sources
+    ./axera/ax-engine-guard.cc
     ./axera/offline-sense-voice-model-axera.cc
     ./axera/utils.cc
   )
-
 endif()
 
 if(SHERPA_ONNX_ENABLE_AXCL)
@@ -209,7 +209,6 @@ if(SHERPA_ONNX_ENABLE_AXCL)
     ./axcl/offline-sense-voice-model-axcl.cc
     ./axcl/ax_model_runner_axcl.cc
   )
-
 endif()
 
 if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN OR SHERPA_ONNX_ENABLE_AXERA OR SHERPA_ONNX_ENABLE_AXCL)
diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
index 542d0e4f..7143d1ce 100644
--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
@@ -9,7 +9,7 @@
 #include <algorithm>
 #include <array>
 #include <memory>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
index 10391ee4..11a7503b 100644
--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
@@ -8,7 +8,7 @@
 #include <memory>
 #include <vector>
 
-#include "axcl.h"
+#include "axcl.h"  // NOLINT
 #include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
@@ -36,4 +36,4 @@ class OfflineSenseVoiceModelAxcl {
 
 }  // namespace sherpa_onnx
 
-#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
\ No newline at end of file
+#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
diff --git a/sherpa-onnx/csrc/axera/ax-engine-guard.cc b/sherpa-onnx/csrc/axera/ax-engine-guard.cc
new file mode 100644
index 00000000..12331bd6
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/ax-engine-guard.cc
@@ -0,0 +1,51 @@
+// sherpa-onnx/csrc/axera/ax-engine-guard.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/axera/ax-engine-guard.h"
+
+#include <cstring>
+
+#include "ax_engine_api.h"  // NOLINT
+#include "ax_sys_api.h"     // NOLINT
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+thread_local int32_t AxEngineGuard::count_ = 0;
+
+AxEngineGuard::AxEngineGuard() {
+  if (count_ == 0) {
+    auto ret = AX_SYS_Init();
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call AX_SYS_Init. ret code: %d",
+                       static_cast<int32_t>(ret));
+
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    AX_ENGINE_NPU_ATTR_T npu_attr;
+    memset(&npu_attr, 0, sizeof(npu_attr));
+    npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
+    ret = AX_ENGINE_Init(&npu_attr);
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to call AX_ENGINE_Init. ret code: %d",
+                       static_cast<int32_t>(ret));
+
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  ++count_;
+}
+
+AxEngineGuard::~AxEngineGuard() {
+  --count_;
+  if (count_ == 0) {
+    AX_ENGINE_Deinit();
+    AX_SYS_Deinit();
+  }
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axera/ax-engine-guard.h b/sherpa-onnx/csrc/axera/ax-engine-guard.h
new file mode 100644
index 00000000..907563fb
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/ax-engine-guard.h
@@ -0,0 +1,28 @@
+// sherpa-onnx/csrc/axera/ax-engine-guard.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
+#define SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
+#include <cstdint>
+
+namespace sherpa_onnx {
+
+class AxEngineGuard {
+ public:
+  AxEngineGuard();
+  ~AxEngineGuard();
+
+  AxEngineGuard(const AxEngineGuard &) = delete;
+  AxEngineGuard &operator=(const AxEngineGuard &) = delete;
+
+  AxEngineGuard(AxEngineGuard &&) = delete;
+  AxEngineGuard &operator=(AxEngineGuard &&) = delete;
+
+ private:
+  static thread_local int32_t count_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
diff --git a/sherpa-onnx/csrc/axera/io.hpp b/sherpa-onnx/csrc/axera/io.hpp
deleted file mode 100644
index 3d3f2535..00000000
--- a/sherpa-onnx/csrc/axera/io.hpp
+++ /dev/null
@@ -1,255 +0,0 @@
-// sherpa-onnx/csrc/axera/io.hpp
-//
-// This file is adapted from AXERA's ax-samples project.
-// See the original BSD 3-Clause license below.
-//
-// Copyright (c)  2025  M5Stack Technology CO LTD
-
-/*
- * AXERA is pleased to support the open source community by making ax-samples
- * available.
- *
- * Copyright (c) 2022, AXERA Semiconductor (Shanghai) Co., Ltd. All rights
- * reserved.
- *
- * Licensed under the BSD 3-Clause License (the "License"); you may not use this
- * file except in compliance with the License. You may obtain a copy of the
- * License at
- *
- * https://opensource.org/licenses/BSD-3-Clause
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-/*
- * Author: AXERA Corporation
- */
-
-#pragma once
-
-#include <ax_engine_api.h>
-#include <ax_sys_api.h>
-
-#include <cstdio>
-#include <cstring>
-#include <map>
-#include <utility>
-#include <vector>
-
-#define AX_CMM_ALIGN_SIZE 128
-
-inline const char *AX_CMM_SESSION_NAME = "ax-samples-cmm";
-
-typedef enum {
-  AX_ENGINE_ABST_DEFAULT = 0,
-  AX_ENGINE_ABST_CACHED = 1,
-} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
-
-typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
-                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
-    INPUT_OUTPUT_ALLOC_STRATEGY;
-
-#define SAMPLE_AX_ENGINE_DEAL_HANDLE        \
-  if (0 != ret) {                           \
-    return AX_ENGINE_DestroyHandle(handle); \
-  }
-
-#define SAMPLE_AX_ENGINE_DEAL_HANDLE_IO     \
-  if (0 != ret) {                           \
-    middleware::free_io(&io_data);          \
-    return AX_ENGINE_DestroyHandle(handle); \
-  }
-
-namespace middleware {
-
-inline void free_io_index(AX_ENGINE_IO_BUFFER_T *io_buf, size_t index) {
-  for (int i = 0; i < (int)index; ++i) {
-    AX_ENGINE_IO_BUFFER_T *pBuf = io_buf + i;
-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
-  }
-}
-
-inline void free_io(AX_ENGINE_IO_T *io) {
-  for (size_t j = 0; j < io->nInputSize; ++j) {
-    AX_ENGINE_IO_BUFFER_T *pBuf = io->pInputs + j;
-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
-  }
-  for (size_t j = 0; j < io->nOutputSize; ++j) {
-    AX_ENGINE_IO_BUFFER_T *pBuf = io->pOutputs + j;
-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
-  }
-  delete[] io->pInputs;
-  delete[] io->pOutputs;
-}
-
-static inline int prepare_io(AX_ENGINE_IO_INFO_T *info, AX_ENGINE_IO_T *io_data,
-                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
-  memset(io_data, 0, sizeof(*io_data));
-  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[info->nInputSize];
-  memset(io_data->pInputs, 0, sizeof(AX_ENGINE_IO_BUFFER_T) * info->nInputSize);
-  io_data->nInputSize = info->nInputSize;
-
-  auto ret = 0;
-  for (int i = 0; i < (int)info->nInputSize; ++i) {
-    auto meta = info->pInputs[i];
-    auto buffer = &io_data->pInputs[i];
-    if (strategy.first == AX_ENGINE_ABST_CACHED) {
-      ret = AX_SYS_MemAllocCached(
-          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
-          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
-    } else {
-      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
-                            meta.nSize, AX_CMM_ALIGN_SIZE,
-                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
-    }
-
-    if (ret != 0) {
-      free_io_index(io_data->pInputs, i);
-      fprintf(
-          stderr,
-          "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
-          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
-      return ret;
-    }
-    // fprintf(stderr, "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes
-    // }. \n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
-  }
-
-  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[info->nOutputSize];
-  memset(io_data->pOutputs, 0,
-         sizeof(AX_ENGINE_IO_BUFFER_T) * info->nOutputSize);
-  io_data->nOutputSize = info->nOutputSize;
-  for (int i = 0; i < (int)info->nOutputSize; ++i) {
-    auto meta = info->pOutputs[i];
-    auto buffer = &io_data->pOutputs[i];
-    buffer->nSize = meta.nSize;
-    if (strategy.second == AX_ENGINE_ABST_CACHED) {
-      ret = AX_SYS_MemAllocCached(
-          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
-          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
-    } else {
-      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
-                            meta.nSize, AX_CMM_ALIGN_SIZE,
-                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
-    }
-    if (ret != 0) {
-      fprintf(
-          stderr,
-          "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
-          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
-      free_io_index(io_data->pInputs, io_data->nInputSize);
-      free_io_index(io_data->pOutputs, i);
-      return ret;
-    }
-    // fprintf(stderr, "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes
-    // }.\n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
-  }
-
-  return 0;
-}
-
-static int push_input(const std::vector<uint8_t> &data, AX_ENGINE_IO_T *io_t,
-                      AX_ENGINE_IO_INFO_T *info_t) {
-  if (info_t->nInputSize != 1) {
-    fprintf(stderr, "Only support Input size == 1 current now");
-    return -1;
-  }
-
-  if (data.size() != info_t->pInputs[0].nSize) {
-    fprintf(stderr,
-            "The input data size is not matched with tensor {name: %s, size: "
-            "%d}.\n",
-            info_t->pInputs[0].pName, info_t->pInputs[0].nSize);
-    return -1;
-  }
-
-  memcpy(io_t->pInputs[0].pVirAddr, data.data(), data.size());
-
-  return 0;
-}
-
-static void print_io_info(AX_ENGINE_IO_INFO_T *io_info) {
-  static std::map<AX_ENGINE_DATA_TYPE_T, const char *> data_type = {
-      {AX_ENGINE_DT_UNKNOWN, "UNKNOWN"},
-      {AX_ENGINE_DT_UINT8, "UINT8"},
-      {AX_ENGINE_DT_UINT16, "UINT16"},
-      {AX_ENGINE_DT_FLOAT32, "FLOAT32"},
-      {AX_ENGINE_DT_SINT16, "SINT16"},
-      {AX_ENGINE_DT_SINT8, "SINT8"},
-      {AX_ENGINE_DT_SINT32, "SINT32"},
-      {AX_ENGINE_DT_UINT32, "UINT32"},
-      {AX_ENGINE_DT_FLOAT64, "FLOAT64"},
-      {AX_ENGINE_DT_UINT10_PACKED, "UINT10_PACKED"},
-      {AX_ENGINE_DT_UINT12_PACKED, "UINT12_PACKED"},
-      {AX_ENGINE_DT_UINT14_PACKED, "UINT14_PACKED"},
-      {AX_ENGINE_DT_UINT16_PACKED, "UINT16_PACKED"},
-  };
-
-  static std::map<AX_ENGINE_COLOR_SPACE_T, const char *> color_type = {
-      {AX_ENGINE_CS_FEATUREMAP, "FEATUREMAP"},
-      {AX_ENGINE_CS_RAW8, "RAW8"},
-      {AX_ENGINE_CS_RAW10, "RAW10"},
-      {AX_ENGINE_CS_RAW12, "RAW12"},
-      {AX_ENGINE_CS_RAW14, "RAW14"},
-      {AX_ENGINE_CS_RAW16, "RAW16"},
-      {AX_ENGINE_CS_NV12, "NV12"},
-      {AX_ENGINE_CS_NV21, "NV21"},
-      {AX_ENGINE_CS_RGB, "RGB"},
-      {AX_ENGINE_CS_BGR, "BGR"},
-      {AX_ENGINE_CS_RGBA, "RGBA"},
-      {AX_ENGINE_CS_GRAY, "GRAY"},
-      {AX_ENGINE_CS_YUV444, "YUV444"},
-  };
-  printf("\ninput size: %d\n", io_info->nInputSize);
-  for (uint32_t i = 0; i < io_info->nInputSize; ++i) {
-    // print shape info,like [batchsize x channel x height x width]
-    auto &info = io_info->pInputs[i];
-    printf("    name: \e[1;32m%8s", info.pName);
-
-    std::string dt = "unknown";
-    if (data_type.find(info.eDataType) != data_type.end()) {
-      dt = data_type[info.eDataType];
-      printf(" \e[1;34m[%s] ", dt.c_str());
-    } else {
-      printf(" \e[1;31m[%s] ", dt.c_str());
-    }
-
-    std::string ct = "unknown";
-    if (info.pExtraMeta &&
-        color_type.find(info.pExtraMeta->eColorSpace) != color_type.end()) {
-      ct = color_type[info.pExtraMeta->eColorSpace];
-      printf("\e[1;34m[%s]", ct.c_str());
-    } else {
-      printf("\e[1;31m[%s]", ct.c_str());
-    }
-    printf(" \n        \e[1;31m");
-
-    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
-      printf("%d", info.pShape[s]);
-      if (s != info.nShapeSize - 1) {
-        printf(" x ");
-      }
-    }
-    printf("\e[0m\n\n");
-  }
-
-  printf("\noutput size: %d\n", io_info->nOutputSize);
-  for (uint32_t i = 0; i < io_info->nOutputSize; ++i) {
-    // print shape info,like [batchsize x channel x height x width]
-    auto &info = io_info->pOutputs[i];
-    printf("    name: \e[1;32m%8s \e[1;34m[%s]\e[0m\n        \e[1;31m",
-           info.pName, data_type[info.eDataType]);
-    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
-      printf("%d", info.pShape[s]);
-      if (s != info.nShapeSize - 1) {
-        printf(" x ");
-      }
-    }
-    printf("\e[0m\n\n");
-  }
-}
-
-}  // namespace middleware
diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
index 039976e1..37d9ad2c 100644
--- a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+++ b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
@@ -19,7 +19,7 @@
 
 namespace sherpa_onnx {
 
-// defined in ../online-recognizer-sense-voice-impl.h
+// defined in ../offline-recognizer-sense-voice-impl.h
 OfflineRecognitionResult ConvertSenseVoiceResult(
     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
     int32_t frame_shift_ms, int32_t subsampling_factor);
diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
index 7e964791..904c0800 100644
--- a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <array>
 #include <cstring>
+#include <mutex>
 #include <utility>
 #include <vector>
 
@@ -19,7 +20,9 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
-#include "sherpa-onnx/csrc/axera/io.hpp"
+#include "ax_engine_api.h"  // NOLINT
+#include "ax_sys_api.h"     // NOLINT
+#include "sherpa-onnx/csrc/axera/ax-engine-guard.h"
 #include "sherpa-onnx/csrc/axera/utils.h"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
@@ -29,7 +32,7 @@ namespace sherpa_onnx {
 class OfflineSenseVoiceModelAxera::Impl {
  public:
   ~Impl() {
-    middleware::free_io(&io_data_);
+    FreeIO(&io_data_);
     if (handle_) {
       AX_ENGINE_DestroyHandle(handle_);
     }
@@ -52,15 +55,13 @@ class OfflineSenseVoiceModelAxera::Impl {
 
   std::vector<float> Run(std::vector<float> features, int32_t language,
                          int32_t text_norm) {
+    // TODO(fangjun): Support multi clients
+    std::lock_guard<std::mutex> lock(mutex_);
+
     features = ApplyLFR(std::move(features));
 
     std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
 
-    if (!io_info_ || io_info_->nInputSize < 1) {
-      SHERPA_ONNX_LOGE("Axera model expects at least 1 input tensor");
-      SHERPA_ONNX_EXIT(-1);
-    }
-
     const auto &in0_meta = io_info_->pInputs[0];
     size_t bytes0 = in0_meta.nSize;
 
@@ -73,19 +74,15 @@ class OfflineSenseVoiceModelAxera::Impl {
 
     std::memcpy(io_data_.pInputs[0].pVirAddr, features.data(), bytes0);
 
-    //   io_info_->nInputSize >= 2
-    //   io_info_->pInputs[1].nSize == prompt.size() * sizeof(int32_t)
-    if (io_info_->nInputSize >= 2) {
-      const auto &in1_meta = io_info_->pInputs[1];
-      size_t bytes1 = in1_meta.nSize;
-      if (bytes1 != prompt.size() * sizeof(int32_t)) {
-        SHERPA_ONNX_LOGE(
-            "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
-            in1_meta.nSize, prompt.size() * sizeof(int32_t));
-        SHERPA_ONNX_EXIT(-1);
-      }
-      std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
+    const auto &in1_meta = io_info_->pInputs[1];
+    size_t bytes1 = in1_meta.nSize;
+    if (bytes1 != prompt.size() * sizeof(int32_t)) {
+      SHERPA_ONNX_LOGE(
+          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
+          in1_meta.nSize, prompt.size() * sizeof(int32_t));
+      SHERPA_ONNX_EXIT(-1);
     }
+    std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
 
     auto ret = AX_ENGINE_RunSync(handle_, &io_data_);
     if (ret != 0) {
@@ -93,11 +90,6 @@ class OfflineSenseVoiceModelAxera::Impl {
       SHERPA_ONNX_EXIT(-1);
     }
 
-    if (io_info_->nOutputSize < 1) {
-      SHERPA_ONNX_LOGE("Axera model has no output tensor");
-      SHERPA_ONNX_EXIT(-1);
-    }
-
     const auto &out_meta = io_info_->pOutputs[0];
     auto &out_buf = io_data_.pOutputs[0];
 
@@ -111,17 +103,13 @@ class OfflineSenseVoiceModelAxera::Impl {
 
  private:
   void Init(void *model_data, size_t model_data_length) {
-    InitEngine(config_.debug);
-
     InitContext(model_data, model_data_length, config_.debug, &handle_);
 
     InitInputOutputAttrs(handle_, config_.debug, &io_info_);
 
-    std::memset(&io_data_, 0, sizeof(io_data_));
-
     PrepareIO(io_info_, &io_data_, config_.debug);
 
-    if (!io_info_ || io_info_->nInputSize == 0 || !io_info_->pInputs) {
+    if (!io_info_ || io_info_->nInputSize != 2 || !io_info_->pInputs) {
       SHERPA_ONNX_LOGE("No input tensor in Axera model");
       SHERPA_ONNX_EXIT(-1);
     }
@@ -134,6 +122,11 @@ class OfflineSenseVoiceModelAxera::Impl {
     }
     num_input_frames_ = in0.pShape[1];
 
+    if (io_info_->nOutputSize != 1) {
+      SHERPA_ONNX_LOGE("Axera sense voice model expected only 1 output tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
     if (config_.debug) {
       SHERPA_ONNX_LOGE("Axera SenseVoice model init done.");
       SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
@@ -173,6 +166,9 @@ class OfflineSenseVoiceModelAxera::Impl {
   }
 
  private:
+  std::mutex mutex_;
+  AxEngineGuard ax_engine_guard_;
+
   OfflineModelConfig config_;
   AX_ENGINE_HANDLE handle_ = nullptr;
   AX_ENGINE_IO_INFO_T *io_info_ = nullptr;
@@ -213,4 +209,4 @@ template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
     NativeResourceManager *mgr, const OfflineModelConfig &config);
 #endif
 
-}  // namespace sherpa_onnx
\ No newline at end of file
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
index dcbb2381..5ecdcd64 100644
--- a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
@@ -8,8 +8,6 @@
 #include <memory>
 #include <vector>
 
-#include "ax_engine_api.h"
-#include "ax_sys_api.h"
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
 
@@ -36,4 +34,4 @@ class OfflineSenseVoiceModelAxera {
 
 }  // namespace sherpa_onnx
 
-#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
\ No newline at end of file
+#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
diff --git a/sherpa-onnx/csrc/axera/utils.cc b/sherpa-onnx/csrc/axera/utils.cc
index c42f2a6a..13441cef 100644
--- a/sherpa-onnx/csrc/axera/utils.cc
+++ b/sherpa-onnx/csrc/axera/utils.cc
@@ -9,97 +9,142 @@
 #include <sstream>
 #include <string>
 #include <utility>
-#include <vector>
 
-#include "sherpa-onnx/csrc/axera/io.hpp"
+#include "ax_engine_api.h"   // NOLINT
+#include "ax_engine_type.h"  // NOLINT
+#include "ax_sys_api.h"      // NOLINT
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/text-utils.h"
 
+#define SHERPA_ONNX_TO_STRING(type) \
+  case type:                        \
+    return #type
+
 namespace sherpa_onnx {
 
-void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
-                       int32_t height, int32_t width, float *dst) {
-  for (int32_t i = 0; i < n; ++i) {
-    for (int32_t h = 0; h < height; ++h) {
-      for (int32_t w = 0; w < width; ++w) {
-        for (int32_t c = 0; c < channel; ++c) {
-          dst[i * height * width * channel + h * width * channel + w * channel +
-              c] = src[i * height * width * channel + c * height * width +
-                       h * width + w];
-        }
-      }
-    }
-  }
-}
+static constexpr int32_t kCmnAlignSize = 128;
+static const char *kSherpaOnnxAxeraSessionName = "sherpa-onnx-axera";
 
-std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
+static std::string VectorToString(AX_S32 *arr, AX_U8 n) {
   std::ostringstream os;
-  os << "{";
-  if (!io_info) {
-    os << "null AX_ENGINE_IO_INFO_T}";
-    return os.str();
+  std::string sep;
+  os << "[";
+  for (AX_U8 i = 0; i < n; ++i) {
+    os << sep << arr[i];
+    sep = ", ";
   }
+  os << "]";
 
-  os << "nInputSize: " << io_info->nInputSize;
-  os << ", nOutputSize: " << io_info->nOutputSize;
-  os << ", nMaxBatchSize: " << io_info->nMaxBatchSize;
-  os << ", bDynamicBatchSize: "
-     << (io_info->bDynamicBatchSize ? "true" : "false");
-  os << "}";
   return os.str();
 }
 
-std::unordered_map<std::string, std::string> Parse(const char *custom_string,
-                                                   bool debug /*= false*/) {
-  std::unordered_map<std::string, std::string> ans;
-  if (!custom_string) {
-    SHERPA_ONNX_LOGE("Parse: custom_string is null");
-    SHERPA_ONNX_EXIT(-1);
+static const char *AxEngineDataTypeToString(AX_ENGINE_DATA_TYPE_T type) {
+  switch (type) {
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UNKNOWN);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT8);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT16);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_FLOAT32);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT16);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT8);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT32);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT32);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_FLOAT64);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT10_PACKED);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT12_PACKED);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT14_PACKED);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT16_PACKED);
+    default:
+      return "Unknown data type";
   }
+}
 
-  std::vector<std::string> fields;
-  SplitStringToVector(custom_string, ";", false, &fields);
-  std::vector<std::string> tmp;
-
-  for (const auto &f : fields) {
-    tmp.clear();
-    SplitStringToVector(f, "=", false, &tmp);
-    if (tmp.size() != 2) {
-      SHERPA_ONNX_LOGE("Invalid custom string %s for %s", custom_string,
-                       f.c_str());
-      SHERPA_ONNX_EXIT(-1);
-    }
-    ans[std::move(tmp[0])] = std::move(tmp[1]);
+static const char *AxEngineTensorLayoutToString(
+    AX_ENGINE_TENSOR_LAYOUT_T layout) {
+  switch (layout) {
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_UNKNOWN);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_NHWC);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_NCHW);
+    default:
+      return "Unknown data layout";
   }
+}
 
-  if (debug) {
-    for (const auto &p : ans) {
-      SHERPA_ONNX_LOGE("%s: %s", p.first.c_str(), p.second.c_str());
-    }
+static const char *AxEngineMemoryTypeToString(AX_ENGINE_MEMORY_TYPE_T type) {
+  switch (type) {
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_PHYSICAL);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_VIRTUAL);
+    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_OCM);
+    default:
+      return "Unknown memory type";
   }
-  return ans;
 }
 
-void InitEngine(bool debug) {
-  AX_SYS_Init();
-#ifdef AXERA_TARGET_CHIP_AX620E
-  auto ret = AX_ENGINE_Init();
-#else
-  AX_ENGINE_NPU_ATTR_T npu_attr;
-  memset(&npu_attr, 0, sizeof(npu_attr));
-  npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
-  auto ret = AX_ENGINE_Init(&npu_attr);
-#endif
-  if (ret != 0) {
-    SHERPA_ONNX_LOGE("AX_ENGINE_Init failed, ret = %d", ret);
-    SHERPA_ONNX_EXIT(-1);
+/*
+num_inputs: 2
+num_outputs: 1
+max_bach_size: 1
+dynamic_bach_size: false
+---input 0---
+ name: x
+ shape: [1, 167, 560]
+ layout: AX_ENGINE_TENSOR_LAYOUT_NCHW
+ memory_type: AX_ENGINE_MT_PHYSICAL
+ data_type: AX_ENGINE_DT_FLOAT32
+ n_size (number of bytes): 374080
+---input 1---
+ name: prompt
+ shape: [4]
+ layout: AX_ENGINE_TENSOR_LAYOUT_NCHW
+ memory_type: AX_ENGINE_MT_PHYSICAL
+ data_type: AX_ENGINE_DT_SINT32
+ n_size (number of bytes): 16
+
+---output 0---
+ name: logits
+ shape: [1, 171, 25055]
+ layout: AX_ENGINE_TENSOR_LAYOUT_UNKNOWN
+ memory_type: AX_ENGINE_MT_PHYSICAL
+ data_type: AX_ENGINE_DT_FLOAT32
+ n_size: 17137620
+ */
+static std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
+  std::ostringstream os;
+  os << "num_inputs: " << io_info->nInputSize << "\n";
+  os << "num_outputs: " << io_info->nOutputSize << "\n";
+  os << "max_bach_size: " << io_info->nMaxBatchSize << "\n";
+  os << "dynamic_bach_size: " << (io_info->bDynamicBatchSize ? "true" : "false")
+     << "\n";
+
+  for (AX_U32 i = 0; i < io_info->nInputSize; ++i) {
+    const auto &input = io_info->pInputs[i];
+    os << "---input " << i << "---\n";
+    os << " name: " << input.pName << "\n";
+    os << " shape: " << VectorToString(input.pShape, input.nShapeSize) << "\n";
+    os << " layout: " << AxEngineTensorLayoutToString(input.eLayout) << "\n";
+    os << " memory_type: " << AxEngineMemoryTypeToString(input.eMemoryType)
+       << "\n";
+    os << " data_type: " << AxEngineDataTypeToString(input.eDataType) << "\n";
+    os << " n_size (number of bytes): " << input.nSize << "\n";
   }
-  if (debug) {
-    SHERPA_ONNX_LOGE("AX_ENGINE_Init done.");
+  os << "\n";
+
+  for (AX_U32 i = 0; i < io_info->nOutputSize; ++i) {
+    const auto &output = io_info->pOutputs[i];
+    os << "---output " << i << "---\n";
+    os << " name: " << output.pName << "\n";
+    os << " shape: " << VectorToString(output.pShape, output.nShapeSize)
+       << "\n";
+    os << " layout: " << AxEngineTensorLayoutToString(output.eLayout) << "\n";
+    os << " memory_type: " << AxEngineMemoryTypeToString(output.eMemoryType)
+       << "\n";
+    os << " data_type: " << AxEngineDataTypeToString(output.eDataType) << "\n";
+    os << " n_size: " << output.nSize << "\n";
   }
+
+  return os.str();
 }
 
-void InitContext(void *model_data, size_t model_data_length, bool debug,
+void InitContext(const void *model_data, size_t model_data_length, bool debug,
                  AX_ENGINE_HANDLE *handle) {
   if (!handle) {
     SHERPA_ONNX_LOGE("InitContext: handle is null");
@@ -111,9 +156,9 @@ void InitContext(void *model_data, size_t model_data_length, bool debug,
     SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle failed, ret = %d", ret);
     SHERPA_ONNX_EXIT(-1);
   }
+
   if (debug) {
-    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p",
-                     (void *)(*handle));
+    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p", *handle);
   }
 
   ret = AX_ENGINE_CreateContext(*handle);
@@ -121,6 +166,7 @@ void InitContext(void *model_data, size_t model_data_length, bool debug,
     SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext failed, ret = %d", ret);
     SHERPA_ONNX_EXIT(-1);
   }
+
   if (debug) {
     SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext done.");
   }
@@ -133,6 +179,7 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
     SHERPA_ONNX_EXIT(-1);
   }
 
+  // Note(fangjun): No need to free *io_info
   auto ret = AX_ENGINE_GetIOInfo(handle, io_info);
   if (ret != 0) {
     SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo failed, ret = %d", ret);
@@ -141,8 +188,7 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
 
   if (debug) {
     SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo done.");
-    SHERPA_ONNX_LOGE("IO_INFO: %s", ToString(*io_info).c_str());
-    middleware::print_io_info(*io_info);
+    SHERPA_ONNX_LOGE("IO_INFO:\n%s", ToString(*io_info).c_str());
   }
 }
 
@@ -153,17 +199,69 @@ void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
     SHERPA_ONNX_EXIT(-1);
   }
 
-  auto ret = middleware::prepare_io(
-      io_info, io_data,
-      std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_CACHED));
-  if (ret != 0) {
-    SHERPA_ONNX_LOGE("middleware::prepare_io failed, ret = %d", ret);
-    SHERPA_ONNX_EXIT(-1);
+  memset(io_data, 0, sizeof(AX_ENGINE_IO_T));
+
+  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[io_info->nInputSize];
+
+  memset(io_data->pInputs, 0,
+         sizeof(AX_ENGINE_IO_BUFFER_T) * io_info->nInputSize);
+
+  io_data->nInputSize = io_info->nInputSize;
+
+  for (AX_U32 i = 0; i < io_info->nInputSize; ++i) {
+    const auto &input = io_info->pInputs[i];
+    auto &buffer = io_data->pInputs[i];
+
+    buffer.nSize = input.nSize;
+
+    auto ret = AX_SYS_MemAlloc(
+        reinterpret_cast<AX_U64 *>(&buffer.phyAddr), &buffer.pVirAddr,
+        input.nSize, kCmnAlignSize,
+        reinterpret_cast<const AX_S8 *>(kSherpaOnnxAxeraSessionName));
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to allocate memory for Input %d",
+                       static_cast<int32_t>(i));
+      SHERPA_ONNX_EXIT(-1);
+    }
   }
 
-  if (debug) {
-    SHERPA_ONNX_LOGE("PrepareIO (middleware::prepare_io) done.");
+  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[io_info->nOutputSize];
+
+  memset(io_data->pOutputs, 0,
+         sizeof(AX_ENGINE_IO_BUFFER_T) * io_info->nOutputSize);
+
+  io_data->nOutputSize = io_info->nOutputSize;
+
+  for (AX_U32 i = 0; i < io_info->nOutputSize; ++i) {
+    const auto &output = io_info->pOutputs[i];
+    auto &buffer = io_data->pOutputs[i];
+    buffer.nSize = output.nSize;
+    auto ret = AX_SYS_MemAllocCached(
+        reinterpret_cast<AX_U64 *>(&buffer.phyAddr), &buffer.pVirAddr,
+        output.nSize, kCmnAlignSize,
+        reinterpret_cast<const AX_S8 *>(kSherpaOnnxAxeraSessionName));
+
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Failed to allocate memory for Output %d",
+                       static_cast<int32_t>(i));
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+}
+
+void FreeIO(AX_ENGINE_IO_T *io_data) {
+  for (AX_U32 i = 0; i < io_data->nInputSize; ++i) {
+    auto &buf = io_data->pInputs[i];
+    AX_SYS_MemFree(buf.phyAddr, buf.pVirAddr);
+  }
+
+  for (AX_U32 i = 0; i < io_data->nOutputSize; ++i) {
+    auto &buf = io_data->pOutputs[i];
+    AX_SYS_MemFree(buf.phyAddr, buf.pVirAddr);
   }
+  delete[] io_data->pInputs;
+  delete[] io_data->pOutputs;
 }
 
-}  // namespace sherpa_onnx
\ No newline at end of file
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/axera/utils.h b/sherpa-onnx/csrc/axera/utils.h
index 039eefc8..b1896fed 100644
--- a/sherpa-onnx/csrc/axera/utils.h
+++ b/sherpa-onnx/csrc/axera/utils.h
@@ -5,25 +5,13 @@
 #ifndef SHERPA_ONNX_CSRC_AXERA_UTILS_H_
 #define SHERPA_ONNX_CSRC_AXERA_UTILS_H_
 
-#include <string>
-#include <unordered_map>
-#include <vector>
+#include <cstddef>
 
-#include "ax_engine_api.h"
+#include "ax_engine_api.h"  // NOLINT
 
 namespace sherpa_onnx {
 
-void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
-                       int32_t height, int32_t width, float *dst);
-
-std::string ToString(const AX_ENGINE_IO_INFO_T *io_info);
-
-std::unordered_map<std::string, std::string> Parse(const char *custom_string,
-                                                   bool debug = false);
-
-void InitEngine(bool debug);
-
-void InitContext(void *model_data, size_t model_data_length, bool debug,
+void InitContext(const void *model_data, size_t model_data_length, bool debug,
                  AX_ENGINE_HANDLE *handle);
 
 void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
@@ -32,6 +20,8 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
 void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
                bool debug);
 
+void FreeIO(AX_ENGINE_IO_T *io_data);
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_AXERA_UTILS_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index ae45d304..683fbddc 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -89,14 +89,14 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 
   if (config.model_config.provider == "axera") {
 #if SHERPA_ONNX_ENABLE_AXERA
-    if (config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
+    } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
-          "by axera for non-streaming ASR.");
+          "Only SenseVoice models are currently supported by Axera NPU for "
+          "non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
-    } else if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -110,15 +110,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 
   if (config.model_config.provider == "axcl") {
 #if SHERPA_ONNX_ENABLE_AXCL
-    if (config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
+    } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
-          "by axcl for non-streaming ASR.");
+          "Only SenseVoice models are currently supported by axcl for "
+          "non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
-    } else if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
     }
+
 #else
     SHERPA_ONNX_LOGE(
         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
@@ -391,6 +392,50 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #endif
   }
 
+  if (config.model_config.provider == "axera") {
+#if SHERPA_ONNX_ENABLE_AXERA
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(mgr,
+                                                                    config);
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported by Axera NPU for "
+          "non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    }
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXERA=ON if you "
+        "want to use axera. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/axera/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
+  if (config.model_config.provider == "axcl") {
+#if SHERPA_ONNX_ENABLE_AXCL
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(mgr, config);
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported by axcl for "
+          "non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    }
+
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
+        "want to use axcl. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/axcl/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
   if (config.model_config.provider == "ascend") {
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
     if (!config.model_config.sense_voice.model.empty()) {
diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
index 2427d412..8cb39349 100644
--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
@@ -54,12 +54,11 @@ OfflineTransducerGreedySearchDecoder::Decode(Ort::Value encoder_out,
       if (blank_penalty_ > 0.0) {
         p_logit[0] -= blank_penalty_;  // assuming blank id is 0
       }
-      
+
       LogSoftmax(p_logit, vocab_size);
 
       auto y = static_cast<int32_t>(std::distance(
-          p_logit,
-          std::max_element(p_logit, p_logit + vocab_size)));
+          p_logit, std::max_element(p_logit, p_logit + vocab_size)));
 
       float log_prob = p_logit[y];
 
diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
index 335b0dec..60b3a99e 100644
--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
@@ -137,15 +137,14 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
           new_hyp.ys.push_back(new_token);
           new_hyp.timestamps.push_back(t);
 
-          // Store the token log probability (subtract prev log_prob to get original)
+          // Store the token log probability (subtract prev log_prob to get
+          // original)
           float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
           new_hyp.ys_probs.push_back(token_log_prob);
 
           if (context_graphs[i] != nullptr) {
-            auto context_res =
-                context_graphs[i]->ForwardOneStep(context_state,
-                  new_token,
-                  false /* non-strict mode */);
+            auto context_res = context_graphs[i]->ForwardOneStep(
+                context_state, new_token, false /* non-strict mode */);
             context_score = std::get<0>(context_res);
             new_hyp.context_state = std::get<1>(context_res);
           }
diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
index d3d223c3..cba83f1a 100755
--- a/sherpa-onnx/csrc/session.cc
+++ b/sherpa-onnx/csrc/session.cc
@@ -6,6 +6,7 @@
 
 #include <algorithm>
 #include <string>
+#include <unordered_map>
 #include <utility>
 #include <vector>
 
@@ -259,17 +260,22 @@ Ort::SessionOptions GetSessionOptionsImpl(
       SHERPA_ONNX_LOGE("Set InterOpNumThreads to 1");
       sess_opts.SetInterOpNumThreads(1);
       SHERPA_ONNX_LOGE("Set SPACEMIT_EP_INTRA_THREAD_NUM to %d", num_threads);
-      provider_options.insert(
-          std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM", std::to_string(num_threads)));
-      OrtStatus* sts = Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
+      provider_options.insert(std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM",
+                                             std::to_string(num_threads)));
+      OrtStatus *sts =
+          Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
       if (sts) {
         const auto &api = Ort::GetApi();
         const char *msg = api.GetErrorMessage(sts);
-        SHERPA_ONNX_LOGE("Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu", msg);
+        SHERPA_ONNX_LOGE(
+            "Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu",
+            msg);
         api.ReleaseStatus(sts);
       }
 #else
-      SHERPA_ONNX_LOGE("SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback to cpu!");
+      SHERPA_ONNX_LOGE(
+          "SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback "
+          "to cpu!");
 #endif
       break;
     }
diff --git a/sherpa-onnx/csrc/wave-reader-test.cc b/sherpa-onnx/csrc/wave-reader-test.cc
index 286aab06..d63cba2a 100644
--- a/sherpa-onnx/csrc/wave-reader-test.cc
+++ b/sherpa-onnx/csrc/wave-reader-test.cc
@@ -7,6 +7,7 @@
 #include <cstdio>
 #include <fstream>
 #include <string>
+#include <vector>
 
 #if defined(_WIN32)
 #include <windows.h>
@@ -23,7 +24,7 @@ class TempFile {
  public:
   TempFile() : TempFile("") {}
 
-  explicit TempFile(const std::string& suffix) {
+  explicit TempFile(const std::string &suffix) {
 #if defined(_WIN32)
     char temp_path[MAX_PATH];
     char temp_file[MAX_PATH];
@@ -54,7 +55,7 @@ class TempFile {
     }
   }
 
-  const char* path() const { return path_.c_str(); }
+  const char *path() const { return path_.c_str(); }
 
  private:
   std::string path_;
@@ -70,15 +71,10 @@ TEST(WaveReader, TestNonWavFile) {
     // (webm files typically start with EBML header: 0x1a45dfa3)
     const unsigned char webm_header[] = {
         0x1a, 0x45, 0xdf, 0xa3,  // EBML header signature (NOT RIFF)
-        0x01, 0x00, 0x00, 0x00,
-        0x00, 0x00, 0x00, 0x1f,
-        0x42, 0x86, 0x81, 0x01,
+        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x1f, 0x42, 0x86, 0x81, 0x01,
         // Add some more bytes to make it look like a real file
-        0x42, 0xf7, 0x81, 0x01,
-        0x42, 0xf2, 0x81, 0x04,
-        'w', 'e', 'b', 'm'
-    };
-    out.write(reinterpret_cast<const char*>(webm_header), sizeof(webm_header));
+        0x42, 0xf7, 0x81, 0x01, 0x42, 0xf2, 0x81, 0x04, 'w', 'e', 'b', 'm'};
+    out.write(reinterpret_cast<const char *>(webm_header), sizeof(webm_header));
   }
 
   // Test C++ API - should not segfault
@@ -113,12 +109,15 @@ TEST(WaveReader, TestTruncatedWaveFile) {
     std::ofstream out(temp_file.path(), std::ios::binary);
     // Write only partial WAV header (less than 44 bytes required)
     const unsigned char partial_wav[] = {
-        'R', 'I', 'F', 'F',  // chunk_id
-        0x00, 0x00, 0x00, 0x00,  // chunk_size
-        'W', 'A', 'V', 'E'  // format
-        // Missing the rest of the header
+        'R',  'I',  'F',
+        'F',  // chunk_id
+        0x00, 0x00, 0x00,
+        0x00,  // chunk_size
+        'W',  'A',  'V',
+        'E'  // format
+             // Missing the rest of the header
     };
-    out.write(reinterpret_cast<const char*>(partial_wav), sizeof(partial_wav));
+    out.write(reinterpret_cast<const char *>(partial_wav), sizeof(partial_wav));
   }
 
   // Test C++ API - should not segfault

commit 89b7e9f42712bbd1767acb2b83a3dc119f877b6a
Author: Abandon-ht <64671747+Abandon-ht@users.noreply.github.com>
Date:   Wed Dec 3 11:31:30 2025 +0800

    Support AXERA ax630, ax650, and axcl backends. (#2849)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 10467632..20f63f71 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -60,6 +60,8 @@ option(SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE "True to use pre-i
 option(SHERPA_ONNX_ENABLE_SANITIZER "Whether to enable ubsan and asan" OFF)
 option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUGGEST_BUILD_BINARIES})
 option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
+option(SHERPA_ONNX_ENABLE_AXERA "Whether to build for Axera NPU " OFF)
+option(SHERPA_ONNX_ENABLE_AXCL "Whether to build for Axcl NPU " OFF)
 option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
 option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
 option(SHERPA_ONNX_ENABLE_SPACEMIT "Whether to build for SpacemiT CPUs " OFF)
@@ -180,6 +182,8 @@ message(STATUS "SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE ${SHERPA_
 message(STATUS "SHERPA_ONNX_ENABLE_SANITIZER: ${SHERPA_ONNX_ENABLE_SANITIZER}")
 message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAMPLES}")
 message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
+message(STATUS "SHERPA_ONNX_ENABLE_AXERA: ${SHERPA_ONNX_ENABLE_AXERA}")
+message(STATUS "SHERPA_ONNX_ENABLE_AXCL: ${SHERPA_ONNX_ENABLE_AXCL}")
 message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
 message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
 message(STATUS "SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
@@ -306,6 +310,14 @@ if(SHERPA_ONNX_ENABLE_RKNN)
   add_definitions(-DSHERPA_ONNX_ENABLE_RKNN=1)
 endif()
 
+if(SHERPA_ONNX_ENABLE_AXERA)
+  add_definitions(-DSHERPA_ONNX_ENABLE_AXERA=1)
+endif()
+
+if(SHERPA_ONNX_ENABLE_AXCL)
+  add_definitions(-DSHERPA_ONNX_ENABLE_AXCL=1)
+endif()
+
 if(SHERPA_ONNX_ENABLE_QNN)
   add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
 endif()
diff --git a/build-axcl-linux-aarch64.sh b/build-axcl-linux-aarch64.sh
new file mode 100755
index 00000000..7b767cae
--- /dev/null
+++ b/build-axcl-linux-aarch64.sh
@@ -0,0 +1,124 @@
+#!/usr/bin/env bash
+#
+# Usage of this file
+#
+# Refer to the "How to build static libraries and statically linked binaries"
+# section at:
+#   https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html
+#
+# Use the following toolchain:
+#   aarch64-none-linux-gnu-gcc
+#   (GNU Toolchain for the A-profile Architecture 10.3-2021.07 (arm-10.29))
+#   version 10.3.1 20210621
+#
+# Note: Do NOT set:
+#   export BUILD_SHARED_LIBS=OFF
+#
+# Usage of this file
+#
+
+set -ex
+
+# Before you run this file, make sure you have first cloned
+# https://github.com/Abandon-ht/axcl_bsp_sdk
+# and set the environment variable SHERPA_ONNX_AXERA_PATH
+
+if [ -z "$AXCL_SDK_ROOT" ]; then
+  AXCL_SDK_ROOT=/home/m5stack/Workspace/kaldi/sherpa-onnx/axcl_bsp_sdk/out
+  echo "Please set AXCL_SDK_ROOT to your Axcl SDK path, e.g.:"
+  echo "  export AXCL_SDK_ROOT=$PWD/axcl_bsp_sdk/out"
+  exit 1
+fi
+
+if [ ! -d "$AXCL_SDK_ROOT" ]; then
+  echo "AXCL_SDK_ROOT ($AXCL_SDK_ROOT) does not exist"
+  exit 1
+fi
+
+if [ ! -f "$AXCL_SDK_ROOT/include/axcl.h" ]; then
+  echo "$AXCL_SDK_ROOT/include/axcl.h does not exist"
+  exit 1
+fi
+
+if [ ! -f "$AXCL_SDK_ROOT/lib/libaxcl_comm.so" ]; then
+  echo "$AXCL_SDK_ROOT/lib/libaxcl_comm.so does not exist"
+  exit 1
+fi
+
+export CPLUS_INCLUDE_PATH="$AXCL_SDK_ROOT/include:$AXCL_SDK_ROOT/bsp:$CPLUS_INCLUDE_PATH"
+export SHERPA_ONNX_AXCL_LIB_DIR="$AXCL_SDK_ROOT/lib"
+
+if command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
+  ln -svf $(which aarch64-none-linux-gnu-gcc) ./aarch64-linux-gnu-gcc
+  ln -svf $(which aarch64-none-linux-gnu-g++) ./aarch64-linux-gnu-g++
+  export PATH=$PWD:$PATH
+fi
+
+if ! command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
+  echo "Please install a toolchain for cross-compiling."
+  echo "You can refer to: "
+  echo "  https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html"
+  echo "for help."
+  exit 1
+fi
+
+
+dir=$PWD/build-axcl-linux-aarch64
+mkdir -p $dir
+
+cd $dir
+
+if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
+  echo "Start to cross-compile alsa-lib"
+  if [ ! -d alsa-lib ]; then
+    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+  fi
+  # If it shows:
+  #  ./gitcompile: line 79: libtoolize: command not found
+  # Please use:
+  #  sudo apt-get install libtool m4 automake
+  #
+  pushd alsa-lib
+  CC=aarch64-linux-gnu-gcc ./gitcompile --host=aarch64-linux-gnu
+  popd
+  echo "Finish cross-compiling alsa-lib"
+fi
+
+export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+
+if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
+  # By default, use shared link
+  BUILD_SHARED_LIBS=ON
+fi
+
+cmake \
+  -DALSA_INCLUDE_DIR=$PWD/alsa-lib/include \
+  -DALSA_LIBRARY=$PWD/alsa-lib/src/.libs/libasound.so \
+  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
+  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
+  -DBUILD_ESPEAK_NG_EXE=OFF \
+  -DBUILD_ESPEAK_NG_TESTS=OFF \
+  -DCMAKE_INSTALL_PREFIX=./install \
+  -DCMAKE_BUILD_TYPE=Release \
+  -DSHERPA_ONNX_ENABLE_GPU=OFF \
+  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
+  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+  -DSHERPA_ONNX_ENABLE_PORTAUDIO=ON \
+  -DSHERPA_ONNX_ENABLE_JNI=OFF \
+  -DSHERPA_ONNX_ENABLE_C_API=ON \
+  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
+  -DSHERPA_ONNX_ENABLE_AXCL=ON \
+  -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
+  ..
+
+make VERBOSE=1 -j22
+make install/strip
+
+# Enable it if only needed
+# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
+
+# See also
+# https://github.com/airockchip/rknn-toolkit2/blob/master/rknpu2/examples/rknn_api_demo/build-linux.sh
diff --git a/build-axera-linux-aarch64.sh b/build-axera-linux-aarch64.sh
new file mode 100755
index 00000000..a6716712
--- /dev/null
+++ b/build-axera-linux-aarch64.sh
@@ -0,0 +1,210 @@
+#!/usr/bin/env bash
+#
+# Usage of this file
+#
+# Refer to the "How to build static libraries and statically linked binaries"
+# section at:
+#   https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html
+#
+# Use the following toolchain:
+#   aarch64-none-linux-gnu-gcc
+#   (GNU Toolchain for the A-profile Architecture 10.3-2021.07 (arm-10.29))
+#   version 10.3.1 20210621
+#
+# Note: Do NOT set:
+#   export BUILD_SHARED_LIBS=OFF
+#
+# Usage of this file
+#
+# ./build-axera-linux-aarch64.sh ax650
+# ./build-axera-linux-aarch64.sh ax630c
+# ./build-axera-linux-aarch64.sh ax620q
+
+set -ex
+
+SUPPORTED_TARGETS=("ax650" "ax630c" "ax620q")
+
+function print_info() {
+    echo -e "\033[32m[INFO]\033[0m $1"
+}
+
+function print_error() {
+    echo -e "\033[31m[ERROR]\033[0m $1"
+}
+
+function print_warn() {
+    echo -e "\033[33m[WARN]\033[0m $1"
+}
+
+function usage() {
+    print_info "Usage: $0 <axera_target_chip>"
+    print_info "Supported chips: ${SUPPORTED_TARGETS[*]}"
+    print_info "Example: $0 ax650"
+    print_info "Example: $0 ax630c"
+    print_info "Example: $0 ax620q"
+}
+
+function download_650_bsp_sdk() {
+  local version=1.45.0_p39
+  if [ -d ax650n_bsp_sdk-$version ]; then
+    echo $PWD/ax650n_bsp_sdk-$version/msp/out
+    return 0
+  fi
+
+  # 166 MB
+  if [ ! -f v$version.zip ]; then
+    wget https://github.com/AXERA-TECH/ax650n_bsp_sdk/archive/refs/tags/v$version.zip
+  fi
+
+  unzip -qq v$version.zip
+
+  echo $PWD/ax650n_bsp_sdk-$version/msp/out
+
+  return 0
+}
+
+function download_620e_bsp_sdk() {
+  local version=2.0.0_P7
+  if [ -d ax620e_bsp_sdk-$version ]; then
+    echo $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc
+    return 0
+  fi
+
+  # 166 MB
+  if [ ! -f v$version.zip ]; then
+    wget https://github.com/AXERA-TECH/ax620e_bsp_sdk/archive/refs/tags/v2.0.0_P7.zip
+  fi
+
+  unzip -qq v$version.zip
+
+  echo $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc
+
+  return 0
+}
+
+if [ $# -ne 1 ]; then
+    print_error "Error: You need to provide the axera target chip"
+    usage
+    exit 1
+fi
+
+target_chip=$(echo "$1" | tr '[:upper:]' '[:lower:]')
+
+if ! [[ " ${SUPPORTED_TARGETS[*]} " =~ " ${target_chip} " ]]; then
+    print_error "Unsupported target chip '$target_chip'!"
+    print_info "Supported target chips are ${SUPPORTED_TARGETS[*]}"
+    exit 1
+fi
+
+
+if [ -z "$AXERA_SDK_ROOT" ]; then
+  case "$target_chip" in
+    ax650)
+      AXERA_SDK_ROOT=$(download_650_bsp_sdk)
+      ;;
+    ax630c|ax620q)
+      AXERA_SDK_ROOT=$(download_620e_bsp_sdk)
+      ;;
+    *)
+      print_error "Unsupported target chip $target_chip"
+      exit 1
+      ;;
+  esac
+fi
+
+echo "AXERA_SDK_ROOT: $AXERA_SDK_ROOT"
+
+if [ ! -d "$AXERA_SDK_ROOT" ]; then
+  echo "AXERA_SDK_ROOT ($AXERA_SDK_ROOT) does not exist"
+  exit 1
+fi
+
+if [ ! -f "$AXERA_SDK_ROOT/include/ax_engine_api.h" ]; then
+  echo "$AXERA_SDK_ROOT/include/ax_engine_api.h does not exist"
+  exit 1
+fi
+
+if [ ! -f "$AXERA_SDK_ROOT/lib/libax_engine.so" ]; then
+  echo "$AXERA_SDK_ROOT/lib/libax_engine.so does not exist"
+  exit 1
+fi
+
+export CPLUS_INCLUDE_PATH="$AXERA_SDK_ROOT/include:$CPLUS_INCLUDE_PATH"
+
+export SHERPA_ONNX_AXERA_LIB_DIR="$AXERA_SDK_ROOT/lib"
+
+if command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
+  ln -svf $(which aarch64-none-linux-gnu-gcc) ./aarch64-linux-gnu-gcc
+  ln -svf $(which aarch64-none-linux-gnu-g++) ./aarch64-linux-gnu-g++
+  export PATH=$PWD:$PATH
+fi
+
+if ! command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
+  echo "Please install a toolchain for cross-compiling."
+  echo "You can refer to: "
+  echo "  https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html"
+  echo "for help."
+  exit 1
+fi
+
+
+dir=$PWD/build-axera-linux-aarch64-$target_chip
+mkdir -p $dir
+
+cd $dir
+
+if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
+  echo "Start to cross-compile alsa-lib"
+  if [ ! -d alsa-lib ]; then
+    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+  fi
+  # If it shows:
+  #  ./gitcompile: line 79: libtoolize: command not found
+  # Please use:
+  #  sudo apt-get install libtool m4 automake
+  #
+  # If it shows plantuml: command not found
+  # Please use
+  #   sudo apt-get install plantuml
+  pushd alsa-lib
+  CC=aarch64-linux-gnu-gcc ./gitcompile --host=aarch64-linux-gnu
+  popd
+  echo "Finish cross-compiling alsa-lib"
+fi
+
+export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+
+if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
+  # By default, use shared link
+  BUILD_SHARED_LIBS=ON
+fi
+
+cmake \
+  -DALSA_INCLUDE_DIR=$PWD/alsa-lib/include \
+  -DALSA_LIBRARY=$PWD/alsa-lib/src/.libs/libasound.so \
+  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
+  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
+  -DBUILD_ESPEAK_NG_EXE=OFF \
+  -DBUILD_ESPEAK_NG_TESTS=OFF \
+  -DCMAKE_INSTALL_PREFIX=./install \
+  -DCMAKE_BUILD_TYPE=Release \
+  -DSHERPA_ONNX_ENABLE_GPU=OFF \
+  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
+  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+  -DSHERPA_ONNX_ENABLE_PORTAUDIO=ON \
+  -DSHERPA_ONNX_ENABLE_JNI=OFF \
+  -DSHERPA_ONNX_ENABLE_C_API=ON \
+  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
+  -DSHERPA_ONNX_ENABLE_AXERA=ON \
+  -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
+  ..
+
+make VERBOSE=1 -j2
+make install/strip
+
+
+# Enable it if only needed
+# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 9e6e5647..a9fd3c06 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -196,7 +196,23 @@ if(SHERPA_ONNX_ENABLE_RKNN)
 
 endif()
 
-if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN)
+if(SHERPA_ONNX_ENABLE_AXERA)
+  list(APPEND sources
+    ./axera/offline-sense-voice-model-axera.cc
+    ./axera/utils.cc
+  )
+
+endif()
+
+if(SHERPA_ONNX_ENABLE_AXCL)
+  list(APPEND sources
+    ./axcl/offline-sense-voice-model-axcl.cc
+    ./axcl/ax_model_runner_axcl.cc
+  )
+
+endif()
+
+if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN OR SHERPA_ONNX_ENABLE_AXERA OR SHERPA_ONNX_ENABLE_AXCL)
   list(APPEND sources
     ./rknn/offline-ctc-greedy-search-decoder-rknn.cc
   )
@@ -327,6 +343,38 @@ if(SHERPA_ONNX_ENABLE_RKNN)
   endif()
 endif()
 
+if(SHERPA_ONNX_ENABLE_AXERA)
+  if(DEFINED ENV{SHERPA_ONNX_AXERA_LIB_DIR})
+    target_link_libraries(sherpa-onnx-core
+      -L$ENV{SHERPA_ONNX_AXERA_LIB_DIR}
+      -lax_engine
+      -lax_interpreter
+      -lax_sys
+      -lpthread
+    )
+  else()
+    target_link_libraries(sherpa-onnx-core
+      ax_engine
+      ax_interpreter
+      ax_sys
+      pthread
+    )
+  endif()
+endif()
+
+if(SHERPA_ONNX_ENABLE_AXCL)
+  if(DEFINED ENV{SHERPA_ONNX_AXCL_LIB_DIR})
+    target_link_libraries(sherpa-onnx-core
+      -L$ENV{SHERPA_ONNX_AXCL_LIB_DIR}
+      -laxcl_rt
+      )
+  else()
+    target_link_libraries(sherpa-onnx-core
+      axcl_rt
+    )
+  endif()
+endif()
+
 if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
     target_include_directories(sherpa-onnx-core PRIVATE ${ASCEND_TOOLKIT_HOME}/include)
     target_link_libraries(sherpa-onnx-core
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
new file mode 100644
index 00000000..42baad0d
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
@@ -0,0 +1,148 @@
+// sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#pragma once
+#include <map>
+#include <stdexcept>
+#include <string>
+#include <vector>
+
+typedef enum _color_space_e {
+  ax_color_space_unknown,
+  ax_color_space_nv12,
+  ax_color_space_nv21,
+  ax_color_space_bgr,
+  ax_color_space_rgb,
+} ax_color_space_e;
+
+typedef struct {
+  std::string sName;
+  unsigned int nIdx;
+  std::vector<unsigned int> vShape;
+  int nSize;
+  unsigned long long phyAddr;
+  void *pVirAddr;
+} ax_runner_tensor_t;
+
+class ax_runner_base {
+ public:
+  std::vector<ax_runner_tensor_t> moutput_tensors;
+  std::vector<ax_runner_tensor_t> minput_tensors;
+
+  std::vector<std::vector<ax_runner_tensor_t>> mgroup_output_tensors;
+  std::vector<std::vector<ax_runner_tensor_t>> mgroup_input_tensors;
+
+  std::map<std::string, ax_runner_tensor_t> map_output_tensors;
+  std::map<std::string, ax_runner_tensor_t> map_input_tensors;
+
+  std::map<std::string, std::vector<ax_runner_tensor_t>>
+      map_group_output_tensors;
+  std::map<std::string, std::vector<ax_runner_tensor_t>>
+      map_group_input_tensors;
+
+  bool _auto_sync_before_inference = true;
+  bool _auto_sync_after_inference = true;
+
+  float cost_host_to_device = 0;
+  float cost_inference = 0;
+  float cost_device_to_host = 0;
+
+ public:
+  virtual int init(const char *model_file) = 0;
+  virtual int init(char *model_buffer, size_t model_size) = 0;
+
+  virtual void deinit() = 0;
+
+  float get_inference_time() { return cost_inference; }
+
+  int get_num_inputs() { return minput_tensors.size(); };
+  int get_num_outputs() { return moutput_tensors.size(); };
+
+  const ax_runner_tensor_t &get_input(int idx) { return minput_tensors[idx]; }
+  const ax_runner_tensor_t *get_inputs_ptr() { return minput_tensors.data(); }
+  const ax_runner_tensor_t &get_input(std::string name) {
+    if (map_input_tensors.size() == 0) {
+      for (size_t i = 0; i < minput_tensors.size(); i++) {
+        map_input_tensors[minput_tensors[i].sName] = minput_tensors[i];
+      }
+    }
+    if (map_input_tensors.find(name) == map_input_tensors.end()) {
+      throw std::runtime_error("input tensor not found: " + name);
+    }
+
+    return map_input_tensors[name];
+  }
+
+  const ax_runner_tensor_t &get_input(int grpid, int idx) {
+    return mgroup_input_tensors[grpid][idx];
+  }
+  const ax_runner_tensor_t *get_inputs_ptr(int grpid) {
+    return mgroup_input_tensors[grpid].data();
+  }
+  const ax_runner_tensor_t &get_input(int grpid, std::string name) {
+    if (map_group_input_tensors.size() == 0) {
+      for (size_t i = 0; i < mgroup_input_tensors.size(); i++) {
+        for (size_t j = 0; j < mgroup_input_tensors[i].size(); j++) {
+          map_group_input_tensors[mgroup_input_tensors[i][j].sName].push_back(
+              mgroup_input_tensors[i][j]);
+        }
+      }
+    }
+    if (map_group_input_tensors.find(name) == map_group_input_tensors.end()) {
+      throw std::runtime_error("input tensor not found: " + name);
+    }
+    return map_group_input_tensors[name][grpid];
+    // return map_input_tensors[name];
+  }
+
+  const ax_runner_tensor_t &get_output(int idx) { return moutput_tensors[idx]; }
+  const ax_runner_tensor_t *get_outputs_ptr() { return moutput_tensors.data(); }
+  const ax_runner_tensor_t &get_output(std::string name) {
+    if (map_output_tensors.size() == 0) {
+      for (size_t i = 0; i < moutput_tensors.size(); i++) {
+        map_output_tensors[moutput_tensors[i].sName] = moutput_tensors[i];
+      }
+    }
+    if (map_output_tensors.find(name) == map_output_tensors.end()) {
+      throw std::runtime_error("output tensor not found: " + name);
+    }
+
+    return map_output_tensors[name];
+  }
+
+  const ax_runner_tensor_t &get_output(int grpid, int idx) {
+    return mgroup_output_tensors[grpid][idx];
+  }
+  const ax_runner_tensor_t *get_outputs_ptr(int grpid) {
+    return mgroup_output_tensors[grpid].data();
+  }
+  const ax_runner_tensor_t &get_output(int grpid, std::string name) {
+    if (map_group_output_tensors.size() == 0) {
+      for (size_t i = 0; i < mgroup_output_tensors.size(); i++) {
+        for (size_t j = 0; j < mgroup_output_tensors[i].size(); j++) {
+          map_group_output_tensors[mgroup_output_tensors[i][j].sName].push_back(
+              mgroup_output_tensors[i][j]);
+        }
+      }
+    }
+    if (map_group_output_tensors.find(name) == map_group_output_tensors.end()) {
+      throw std::runtime_error("input tensor not found: " + name);
+    }
+    return map_group_output_tensors[name][grpid];
+  }
+
+  virtual int get_algo_width() = 0;
+  virtual int get_algo_height() = 0;
+  virtual ax_color_space_e get_color_space() = 0;
+
+  void set_auto_sync_before_inference(bool sync) {
+    _auto_sync_before_inference = sync;
+  }
+  void set_auto_sync_after_inference(bool sync) {
+    _auto_sync_after_inference = sync;
+  }
+
+  virtual int inference() = 0;
+  virtual int inference(int grpid) = 0;
+};
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
new file mode 100644
index 00000000..7f3733a9
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
@@ -0,0 +1,470 @@
+// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#include "ax_model_runner_axcl.hpp"
+
+#include <axcl.h>
+#include <fcntl.h>
+#include <string.h>
+
+#include <fstream>
+#include <memory>
+
+typedef enum {
+  AX_ENGINE_ABST_DEFAULT = 0,
+  AX_ENGINE_ABST_CACHED = 1,
+} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
+
+typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
+                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
+    INPUT_OUTPUT_ALLOC_STRATEGY;
+
+static void print_io_info(std::vector<ax_runner_tensor_t> &input,
+                          std::vector<ax_runner_tensor_t> &output) {
+  printf("\ninput size: %ld\n", input.size());
+  for (size_t i = 0; i < input.size(); ++i) {
+    // print shape info,like [batchsize x channel x height x width]
+    auto &info = input[i];
+    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
+    for (size_t s = 0; s < info.vShape.size(); s++) {
+      printf("%d", info.vShape[s]);
+      if (s != info.vShape.size() - 1) {
+        printf(" x ");
+      }
+    }
+    printf("\e[0m\n\n");
+  }
+
+  printf("\noutput size: %ld\n", output.size());
+  for (size_t i = 0; i < output.size(); ++i) {
+    // print shape info,like [batchsize x channel x height x width]
+    auto &info = output[i];
+    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
+    for (size_t s = 0; s < info.vShape.size(); s++) {
+      printf("%d", info.vShape[s]);
+      if (s != info.vShape.size() - 1) {
+        printf(" x ");
+      }
+    }
+    printf("\e[0m\n\n");
+  }
+}
+
+static bool read_file(const char *fn, std::vector<unsigned char> &data) {
+  FILE *fp = fopen(fn, "r");
+  if (fp != nullptr) {
+    fseek(fp, 0L, SEEK_END);
+    auto len = ftell(fp);
+    fseek(fp, 0, SEEK_SET);
+    data.clear();
+    size_t read_size = 0;
+    if (len > 0) {
+      data.resize(len);
+      read_size = fread(data.data(), 1, len, fp);
+    }
+    fclose(fp);
+    return read_size == (size_t)len;
+  }
+  return false;
+}
+
+typedef struct {
+  int nIndex;
+  int nSize;
+  void *pBuf;
+  void *pVirAddr;
+
+  std::string Name;
+
+  axclrtEngineIODims dims;
+} AXCL_IO_BUF_T;
+
+typedef struct {
+  uint32_t nInputSize;
+  uint32_t nOutputSize;
+  AXCL_IO_BUF_T *pInputs;
+  AXCL_IO_BUF_T *pOutputs;
+} AXCL_IO_DATA_T;
+
+static void free_io_index(AXCL_IO_BUF_T *pBuf, size_t index) {
+  for (size_t i = 0; i < index; ++i) {
+    axclrtFree(pBuf[i].pBuf);
+  }
+}
+
+static void free_io(AXCL_IO_DATA_T *io_data) {
+  for (size_t j = 0; j < io_data->nInputSize; ++j) {
+    axclrtFree(io_data->pInputs[j].pBuf);
+    free(io_data->pInputs[j].pVirAddr);
+  }
+  for (size_t j = 0; j < io_data->nOutputSize; ++j) {
+    axclrtFree(io_data->pOutputs[j].pBuf);
+    free(io_data->pOutputs[j].pVirAddr);
+  }
+  delete[] io_data->pInputs;
+  delete[] io_data->pOutputs;
+}
+
+static inline int prepare_io(int grpid, axclrtEngineIOInfo io_info,
+                             axclrtEngineIO io, AXCL_IO_DATA_T *io_data,
+                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
+  memset(io_data, 0, sizeof(AXCL_IO_DATA_T));
+
+  auto inputNum = axclrtEngineGetNumInputs(io_info);
+  auto outputNum = axclrtEngineGetNumOutputs(io_info);
+  io_data->nInputSize = inputNum;
+  io_data->nOutputSize = outputNum;
+  io_data->pInputs = new AXCL_IO_BUF_T[inputNum];
+  io_data->pOutputs = new AXCL_IO_BUF_T[outputNum];
+
+  // 1. alloc inputs
+  for (uint32_t i = 0; i < inputNum; i++) {
+    auto bufSize = axclrtEngineGetInputSizeByIndex(io_info, grpid, i);
+    void *devPtr = nullptr;
+    axclError ret = 0;
+    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
+      ret = axclrtMalloc(&devPtr, bufSize,
+                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+    } else {
+      ret = axclrtMallocCached(
+          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+    }
+
+    if (ret != 0) {
+      free_io_index(io_data->pInputs, i);
+      fprintf(stderr, "Malloc input(index: %d, size: %ld) failed! ret=0x%x\n",
+              i, bufSize, ret);
+      return -1;
+    }
+    std::vector<char> tmp(bufSize, 0);
+    axclrtMemcpy(devPtr, tmp.data(), bufSize,
+                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
+    // axclrtMemset(devPtr, 0, bufSize);
+
+    axclrtEngineIODims dims;
+    ret = axclrtEngineGetInputDims(io_info, grpid, i, &dims);
+    if (ret != 0) {
+      free_io_index(io_data->pInputs, i);
+      fprintf(stderr, "Get input dims(index: %d) failed! ret=0x%x\n", i, ret);
+      return -1;
+    }
+
+    io_data->pInputs[i].nIndex = i;
+    io_data->pInputs[i].nSize = bufSize;
+    io_data->pInputs[i].pBuf = devPtr;
+    io_data->pInputs[i].dims = dims;
+    io_data->pInputs[i].Name = axclrtEngineGetInputNameByIndex(io_info, i);
+    io_data->pInputs[i].pVirAddr = malloc(bufSize);
+    memset(io_data->pInputs[i].pVirAddr, 0, bufSize);
+    ret = axclrtEngineSetInputBufferByIndex(io, i, devPtr, bufSize);
+    if (ret != 0) {
+      free_io_index(io_data->pInputs, i);
+      fprintf(stderr,
+              "Set input buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
+              bufSize, ret);
+      return -1;
+    }
+  }
+
+  // 2. alloc outputs
+  for (uint32_t i = 0; i < outputNum; i++) {
+    auto bufSize = axclrtEngineGetOutputSizeByIndex(io_info, grpid, i);
+    void *devPtr = NULL;
+    axclError ret = 0;
+    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
+      ret = axclrtMalloc(&devPtr, bufSize,
+                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+    } else {
+      ret = axclrtMallocCached(
+          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+    }
+
+    if (ret != 0) {
+      free_io_index(io_data->pOutputs, i);
+      fprintf(stderr, "Malloc output(index: %d, size: %ld) failed! ret=0x%x\n",
+              i, bufSize, ret);
+      return -1;
+    }
+    std::vector<char> tmp(bufSize, 0);
+    axclrtMemcpy(devPtr, tmp.data(), bufSize,
+                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
+    axclrtEngineIODims dims;
+    ret = axclrtEngineGetOutputDims(io_info, grpid, i, &dims);
+    if (ret != 0) {
+      free_io_index(io_data->pOutputs, i);
+      fprintf(stderr, "Get output dims(index: %d) failed! ret=0x%x\n", i, ret);
+      return -1;
+    }
+
+    io_data->pOutputs[i].nIndex = i;
+    io_data->pOutputs[i].nSize = bufSize;
+    io_data->pOutputs[i].pBuf = devPtr;
+    io_data->pOutputs[i].dims = dims;
+    io_data->pOutputs[i].Name = axclrtEngineGetOutputNameByIndex(io_info, i);
+    io_data->pOutputs[i].pVirAddr = malloc(bufSize);
+    memset(io_data->pOutputs[i].pVirAddr, 0, bufSize);
+    ret = axclrtEngineSetOutputBufferByIndex(io, i, devPtr, bufSize);
+    if (ret != 0) {
+      free_io_index(io_data->pOutputs, i);
+      fprintf(stderr,
+              "Set output buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
+              bufSize, ret);
+      return -1;
+    }
+  }
+
+  return 0;
+}
+
+struct ax_joint_runner_axcl_handle_t {
+  uint64_t handle = 0;
+  uint64_t context = 0;
+  axclrtEngineIOInfo io_info = 0;
+  std::vector<axclrtEngineIO> ios;
+  std::vector<AXCL_IO_DATA_T> io_datas;
+
+  // int algo_width, algo_height;
+  // int algo_colorformat;
+};
+
+int ax_runner_axcl::sub_init() {
+  // 4. create context
+  int ret = axclrtEngineCreateContext(m_handle->handle, &m_handle->context);
+  if (0 != ret) {
+    fprintf(stderr, "axclrtEngineCreateContext failed.\n");
+    return ret;
+  }
+  fprintf(stdout, "axclrtEngineCreateContextt is done. \n");
+
+  // 5. set io
+
+  ret = axclrtEngineGetIOInfo(m_handle->handle, &m_handle->io_info);
+  if (0 != ret) {
+    fprintf(stderr, "axclrtEngineGetIOInfo failed.\n");
+    return ret;
+  }
+  fprintf(stdout, "axclrtEngineGetIOInfo is done. \n");
+
+  ret = axclrtEngineGetShapeGroupsCount(m_handle->io_info, &group_count);
+  if (ret != 0) {
+    axclrtEngineUnload(m_handle->handle);
+    return ret;
+  }
+
+  // 6. alloc io
+  if (!_parepare_io) {
+    m_handle->ios.resize(group_count);
+    m_handle->io_datas.resize(group_count);
+    mgroup_input_tensors.resize(group_count);
+    mgroup_output_tensors.resize(group_count);
+
+    memset(&m_handle->io_datas[0], 0, sizeof(AXCL_IO_DATA_T) * group_count);
+
+    auto malloc_strategy =
+        std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_DEFAULT);
+
+    for (int grpid = 0; grpid < group_count; grpid++) {
+      ret = axclrtEngineCreateIO(m_handle->io_info, &m_handle->ios[grpid]);
+      if (ret != 0) {
+        axclrtEngineUnload(m_handle->handle);
+        fprintf(stderr, "Create io failed. ret=0x%x\n", ret);
+        return -1;
+      }
+
+      ret = prepare_io(grpid, m_handle->io_info, m_handle->ios[grpid],
+                       &m_handle->io_datas[grpid], malloc_strategy);
+      if (ret != 0) {
+        free_io(&m_handle->io_datas[grpid]);
+        axclrtEngineDestroyIO(m_handle->ios[grpid]);
+        axclrtEngineUnload(m_handle->handle);
+
+        fprintf(stderr, "prepare_io failed.\n");
+        return ret;
+      }
+    }
+
+    for (int grpid = 0; grpid < group_count; grpid++) {
+      // auto &io_info = m_handle->io_info[grpid];
+      auto &io_data = m_handle->io_datas[grpid];
+      for (uint32_t i = 0; i < io_data.nOutputSize; i++) {
+        ax_runner_tensor_t tensor;
+        tensor.nIdx = i;
+        tensor.sName = std::string(io_data.pOutputs[i].Name);
+        tensor.nSize = io_data.pOutputs[i].nSize;
+        for (int32_t j = 0; j < io_data.pOutputs[i].dims.dimCount; j++) {
+          tensor.vShape.push_back(io_data.pOutputs[i].dims.dims[j]);
+        }
+        tensor.phyAddr = (unsigned long long)io_data.pOutputs[i].pBuf;
+        tensor.pVirAddr = io_data.pOutputs[i].pVirAddr;
+        mgroup_output_tensors[grpid].push_back(tensor);
+      }
+
+      for (size_t i = 0; i < io_data.nInputSize; i++) {
+        ax_runner_tensor_t tensor;
+        tensor.nIdx = i;
+        tensor.sName = std::string(io_data.pInputs[i].Name);
+        tensor.nSize = io_data.pInputs[i].nSize;
+        for (int32_t j = 0; j < io_data.pInputs[i].dims.dimCount; j++) {
+          tensor.vShape.push_back(io_data.pInputs[i].dims.dims[j]);
+        }
+        tensor.phyAddr = (unsigned long long)io_data.pInputs[i].pBuf;
+        tensor.pVirAddr = io_data.pInputs[i].pVirAddr;
+        mgroup_input_tensors[grpid].push_back(tensor);
+      }
+    }
+
+    moutput_tensors = mgroup_output_tensors[0];
+    minput_tensors = mgroup_input_tensors[0];
+    _parepare_io = true;
+  } else {
+  }
+  // for (int grpid = 0; grpid < group_count; grpid++) {
+  //   printf("\ngrpid: %d\n", grpid);
+  //   print_io_info(mgroup_input_tensors[grpid], mgroup_output_tensors[grpid]);
+  //   printf("==================================================\n\n");
+  // }
+
+  return ret;
+}
+
+int ax_runner_axcl::init(const char *model_file) {
+  std::vector<unsigned char> model_buffer;
+  if (!read_file(model_file, model_buffer)) {
+    fprintf(stderr, "read_file failed.\n");
+    return -1;
+  }
+  auto ret = init((char *)model_buffer.data(), model_buffer.size());
+  return ret;
+}
+
+int ax_runner_axcl::init(char *model_buffer, size_t model_size) {
+  if (!m_handle) {
+    m_handle = new ax_joint_runner_axcl_handle_t;
+  }
+  memset((void *)m_handle, 0, sizeof(ax_joint_runner_axcl_handle_t));
+
+  // 3. create handle
+  void *devMem = nullptr;
+  axclrtMalloc(&devMem, model_size, AXCL_MEM_MALLOC_NORMAL_ONLY);
+
+  // 4. copy model to device
+  axclrtMemcpy(devMem, model_buffer, model_size, AXCL_MEMCPY_HOST_TO_DEVICE);
+
+  int ret = axclrtEngineLoadFromMem(devMem, model_size, &m_handle->handle);
+  if (0 != ret) {
+    fprintf(stderr, "AX_ENGINE_CreateHandle");
+    return ret;
+  }
+  axclrtFree(devMem);
+
+  return sub_init();
+}
+
+void ax_runner_axcl::release() {
+  if (m_handle && m_handle->handle) {
+    for (int grpid = 0; grpid < group_count; grpid++) {
+      free_io(&m_handle->io_datas[grpid]);
+      axclrtEngineDestroyIO(m_handle->ios[grpid]);
+    }
+
+    axclrtEngineUnload(m_handle->handle);
+    m_handle->handle = 0;
+  }
+
+  if (m_handle) {
+    delete m_handle;
+    m_handle = nullptr;
+  }
+
+  minput_tensors.clear();
+  moutput_tensors.clear();
+
+  map_input_tensors.clear();
+  map_output_tensors.clear();
+
+  mgroup_input_tensors.clear();
+  mgroup_output_tensors.clear();
+
+  map_group_input_tensors.clear();
+  map_group_output_tensors.clear();
+}
+
+void ax_runner_axcl::deinit() {
+  if (m_handle && m_handle->handle) {
+    axclrtEngineUnload(m_handle->handle);
+    m_handle->handle = 0;
+  }
+}
+
+int ax_runner_axcl::get_algo_width() {
+  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
+    return minput_tensors[0].vShape[2];
+  }
+  return -1;
+}
+int ax_runner_axcl::get_algo_height() {
+  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
+    return minput_tensors[0].vShape[1];
+  }
+  return -1;
+}
+
+int ax_runner_axcl::set_input(int grpid, int idx,
+                              unsigned long long int phy_addr,
+                              unsigned long size) {
+  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid], idx,
+                                           (void *)phy_addr, size);
+}
+int ax_runner_axcl::set_output(int grpid, int idx,
+                               unsigned long long int phy_addr,
+                               unsigned long size) {
+  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid], idx,
+                                            (void *)phy_addr, size);
+}
+
+int ax_runner_axcl::set_input(int grpid, std::string name,
+                              unsigned long long int phy_addr,
+                              unsigned long size) {
+  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid],
+                                           get_input(grpid, name).nIdx,
+                                           (void *)phy_addr, size);
+}
+
+int ax_runner_axcl::set_output(int grpid, std::string name,
+                               unsigned long long int phy_addr,
+                               unsigned long size) {
+  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid],
+                                            get_output(grpid, name).nIdx,
+                                            (void *)phy_addr, size);
+}
+
+ax_color_space_e ax_runner_axcl::get_color_space() {
+  return ax_color_space_unknown;
+}
+
+int ax_runner_axcl::inference() { return inference(0); }
+
+int ax_runner_axcl::inference(int grpid) {
+  if (_auto_sync_before_inference)
+    for (size_t i = 0; i < mgroup_input_tensors[grpid].size(); i++)
+      axclrtMemcpy((void *)mgroup_input_tensors[grpid][i].phyAddr,
+                   mgroup_input_tensors[grpid][i].pVirAddr,
+                   mgroup_input_tensors[grpid][i].nSize,
+                   AXCL_MEMCPY_HOST_TO_DEVICE);
+
+  auto ret = axclrtEngineExecute(m_handle->handle, m_handle->context, grpid,
+                                 m_handle->ios[grpid]);
+  if (ret != 0) {
+    fprintf(stderr, "axclrtEngineExecute failed. ret=0x%x\n", ret);
+    return ret;
+  }
+
+  if (_auto_sync_after_inference)
+    for (size_t i = 0; i < mgroup_output_tensors[grpid].size(); i++)
+      axclrtMemcpy(mgroup_output_tensors[grpid][i].pVirAddr,
+                   (void *)mgroup_output_tensors[grpid][i].phyAddr,
+                   mgroup_output_tensors[grpid][i].nSize,
+                   AXCL_MEMCPY_DEVICE_TO_HOST);
+  return 0;
+}
diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
new file mode 100644
index 00000000..4aba11b0
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
@@ -0,0 +1,40 @@
+// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#pragma once
+#include "ax_model_runner.hpp"
+
+class ax_runner_axcl : public ax_runner_base {
+ protected:
+  struct ax_joint_runner_axcl_handle_t *m_handle = nullptr;
+  int group_count = 0;
+  bool _parepare_io = false;
+
+  int sub_init();
+
+ public:
+  int init(const char *model_file) override;
+  int init(char *model_buffer, size_t model_size) override;
+
+  void release();
+  void deinit() override;
+
+  int get_algo_width() override;
+  int get_algo_height() override;
+  ax_color_space_e get_color_space() override;
+
+  int set_input(int grpid, int idx, unsigned long long int phy_addr,
+                unsigned long size);
+  int set_output(int grpid, int idx, unsigned long long int phy_addr,
+                 unsigned long size);
+
+  int set_input(int grpid, std::string name, unsigned long long int phy_addr,
+                unsigned long size);
+  int set_output(int grpid, std::string name, unsigned long long int phy_addr,
+                 unsigned long size);
+
+  // int inference(ax_image_t *pstFrame) override;
+  int inference() override;
+  int inference(int grpid) override;
+};
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
new file mode 100644
index 00000000..59062ac4
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
@@ -0,0 +1,138 @@
+// sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../online-recognizer-sense-voice-impl.h
+OfflineRecognitionResult ConvertSenseVoiceResult(
+    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+    int32_t frame_shift_ms, int32_t subsampling_factor);
+
+class OfflineRecognizerSenseVoiceAxclImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerSenseVoiceAxclImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(
+            std::make_unique<OfflineSenseVoiceModelAxcl>(config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerSenseVoiceAxclImpl(Manager *mgr,
+                                      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineSenseVoiceModelAxcl>(
+            mgr, config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i < n; ++i) {
+      DecodeOneStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void InitFeatConfig() {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+    config_.feat_config.window_type = "hamming";
+    config_.feat_config.high_freq = 0;
+    config_.feat_config.snip_edges = true;
+  }
+
+  void DecodeOneStream(OfflineStream *s) const {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    std::vector<float> f = s->GetFrames();
+
+    int32_t language = 0;
+    if (config_.model_config.sense_voice.language.empty()) {
+      language = 0;
+    } else if (meta_data.lang2id.count(
+                   config_.model_config.sense_voice.language)) {
+      language =
+          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+    } else {
+      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+                       config_.model_config.sense_voice.language.c_str());
+    }
+
+    int32_t text_norm = config_.model_config.sense_voice.use_itn
+                            ? meta_data.with_itn_id
+                            : meta_data.without_itn_id;
+
+    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+
+    auto result =
+        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+
+    int32_t frame_shift_ms = 10;
+    int32_t subsampling_factor = meta_data.window_shift;
+    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+                                     subsampling_factor);
+
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineSenseVoiceModelAxcl> model_;
+  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
new file mode 100644
index 00000000..54461590
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
@@ -0,0 +1,196 @@
+// sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
+
+#include <algorithm>
+#include <array>
+#include <cstring>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelAxcl::Impl {
+ public:
+  ~Impl() { runner_.release(); }
+
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    auto buf = ReadFile(config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    auto buf = ReadFile(mgr, config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+  }
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+    return meta_data_;
+  }
+
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) {
+    features = ApplyLFR(std::move(features));
+    std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
+
+    // input 0: features
+    auto &in0 = runner_.get_input(0);
+    size_t bytes0 = in0.nSize;
+    if (bytes0 != features.size() * sizeof(float)) {
+      SHERPA_ONNX_LOGE(
+          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
+          in0.nSize, features.size() * sizeof(float));
+      SHERPA_ONNX_EXIT(-1);
+    }
+    std::memcpy(in0.pVirAddr, features.data(), bytes0);
+
+    auto &in1 = runner_.get_input(1);
+    size_t bytes1 = in1.nSize;
+    if (bytes1 != prompt.size() * sizeof(int32_t)) {
+      SHERPA_ONNX_LOGE(
+          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
+          in1.nSize, prompt.size() * sizeof(int32_t));
+      SHERPA_ONNX_EXIT(-1);
+    }
+    std::memcpy(in1.pVirAddr, prompt.data(), bytes1);
+
+    int ret = runner_.inference();
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("ax_runner_axcl inference failed, ret = %d", ret);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    // output 0
+    auto &out0 = runner_.get_output(0);
+    size_t out_elems = out0.nSize / sizeof(float);
+    std::vector<float> out(out_elems);
+    std::memcpy(out.data(), out0.pVirAddr, out0.nSize);
+    return out;
+  }
+
+ private:
+  void Init(void *model_data, size_t model_data_length) {
+    {
+      if (auto ret = axclInit(0); 0 != ret) {
+        fprintf(stderr, "Init AXCL failed{0x%8x}.\n", ret);
+        return;
+      }
+      axclrtDeviceList lst;
+      if (const auto ret = axclrtGetDeviceList(&lst);
+          0 != ret || 0 == lst.num) {
+        fprintf(stderr,
+                "Get AXCL device failed{0x%8x}, find total %d device.\n", ret,
+                lst.num);
+        return;
+      }
+      if (const auto ret = axclrtSetDevice(lst.devices[0]); 0 != ret) {
+        fprintf(stderr, "Set AXCL device failed{0x%8x}.\n", ret);
+        return;
+      }
+      int ret = axclrtEngineInit(AXCL_VNPU_DISABLE);
+      if (0 != ret) {
+        fprintf(stderr, "axclrtEngineInit %d\n", ret);
+        return;
+      }
+    }
+
+    int ret =
+        runner_.init(reinterpret_cast<char *>(model_data), model_data_length);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("Init ax_runner_axcl failed, ret = %d", ret);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    auto &in0 = runner_.get_input(0);
+    if (in0.vShape.size() < 2) {
+      SHERPA_ONNX_LOGE(
+          "Input tensor rank is too small (rank = %zu). Shape vector is empty "
+          "or has only 1 dim.",
+          in0.vShape.size());
+      SHERPA_ONNX_EXIT(-1);
+    }
+    num_input_frames_ = in0.vShape[1];
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Axcl SenseVoice model init done with ax_runner_axcl.");
+      SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
+    }
+  }
+
+  std::vector<float> ApplyLFR(std::vector<float> in) const {
+    int32_t lfr_window_size = meta_data_.window_size;
+    int32_t lfr_window_shift = meta_data_.window_shift;
+    int32_t in_feat_dim = 80;
+    int32_t in_num_frames = in.size() / in_feat_dim;
+    int32_t out_num_frames =
+        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+
+    if (out_num_frames > num_input_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          out_num_frames, num_input_frames_);
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios.");
+      out_num_frames = num_input_frames_;
+    }
+
+    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+    std::vector<float> out(num_input_frames_ * out_feat_dim);
+    const float *p_in = in.data();
+    float *p_out = out.data();
+    for (int32_t i = 0; i != out_num_frames; ++i) {
+      std::copy(p_in, p_in + out_feat_dim, p_out);
+      p_out += out_feat_dim;
+      p_in += lfr_window_shift * in_feat_dim;
+    }
+    return out;
+  }
+
+ private:
+  OfflineModelConfig config_;
+  ax_runner_axcl runner_;
+  OfflineSenseVoiceModelMetaData meta_data_;
+  int32_t num_input_frames_ = -1;
+};
+
+OfflineSenseVoiceModelAxcl::~OfflineSenseVoiceModelAxcl() = default;
+
+OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+std::vector<float> OfflineSenseVoiceModelAxcl::Run(std::vector<float> features,
+                                                   int32_t language,
+                                                   int32_t text_norm) const {
+  return impl_->Run(std::move(features), language, text_norm);
+}
+
+const OfflineSenseVoiceModelMetaData &
+OfflineSenseVoiceModelAxcl::GetModelMetadata() const {
+  return impl_->GetModelMetadata();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
new file mode 100644
index 00000000..10391ee4
--- /dev/null
+++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
+#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
+
+#include <memory>
+#include <vector>
+
+#include "axcl.h"
+#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelAxcl {
+ public:
+  ~OfflineSenseVoiceModelAxcl();
+
+  explicit OfflineSenseVoiceModelAxcl(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineSenseVoiceModelAxcl(Manager *mgr, const OfflineModelConfig &config);
+
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) const;
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axera/io.hpp b/sherpa-onnx/csrc/axera/io.hpp
new file mode 100644
index 00000000..3d3f2535
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/io.hpp
@@ -0,0 +1,255 @@
+// sherpa-onnx/csrc/axera/io.hpp
+//
+// This file is adapted from AXERA's ax-samples project.
+// See the original BSD 3-Clause license below.
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+/*
+ * AXERA is pleased to support the open source community by making ax-samples
+ * available.
+ *
+ * Copyright (c) 2022, AXERA Semiconductor (Shanghai) Co., Ltd. All rights
+ * reserved.
+ *
+ * Licensed under the BSD 3-Clause License (the "License"); you may not use this
+ * file except in compliance with the License. You may obtain a copy of the
+ * License at
+ *
+ * https://opensource.org/licenses/BSD-3-Clause
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+/*
+ * Author: AXERA Corporation
+ */
+
+#pragma once
+
+#include <ax_engine_api.h>
+#include <ax_sys_api.h>
+
+#include <cstdio>
+#include <cstring>
+#include <map>
+#include <utility>
+#include <vector>
+
+#define AX_CMM_ALIGN_SIZE 128
+
+inline const char *AX_CMM_SESSION_NAME = "ax-samples-cmm";
+
+typedef enum {
+  AX_ENGINE_ABST_DEFAULT = 0,
+  AX_ENGINE_ABST_CACHED = 1,
+} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
+
+typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
+                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
+    INPUT_OUTPUT_ALLOC_STRATEGY;
+
+#define SAMPLE_AX_ENGINE_DEAL_HANDLE        \
+  if (0 != ret) {                           \
+    return AX_ENGINE_DestroyHandle(handle); \
+  }
+
+#define SAMPLE_AX_ENGINE_DEAL_HANDLE_IO     \
+  if (0 != ret) {                           \
+    middleware::free_io(&io_data);          \
+    return AX_ENGINE_DestroyHandle(handle); \
+  }
+
+namespace middleware {
+
+inline void free_io_index(AX_ENGINE_IO_BUFFER_T *io_buf, size_t index) {
+  for (int i = 0; i < (int)index; ++i) {
+    AX_ENGINE_IO_BUFFER_T *pBuf = io_buf + i;
+    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+  }
+}
+
+inline void free_io(AX_ENGINE_IO_T *io) {
+  for (size_t j = 0; j < io->nInputSize; ++j) {
+    AX_ENGINE_IO_BUFFER_T *pBuf = io->pInputs + j;
+    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+  }
+  for (size_t j = 0; j < io->nOutputSize; ++j) {
+    AX_ENGINE_IO_BUFFER_T *pBuf = io->pOutputs + j;
+    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+  }
+  delete[] io->pInputs;
+  delete[] io->pOutputs;
+}
+
+static inline int prepare_io(AX_ENGINE_IO_INFO_T *info, AX_ENGINE_IO_T *io_data,
+                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
+  memset(io_data, 0, sizeof(*io_data));
+  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[info->nInputSize];
+  memset(io_data->pInputs, 0, sizeof(AX_ENGINE_IO_BUFFER_T) * info->nInputSize);
+  io_data->nInputSize = info->nInputSize;
+
+  auto ret = 0;
+  for (int i = 0; i < (int)info->nInputSize; ++i) {
+    auto meta = info->pInputs[i];
+    auto buffer = &io_data->pInputs[i];
+    if (strategy.first == AX_ENGINE_ABST_CACHED) {
+      ret = AX_SYS_MemAllocCached(
+          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
+          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
+    } else {
+      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
+                            meta.nSize, AX_CMM_ALIGN_SIZE,
+                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
+    }
+
+    if (ret != 0) {
+      free_io_index(io_data->pInputs, i);
+      fprintf(
+          stderr,
+          "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
+          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+      return ret;
+    }
+    // fprintf(stderr, "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes
+    // }. \n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+  }
+
+  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[info->nOutputSize];
+  memset(io_data->pOutputs, 0,
+         sizeof(AX_ENGINE_IO_BUFFER_T) * info->nOutputSize);
+  io_data->nOutputSize = info->nOutputSize;
+  for (int i = 0; i < (int)info->nOutputSize; ++i) {
+    auto meta = info->pOutputs[i];
+    auto buffer = &io_data->pOutputs[i];
+    buffer->nSize = meta.nSize;
+    if (strategy.second == AX_ENGINE_ABST_CACHED) {
+      ret = AX_SYS_MemAllocCached(
+          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
+          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
+    } else {
+      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
+                            meta.nSize, AX_CMM_ALIGN_SIZE,
+                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
+    }
+    if (ret != 0) {
+      fprintf(
+          stderr,
+          "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
+          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+      free_io_index(io_data->pInputs, io_data->nInputSize);
+      free_io_index(io_data->pOutputs, i);
+      return ret;
+    }
+    // fprintf(stderr, "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes
+    // }.\n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+  }
+
+  return 0;
+}
+
+static int push_input(const std::vector<uint8_t> &data, AX_ENGINE_IO_T *io_t,
+                      AX_ENGINE_IO_INFO_T *info_t) {
+  if (info_t->nInputSize != 1) {
+    fprintf(stderr, "Only support Input size == 1 current now");
+    return -1;
+  }
+
+  if (data.size() != info_t->pInputs[0].nSize) {
+    fprintf(stderr,
+            "The input data size is not matched with tensor {name: %s, size: "
+            "%d}.\n",
+            info_t->pInputs[0].pName, info_t->pInputs[0].nSize);
+    return -1;
+  }
+
+  memcpy(io_t->pInputs[0].pVirAddr, data.data(), data.size());
+
+  return 0;
+}
+
+static void print_io_info(AX_ENGINE_IO_INFO_T *io_info) {
+  static std::map<AX_ENGINE_DATA_TYPE_T, const char *> data_type = {
+      {AX_ENGINE_DT_UNKNOWN, "UNKNOWN"},
+      {AX_ENGINE_DT_UINT8, "UINT8"},
+      {AX_ENGINE_DT_UINT16, "UINT16"},
+      {AX_ENGINE_DT_FLOAT32, "FLOAT32"},
+      {AX_ENGINE_DT_SINT16, "SINT16"},
+      {AX_ENGINE_DT_SINT8, "SINT8"},
+      {AX_ENGINE_DT_SINT32, "SINT32"},
+      {AX_ENGINE_DT_UINT32, "UINT32"},
+      {AX_ENGINE_DT_FLOAT64, "FLOAT64"},
+      {AX_ENGINE_DT_UINT10_PACKED, "UINT10_PACKED"},
+      {AX_ENGINE_DT_UINT12_PACKED, "UINT12_PACKED"},
+      {AX_ENGINE_DT_UINT14_PACKED, "UINT14_PACKED"},
+      {AX_ENGINE_DT_UINT16_PACKED, "UINT16_PACKED"},
+  };
+
+  static std::map<AX_ENGINE_COLOR_SPACE_T, const char *> color_type = {
+      {AX_ENGINE_CS_FEATUREMAP, "FEATUREMAP"},
+      {AX_ENGINE_CS_RAW8, "RAW8"},
+      {AX_ENGINE_CS_RAW10, "RAW10"},
+      {AX_ENGINE_CS_RAW12, "RAW12"},
+      {AX_ENGINE_CS_RAW14, "RAW14"},
+      {AX_ENGINE_CS_RAW16, "RAW16"},
+      {AX_ENGINE_CS_NV12, "NV12"},
+      {AX_ENGINE_CS_NV21, "NV21"},
+      {AX_ENGINE_CS_RGB, "RGB"},
+      {AX_ENGINE_CS_BGR, "BGR"},
+      {AX_ENGINE_CS_RGBA, "RGBA"},
+      {AX_ENGINE_CS_GRAY, "GRAY"},
+      {AX_ENGINE_CS_YUV444, "YUV444"},
+  };
+  printf("\ninput size: %d\n", io_info->nInputSize);
+  for (uint32_t i = 0; i < io_info->nInputSize; ++i) {
+    // print shape info,like [batchsize x channel x height x width]
+    auto &info = io_info->pInputs[i];
+    printf("    name: \e[1;32m%8s", info.pName);
+
+    std::string dt = "unknown";
+    if (data_type.find(info.eDataType) != data_type.end()) {
+      dt = data_type[info.eDataType];
+      printf(" \e[1;34m[%s] ", dt.c_str());
+    } else {
+      printf(" \e[1;31m[%s] ", dt.c_str());
+    }
+
+    std::string ct = "unknown";
+    if (info.pExtraMeta &&
+        color_type.find(info.pExtraMeta->eColorSpace) != color_type.end()) {
+      ct = color_type[info.pExtraMeta->eColorSpace];
+      printf("\e[1;34m[%s]", ct.c_str());
+    } else {
+      printf("\e[1;31m[%s]", ct.c_str());
+    }
+    printf(" \n        \e[1;31m");
+
+    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
+      printf("%d", info.pShape[s]);
+      if (s != info.nShapeSize - 1) {
+        printf(" x ");
+      }
+    }
+    printf("\e[0m\n\n");
+  }
+
+  printf("\noutput size: %d\n", io_info->nOutputSize);
+  for (uint32_t i = 0; i < io_info->nOutputSize; ++i) {
+    // print shape info,like [batchsize x channel x height x width]
+    auto &info = io_info->pOutputs[i];
+    printf("    name: \e[1;32m%8s \e[1;34m[%s]\e[0m\n        \e[1;31m",
+           info.pName, data_type[info.eDataType]);
+    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
+      printf("%d", info.pShape[s]);
+      if (s != info.nShapeSize - 1) {
+        printf(" x ");
+      }
+    }
+    printf("\e[0m\n\n");
+  }
+}
+
+}  // namespace middleware
diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
new file mode 100644
index 00000000..039976e1
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
@@ -0,0 +1,138 @@
+// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-axera-impl.h
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../online-recognizer-sense-voice-impl.h
+OfflineRecognitionResult ConvertSenseVoiceResult(
+    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+    int32_t frame_shift_ms, int32_t subsampling_factor);
+
+class OfflineRecognizerSenseVoiceAxeraImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerSenseVoiceAxeraImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
+            config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerSenseVoiceAxeraImpl(Manager *mgr,
+                                       const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
+            mgr, config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i < n; ++i) {
+      DecodeOneStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void InitFeatConfig() {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+    config_.feat_config.window_type = "hamming";
+    config_.feat_config.high_freq = 0;
+    config_.feat_config.snip_edges = true;
+  }
+
+  void DecodeOneStream(OfflineStream *s) const {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    std::vector<float> f = s->GetFrames();
+
+    int32_t language = 0;
+    if (config_.model_config.sense_voice.language.empty()) {
+      language = 0;
+    } else if (meta_data.lang2id.count(
+                   config_.model_config.sense_voice.language)) {
+      language =
+          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+    } else {
+      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+                       config_.model_config.sense_voice.language.c_str());
+    }
+
+    int32_t text_norm = config_.model_config.sense_voice.use_itn
+                            ? meta_data.with_itn_id
+                            : meta_data.without_itn_id;
+
+    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+
+    auto result =
+        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+
+    int32_t frame_shift_ms = 10;
+    int32_t subsampling_factor = meta_data.window_shift;
+    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+                                     subsampling_factor);
+
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineSenseVoiceModelAxera> model_;
+  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
new file mode 100644
index 00000000..7e964791
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
@@ -0,0 +1,216 @@
+// sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
+
+#include <algorithm>
+#include <array>
+#include <cstring>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/axera/io.hpp"
+#include "sherpa-onnx/csrc/axera/utils.h"
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelAxera::Impl {
+ public:
+  ~Impl() {
+    middleware::free_io(&io_data_);
+    if (handle_) {
+      AX_ENGINE_DestroyHandle(handle_);
+    }
+  }
+
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    auto buf = ReadFile(config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    auto buf = ReadFile(mgr, config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+  }
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+    return meta_data_;
+  }
+
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) {
+    features = ApplyLFR(std::move(features));
+
+    std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
+
+    if (!io_info_ || io_info_->nInputSize < 1) {
+      SHERPA_ONNX_LOGE("Axera model expects at least 1 input tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    const auto &in0_meta = io_info_->pInputs[0];
+    size_t bytes0 = in0_meta.nSize;
+
+    if (bytes0 != features.size() * sizeof(float)) {
+      SHERPA_ONNX_LOGE(
+          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
+          in0_meta.nSize, features.size() * sizeof(float));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::memcpy(io_data_.pInputs[0].pVirAddr, features.data(), bytes0);
+
+    //   io_info_->nInputSize >= 2
+    //   io_info_->pInputs[1].nSize == prompt.size() * sizeof(int32_t)
+    if (io_info_->nInputSize >= 2) {
+      const auto &in1_meta = io_info_->pInputs[1];
+      size_t bytes1 = in1_meta.nSize;
+      if (bytes1 != prompt.size() * sizeof(int32_t)) {
+        SHERPA_ONNX_LOGE(
+            "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
+            in1_meta.nSize, prompt.size() * sizeof(int32_t));
+        SHERPA_ONNX_EXIT(-1);
+      }
+      std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
+    }
+
+    auto ret = AX_ENGINE_RunSync(handle_, &io_data_);
+    if (ret != 0) {
+      SHERPA_ONNX_LOGE("AX_ENGINE_RunSync failed, ret = %d", ret);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (io_info_->nOutputSize < 1) {
+      SHERPA_ONNX_LOGE("Axera model has no output tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    const auto &out_meta = io_info_->pOutputs[0];
+    auto &out_buf = io_data_.pOutputs[0];
+
+    size_t out_elems = out_meta.nSize / sizeof(float);
+    std::vector<float> out(out_elems);
+
+    std::memcpy(out.data(), out_buf.pVirAddr, out_meta.nSize);
+
+    return out;
+  }
+
+ private:
+  void Init(void *model_data, size_t model_data_length) {
+    InitEngine(config_.debug);
+
+    InitContext(model_data, model_data_length, config_.debug, &handle_);
+
+    InitInputOutputAttrs(handle_, config_.debug, &io_info_);
+
+    std::memset(&io_data_, 0, sizeof(io_data_));
+
+    PrepareIO(io_info_, &io_data_, config_.debug);
+
+    if (!io_info_ || io_info_->nInputSize == 0 || !io_info_->pInputs) {
+      SHERPA_ONNX_LOGE("No input tensor in Axera model");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    auto &in0 = io_info_->pInputs[0];
+    if (in0.nShapeSize < 2) {
+      SHERPA_ONNX_LOGE("Input tensor rank is too small (nShapeSize = %u)",
+                       in0.nShapeSize);
+      SHERPA_ONNX_EXIT(-1);
+    }
+    num_input_frames_ = in0.pShape[1];
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Axera SenseVoice model init done.");
+      SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
+    }
+  }
+
+  std::vector<float> ApplyLFR(std::vector<float> in) const {
+    int32_t lfr_window_size = meta_data_.window_size;
+    int32_t lfr_window_shift = meta_data_.window_shift;
+    int32_t in_feat_dim = 80;
+    int32_t in_num_frames = in.size() / in_feat_dim;
+    int32_t out_num_frames =
+        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+
+    if (out_num_frames > num_input_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          out_num_frames, num_input_frames_);
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios.");
+      out_num_frames = num_input_frames_;
+    }
+
+    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+    std::vector<float> out(num_input_frames_ * out_feat_dim);
+    const float *p_in = in.data();
+    float *p_out = out.data();
+
+    for (int32_t i = 0; i != out_num_frames; ++i) {
+      std::copy(p_in, p_in + out_feat_dim, p_out);
+      p_out += out_feat_dim;
+      p_in += lfr_window_shift * in_feat_dim;
+    }
+
+    return out;
+  }
+
+ private:
+  OfflineModelConfig config_;
+  AX_ENGINE_HANDLE handle_ = nullptr;
+  AX_ENGINE_IO_INFO_T *io_info_ = nullptr;
+  AX_ENGINE_IO_T io_data_;
+  OfflineSenseVoiceModelMetaData meta_data_;
+  int32_t num_input_frames_ = -1;
+};
+
+OfflineSenseVoiceModelAxera::~OfflineSenseVoiceModelAxera() = default;
+
+OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+std::vector<float> OfflineSenseVoiceModelAxera::Run(std::vector<float> features,
+                                                    int32_t language,
+                                                    int32_t text_norm) const {
+  return impl_->Run(std::move(features), language, text_norm);
+}
+
+const OfflineSenseVoiceModelMetaData &
+OfflineSenseVoiceModelAxera::GetModelMetadata() const {
+  return impl_->GetModelMetadata();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
new file mode 100644
index 00000000..dcbb2381
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
+#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
+
+#include <memory>
+#include <vector>
+
+#include "ax_engine_api.h"
+#include "ax_sys_api.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelAxera {
+ public:
+  ~OfflineSenseVoiceModelAxera();
+
+  explicit OfflineSenseVoiceModelAxera(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineSenseVoiceModelAxera(Manager *mgr, const OfflineModelConfig &config);
+
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) const;
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axera/utils.cc b/sherpa-onnx/csrc/axera/utils.cc
new file mode 100644
index 00000000..c42f2a6a
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/utils.cc
@@ -0,0 +1,169 @@
+// sherpa-onnx/csrc/axera/utils.cc
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#include "sherpa-onnx/csrc/axera/utils.h"
+
+#include <string.h>
+
+#include <sstream>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/axera/io.hpp"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
+                       int32_t height, int32_t width, float *dst) {
+  for (int32_t i = 0; i < n; ++i) {
+    for (int32_t h = 0; h < height; ++h) {
+      for (int32_t w = 0; w < width; ++w) {
+        for (int32_t c = 0; c < channel; ++c) {
+          dst[i * height * width * channel + h * width * channel + w * channel +
+              c] = src[i * height * width * channel + c * height * width +
+                       h * width + w];
+        }
+      }
+    }
+  }
+}
+
+std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
+  std::ostringstream os;
+  os << "{";
+  if (!io_info) {
+    os << "null AX_ENGINE_IO_INFO_T}";
+    return os.str();
+  }
+
+  os << "nInputSize: " << io_info->nInputSize;
+  os << ", nOutputSize: " << io_info->nOutputSize;
+  os << ", nMaxBatchSize: " << io_info->nMaxBatchSize;
+  os << ", bDynamicBatchSize: "
+     << (io_info->bDynamicBatchSize ? "true" : "false");
+  os << "}";
+  return os.str();
+}
+
+std::unordered_map<std::string, std::string> Parse(const char *custom_string,
+                                                   bool debug /*= false*/) {
+  std::unordered_map<std::string, std::string> ans;
+  if (!custom_string) {
+    SHERPA_ONNX_LOGE("Parse: custom_string is null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  std::vector<std::string> fields;
+  SplitStringToVector(custom_string, ";", false, &fields);
+  std::vector<std::string> tmp;
+
+  for (const auto &f : fields) {
+    tmp.clear();
+    SplitStringToVector(f, "=", false, &tmp);
+    if (tmp.size() != 2) {
+      SHERPA_ONNX_LOGE("Invalid custom string %s for %s", custom_string,
+                       f.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+    ans[std::move(tmp[0])] = std::move(tmp[1]);
+  }
+
+  if (debug) {
+    for (const auto &p : ans) {
+      SHERPA_ONNX_LOGE("%s: %s", p.first.c_str(), p.second.c_str());
+    }
+  }
+  return ans;
+}
+
+void InitEngine(bool debug) {
+  AX_SYS_Init();
+#ifdef AXERA_TARGET_CHIP_AX620E
+  auto ret = AX_ENGINE_Init();
+#else
+  AX_ENGINE_NPU_ATTR_T npu_attr;
+  memset(&npu_attr, 0, sizeof(npu_attr));
+  npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
+  auto ret = AX_ENGINE_Init(&npu_attr);
+#endif
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_Init failed, ret = %d", ret);
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (debug) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_Init done.");
+  }
+}
+
+void InitContext(void *model_data, size_t model_data_length, bool debug,
+                 AX_ENGINE_HANDLE *handle) {
+  if (!handle) {
+    SHERPA_ONNX_LOGE("InitContext: handle is null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  auto ret = AX_ENGINE_CreateHandle(handle, model_data, model_data_length);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle failed, ret = %d", ret);
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (debug) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p",
+                     (void *)(*handle));
+  }
+
+  ret = AX_ENGINE_CreateContext(*handle);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext failed, ret = %d", ret);
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (debug) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext done.");
+  }
+}
+
+void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+                          AX_ENGINE_IO_INFO_T **io_info) {
+  if (!io_info) {
+    SHERPA_ONNX_LOGE("InitInputOutputAttrs: io_info is null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  auto ret = AX_ENGINE_GetIOInfo(handle, io_info);
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo failed, ret = %d", ret);
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (debug) {
+    SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo done.");
+    SHERPA_ONNX_LOGE("IO_INFO: %s", ToString(*io_info).c_str());
+    middleware::print_io_info(*io_info);
+  }
+}
+
+void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
+               bool debug) {
+  if (!io_info || !io_data) {
+    SHERPA_ONNX_LOGE("PrepareIO: io_info or io_data is null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  auto ret = middleware::prepare_io(
+      io_info, io_data,
+      std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_CACHED));
+  if (ret != 0) {
+    SHERPA_ONNX_LOGE("middleware::prepare_io failed, ret = %d", ret);
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (debug) {
+    SHERPA_ONNX_LOGE("PrepareIO (middleware::prepare_io) done.");
+  }
+}
+
+}  // namespace sherpa_onnx
\ No newline at end of file
diff --git a/sherpa-onnx/csrc/axera/utils.h b/sherpa-onnx/csrc/axera/utils.h
new file mode 100644
index 00000000..039eefc8
--- /dev/null
+++ b/sherpa-onnx/csrc/axera/utils.h
@@ -0,0 +1,37 @@
+// sherpa-onnx/csrc/axera/utils.h
+//
+// Copyright (c)  2025  M5Stack Technology CO LTD
+
+#ifndef SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+#define SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+
+#include <string>
+#include <unordered_map>
+#include <vector>
+
+#include "ax_engine_api.h"
+
+namespace sherpa_onnx {
+
+void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
+                       int32_t height, int32_t width, float *dst);
+
+std::string ToString(const AX_ENGINE_IO_INFO_T *io_info);
+
+std::unordered_map<std::string, std::string> Parse(const char *custom_string,
+                                                   bool debug = false);
+
+void InitEngine(bool debug);
+
+void InitContext(void *model_data, size_t model_data_length, bool debug,
+                 AX_ENGINE_HANDLE *handle);
+
+void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+                          AX_ENGINE_IO_INFO_T **io_info);
+
+void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
+               bool debug);
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_AXERA_UTILS_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 2ff1a4a7..ae45d304 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -41,6 +41,14 @@
 #include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
 #endif
 
+#if SHERPA_ONNX_ENABLE_AXERA
+#include "sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h"
+#endif
+
+#if SHERPA_ONNX_ENABLE_AXCL
+#include "sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h"
+#endif
+
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
@@ -79,6 +87,48 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #endif
   }
 
+  if (config.model_config.provider == "axera") {
+#if SHERPA_ONNX_ENABLE_AXERA
+    if (config.model_config.sense_voice.model.empty()) {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported "
+          "by axera for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    } else if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
+    }
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXERA=ON if you "
+        "want to use axera. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/axera/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
+  if (config.model_config.provider == "axcl") {
+#if SHERPA_ONNX_ENABLE_AXCL
+    if (config.model_config.sense_voice.model.empty()) {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported "
+          "by axcl for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    } else if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
+    }
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
+        "want to use axcl. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/axcl/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
   if (config.model_config.provider == "ascend") {
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
     if (!config.model_config.sense_voice.model.empty()) {

commit 4c27d9b9b37ee0f2d23f86761f1aabfa1d003190
Author: Mahmoud ghareeb <mahmoudghareeb11111@gmail.com>
Date:   Tue Dec 2 15:04:41 2025 +0200

    Fix token log probabilities in offline transducer modified beam search decoder (#2846)

diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
index e97fd636..335b0dec 100644
--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
@@ -139,7 +139,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
 
           // Store the token log probability (subtract prev log_prob to get original)
           float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
-          new_hyp.ys_log_probs.push_back(token_log_prob);
+          new_hyp.ys_probs.push_back(token_log_prob);
 
           if (context_graphs[i] != nullptr) {
             auto context_res =
@@ -191,7 +191,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
     // strip leading blanks
     r.tokens = {hyp.ys.begin() + context_size, hyp.ys.end()};
     r.timestamps = std::move(hyp.timestamps);
-    r.ys_log_probs = std::move(hyp.ys_log_probs);
+    r.ys_log_probs = std::move(hyp.ys_probs);
   }
 
   return unsorted_ans;

commit 0f369acd518db09f141327780b1acb82410c48ed
Author: Mahmoud ghareeb <mahmoudghareeb11111@gmail.com>
Date:   Tue Dec 2 13:00:52 2025 +0200

    Add token-level confidence scores (ys_probs) for offline transducer models (#2843)
    
    This PR adds support for retrieving per-token log probabilities (confidence scores) from offline transducer-based ASR models. The changes are comprehensive, touching the C++ core logic to compute the scores, the C-API to expose them, and the Python bindings.

diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 08702f9c..99422a06 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -703,12 +703,20 @@ const SherpaOnnxOfflineRecognizerResult *SherpaOnnxGetOfflineStreamResult(
       r->durations = nullptr;
     }
 
+    if (!result.ys_log_probs.empty() && result.ys_log_probs.size() == r->count) {
+      r->ys_log_probs = new float[r->count];
+      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(), r->ys_log_probs);
+    } else {
+      r->ys_log_probs = nullptr;
+    }
+
     r->tokens = tokens;
   } else {
     r->count = 0;
     r->timestamps = nullptr;
     r->tokens = nullptr;
     r->tokens_arr = nullptr;
+    r->ys_log_probs = nullptr;
   }
 
   return r;
@@ -720,6 +728,7 @@ void SherpaOnnxDestroyOfflineRecognizerResult(
     delete[] r->text;
     delete[] r->timestamps;
     delete[] r->durations;
+    delete[] r->ys_log_probs;
     delete[] r->tokens;
     delete[] r->tokens_arr;
     delete[] r->json;
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 36731e66..e3f7e016 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -666,6 +666,11 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerResult {
   // Pointer to continuous memory which holds durations (in seconds) for each
   // token It is NULL if the model does not support durations
   float *durations;
+
+  // Pointer to continuous memory which holds log probabilities (confidence)
+  // for each token. It is NULL if the model does not support probabilities.
+  // ys_log_probs[i] is the log probability for token i.
+  float *ys_log_probs;
 } SherpaOnnxOfflineRecognizerResult;
 
 /// Get the result of the offline stream.
diff --git a/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h b/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
index fefd3325..b79bac18 100644
--- a/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
@@ -72,6 +72,9 @@ static OfflineRecognitionResult Convert(
     r.durations.push_back(d * frame_shift_s);
   }
 
+  // Copy token log probabilities (confidence scores)
+  r.ys_log_probs = src.ys_log_probs;
+
   return r;
 }
 
diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
index 8f757e6a..265c8dbe 100644
--- a/sherpa-onnx/csrc/offline-stream.cc
+++ b/sherpa-onnx/csrc/offline-stream.cc
@@ -442,6 +442,18 @@ std::string OfflineRecognitionResult::AsJsonString() const {
   }
   os << "], ";
 
+  os << "\""
+     << "ys_log_probs"
+     << "\""
+     << ": ";
+  os << "[";
+  sep = "";
+  for (auto p : ys_log_probs) {
+    os << sep << std::fixed << std::setprecision(6) << p;
+    sep = ", ";
+  }
+  os << "], ";
+
   sep = "";
 
   os << "\""
diff --git a/sherpa-onnx/csrc/offline-stream.h b/sherpa-onnx/csrc/offline-stream.h
index 5e2a514b..c4838fd6 100644
--- a/sherpa-onnx/csrc/offline-stream.h
+++ b/sherpa-onnx/csrc/offline-stream.h
@@ -42,6 +42,9 @@ struct OfflineRecognitionResult {
   /// only)
   std::vector<float> durations;
 
+  /// ys_log_probs[i] contains the log probability (confidence) for tokens[i].
+  std::vector<float> ys_log_probs;
+
   std::vector<int32_t> words;
 
   std::string AsJsonString() const;
diff --git a/sherpa-onnx/csrc/offline-transducer-decoder.h b/sherpa-onnx/csrc/offline-transducer-decoder.h
index 74d2b9e9..82a4e1bb 100644
--- a/sherpa-onnx/csrc/offline-transducer-decoder.h
+++ b/sherpa-onnx/csrc/offline-transducer-decoder.h
@@ -24,6 +24,9 @@ struct OfflineTransducerDecoderResult {
   /// (post-subsampling). It is converted to seconds by higher layers
   /// (e.g., Convert() in offline-recognizer-transducer-impl.h).
   std::vector<float> durations;
+
+  /// ys_log_probs[i] contains the log probability (confidence) for tokens[i].
+  std::vector<float> ys_log_probs;
 };
 
 class OfflineTransducerDecoder {
diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
index 98fe78da..2427d412 100644
--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
@@ -9,6 +9,7 @@
 #include <utility>
 #include <vector>
 
+#include "sherpa-onnx/csrc/math.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/packed-sequence.h"
 #include "sherpa-onnx/csrc/slice.h"
@@ -53,16 +54,22 @@ OfflineTransducerGreedySearchDecoder::Decode(Ort::Value encoder_out,
       if (blank_penalty_ > 0.0) {
         p_logit[0] -= blank_penalty_;  // assuming blank id is 0
       }
+      
+      LogSoftmax(p_logit, vocab_size);
+
       auto y = static_cast<int32_t>(std::distance(
-          static_cast<const float *>(p_logit),
-          std::max_element(static_cast<const float *>(p_logit),
-                           static_cast<const float *>(p_logit) + vocab_size)));
+          p_logit,
+          std::max_element(p_logit, p_logit + vocab_size)));
+
+      float log_prob = p_logit[y];
+
       p_logit += vocab_size;
       // blank id is hardcoded to 0
       // also, it treats unk as blank
       if (y != 0 && y != unk_id_) {
         ans[i].tokens.push_back(y);
         ans[i].timestamps.push_back(t);
+        ans[i].ys_log_probs.push_back(log_prob);
         emitted = true;
       }
     }
diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
index d053a6fd..e97fd636 100644
--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
@@ -136,6 +136,11 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
         if (new_token != 0 && new_token != unk_id_) {
           new_hyp.ys.push_back(new_token);
           new_hyp.timestamps.push_back(t);
+
+          // Store the token log probability (subtract prev log_prob to get original)
+          float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
+          new_hyp.ys_log_probs.push_back(token_log_prob);
+
           if (context_graphs[i] != nullptr) {
             auto context_res =
                 context_graphs[i]->ForwardOneStep(context_state,
@@ -186,6 +191,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
     // strip leading blanks
     r.tokens = {hyp.ys.begin() + context_size, hyp.ys.end()};
     r.timestamps = std::move(hyp.timestamps);
+    r.ys_log_probs = std::move(hyp.ys_log_probs);
   }
 
   return unsorted_ans;
diff --git a/sherpa-onnx/python/csrc/offline-stream.cc b/sherpa-onnx/python/csrc/offline-stream.cc
index fbae871b..203bcf90 100644
--- a/sherpa-onnx/python/csrc/offline-stream.cc
+++ b/sherpa-onnx/python/csrc/offline-stream.cc
@@ -45,7 +45,9 @@ static void PybindOfflineRecognitionResult(py::module *m) {  // NOLINT
       .def_property_readonly("timestamps",
         [](const PyClass &self) { return self.timestamps; })
       .def_property_readonly("durations",
-        [](const PyClass &self) { return self.durations; });
+        [](const PyClass &self) { return self.durations; })
+      .def_property_readonly("ys_log_probs",
+        [](const PyClass &self) { return self.ys_log_probs; });
 }
 
 void PybindOfflineStream(py::module *m) {

commit 586cd19e225bed3a50c3d0a554736a61a0b33191
Author: alex-spacemit <jinghui.huang@spacemit.com>
Date:   Tue Dec 2 14:36:31 2025 +0800

    Add spacemit ort ep for spacemit riscv cpus (#2837)
    
    This pull request significantly extends the project's hardware compatibility by integrating a dedicated SpacemiT Execution Provider for ONNX Runtime. The changes enable efficient model inference on SpacemiT RISC-V CPUs, leveraging their RVV1.0 capabilities. This involves updates to the build system, new CMake modules for toolchain and ONNX Runtime package handling, and modifications to the core provider and session management logic to recognize and configure the SpacemiT EP.

diff --git a/.github/workflows/riscv64-spacemit-linux.yaml b/.github/workflows/riscv64-spacemit-linux.yaml
new file mode 100755
index 00000000..ccf3b155
--- /dev/null
+++ b/.github/workflows/riscv64-spacemit-linux.yaml
@@ -0,0 +1,299 @@
+name: riscv64-spacemit-linux
+
+on:
+  push:
+    branches:
+      - master
+    paths:
+      - '.github/workflows/riscv64-spacemit-linux.yaml'
+      - 'cmake/**'
+      - 'sherpa-onnx/csrc/*'
+      - 'sherpa-onnx/c-api/*'
+      - 'toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake'
+      - 'build-riscv64-linux-gnu-spacemit.sh'
+    tags:
+      - 'v[0-9]+.[0-9]+.[0-9]+*'
+
+  workflow_dispatch:
+
+concurrency:
+  group: riscv64-spacemit-linux-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  riscv64_spacemit_linux:
+    runs-on: ${{ matrix.os }}
+    name: ${{ matrix.os }} ${{ matrix.lib_type }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        lib_type: [shared] #, static]
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      - name: ccache
+        uses: hendrikmuhs/ccache-action@v1.2
+        with:
+          key: ${{ matrix.os }}-riscv64-spacemit-${{ matrix.lib_type }}
+
+      - name: cache-qemu
+        id: cache-qemu
+        uses: actions/cache@v4
+        with:
+          path: qemu-install
+          key: qemu-riscv-spacemit-install-20250818
+
+      - name: qemu
+        if: steps.cache-qemu.outputs.cache-hit != 'true'
+        run: |
+          wget -q https://archive.spacemit.com/spacemit-ai/qemu/jdsk-qemu-v10.0.2.tar.gz
+          tar -xf jdsk-qemu-v10.0.2.tar.gz
+          mkdir -p qemu-install/bin
+
+          cp -v ./jdsk-qemu/bin/qemu-riscv64 ./qemu-install/bin
+
+      - name: cache-toolchain
+        id: cache-toolchain
+        uses: actions/cache@v4
+        with:
+          path: toolchain
+          key: https://archive.spacemit.com/toolchain/spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz
+
+      - name: Download toolchain
+        if: steps.cache-toolchain.outputs.cache-hit != 'true'
+        shell: bash
+        run: |
+          wget -q https://archive.spacemit.com/toolchain/spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz
+
+          mkdir $GITHUB_WORKSPACE/toolchain
+
+          tar xvf spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz --strip-components 1 -C $GITHUB_WORKSPACE/toolchain
+          ls -lh $GITHUB_WORKSPACE/toolchain/bin
+
+      - name: Display toolchain info
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+          riscv64-unknown-linux-gnu-gcc --version
+
+      - name: Display qemu-riscv64 -h
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
+          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
+          qemu-riscv64 -h
+
+      - name: build riscv64-spacemit-linux
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+
+          export CMAKE_CXX_COMPILER_LAUNCHER=ccache
+          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
+
+          cmake --version
+
+          lib_type=${{ matrix.lib_type }}
+
+          if [[ $lib_type == "shared" ]]; then
+            export BUILD_SHARED_LIBS=ON
+          else
+            export BUILD_SHARED_LIBS=OFF
+          fi
+
+          export RISCV_ROOT_PATH=$GITHUB_WORKSPACE/toolchain
+          ./build-riscv64-linux-gnu-spacemit.sh
+
+          ls -lh build-riscv64-linux-gnu-spacemit/bin
+          ls -lh build-riscv64-linux-gnu-spacemit/lib
+
+          echo "---install/lib---"
+          ls -lh build-riscv64-linux-gnu-spacemit/install/lib
+
+          echo "---install/bin---"
+          ls -lh build-riscv64-linux-gnu-spacemit/install/bin
+
+          file build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
+
+          readelf -d build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
+
+      - name: Copy files
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+          riscv64-unknown-linux-gnu-strip --version
+
+          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-linux-riscv64-spacemit-${{ matrix.lib_type }}
+          mkdir $dst
+
+          cp -v $GITHUB_WORKSPACE/toolchain/sysroot/lib/ld-linux-riscv64-lp64d.so.1 build-riscv64-linux-gnu-spacemit/install/lib/
+
+          ls -lh build-riscv64-linux-gnu-spacemit/install/lib
+
+          cp -a build-riscv64-linux-gnu-spacemit/install/bin $dst/
+          ls -lh $dst/bin/*
+          riscv64-unknown-linux-gnu-strip $dst/bin/*
+          ls -lh $dst
+
+          lib_type=${{ matrix.lib_type }}
+          if [[ $lib_type == "shared" ]]; then
+            cp -a build-riscv64-linux-gnu-spacemit/install/lib $dst/
+            rm -fv $dst/lib/libasound.so
+          fi
+
+          tree $dst
+
+          tar cjvf ${dst}.tar.bz2 $dst
+
+      - uses: actions/upload-artifact@v4
+        if: matrix.lib_type == 'shared'
+        with:
+          name: sherpa-onnx-linux-riscv64-spacemit-shared
+          path: sherpa-onnx-*linux-riscv64-spacemit-shared.tar.bz2
+
+      # https://huggingface.co/docs/hub/spaces-github-actions
+      - name: Publish to huggingface
+        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+
+            GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
+
+            cd huggingface
+            dst=riscv64-spacemit/$SHERPA_ONNX_VERSION
+            mkdir -p $dst
+
+            cp -v ../sherpa-onnx-*-shared.tar.bz2 $dst/
+
+            git status
+            git lfs track "*.bz2"
+
+            git add .
+
+            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-linux-riscv64-spacemit-shared.tar.bz2"
+
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
+
+      - uses: actions/upload-artifact@v4
+        if: matrix.lib_type == 'static'
+        with:
+          name: sherpa-onnx-linux-riscv64-spacemit-static
+          path: sherpa-onnx-*linux-riscv64-spacemit-static.tar.bz2
+
+      - name: Release pre-compiled binaries and libs for riscv64 linux ${{ matrix.lib_type }}
+        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-riscv64*.tar.bz2
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: v1.12.11
+
+      - name: Release pre-compiled binaries and libs for riscv64 linux ${{ matrix.lib_type }}
+        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*linux-riscv64*.tar.bz2
+
+      - name: Test sherpa-onnx
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
+          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
+          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
+          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
+
+          ls -lh ./build-riscv64-linux-gnu-spacemit/bin
+
+          echo "----------sherpa-onnx----------"
+          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx --help
+          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
+
+          echo "----------sherpa-onnx-offline----------"
+          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline --help
+          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline
+
+          echo "----------sherpa-onnx-offline-tts----------"
+          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts --help
+          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts
+
+      - name: Test streaming speech recognition
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
+          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
+          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
+          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
+          echo "Some mistakes in ep graph partition, disable op Gather for spacemit-ep now, will be fixed soon."
+          export SPACEMIT_EP_DISABLE_OP_TYPE_FILTER="Gather"
+
+          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
+          tar xvf sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
+          rm sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
+
+          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx \
+            --provider=spacemit \
+            --tokens=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/tokens.txt \
+            --encoder=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/encoder-epoch-99-avg-1.onnx \
+            --decoder=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/decoder-epoch-99-avg-1.onnx \
+            --joiner=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/joiner-epoch-99-avg-1.onnx \
+            ./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/test_wavs/0.wav
+
+      - name: Test offline tts
+        shell: bash
+        run: |
+          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
+          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
+          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
+          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
+          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
+          echo "Some mistakes in ep graph partition, disable op Gather;Cast;ConvTranspose for spacemit-ep now, will be fixed soon."
+          export SPACEMIT_EP_DISABLE_OP_TYPE_FILTER="Gather;Cast;ConvTranspose"
+
+          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-en_US-lessac-medium.tar.bz2
+          tar xf vits-piper-en_US-lessac-medium.tar.bz2
+          rm vits-piper-en_US-lessac-medium.tar.bz2
+
+          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts \
+            --provider=spacemit \
+            --vits-model=./vits-piper-en_US-lessac-medium/en_US-lessac-medium.onnx \
+            --vits-data-dir=./vits-piper-en_US-lessac-medium/espeak-ng-data \
+            --vits-tokens=./vits-piper-en_US-lessac-medium/tokens.txt \
+            --output-filename=./liliana-piper-en_US-lessac-medium.wav \
+            'liliana, the most beautiful and lovely assistant of our team!'
+
+      - uses: actions/upload-artifact@v4
+        if: matrix.lib_type == 'shared'
+        with:
+          name: wave
+          path: ./*.wav
diff --git a/.gitignore b/.gitignore
old mode 100644
new mode 100755
index 6ff6eb29..813b50a2
--- a/.gitignore
+++ b/.gitignore
@@ -161,3 +161,5 @@ sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-a
 sherpa-onnx-paraformer-zh-int8-2025-10-07
 sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
 sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+build-riscv64-linux-gnu-spacemit/
+spacemit-toolchain*
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 64efc4ba..10467632 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -62,6 +62,7 @@ option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUG
 option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
 option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
 option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
+option(SHERPA_ONNX_ENABLE_SPACEMIT "Whether to build for SpacemiT CPUs " OFF)
 
 set(SHERPA_ONNX_LINUX_ARM64_GPU_ONNXRUNTIME_VERSION "1.11.0" CACHE STRING "Used only for Linux ARM64 GPU. Set to 1.11.0 if you use CUDA 10.2 and cudnn8. Set it to 1.16.0 if you use CUDA 11.4 and cudnn8. Set it to 1.18.0 if you use CUDA 12.2 and cudnn8. Set it to 1.18.1 if you use CUDA 12.6 and cudnn9")
 
@@ -181,6 +182,7 @@ message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAM
 message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
 message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
 message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
+message(STATUS "SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
 message(STATUS "SHERPA_ONNX_LINK_D3D: ${SHERPA_ONNX_LINK_D3D}")
 
 if(BUILD_SHARED_LIBS OR SHERPA_ONNX_ENABLE_JNI)
@@ -308,6 +310,10 @@ if(SHERPA_ONNX_ENABLE_QNN)
   add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
 endif()
 
+if(SHERPA_ONNX_ENABLE_SPACEMIT)
+  add_definitions(-DSHERPA_ONNX_ENABLE_SPACEMIT=1)
+endif()
+
 if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   set(ASCEND_TOOLKIT_HOME)
   if(NOT DEFINED ENV{ASCEND_TOOLKIT_HOME})
diff --git a/build-riscv64-linux-gnu-spacemit.sh b/build-riscv64-linux-gnu-spacemit.sh
new file mode 100755
index 00000000..0d50b9f6
--- /dev/null
+++ b/build-riscv64-linux-gnu-spacemit.sh
@@ -0,0 +1,71 @@
+#!/usr/bin/env bash
+set -ex
+
+SPACEMIT_TOOLCHAIN=spacemit-toolchain-linux-glibc-x86_64-v1.1.2
+DOWNLOAD_URL="https://archive.spacemit.com/toolchain/${SPACEMIT_TOOLCHAIN}.tar.xz"
+DOWNLOAD_FILE="./riscv-spacemit-toolchain.tar.gz"
+
+if [ -n "$RISCV_ROOT_PATH" ] && [ -d "$RISCV_ROOT_PATH" ]; then
+    echo "LOCAL RISCV_ROOT_PATH: $RISCV_ROOT_PATH"
+else
+    wget -O "$DOWNLOAD_FILE" "$DOWNLOAD_URL" --quiet --show-progress
+    tar -xf "$DOWNLOAD_FILE"
+    export RISCV_ROOT_PATH=$PWD/${SPACEMIT_TOOLCHAIN}
+    echo "DOWNLOAD RISCV_ROOT_PATH: $RISCV_ROOT_PATH"
+fi
+
+
+if [ x$dir = x"" ]; then
+  dir=build-riscv64-linux-gnu-spacemit
+fi
+mkdir -p $dir
+cd $dir
+
+if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
+  echo "Start to cross-compile alsa-lib"
+  if [ ! -d alsa-lib ]; then
+    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+  fi
+  # If it shows:
+  #  ./gitcompile: line 79: libtoolize: command not found
+  # Please use:
+  #  sudo apt-get install libtool m4 automake
+  #
+  pushd alsa-lib
+  CC=$RISCV_ROOT_PATH/bin/riscv64-unknown-linux-gnu-gcc ./gitcompile --host=riscv64-unknown-linux-gnu
+  popd
+  echo "Finish cross-compiling alsa-lib"
+fi
+
+export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+
+if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
+  # By default, use shared libraries
+  BUILD_SHARED_LIBS=ON
+fi
+
+cmake \
+  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
+  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
+  -DBUILD_ESPEAK_NG_EXE=OFF \
+  -DBUILD_ESPEAK_NG_TESTS=OFF \
+  -DCMAKE_INSTALL_PREFIX=./install \
+  -DCMAKE_BUILD_TYPE=Release \
+  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
+  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+  -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+  -DSHERPA_ONNX_ENABLE_JNI=OFF \
+  -DSHERPA_ONNX_ENABLE_C_API=ON \
+  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
+  -DSHERPA_ONNX_ENABLE_SPACEMIT=ON \
+  -DCMAKE_TOOLCHAIN_FILE=../toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake \
+  ..
+
+make VERBOSE=1 -j4
+make install/strip
+
+# Enable it if only needed
+# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
diff --git a/cmake/onnxruntime-linux-riscv64-spacemit.cmake b/cmake/onnxruntime-linux-riscv64-spacemit.cmake
new file mode 100755
index 00000000..bea0ed5d
--- /dev/null
+++ b/cmake/onnxruntime-linux-riscv64-spacemit.cmake
@@ -0,0 +1,94 @@
+message(STATUS "CMAKE_SYSTEM_NAME: ${CMAKE_SYSTEM_NAME}")
+message(STATUS "CMAKE_SYSTEM_PROCESSOR: ${CMAKE_SYSTEM_PROCESSOR}")
+
+if(NOT CMAKE_SYSTEM_NAME STREQUAL Linux)
+  message(FATAL_ERROR "This file is for Linux only. Given: ${CMAKE_SYSTEM_NAME}")
+endif()
+
+if(NOT CMAKE_SYSTEM_PROCESSOR STREQUAL riscv64)
+  message(FATAL_ERROR "This file is for riscv64 only. Given: ${CMAKE_SYSTEM_PROCESSOR}")
+endif()
+
+if(NOT BUILD_SHARED_LIBS)
+  message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}, SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
+endif()
+
+set(onnxruntime_pkg_name "spacemit-ort.riscv64.2.0.1.tar.gz")
+set(onnxruntime_URL  "https://archive.spacemit.com/spacemit-ai/onnxruntime/${onnxruntime_pkg_name}")
+set(onnxruntime_HASH "SHA256=8a15035aca34d5fd95f24444d4c7843265c1a81f49d84ec6fe9c6d0fdf5b55cf")
+
+# If you don't have access to the Internet,
+# please download onnxruntime to one of the following locations.
+# You can add more if you want.
+set(possible_file_locations
+  $ENV{HOME}/Downloads/${onnxruntime_pkg_name}
+  ${CMAKE_SOURCE_DIR}/${onnxruntime_pkg_name}
+  ${CMAKE_BINARY_DIR}/${onnxruntime_pkg_name}
+  /tmp/${onnxruntime_pkg_name}
+  /star-fj/fangjun/download/github/${onnxruntime_pkg_name}
+)
+
+foreach(f IN LISTS possible_file_locations)
+  if(EXISTS ${f})
+    set(onnxruntime_URL  "${f}")
+    file(TO_CMAKE_PATH "${onnxruntime_URL}" onnxruntime_URL)
+    message(STATUS "Found local downloaded onnxruntime: ${onnxruntime_URL}")
+    set(onnxruntime_URL2)
+    break()
+  endif()
+endforeach()
+
+FetchContent_Declare(onnxruntime
+  URL
+    ${onnxruntime_URL}
+    ${onnxruntime_URL2}
+  URL_HASH          ${onnxruntime_HASH}
+)
+
+FetchContent_GetProperties(onnxruntime)
+if(NOT onnxruntime_POPULATED)
+  message(STATUS "Downloading onnxruntime from ${onnxruntime_URL}")
+  FetchContent_Populate(onnxruntime)
+endif()
+message(STATUS "onnxruntime is downloaded to ${onnxruntime_SOURCE_DIR}")
+
+find_library(location_onnxruntime
+  NAMES onnxruntime
+  PATHS "${onnxruntime_SOURCE_DIR}/lib"
+  NO_CMAKE_SYSTEM_PATH
+)
+
+message(STATUS "location_onnxruntime: ${location_onnxruntime}")
+
+find_library(location_spacemit_ep
+  NAMES spacemit_ep
+  PATHS "${onnxruntime_SOURCE_DIR}/lib"
+  NO_CMAKE_SYSTEM_PATH
+)
+
+message(STATUS "location_spacemit_ep: ${location_spacemit_ep}")
+
+add_library(onnxruntime SHARED IMPORTED)
+add_library(spacemit_ep SHARED IMPORTED)
+
+set_target_properties(onnxruntime PROPERTIES
+  IMPORTED_LOCATION ${location_onnxruntime}
+  IMPORTED_LOCATION "${onnxruntime_SOURCE_DIR}/lib/libonnxruntime.so"
+  INTERFACE_INCLUDE_DIRECTORIES "${onnxruntime_SOURCE_DIR}/include/"
+)
+
+set_target_properties(spacemit_ep PROPERTIES
+  IMPORTED_LOCATION ${location_spacemit_ep}
+  IMPORTED_LOCATION "${onnxruntime_SOURCE_DIR}/lib/libspacemit_ep.so"
+  INTERFACE_INCLUDE_DIRECTORIES "${onnxruntime_SOURCE_DIR}/include/"
+)
+
+file(GLOB onnxruntime_lib_files
+  "${onnxruntime_SOURCE_DIR}/lib/libonnxruntime*")
+message(STATUS "onnxruntime lib files: ${onnxruntime_lib_files}")
+install(FILES ${onnxruntime_lib_files} DESTINATION lib)
+
+file(GLOB spacemit_ep_lib_files
+  "${onnxruntime_SOURCE_DIR}/lib/libspacemit_ep*")
+message(STATUS "spacemit_ep lib files: ${spacemit_ep_lib_files}")
+install(FILES ${spacemit_ep_lib_files} DESTINATION lib)
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
old mode 100644
new mode 100755
index e96ff9ff..2e709b60
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -7,7 +7,9 @@ function(download_onnxruntime)
   if(SHERPA_ONNX_ENABLE_WASM)
     include(onnxruntime-wasm-simd)
   elseif(CMAKE_SYSTEM_PROCESSOR STREQUAL riscv64)
-    if(BUILD_SHARED_LIBS)
+    if(SHERPA_ONNX_ENABLE_SPACEMIT)
+      include(onnxruntime-linux-riscv64-spacemit)
+    elseif(BUILD_SHARED_LIBS)
       include(onnxruntime-linux-riscv64)
     else()
       include(onnxruntime-linux-riscv64-static)
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
old mode 100644
new mode 100755
index 2e771d7e..9e6e5647
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -339,6 +339,14 @@ if(SHERPA_ONNX_ENABLE_QNN)
   target_include_directories(sherpa-onnx-core PRIVATE ${QNN_SDK_ROOT}/include/QNN)
 endif()
 
+if(SHERPA_ONNX_ENABLE_SPACEMIT)
+  if(TARGET spacemit_ep)
+    target_link_libraries(sherpa-onnx-core spacemit_ep)
+  else()
+    target_link_libraries(sherpa-onnx-core ${spacemit_ep_lib_files})
+  endif()
+endif()
+
 if(TARGET onnxruntime)
   target_link_libraries(sherpa-onnx-core onnxruntime)
 else()
diff --git a/sherpa-onnx/csrc/provider.cc b/sherpa-onnx/csrc/provider.cc
index af3759c7..8f7fe317 100644
--- a/sherpa-onnx/csrc/provider.cc
+++ b/sherpa-onnx/csrc/provider.cc
@@ -29,6 +29,8 @@ Provider StringToProvider(std::string s) {
     return Provider::kTRT;
   } else if (s == "directml") {
     return Provider::kDirectML;
+  } else if (s == "spacemit") {
+    return Provider::kSpacemiT;
   } else {
     SHERPA_ONNX_LOGE("Unsupported string: %s. Fallback to cpu", s.c_str());
     return Provider::kCPU;
diff --git a/sherpa-onnx/csrc/provider.h b/sherpa-onnx/csrc/provider.h
index 2b85b8a2..8842c797 100644
--- a/sherpa-onnx/csrc/provider.h
+++ b/sherpa-onnx/csrc/provider.h
@@ -21,6 +21,7 @@ enum class Provider {
   kNNAPI = 4,     // NnapiExecutionProvider
   kTRT = 5,       // TensorRTExecutionProvider
   kDirectML = 6,  // DmlExecutionProvider
+  kSpacemiT = 7,  // SpacemiTExecutionProvider
 };
 
 /**
diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
old mode 100644
new mode 100755
index 24a63bc4..d3d223c3
--- a/sherpa-onnx/csrc/session.cc
+++ b/sherpa-onnx/csrc/session.cc
@@ -23,6 +23,10 @@
 #include "dml_provider_factory.h"  // NOLINT
 #endif
 
+#if defined(SHERPA_ONNX_ENABLE_SPACEMIT)
+#include "spacemit_ort_env.h"  // NOLINT
+#endif
+
 namespace sherpa_onnx {
 
 static void OrtStatusFailure(OrtStatus *status, const char *s) {
@@ -239,6 +243,33 @@ Ort::SessionOptions GetSessionOptionsImpl(
           (int32_t)__ANDROID_API__);
 #else
       SHERPA_ONNX_LOGE("NNAPI is for Android only. Fallback to cpu");
+#endif
+      break;
+    }
+    case Provider::kSpacemiT: {
+#if defined(SHERPA_ONNX_ENABLE_SPACEMIT)
+      SHERPA_ONNX_LOGE("Use SpacemiT Execution Provider");
+      // when using SpacemiT Execution Provider, set intra_op_num_threads and
+      // inter_op_num_threads to 1 can improve performance.
+      // all ops run on ep, no need to create multiple threads in onnxruntime.
+      // ep will create SPACEMIT_EP_INTRA_THREAD_NUM threads as intra threads.
+      std::unordered_map<std::string, std::string> provider_options;
+      SHERPA_ONNX_LOGE("Set IntraOpNumThreads to 1");
+      sess_opts.SetIntraOpNumThreads(1);
+      SHERPA_ONNX_LOGE("Set InterOpNumThreads to 1");
+      sess_opts.SetInterOpNumThreads(1);
+      SHERPA_ONNX_LOGE("Set SPACEMIT_EP_INTRA_THREAD_NUM to %d", num_threads);
+      provider_options.insert(
+          std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM", std::to_string(num_threads)));
+      OrtStatus* sts = Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
+      if (sts) {
+        const auto &api = Ort::GetApi();
+        const char *msg = api.GetErrorMessage(sts);
+        SHERPA_ONNX_LOGE("Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu", msg);
+        api.ReleaseStatus(sts);
+      }
+#else
+      SHERPA_ONNX_LOGE("SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback to cpu!");
 #endif
       break;
     }
diff --git a/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake b/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake
new file mode 100755
index 00000000..42ef0fd6
--- /dev/null
+++ b/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake
@@ -0,0 +1,30 @@
+set(CMAKE_SYSTEM_NAME Linux)
+set(CMAKE_SYSTEM_PROCESSOR riscv64)
+set(CMAKE_SYSTEM_VERSION 1)
+
+if(CMAKE_HOST_SYSTEM_PROCESSOR MATCHES "^(riscv)")
+    message(STATUS "HOST SYSTEM ${CMAKE_HOST_SYSTEM_PROCESSOR}")
+else()
+    set(GNU_MACHINE riscv64-unknown-linux-gnu CACHE STRING "GNU compiler triple")
+    if(DEFINED ENV{RISCV_ROOT_PATH})
+        file(TO_CMAKE_PATH $ENV{RISCV_ROOT_PATH} RISCV_ROOT_PATH)
+    else()
+        message(FATAL_ERROR "RISCV_ROOT_PATH env must be defined")
+    endif()
+
+    set(RISCV_ROOT_PATH ${RISCV_ROOT_PATH} CACHE STRING "root path to riscv toolchain")
+    set(CMAKE_C_COMPILER ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-gcc)
+    set(CMAKE_CXX_COMPILER ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-g++)
+    set(CMAKE_STRIP ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-strip)
+    set(CMAKE_FIND_ROOT_PATH "${RISCV_ROOT_PATH}/sysroot")
+    set(CMAKE_SYSROOT "${RISCV_ROOT_PATH}/sysroot")
+endif()
+
+set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
+set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
+set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)
+set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)
+set(CMAKE_C_FLAGS "-march=rv64gcv_zfh_zvfh_zba_zicbop_zihintpause -mabi=lp64d -ftree-vectorize ${CMAKE_C_FLAGS}")
+set(CMAKE_CXX_FLAGS "-march=rv64gcv_zfh_zvfh_zba_zicbop_zihintpause -mabi=lp64d  -ftree-vectorize ${CXX_FLAGS}")
+set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} -latomic -lrt -lpthread")
+set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} --sysroot=${CMAKE_SYSROOT}")

commit 3fd5a5d55191d3179d72ec5a08c2ab5723e356b0
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Dec 2 13:49:28 2025 +0800

    Avoid NaN in NeMo speaker embedding models. (#2844)
    
    Fixes #2818
    
    This pull request addresses a critical numerical stability issue within the NeMo speaker embedding models. Due to floating-point precision, the calculated variance could occasionally become negative, leading to NaN values in subsequent computations. The change ensures that the variance is always a positive value, thereby preventing these numerical errors and improving the robustness of the speaker embedding extraction process.

diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
index ec1c44d6..40253d55 100644
--- a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
+++ b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
@@ -130,7 +130,8 @@ class SpeakerEmbeddingExtractorNeMoImpl : public SpeakerEmbeddingExtractorImpl {
 
     auto EX = m.colwise().mean();
     auto EX2 = m.array().pow(2).colwise().sum() / num_frames;
-    auto variance = EX2 - EX.array().pow(2);
+    auto variance = (EX2 - EX.array().pow(2)).max(1e-5);
+
     auto stddev = variance.array().sqrt();
 
     m = (m.rowwise() - EX).array().rowwise() / (stddev.array() + 1e-5);

commit c66442d219a4f2abfdabf538bbcd70d18b87231f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 1 11:57:23 2025 +0800

    Fix building JNI for Windows (#2840)

diff --git a/.github/workflows/windows-x64-jni.yaml b/.github/workflows/windows-x64-jni.yaml
index d20fb223..2f9682f5 100644
--- a/.github/workflows/windows-x64-jni.yaml
+++ b/.github/workflows/windows-x64-jni.yaml
@@ -52,7 +52,8 @@ jobs:
             -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF \
             -DBUILD_ESPEAK_NG_EXE=OFF \
             -DSHERPA_ONNX_BUILD_C_API_EXAMPLES=OFF  \
-            -DSHERPA_ONNX_ENABLE_BINARY=ON \
+            -DSHERPA_ONNX_ENABLE_BINARY=OFF \
+            -DSHERPA_ONNX_ENABLE_C_API=OFF \
             ..
 
       - name: Build sherpa-onnx for windows
@@ -62,7 +63,6 @@ jobs:
           cmake --build . --config Release -- -m:2
           cmake --build . --config Release --target install -- -m:2
 
-          ls -lh ./bin/Release/sherpa-onnx.exe
           rm -rf install/share
           rm -rf install/lib/share
           rm -rf install/lib/pkgconfig
@@ -81,9 +81,9 @@ jobs:
           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-win-x64-jni
           mkdir -p $dst
 
-          cp -a build/install/bin $dst/
-          cp -a build/install/lib $dst/
-          cp -a build/install/include $dst/
+          cp -a build/install/bin $dst/ || true
+          cp -a build/install/lib $dst/ || true
+          cp -a build/install/include $dst/ || true
 
           tar cjvf ${dst}.tar.bz2 $dst
 
@@ -130,4 +130,4 @@ jobs:
           file: sherpa-onnx-*.tar.bz2
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.12.0
+          # tag: v1.12.18
diff --git a/sherpa-onnx/jni/common.cc b/sherpa-onnx/jni/common.cc
index 2de20727..c620016a 100644
--- a/sherpa-onnx/jni/common.cc
+++ b/sherpa-onnx/jni/common.cc
@@ -15,6 +15,11 @@ namespace sherpa_onnx {
 
 https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
  */
+#if defined(_WIN32)
+void PrependAdspLibraryPath(const std::string &new_path) {
+  SHERPA_ONNX_LOGE("This function is not for Windows. Ignore it");
+}
+#else
 void PrependAdspLibraryPath(const std::string &new_path) {
   const char *old_path = getenv("ADSP_LIBRARY_PATH");
   std::string updated_path;
@@ -43,5 +48,6 @@ Successfully set ADSP_LIBRARY_PATH to
 
    */
 }
+#endif
 
 }  // namespace sherpa_onnx

commit 1a48531bbd8d7a3ed3c5e6afcf87bc1e673f184c
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 1 11:54:11 2025 +0800

    Add simulate streaming ASR Python example for Paraformer (#2839)

diff --git a/python-api-examples/simulate-streaming-paraformer-microphone.py b/python-api-examples/simulate-streaming-paraformer-microphone.py
new file mode 100755
index 00000000..b95e6a21
--- /dev/null
+++ b/python-api-examples/simulate-streaming-paraformer-microphone.py
@@ -0,0 +1,243 @@
+#!/usr/bin/env python3
+#
+# Copyright (c)  2025  Xiaomi Corporation
+
+"""
+This file demonstrates how to use sherpa-onnx Python APIs
+with VAD and non-streaming Paraformer for real-time speech recognition
+from a microphone.
+
+Usage:
+
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-paraformer-zh-int8-2025-10-07.tar.bz2
+tar xvf sherpa-onnx-paraformer-zh-int8-2025-10-07.tar.bz2
+
+./python-api-examples/simulate-streaming-paraformer-microphone.py  \
+  --silero-vad-model=./silero_vad.onnx \
+  --paraformer=./sherpa-onnx-paraformer-zh-int8-2025-10-07/model.int8.onnx \
+  --tokens=./sherpa-onnx-paraformer-zh-int8-2025-10-07/tokens.txt
+"""
+import argparse
+import queue
+import sys
+import threading
+import time
+from pathlib import Path
+
+import numpy as np
+
+try:
+    import sounddevice as sd
+except ImportError:
+    print("Please install sounddevice first. You can use")
+    print()
+    print("  pip install sounddevice")
+    print()
+    print("to install it")
+    sys.exit(-1)
+
+import sherpa_onnx
+
+killed = False
+recording_thread = None
+sample_rate = 16000  # Please don't change it
+
+# buffer saves audio samples to be played
+samples_queue = queue.Queue()
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--silero-vad-model",
+        type=str,
+        required=True,
+        help="Path to silero_vad.onnx",
+    )
+
+    parser.add_argument(
+        "--tokens",
+        type=str,
+        help="Path to tokens.txt",
+    )
+
+    parser.add_argument(
+        "--paraformer",
+        default="",
+        type=str,
+        help="Path to the model.onnx from Paraformer",
+    )
+
+    parser.add_argument(
+        "--num-threads",
+        type=int,
+        default=2,
+        help="Number of threads for neural network computation",
+    )
+
+    parser.add_argument(
+        "--hr-lexicon",
+        type=str,
+        default="",
+        help="If not empty, it is the lexicon.txt for homophone replacer",
+    )
+
+    parser.add_argument(
+        "--hr-rule-fsts",
+        type=str,
+        default="",
+        help="If not empty, it is the replace.fst for homophone replacer",
+    )
+
+    return parser.parse_args()
+
+
+def assert_file_exists(filename: str):
+    assert Path(filename).is_file(), (
+        f"{filename} does not exist!\n"
+        "Please refer to "
+        "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html to download it"
+    )
+
+
+def create_recognizer(args) -> sherpa_onnx.OfflineRecognizer:
+    assert_file_exists(args.paraformer)
+    recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer(
+        paraformer=args.paraformer,
+        tokens=args.tokens,
+        num_threads=args.num_threads,
+        debug=False,
+        hr_rule_fsts=args.hr_rule_fsts,
+        hr_lexicon=args.hr_lexicon,
+    )
+
+    return recognizer
+
+
+def start_recording():
+    # You can use any value you like for samples_per_read
+    samples_per_read = int(0.1 * sample_rate)  # 0.1 second = 100 ms
+
+    with sd.InputStream(channels=1, dtype="float32", samplerate=sample_rate) as s:
+        while not killed:
+            samples, _ = s.read(samples_per_read)  # a blocking read
+            samples = samples.reshape(-1)
+            samples = np.copy(samples)
+            samples_queue.put(samples)
+
+
+def main():
+    devices = sd.query_devices()
+    if len(devices) == 0:
+        print("No microphone devices found")
+        sys.exit(0)
+
+    print(devices)
+
+    # If you want to select a different input device, please use
+    # sd.default.device[0] = xxx
+    # where xxx is the device number
+
+    default_input_device_idx = sd.default.device[0]
+    print(f'Use default device: {devices[default_input_device_idx]["name"]}')
+
+    args = get_args()
+    assert_file_exists(args.tokens)
+    assert_file_exists(args.silero_vad_model)
+
+    assert args.num_threads > 0, args.num_threads
+
+    print("Creating recognizer. Please wait...")
+    recognizer = create_recognizer(args)
+
+    config = sherpa_onnx.VadModelConfig()
+    config.silero_vad.model = args.silero_vad_model
+    config.silero_vad.threshold = 0.5
+    config.silero_vad.min_silence_duration = 0.1  # seconds
+    config.silero_vad.min_speech_duration = 0.25  # seconds
+    # If the current segment is larger than this value, then it increases
+    # the threshold to 0.9 internally. After detecting this segment,
+    # it resets the threshold to its original value.
+    config.silero_vad.max_speech_duration = 8  # seconds
+    config.sample_rate = sample_rate
+
+    window_size = config.silero_vad.window_size
+
+    vad = sherpa_onnx.VoiceActivityDetector(config, buffer_size_in_seconds=100)
+
+    print("Started! Please speak")
+
+    buffer = []
+
+    global recording_thread
+    recording_thread = threading.Thread(target=start_recording)
+    recording_thread.start()
+
+    display = sherpa_onnx.Display()
+
+    started = False
+    started_time = None
+
+    offset = 0
+    while not killed:
+        samples = samples_queue.get()  # a blocking read
+
+        buffer = np.concatenate([buffer, samples])
+        while offset + window_size < len(buffer):
+            vad.accept_waveform(buffer[offset : offset + window_size])
+            if not started and vad.is_speech_detected():
+                started = True
+                started_time = time.time()
+            offset += window_size
+
+        if not started:
+            if len(buffer) > 10 * window_size:
+                offset -= len(buffer) - 10 * window_size
+                buffer = buffer[-10 * window_size :]
+
+        if started and time.time() - started_time > 0.2:
+            stream = recognizer.create_stream()
+            stream.accept_waveform(sample_rate, buffer)
+            recognizer.decode_stream(stream)
+            text = stream.result.text.strip()
+            if text:
+                display.update_text(text)
+                display.display()
+
+            started_time = time.time()
+
+        while not vad.empty():
+            # In general, this while loop is executed only once
+            stream = recognizer.create_stream()
+            stream.accept_waveform(sample_rate, vad.front.samples)
+
+            vad.pop()
+            recognizer.decode_stream(stream)
+
+            text = stream.result.text.strip()
+
+            display.update_text(text)
+
+            buffer = []
+            offset = 0
+            started = False
+            started_time = None
+
+            display.finalize_current_sentence()
+            display.display()
+
+
+if __name__ == "__main__":
+    try:
+        main()
+    except KeyboardInterrupt:
+        killed = True
+        if recording_thread:
+            recording_thread.join()
+        print("\nCaught Ctrl + C. Exiting")

commit 4a49d13938d6fbd7b8aafc032bb8542c8fb890f4
Author: Wei Kang <wkang.pku@gmail.com>
Date:   Mon Dec 1 09:51:24 2025 +0800

    [ZipVoice] Fix english tokenization error (#2834)

diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
index 075b8d0c..26ca18f4 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
@@ -342,7 +342,7 @@ std::vector<TokenIDs> OfflineTtsZipvoiceFrontend::ConvertTextToTokenIds(
       TokenizeZh(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
                  &tokens);
     } else if (pt.second == "en") {
-      TokenizeEn(pt.first, token2id_, voice, &token_ids, &tokens);
+      TokenizeEn(pt.first, token2id_, "en-us", &token_ids, &tokens);
     } else if (pt.second == "pinyin") {
       TokenizePinyin(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
                      &tokens);

commit 4b60dc3940d7d9572c64dfa0c4dc576c429c88ff
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Sat Nov 29 10:40:32 2025 +0800

    Fix building without TTS for C API (#2838)

diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 4d612ae1..08702f9c 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -1473,6 +1473,13 @@ const SherpaOnnxGeneratedAudio *SherpaOnnxOfflineTtsGenerateWithCallbackWithArg(
   return nullptr;
 }
 
+const SherpaOnnxGeneratedAudio *SherpaOnnxOfflineTtsGenerateWithZipvoice(
+    const SherpaOnnxOfflineTts *tts, const char *text, const char *prompt_text,
+    const float *prompt_samples, int32_t n_prompt, int32_t prompt_sr,
+    float speed, int32_t num_steps) {
+  return nullptr;
+}
+
 void SherpaOnnxDestroyOfflineTtsGeneratedAudio(
     const SherpaOnnxGeneratedAudio *p) {
   SHERPA_ONNX_LOGE("TTS is not enabled. Please rebuild sherpa-onnx");

commit 624b41c43b63167a0f238db5cb773c6da10a9d39
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 27 18:17:34 2025 +0800

    Release v1.12.18 (#2831)

diff --git a/.github/workflows/pkg-config.yaml b/.github/workflows/pkg-config.yaml
index 271a8c61..6276ad51 100644
--- a/.github/workflows/pkg-config.yaml
+++ b/.github/workflows/pkg-config.yaml
@@ -7,22 +7,6 @@ on:
       - pkg-config
     tags:
       - 'v[0-9]+.[0-9]+.[0-9]+*'
-    paths:
-      - '.github/workflows/pkg-config.yaml'
-      - '.github/scripts/test-offline-tts.sh'
-      - 'cmake/**'
-      - 'sherpa-onnx/csrc/*'
-      - 'sherpa-onnx/c-api/*'
-      - 'c-api-examples/**'
-  pull_request:
-    branches:
-      - master
-    paths:
-      - '.github/workflows/pkg-config.yaml'
-      - '.github/scripts/test-offline-tts.sh'
-      - 'cmake/**'
-      - 'sherpa-onnx/csrc/*'
-      - 'sherpa-onnx/c-api/*'
 
   workflow_dispatch:
 
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 8c42ad5e..78bf0cb7 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,25 @@
+## 1.12.18
+
+* Fix building wheels (#2786)
+* export omniASR_CTC_1B (#2788)
+* Add C++ QNN support for SenseVoice (#2793)
+* Export models for CANN toolkit 7.0 (#2795)
+* Support hotwords with byte level bpe (#2802)
+* Add Android demo with QNN (Qualcomm NPU) for SenseVoice ASR (#2803)
+* Export zipformer ctc models to QNN (#2815)
+* Add spaces between English words for Homophone replacer. (#2817)
+* Add C++ QNN support for Zipformer CTC models. (#2809)
+* Limit symbol visibility in the shared libraries (#2822)
+* Fix warnings for initializing tts lexicon. (#2823)
+* Export zipformer ctc models to Ascend NPU (#2824)
+* Refactor scripts for exporting models to Ascend NPU. (#2825)
+* Add C++ support for Zipformer CTC on Ascend NPU (#2826)
+* Fix segfault when non-wav file is passed to ReadWave (#2821)
+* Avoid calling rknn_dup_context(). (#2828)
+* Add C++ support for Paraformer with RK NPU (#2829)
+* Update README to include NPU support (#2830)
+* Support running whisper large v3 with external data weight (#2807)
+
 ## 1.12.17
 
 * Fix releasing
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 4bc8cb40..64efc4ba 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.17")
+set(SHERPA_ONNX_VERSION "1.12.18")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index 76024de6..214a87cd 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index 76024de6..214a87cd 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index c3c5ac3d..7d1505bd 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-v1.12.17-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.17-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-v1.12.18-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.18-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.17.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.18.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index bd6e098c..cf40dd2a 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index 12638bee..27ad7ac2 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index 93848bc6..f9d0c62b 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.17'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.18'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index 76024de6..214a87cd 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index 082ea5c8..43b998bf 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index ef491de9..3a5e95cc 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.17")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.18")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index a5ce9201..34f1fcd5 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index ddb6f444..1af89d4a 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index a7617b14..41a2bdf9 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 034325ac..b1151b94 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 53af0d02..53ad4569 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251113
-        versionName = "1.12.17"
+        versionCode = 20251127
+        versionName = "1.12.18"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index 15784a18..b65311a6 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index 15784a18..b65311a6 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 7f10eb5d..45e27edd 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251113
-        versionName "1.12.17"
+        versionCode 20251127
+        versionName "1.12.18"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index c1477eb7..a52287f5 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.17</string>
+	<string>1.12.18</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index 3fde5ed0..9eec66cc 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index 909c3751..c48cb85a 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index 762f3493..4b90ab48 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index 187661b5..4f89b81f 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index c2893c4c..1850039f 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index 2d68710a..e5122ffa 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index 32ec2161..0292fd08 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index b9e2166d..90ff0b20 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index 577b8efd..f6abcf2f 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index 54c9abf7..d551987f 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index b544cba8..de635d33 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 6c124a31..83a6e030 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index 1fe5515f..c3588163 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.17
+version: 1.12.18
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index 455b6f1f..0a259b1d 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.17
+version: 1.12.18
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index dd4d2433..21d23eb4 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.17
+version: 1.12.18
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.17
+  sherpa_onnx: ^1.12.18
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index 825f975f..3491fead 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.17
+version: 1.12.18
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.17
+  sherpa_onnx_android: ^1.12.18
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.17
+  sherpa_onnx_macos: ^1.12.18
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.17
+  sherpa_onnx_linux: ^1.12.18
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.17
+  sherpa_onnx_windows: ^1.12.18
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.17
+  sherpa_onnx_ios: ^1.12.18
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index 3e674158..a4b7b606 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.17'
+  s.version          = '1.12.18'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index dee37a00..dcd8f6b5 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.17'
+  s.version          = '1.12.18'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index dd8845a8..b04e4790 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.17';
+export const HAR_VERSION = '1.12.18';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index 187519e9..9467e26d 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.17",
+    "sherpa_onnx": "1.12.18",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index 1bfb0d94..7438ad28 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.17",
+  "version": "1.12.18",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index ec30333f..4fc965d4 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.17"
+    "sherpa_onnx": "1.12.18"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 992dd9d0..88735c1c 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.17",
+    "sherpa_onnx": "1.12.18",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 992dd9d0..88735c1c 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.17",
+    "sherpa_onnx": "1.12.18",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 992dd9d0..88735c1c 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.17",
+    "sherpa_onnx": "1.12.18",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index acc4feb5..dff1eb6b 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.17.har
+Please download ./sherpa_onnx-v1.12.18.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index 27182c9b..cf0d6dee 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.17",
+    "sherpa_onnx": "1.12.18",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index 9e83e15c..bd8c3c5d 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-1.12.17.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-1.12.18.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.17.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.17 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.18.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.18 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index 2c7edb25..683034ac 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x86-v1.12.17.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x86-v1.12.17.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x86-v1.12.17.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x86-v1.12.18.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x86-v1.12.18.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x86-v1.12.18.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 3758063c..5d4a89c5 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20251022
-new_version_code=20251113
+old_version_code=20251113
+new_version_code=20251127
 
-old_version="1\.12\.16"
-new_version="1\.12\.17"
+old_version="1\.12\.17"
+new_version="1\.12\.18"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index bf6beed9..3555e6ed 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.17"
+    "sherpa-onnx-node": "^1.12.18"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index ab2c37d2..db474390 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.17",
+    "sherpa-onnx": "^1.12.18",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index e8191a24..f54d3c84 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.17</version>
+    <version>1.12.18</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index b7a22878..c86f3727 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.17",
+    version="1.12.18",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.17",
+        "sherpa-onnx-core==1.12.18",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index 071319fb..456e2b7f 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.17",
+    version="1.12.18",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 061a2f03..329bddd4 100644
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.17"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.18"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index 02c48a40..e118dcc5 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Thu Nov 13 19:08:29 2025";
+  static const char *date = "Thu Nov 27 15:24:39 2025";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "6d199b18";
+  static const char *sha1 = "7d1d2270";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.17";
+  static const char *version = "1.12.18";
   return version;
 }
 

commit ec5e631cf70b897e390df673bdd0549bcc9ff334
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 27 18:12:56 2025 +0800

    Support running whisper large v3 with external data weight (#2807)

diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
index 3921c3b0..22c395b2 100644
--- a/sherpa-onnx/csrc/offline-whisper-model.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model.cc
@@ -37,15 +37,13 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    {
-      auto buf = ReadFile(config.whisper.encoder);
-      InitEncoder(buf.data(), buf.size());
-    }
+    encoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
+    InitEncoder(nullptr, 0);
 
-    {
-      auto buf = ReadFile(config.whisper.decoder);
-      InitDecoder(buf.data(), buf.size());
-    }
+    decoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
+    InitDecoder(nullptr, 0);
   }
 
   explicit Impl(const SpokenLanguageIdentificationConfig &config)
@@ -53,15 +51,13 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    {
-      auto buf = ReadFile(config.whisper.encoder);
-      InitEncoder(buf.data(), buf.size());
-    }
+    encoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
+    InitEncoder(nullptr, 0);
 
-    {
-      auto buf = ReadFile(config.whisper.decoder);
-      InitDecoder(buf.data(), buf.size());
-    }
+    decoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
+    InitDecoder(nullptr, 0);
   }
 
   template <typename Manager>
@@ -234,8 +230,16 @@ class OfflineWhisperModel::Impl {
 
  private:
   void InitEncoder(void *model_data, size_t model_data_length) {
-    encoder_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_);
+    if (model_data) {
+      encoder_sess_ = std::make_unique<Ort::Session>(
+          env_, model_data, model_data_length, sess_opts_);
+    } else if (!encoder_sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize encoder session outside of "
+          "this "
+          "function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(encoder_sess_.get(), &encoder_input_names_,
                   &encoder_input_names_ptr_);
@@ -293,8 +297,15 @@ class OfflineWhisperModel::Impl {
   }
 
   void InitDecoder(void *model_data, size_t model_data_length) {
-    decoder_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_);
+    if (model_data) {
+      decoder_sess_ = std::make_unique<Ort::Session>(
+          env_, model_data, model_data_length, sess_opts_);
+    } else if (!decoder_sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize decoder session outside of "
+          "this function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(decoder_sess_.get(), &decoder_input_names_,
                   &decoder_input_names_ptr_);

commit 7d1d2270a1fddeed4851abc1f1985189679e6a32
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 27 15:24:39 2025 +0800

    Update README to include NPU support (#2830)

diff --git a/README.md b/README.md
index ed9ffd1f..6eb4cd4e 100644
--- a/README.md
+++ b/README.md
@@ -45,6 +45,12 @@ For Rust support, please see [sherpa-rs][sherpa-rs]
 
 It also supports WebAssembly.
 
+### Supported NPUs
+
+| [1. Rockchip NPU (RKNN)][rknpu-doc] | [2. Qualcomm NPU (QNN)][qnn-doc]  | [3. Ascend NPU][ascend-doc] |
+|-------------------------------------|-----------------------------------|-----------------------------|
+|                                   |                                 |                           |
+
 [Join our discord](https://discord.gg/fJdxzg2VbG)
 
 
@@ -573,3 +579,6 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
 [kws-url]: https://k2-fsa.github.io/sherpa/onnx/kws/index.html
 [punct-url]: https://k2-fsa.github.io/sherpa/onnx/punctuation/index.html
 [se-url]: https://k2-fsa.github.io/sherpa/onnx/speech-enhancement/index.html
+[rknpu-doc]: https://k2-fsa.github.io/sherpa/onnx/rknn/index.html
+[qnn-doc]: https://k2-fsa.github.io/sherpa/onnx/qnn/index.html
+[ascend-doc]: https://k2-fsa.github.io/sherpa/onnx/ascend/index.html

commit 6873c3bf609f64486e40b3c7f71e1d8d7f3a175a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 27 15:03:08 2025 +0800

    Add C++ support for Paraformer with RK NPU (#2829)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 6ad708f8..2e771d7e 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -25,6 +25,7 @@ set(sources
   keyword-spotter-impl.cc
   keyword-spotter.cc
   lodr-fst.cc
+  math.cc
   offline-canary-model-config.cc
   offline-canary-model.cc
   offline-ctc-fst-decoder-config.cc
@@ -182,6 +183,7 @@ if(SHERPA_ONNX_ENABLE_RKNN)
   list(APPEND sources
     ./rknn/context-blocking-queue-rknn.cc
     ./rknn/offline-sense-voice-model-rknn.cc
+    ./rknn/offline-paraformer-model-rknn.cc
     ./rknn/online-stream-rknn.cc
     ./rknn/online-transducer-greedy-search-decoder-rknn.cc
     ./rknn/online-transducer-modified-beam-search-decoder-rknn.cc
diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
index 739b752b..8acd020d 100644
--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
@@ -27,57 +27,11 @@
 #include "sherpa-onnx/csrc/ascend/utils.h"
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/math.h"
 #include "sherpa-onnx/csrc/text-utils.h"
 
 namespace sherpa_onnx {
 
-static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
-  for (int32_t i = 0; i < n; ++i) {
-    in_out[i] += scale * src[i];
-  }
-}
-
-static void Scale(const float *src, float scale, int32_t n, float *out) {
-  for (int32_t i = 0; i < n; ++i) {
-    out[i] = scale * src[i];
-  }
-}
-
-static std::vector<float> ComputeAcousticEmbedding(
-    std::vector<float> encoder_out, std::vector<float> alphas,
-    int32_t encoder_dim) {
-  std::vector<float> ans;
-  ans.reserve(encoder_out.size());
-
-  float acc = 0;
-  std::vector<float> cur_emb(encoder_dim);
-  for (int32_t i = 0; i < static_cast<int32_t>(alphas.size()); ++i) {
-    float w = alphas[i];
-
-    acc += w;
-    if (acc >= 1) {
-      float overflow = acc - 1;
-      float remain = w - overflow;
-
-      ScaleAdd(encoder_out.data() + i * encoder_dim, remain, encoder_dim,
-               cur_emb.data());
-
-      ans.insert(ans.end(), cur_emb.begin(), cur_emb.end());
-
-      Scale(encoder_out.data() + i * encoder_dim, overflow, encoder_dim,
-            cur_emb.data());
-
-      acc = overflow;
-    } else {
-      ScaleAdd(encoder_out.data() + i * encoder_dim, w, encoder_dim,
-               cur_emb.data());
-    }
-  }
-  // TODO(fangjun): The last cur_emb is not used
-
-  return ans;
-}
-
 class OfflineParaformerModelAscend::Impl {
  public:
   explicit Impl(const OfflineModelConfig &config) : config_(config) {
@@ -133,6 +87,9 @@ class OfflineParaformerModelAscend::Impl {
     std::lock_guard<std::mutex> lock(mutex_);
 
     features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
 
     int32_t num_frames = features.size() / 560;
 
@@ -154,13 +111,16 @@ class OfflineParaformerModelAscend::Impl {
                     num_frames * sizeof(float), ACL_MEMCPY_DEVICE_TO_HOST);
     SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
 
-    std::vector<float> acoustic_embedding = ComputeAcousticEmbedding(
-        std::move(encoder_out_cpu), std::move(alphas_cpu), encoder_dim_);
+    std::vector<float> acoustic_embedding =
+        ComputeAcousticEmbedding(encoder_out_cpu, alphas_cpu, encoder_dim_);
     if (acoustic_embedding.empty()) {
       // no speech in the audio file
       return {};
     }
 
+    encoder_out_cpu.clear();
+    alphas_cpu.clear();
+
     int32_t num_tokens = acoustic_embedding.size() / encoder_dim_;
 
     RunDecoder(num_frames, std::move(acoustic_embedding));
@@ -366,6 +326,10 @@ class OfflineParaformerModelAscend::Impl {
     int32_t in_feat_dim = 80;
 
     int32_t in_num_frames = in.size() / in_feat_dim;
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
     int32_t out_num_frames =
         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
 
diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
index 97e1ae86..53da0300 100644
--- a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+++ b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
@@ -111,6 +111,10 @@ class OfflineRecognizerSenseVoiceAscendImpl : public OfflineRecognizerImpl {
                             : meta_data.without_itn_id;
 
     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    if (logits.empty()) {
+      return;
+    }
+
     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
 
     auto result =
diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
index a7409206..542d0e4f 100644
--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
@@ -57,6 +57,9 @@ class OfflineSenseVoiceModelAscend::Impl {
     std::lock_guard<std::mutex> lock(mutex_);
 
     features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
 
     int32_t num_frames = features.size() / 560;
 
@@ -161,6 +164,10 @@ class OfflineSenseVoiceModelAscend::Impl {
     int32_t in_feat_dim = 80;
 
     int32_t in_num_frames = in.size() / in_feat_dim;
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
     int32_t out_num_frames =
         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
 
diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
new file mode 100644
index 00000000..d3628191
--- /dev/null
+++ b/sherpa-onnx/csrc/math.cc
@@ -0,0 +1,57 @@
+// sherpa-onnx/csrc/math.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#include "sherpa-onnx/csrc/math.h"
+
+#include <vector>
+namespace sherpa_onnx {
+
+static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
+  for (int32_t i = 0; i < n; ++i) {
+    in_out[i] += scale * src[i];
+  }
+}
+
+static void Scale(const float *src, float scale, int32_t n, float *out) {
+  for (int32_t i = 0; i < n; ++i) {
+    out[i] = scale * src[i];
+  }
+}
+
+// this if for Paraformer
+std::vector<float> ComputeAcousticEmbedding(
+    const std::vector<float> &encoder_out, const std::vector<float> &alphas,
+    int32_t encoder_dim) {
+  std::vector<float> ans;
+  ans.reserve(encoder_out.size());
+
+  float acc = 0;
+  std::vector<float> cur_emb(encoder_dim);
+  for (int32_t i = 0; i < static_cast<int32_t>(alphas.size()); ++i) {
+    float w = alphas[i];
+
+    acc += w;
+    if (acc >= 1) {
+      float overflow = acc - 1;
+      float remain = w - overflow;
+
+      ScaleAdd(encoder_out.data() + i * encoder_dim, remain, encoder_dim,
+               cur_emb.data());
+
+      ans.insert(ans.end(), cur_emb.begin(), cur_emb.end());
+
+      Scale(encoder_out.data() + i * encoder_dim, overflow, encoder_dim,
+            cur_emb.data());
+
+      acc = overflow;
+    } else {
+      ScaleAdd(encoder_out.data() + i * encoder_dim, w, encoder_dim,
+               cur_emb.data());
+    }
+  }
+  // TODO(fangjun): The last cur_emb is not used
+
+  return ans;
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
index 21fd3803..1edc065d 100644
--- a/sherpa-onnx/csrc/math.h
+++ b/sherpa-onnx/csrc/math.h
@@ -131,5 +131,10 @@ std::vector<int32_t> TopkIndex(const std::vector<std::vector<T>> &vec,
   return TopkIndex(flatten.data(), flatten.size(), topk);
 }
 
+// For Paraformer
+std::vector<float> ComputeAcousticEmbedding(
+    const std::vector<float> &encoder_out, const std::vector<float> &alphas,
+    int32_t encoder_dim);
+
 }  // namespace sherpa_onnx
 #endif  // SHERPA_ONNX_CSRC_MATH_H_
diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
index e71b77bb..e7075bc4 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
@@ -17,8 +17,10 @@ namespace sherpa_onnx {
 void OfflineParaformerModelConfig::Register(ParseOptions *po) {
   po->Register(
       "paraformer", &model,
-      "Path to model.onnx of Paraformer or if you use Ascend NPU, it is "
-      "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om");
+      "Path to model.onnx of Paraformer. If you use Ascend NPU, it is "
+      "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
+      "If you use RK NPU, it is "
+      "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn");
 }
 
 bool OfflineParaformerModelConfig::Validate() const {
@@ -47,7 +49,24 @@ bool OfflineParaformerModelConfig::Validate() const {
     return true;
   }
 
-  SHERPA_ONNX_LOGE("Please pass *.onnx or *.om models. Given '%s'",
+  if (EndsWith(model, ".rknn")) {
+    std::vector<std::string> filenames;
+    SplitStringToVector(model, ",", false, &filenames);
+    if (filenames.size() != 3 || !EndsWith(filenames[0], "encoder.rknn") ||
+        !EndsWith(filenames[1], "predictor.rknn") ||
+        !EndsWith(filenames[2], "decoder.rknn")) {
+      SHERPA_ONNX_LOGE(
+          "For RKNN, you should pass "
+          "/path/encoder.rknn,/path/predictor.rknn,/path/decoder.rknn. "
+          "Given '%s'",
+          model.c_str());
+      return false;
+    }
+
+    return true;
+  }
+
+  SHERPA_ONNX_LOGE("Please pass *.onnx, *.om, or *.rknn models. Given '%s'",
                    model.c_str());
   return false;
 }
diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.h b/sherpa-onnx/csrc/offline-paraformer-model-config.h
index 1bb822b8..e4fa94d8 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model-config.h
+++ b/sherpa-onnx/csrc/offline-paraformer-model-config.h
@@ -13,6 +13,10 @@ namespace sherpa_onnx {
 struct OfflineParaformerModelConfig {
   // for ascend npu,
   // model is "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
+  //
+  // for rknn,
+  // model is
+  // "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn"
   std::string model;
 
   OfflineParaformerModelConfig() = default;
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 318be0e0..2ff1a4a7 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -37,6 +37,7 @@
 #include "sherpa-onnx/csrc/text-utils.h"
 
 #if SHERPA_ONNX_ENABLE_RKNN
+#include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
 #include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
 #endif
 
@@ -57,14 +58,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     const OfflineRecognizerConfig &config) {
   if (config.model_config.provider == "rknn") {
 #if SHERPA_ONNX_ENABLE_RKNN
-    if (config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
+    } else if (!config.model_config.paraformer.model.empty()) {
+      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
+    } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
+          "Only SenseVoice and Paraformer models are currently supported "
           "by rknn for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
-    } else if (!config.model_config.sense_voice.model.empty()) {
-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -317,12 +320,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     Manager *mgr, const OfflineRecognizerConfig &config) {
   if (config.model_config.provider == "rknn") {
 #if SHERPA_ONNX_ENABLE_RKNN
-    if (config.model_config.sense_voice.model.empty()) {
-      SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
-          "by rknn for non-streaming ASR. Fallback to CPU");
-    } else if (!config.model_config.sense_voice.model.empty()) {
+    if (!config.model_config.sense_voice.model.empty()) {
       return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(mgr, config);
+    } else if (!config.model_config.paraformer.model.empty()) {
+      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice and Paraformer models are currently supported "
+          "by rknn for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -348,7 +355,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
-          "supported by Ascend NPU for non-streaming ASR. Fallback to CPU");
+          "supported by Ascend NPU for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -666,8 +675,8 @@ OfflineRecognizerImpl::OfflineRecognizerImpl(
         itn_list_.push_back(
             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
       }  // for (; !reader->Done(); reader->Next())
-    }  // for (const auto &f : files)
-  }  // if (!config.rule_fars.empty())
+    }    // for (const auto &f : files)
+  }      // if (!config.rule_fars.empty())
 
   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
     auto hr_config = config.hr;
diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
index 22e0a6ed..a8185cd4 100644
--- a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
@@ -76,7 +76,7 @@ class ContextBlockingQueueRknn::Impl {
 
 ContextBlockingQueueRknn::ContextBlockingQueueRknn(rknn_context context,
                                                    int32_t num_threads,
-                                                   int32_t capacity /*= 20*/)
+                                                   int32_t capacity /*= 10*/)
     : impl_(std::make_unique<Impl>(context, num_threads, capacity)) {}
 
 ContextBlockingQueueRknn::~ContextBlockingQueueRknn() = default;
diff --git a/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
new file mode 100644
index 00000000..c23460e1
--- /dev/null
+++ b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
@@ -0,0 +1,406 @@
+// sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/math.h"
+#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
+#include "sherpa-onnx/csrc/rknn/macros.h"
+#include "sherpa-onnx/csrc/rknn/utils.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+class OfflineParaformerModelRknn::Impl {
+ public:
+  ~Impl() {
+    auto ret = rknn_destroy(encoder_ctx_);
+    if (ret != RKNN_SUCC) {
+      SHERPA_ONNX_LOGE("Failed to destroy the encoder context");
+    }
+
+    ret = rknn_destroy(predictor_ctx_);
+    if (ret != RKNN_SUCC) {
+      SHERPA_ONNX_LOGE("Failed to destroy the predictor context");
+    }
+
+    ret = rknn_destroy(decoder_ctx_);
+    if (ret != RKNN_SUCC) {
+      SHERPA_ONNX_LOGE("Failed to destroy the decoder context");
+    }
+  }
+
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    std::vector<std::string> filenames;
+    SplitStringToVector(config_.paraformer.model, ",", false, &filenames);
+    if (filenames.size() != 3) {
+      SHERPA_ONNX_LOGE("Invalid Paraformer RK NPU model '%s'",
+                       config_.paraformer.model.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    {
+      auto buf = ReadFile(filenames[0]);
+      InitEncoder(buf.data(), buf.size());
+    }
+
+    {
+      auto buf = ReadFile(filenames[1]);
+      InitPredictor(buf.data(), buf.size());
+    }
+
+    {
+      auto buf = ReadFile(filenames[2]);
+      InitDecoder(buf.data(), buf.size());
+    }
+
+    PostInit();
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    std::vector<std::string> filenames;
+    SplitStringToVector(config_.paraformer.model, ",", false, &filenames);
+    if (filenames.size() != 3) {
+      SHERPA_ONNX_LOGE("Invalid Paraformer RK NPU model '%s'",
+                       config_.paraformer.model.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    {
+      auto buf = ReadFile(mgr, filenames[0]);
+      InitEncoder(buf.data(), buf.size());
+    }
+
+    {
+      auto buf = ReadFile(mgr, filenames[1]);
+      InitPredictor(buf.data(), buf.size());
+    }
+
+    {
+      auto buf = ReadFile(mgr, filenames[2]);
+      InitDecoder(buf.data(), buf.size());
+    }
+
+    PostInit();
+  }
+
+  std::vector<float> Run(std::vector<float> features) {
+    std::vector<float> encoder_out = RunEncoder(features);
+    if (encoder_out.empty()) {
+      return {};
+    }
+
+    std::vector<float> alphas = RunPredictor(encoder_out);
+
+    std::vector<float> acoustic_embedding =
+        ComputeAcousticEmbedding(encoder_out, alphas, encoder_out_dim_);
+    if (acoustic_embedding.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("No speech found in the input audio");
+      }
+
+      return {};
+    }
+
+    int32_t num_tokens = acoustic_embedding.size() / encoder_out_dim_;
+
+    acoustic_embedding.resize(encoder_out.size());
+
+    return RunDecoder(std::move(encoder_out), std::move(acoustic_embedding),
+                      num_tokens);
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+
+ private:
+  std::vector<float> RunEncoder(std::vector<float> features) {
+    features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
+
+    std::vector<rknn_input> inputs(encoder_input_attrs_.size());
+
+    inputs[0].index = encoder_input_attrs_[0].index;
+    inputs[0].type = RKNN_TENSOR_FLOAT32;
+    inputs[0].fmt = encoder_input_attrs_[0].fmt;
+    inputs[0].buf = reinterpret_cast<void *>(features.data());
+    inputs[0].size = features.size() * sizeof(float);
+
+    std::vector<float> out(encoder_output_attrs_[0].n_elems);
+
+    std::vector<rknn_output> outputs(encoder_output_attrs_.size());
+    outputs[0].index = encoder_output_attrs_[0].index;
+    outputs[0].is_prealloc = 1;
+    outputs[0].want_float = 1;
+    outputs[0].size = out.size() * sizeof(float);
+    outputs[0].buf = reinterpret_cast<void *>(out.data());
+
+    rknn_context ctx = encoder_ctx_queue_->Take();
+
+    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set encoder inputs");
+
+    ret = rknn_run(ctx, nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the encoder model");
+
+    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get encoder output");
+
+    encoder_ctx_queue_->Put(ctx);
+
+    return out;
+  }
+
+  std::vector<float> RunPredictor(const std::vector<float> &encoder_out) {
+    std::vector<rknn_input> inputs(predictor_input_attrs_.size());
+
+    inputs[0].index = predictor_input_attrs_[0].index;
+    inputs[0].type = RKNN_TENSOR_FLOAT32;
+    inputs[0].fmt = predictor_input_attrs_[0].fmt;
+    inputs[0].buf =
+        reinterpret_cast<void *>(const_cast<float *>(encoder_out.data()));
+    inputs[0].size = encoder_out.size() * sizeof(float);
+
+    std::vector<float> out(predictor_output_attrs_[0].n_elems);
+
+    std::vector<rknn_output> outputs(predictor_output_attrs_.size());
+    outputs[0].index = predictor_output_attrs_[0].index;
+    outputs[0].is_prealloc = 1;
+    outputs[0].want_float = 1;
+    outputs[0].size = out.size() * sizeof(float);
+    outputs[0].buf = reinterpret_cast<void *>(out.data());
+
+    rknn_context ctx = predictor_ctx_queue_->Take();
+
+    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set predictor inputs");
+
+    ret = rknn_run(ctx, nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the predictor model");
+
+    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get predictor output");
+
+    predictor_ctx_queue_->Put(ctx);
+
+    return out;
+  }
+
+  std::vector<float> RunDecoder(std::vector<float> encoder_out,
+                                std::vector<float> acoustic_embedding,
+                                int32_t num_tokens) {
+    int32_t num_frames = encoder_out.size() / encoder_out_dim_;
+
+    std::vector<rknn_input> inputs(decoder_input_attrs_.size());
+
+    inputs[0].index = decoder_input_attrs_[0].index;
+    inputs[0].type = RKNN_TENSOR_FLOAT32;
+    inputs[0].fmt = decoder_input_attrs_[0].fmt;
+    inputs[0].buf = reinterpret_cast<void *>(encoder_out.data());
+    inputs[0].size = encoder_out.size() * sizeof(float);
+
+    inputs[1].index = decoder_input_attrs_[1].index;
+    inputs[1].type = RKNN_TENSOR_FLOAT32;
+    inputs[1].fmt = decoder_input_attrs_[1].fmt;
+    inputs[1].buf = reinterpret_cast<void *>(acoustic_embedding.data());
+    inputs[1].size = acoustic_embedding.size() * sizeof(float);
+
+    std::vector<float> mask(num_frames, 1);
+    std::fill(mask.begin() + num_tokens, mask.end(), 0);
+
+    inputs[2].index = decoder_input_attrs_[2].index;
+    inputs[2].type = RKNN_TENSOR_FLOAT32;
+    inputs[2].fmt = decoder_input_attrs_[2].fmt;
+    inputs[2].buf = reinterpret_cast<void *>(mask.data());
+    inputs[2].size = mask.size() * sizeof(float);
+
+    std::vector<float> out(decoder_output_attrs_[0].n_elems);
+
+    std::vector<rknn_output> outputs(decoder_output_attrs_.size());
+    outputs[0].index = decoder_output_attrs_[0].index;
+    outputs[0].is_prealloc = 1;
+    outputs[0].want_float = 1;
+    outputs[0].size = out.size() * sizeof(float);
+    outputs[0].buf = reinterpret_cast<void *>(out.data());
+
+    rknn_context ctx = decoder_ctx_queue_->Take();
+
+    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set decoder inputs");
+
+    ret = rknn_run(ctx, nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the decoder model");
+
+    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
+    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get decoder output");
+
+    decoder_ctx_queue_->Put(ctx);
+
+    return out;
+  }
+
+  void InitEncoder(void *model_data, size_t model_data_length) {
+    InitContext(model_data, model_data_length, config_.debug, &encoder_ctx_);
+
+    InitInputOutputAttrs(encoder_ctx_, config_.debug, &encoder_input_attrs_,
+                         &encoder_output_attrs_);
+
+    num_input_frames_ = encoder_input_attrs_[0].dims[1];
+    encoder_out_dim_ = encoder_output_attrs_[0].dims[2];
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("num_input_frames_: %d", num_input_frames_);
+      SHERPA_ONNX_LOGE("encoder_out_dim:: %d", encoder_out_dim_);
+    }
+  }
+
+  void InitPredictor(void *model_data, size_t model_data_length) {
+    InitContext(model_data, model_data_length, config_.debug, &predictor_ctx_);
+
+    InitInputOutputAttrs(predictor_ctx_, config_.debug, &predictor_input_attrs_,
+                         &predictor_output_attrs_);
+  }
+
+  void InitDecoder(void *model_data, size_t model_data_length) {
+    InitContext(model_data, model_data_length, config_.debug, &decoder_ctx_);
+
+    InitInputOutputAttrs(decoder_ctx_, config_.debug, &decoder_input_attrs_,
+                         &decoder_output_attrs_);
+    vocab_size_ = decoder_output_attrs_[0].dims[2];
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
+    }
+  }
+
+  std::vector<float> ApplyLFR(std::vector<float> in) const {
+    int32_t lfr_window_size = 7;
+    int32_t lfr_window_shift = 6;
+    int32_t in_feat_dim = 80;
+
+    int32_t in_num_frames = in.size() / in_feat_dim;
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
+    int32_t out_num_frames =
+        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+
+    if (out_num_frames > num_input_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          out_num_frames, num_input_frames_);
+
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios.");
+
+      out_num_frames = num_input_frames_;
+    }
+
+    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+
+    std::vector<float> out(num_input_frames_ * out_feat_dim);
+
+    const float *p_in = in.data();
+    float *p_out = out.data();
+
+    for (int32_t i = 0; i != out_num_frames; ++i) {
+      std::copy(p_in, p_in + out_feat_dim, p_out);
+
+      p_out += out_feat_dim;
+      p_in += lfr_window_shift * in_feat_dim;
+    }
+
+    return out;
+  }
+
+  void PostInit() {
+    if (config_.num_threads > 1) {
+      config_.num_threads = 1;
+    }
+
+    encoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        encoder_ctx_, config_.num_threads);
+
+    predictor_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        predictor_ctx_, config_.num_threads);
+
+    decoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        decoder_ctx_, config_.num_threads);
+  }
+
+ private:
+  OfflineModelConfig config_;
+
+  rknn_context encoder_ctx_ = 0;
+  rknn_context predictor_ctx_ = 0;
+  rknn_context decoder_ctx_ = 0;
+
+  std::unique_ptr<ContextBlockingQueueRknn> encoder_ctx_queue_;
+  std::unique_ptr<ContextBlockingQueueRknn> predictor_ctx_queue_;
+  std::unique_ptr<ContextBlockingQueueRknn> decoder_ctx_queue_;
+
+  std::vector<rknn_tensor_attr> encoder_input_attrs_;
+  std::vector<rknn_tensor_attr> encoder_output_attrs_;
+
+  std::vector<rknn_tensor_attr> predictor_input_attrs_;
+  std::vector<rknn_tensor_attr> predictor_output_attrs_;
+
+  std::vector<rknn_tensor_attr> decoder_input_attrs_;
+  std::vector<rknn_tensor_attr> decoder_output_attrs_;
+
+  int32_t vocab_size_ = 0;
+  int32_t num_input_frames_ = -1;
+  int32_t encoder_out_dim_ = -1;
+};
+
+OfflineParaformerModelRknn::~OfflineParaformerModelRknn() = default;
+
+OfflineParaformerModelRknn::OfflineParaformerModelRknn(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineParaformerModelRknn::OfflineParaformerModelRknn(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+std::vector<float> OfflineParaformerModelRknn::Run(
+    std::vector<float> features) const {
+  return impl_->Run(std::move(features));
+}
+
+int32_t OfflineParaformerModelRknn::VocabSize() const {
+  return impl_->VocabSize();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineParaformerModelRknn::OfflineParaformerModelRknn(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineParaformerModelRknn::OfflineParaformerModelRknn(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
new file mode 100644
index 00000000..17eeb584
--- /dev/null
+++ b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
@@ -0,0 +1,40 @@
+// sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
+#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineParaformerModelRknn {
+ public:
+  ~OfflineParaformerModelRknn();
+
+  explicit OfflineParaformerModelRknn(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineParaformerModelRknn(Manager *mgr, const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (num_frames, feature_dim)
+   *                 before applying LFR.
+   * @returns Return a tensor of shape (num_output_frames, vocab_size)
+   */
+  std::vector<float> Run(std::vector<float> features) const;
+
+  int32_t VocabSize() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
new file mode 100644
index 00000000..28fc17a6
--- /dev/null
+++ b/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
@@ -0,0 +1,121 @@
+// sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../offline-recognizer-paraformer-impl.h
+OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
+                                 const SymbolTable &sym_table);
+
+class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerParaformerRknnImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(
+            std::make_unique<OfflineParaformerModelRknn>(config.model_config)) {
+    if (config.decoding_method != "greedy_search") {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerParaformerRknnImpl(Manager *mgr,
+                                      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineParaformerModelRknn>(
+            mgr, config.model_config)) {
+    if (config.decoding_method != "greedy_search") {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i < n; ++i) {
+      DecodeOneStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void InitFeatConfig() {
+    config_.feat_config.normalize_samples = false;
+    config_.feat_config.window_type = "hamming";
+    config_.feat_config.high_freq = 0;
+    config_.feat_config.snip_edges = true;
+  }
+
+  void DecodeOneStream(OfflineStream *s) const {
+    std::vector<float> f = s->GetFrames();
+
+    std::vector<float> logits = model_->Run(std::move(f));
+    if (logits.empty()) {
+      SHERPA_ONNX_LOGE("No speech detected");
+      return;
+    }
+
+    int32_t vocab_size = model_->VocabSize();
+    int32_t num_tokens = logits.size() / vocab_size;
+
+    int32_t eos_id = symbol_table_["</s>"];
+
+    OfflineParaformerDecoderResult r;
+    const float *p = logits.data();
+    for (int32_t i = 0; i < num_tokens; ++i) {
+      auto max_idx = static_cast<int64_t>(
+          std::distance(p, std::max_element(p, p + vocab_size)));
+
+      if (max_idx == eos_id) {
+        break;
+      }
+      r.tokens.push_back(max_idx);
+      p += vocab_size;
+    }
+
+    auto result = Convert(r, symbol_table_);
+    result.text = ApplyInverseTextNormalization(std::move(result.text));
+    result.text = ApplyHomophoneReplacer(std::move(result.text));
+    s->SetResult(result);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineParaformerModelRknn> model_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
index 7236b6b8..8daccec0 100644
--- a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+++ b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
@@ -111,6 +111,10 @@ class OfflineRecognizerSenseVoiceRknnImpl : public OfflineRecognizerImpl {
                             : meta_data.without_itn_id;
 
     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    if (logits.empty()) {
+      return;
+    }
+
     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
 
     auto result =
diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
index 0c485c69..805ba7ab 100644
--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
@@ -6,6 +6,7 @@
 
 #include <algorithm>
 #include <array>
+#include <memory>
 #include <utility>
 #include <vector>
 
@@ -56,6 +57,9 @@ class OfflineSenseVoiceModelRknn::Impl {
   std::vector<float> Run(std::vector<float> features, int32_t language,
                          int32_t text_norm) {
     features = ApplyLFR(std::move(features));
+    if (features.empty()) {
+      return {};
+    }
 
     std::vector<rknn_input> inputs(input_attrs_.size());
 
@@ -160,6 +164,11 @@ class OfflineSenseVoiceModelRknn::Impl {
     int32_t in_feat_dim = 80;
 
     int32_t in_num_frames = in.size() / in_feat_dim;
+
+    if (in_num_frames < lfr_window_size) {
+      return {};
+    }
+
     int32_t out_num_frames =
         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
 
diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
index ddbc86b1..7b8a86b1 100644
--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
+++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
@@ -8,7 +8,6 @@
 #include <utility>
 #include <vector>
 
-#include "rknn_api.h"  // NOLINT
 #include "sherpa-onnx/csrc/offline-model-config.h"
 #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
 

commit ae9f8700767e257e9f730c8679b1617d05296981
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Nov 26 12:05:32 2025 +0800

    Avoid calling rknn_dup_context(). (#2828)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index c7873d76..6ad708f8 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -130,6 +130,7 @@ set(sources
   ten-vad-model-config.cc
   ten-vad-model.cc
   text-utils.cc
+  timer.cc
   transducer-keyword-decoder.cc
   transpose.cc
   unbind.cc
@@ -179,6 +180,7 @@ list(APPEND sources
 )
 if(SHERPA_ONNX_ENABLE_RKNN)
   list(APPEND sources
+    ./rknn/context-blocking-queue-rknn.cc
     ./rknn/offline-sense-voice-model-rknn.cc
     ./rknn/online-stream-rknn.cc
     ./rknn/online-transducer-greedy-search-decoder-rknn.cc
diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
new file mode 100644
index 00000000..22e0a6ed
--- /dev/null
+++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
@@ -0,0 +1,90 @@
+// sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+//
+// Copyright      2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
+
+#include <condition_variable>
+#include <mutex>
+#include <queue>
+
+#include "sherpa-onnx/csrc/rknn/macros.h"
+#include "sherpa-onnx/csrc/rknn/utils.h"
+
+namespace sherpa_onnx {
+
+class ContextBlockingQueueRknn::Impl {
+ public:
+  Impl(rknn_context context, int32_t num_threads, int32_t capacity) {
+    for (int32_t i = 0; i < capacity; ++i) {
+      rknn_context bak = 0;
+      auto ret = rknn_dup_context(&context, &bak);
+      SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate context");
+
+      SetCoreMask(bak, num_threads);
+      queue_.push(bak);
+    }
+  }
+  rknn_context Take() {
+    std::unique_lock<std::mutex> lock(mutex_);
+
+    cv_.wait(lock, [&] { return stopped_ || !queue_.empty(); });
+
+    if (stopped_ && queue_.empty()) {
+      return 0;
+    }
+
+    rknn_context ctx = queue_.front();
+    queue_.pop();
+    return ctx;
+  }
+
+  void Put(rknn_context ctx) {
+    {
+      std::lock_guard<std::mutex> lock(mutex_);
+      if (stopped_) {
+        rknn_destroy(ctx);
+        return;
+      }
+      queue_.push(ctx);
+    }
+    cv_.notify_one();
+  }
+
+  ~Impl() {
+    {
+      std::lock_guard<std::mutex> lock(mutex_);
+      stopped_ = true;
+    }
+    cv_.notify_all();
+    Cleanup();
+  }
+
+ private:
+  void Cleanup() {
+    while (!queue_.empty()) {
+      rknn_destroy(queue_.front());
+      queue_.pop();
+    }
+  }
+
+  std::queue<rknn_context> queue_;
+  std::mutex mutex_;
+  std::condition_variable cv_;
+  bool stopped_ = false;
+};
+
+ContextBlockingQueueRknn::ContextBlockingQueueRknn(rknn_context context,
+                                                   int32_t num_threads,
+                                                   int32_t capacity /*= 20*/)
+    : impl_(std::make_unique<Impl>(context, num_threads, capacity)) {}
+
+ContextBlockingQueueRknn::~ContextBlockingQueueRknn() = default;
+
+rknn_context ContextBlockingQueueRknn::Take() { return impl_->Take(); }
+
+void ContextBlockingQueueRknn::Put(rknn_context context) {
+  impl_->Put(context);
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
new file mode 100644
index 00000000..c5651e7b
--- /dev/null
+++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
@@ -0,0 +1,29 @@
+// sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
+//
+// Copyright      2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
+#define SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
+
+#include <memory>
+
+#include "rknn_api.h"  // NOLINT
+
+namespace sherpa_onnx {
+
+class ContextBlockingQueueRknn {
+ public:
+  ContextBlockingQueueRknn(rknn_context context, int32_t num_threads,
+                           int32_t capacity = 10);
+  ~ContextBlockingQueueRknn();
+
+  rknn_context Take();
+  void Put(rknn_context context);
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
index f2cbfcaa..0c485c69 100644
--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
@@ -19,6 +19,7 @@
 #endif
 
 #include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
 #include "sherpa-onnx/csrc/rknn/macros.h"
 #include "sherpa-onnx/csrc/rknn/utils.h"
 
@@ -34,18 +35,18 @@ class OfflineSenseVoiceModelRknn::Impl {
   }
 
   explicit Impl(const OfflineModelConfig &config) : config_(config) {
-    {
-      auto buf = ReadFile(config_.sense_voice.model);
-      Init(buf.data(), buf.size());
-    }
+    auto buf = ReadFile(config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+
+    PostInit();
   }
 
   template <typename Manager>
   Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
-    {
-      auto buf = ReadFile(mgr, config_.sense_voice.model);
-      Init(buf.data(), buf.size());
-    }
+    auto buf = ReadFile(mgr, config_.sense_voice.model);
+    Init(buf.data(), buf.size());
+
+    PostInit();
   }
 
   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
@@ -81,13 +82,9 @@ class OfflineSenseVoiceModelRknn::Impl {
     outputs[0].size = out.size() * sizeof(float);
     outputs[0].buf = reinterpret_cast<void *>(out.data());
 
-    rknn_context ctx = 0;
-    auto ret = rknn_dup_context(&ctx_, &ctx);
-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the ctx");
+    rknn_context ctx = ctx_queue_->Take();
 
-    SetCoreMask(ctx, config_.num_threads);
-
-    ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set inputs");
 
     ret = rknn_run(ctx, nullptr);
@@ -96,7 +93,7 @@ class OfflineSenseVoiceModelRknn::Impl {
     ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get model output");
 
-    rknn_destroy(ctx);
+    ctx_queue_->Put(ctx);
 
     return out;
   }
@@ -195,10 +192,16 @@ class OfflineSenseVoiceModelRknn::Impl {
     return out;
   }
 
+  void PostInit() {
+    ctx_queue_ =
+        std::make_unique<ContextBlockingQueueRknn>(ctx_, config_.num_threads);
+  }
+
  private:
   OfflineModelConfig config_;
 
   rknn_context ctx_ = 0;
+  std::unique_ptr<ContextBlockingQueueRknn> ctx_queue_;
 
   std::vector<rknn_tensor_attr> input_attrs_;
   std::vector<rknn_tensor_attr> output_attrs_;
diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
index cc83a3ef..f488f907 100644
--- a/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
@@ -21,6 +21,7 @@
 #endif
 
 #include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
 #include "sherpa-onnx/csrc/rknn/macros.h"
 #include "sherpa-onnx/csrc/rknn/utils.h"
 #include "sherpa-onnx/csrc/text-utils.h"
@@ -37,18 +38,18 @@ class OnlineZipformerCtcModelRknn::Impl {
   }
 
   explicit Impl(const OnlineModelConfig &config) : config_(config) {
-    {
-      auto buf = ReadFile(config.zipformer2_ctc.model);
-      Init(buf.data(), buf.size());
-    }
+    auto buf = ReadFile(config.zipformer2_ctc.model);
+    Init(buf.data(), buf.size());
+
+    PostInit();
   }
 
   template <typename Manager>
   Impl(Manager *mgr, const OnlineModelConfig &config) : config_(config) {
-    {
-      auto buf = ReadFile(mgr, config.zipformer2_ctc.model);
-      Init(buf.data(), buf.size());
-    }
+    auto buf = ReadFile(mgr, config.zipformer2_ctc.model);
+    Init(buf.data(), buf.size());
+
+    PostInit();
   }
 
   std::vector<std::vector<uint8_t>> GetInitStates() const {
@@ -140,13 +141,9 @@ class OnlineZipformerCtcModelRknn::Impl {
       }
     }
 
-    rknn_context ctx = 0;
-    auto ret = rknn_dup_context(&ctx_, &ctx);
-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the ctx");
+    rknn_context ctx = ctx_queue_->Take();
 
-    SetCoreMask(ctx, config_.num_threads);
-
-    ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set inputs");
 
     ret = rknn_run(ctx, nullptr);
@@ -173,7 +170,7 @@ class OnlineZipformerCtcModelRknn::Impl {
       }
     }
 
-    rknn_destroy(ctx);
+    ctx_queue_->Put(ctx);
 
     return {std::move(out), std::move(next_states)};
   }
@@ -232,9 +229,15 @@ class OnlineZipformerCtcModelRknn::Impl {
     }
   }
 
+  void PostInit() {
+    ctx_queue_ =
+        std::make_unique<ContextBlockingQueueRknn>(ctx_, config_.num_threads);
+  }
+
  private:
   OnlineModelConfig config_;
   rknn_context ctx_ = 0;
+  std::unique_ptr<ContextBlockingQueueRknn> ctx_queue_;
 
   std::vector<rknn_tensor_attr> input_attrs_;
   std::vector<rknn_tensor_attr> output_attrs_;
diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
index 2a13deb8..e70a3c54 100644
--- a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
@@ -21,6 +21,7 @@
 #endif
 
 #include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
 #include "sherpa-onnx/csrc/rknn/macros.h"
 #include "sherpa-onnx/csrc/rknn/utils.h"
 #include "sherpa-onnx/csrc/text-utils.h"
@@ -61,6 +62,8 @@ class OnlineZipformerTransducerModelRknn::Impl {
       auto buf = ReadFile(config.transducer.joiner);
       InitJoiner(buf.data(), buf.size());
     }
+
+    PostInit();
   }
 
   template <typename Manager>
@@ -79,6 +82,8 @@ class OnlineZipformerTransducerModelRknn::Impl {
       auto buf = ReadFile(mgr, config.transducer.joiner);
       InitJoiner(buf.data(), buf.size());
     }
+
+    PostInit();
   }
 
   std::vector<std::vector<uint8_t>> GetEncoderInitStates() const {
@@ -170,19 +175,13 @@ class OnlineZipformerTransducerModelRknn::Impl {
       }
     }
 
-    rknn_context encoder_ctx = 0;
-
-    // https://github.com/rockchip-linux/rknpu2/blob/master/runtime/RK3588/Linux/librknn_api/include/rknn_api.h#L444C1-L444C75
-    // rknn_dup_context(rknn_context* context_in, rknn_context* context_out);
-    auto ret = rknn_dup_context(&encoder_ctx_, &encoder_ctx);
-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the encoder ctx");
+    rknn_context encoder_ctx = encoder_ctx_queue_->Take();
 
-    SetCoreMask(encoder_ctx, config_.num_threads);
-
-    ret = rknn_inputs_set(encoder_ctx, inputs.size(), inputs.data());
+    auto ret = rknn_inputs_set(encoder_ctx, inputs.size(), inputs.data());
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set encoder inputs");
 
     ret = rknn_run(encoder_ctx, nullptr);
+
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run encoder");
 
     ret =
@@ -207,7 +206,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
       }
     }
 
-    rknn_destroy(encoder_ctx);
+    encoder_ctx_queue_->Put(encoder_ctx);
 
     return {std::move(encoder_out), std::move(next_states)};
   }
@@ -230,13 +229,9 @@ class OnlineZipformerTransducerModelRknn::Impl {
     output.size = decoder_out.size() * sizeof(float);
     output.buf = decoder_out.data();
 
-    rknn_context decoder_ctx = 0;
-    auto ret = rknn_dup_context(&decoder_ctx_, &decoder_ctx);
-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the decoder ctx");
+    rknn_context decoder_ctx = decoder_ctx_queue_->Take();
 
-    SetCoreMask(decoder_ctx, config_.num_threads);
-
-    ret = rknn_inputs_set(decoder_ctx, 1, &input);
+    auto ret = rknn_inputs_set(decoder_ctx, 1, &input);
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set decoder inputs");
 
     ret = rknn_run(decoder_ctx, nullptr);
@@ -245,7 +240,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
     ret = rknn_outputs_get(decoder_ctx, 1, &output, nullptr);
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get decoder output");
 
-    rknn_destroy(decoder_ctx);
+    decoder_ctx_queue_->Put(decoder_ctx);
 
     return decoder_out;
   }
@@ -273,13 +268,9 @@ class OnlineZipformerTransducerModelRknn::Impl {
     output.size = joiner_out.size() * sizeof(float);
     output.buf = joiner_out.data();
 
-    rknn_context joiner_ctx = 0;
-    auto ret = rknn_dup_context(&joiner_ctx_, &joiner_ctx);
-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the joiner ctx");
-
-    SetCoreMask(joiner_ctx, config_.num_threads);
+    rknn_context joiner_ctx = joiner_ctx_queue_->Take();
 
-    ret = rknn_inputs_set(joiner_ctx, inputs.size(), inputs.data());
+    auto ret = rknn_inputs_set(joiner_ctx, inputs.size(), inputs.data());
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set joiner inputs");
 
     ret = rknn_run(joiner_ctx, nullptr);
@@ -288,7 +279,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
     ret = rknn_outputs_get(joiner_ctx, 1, &output, nullptr);
     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get joiner output");
 
-    rknn_destroy(joiner_ctx);
+    joiner_ctx_queue_->Put(joiner_ctx);
 
     return joiner_out;
   }
@@ -413,12 +404,25 @@ class OnlineZipformerTransducerModelRknn::Impl {
     }
   }
 
+  void PostInit() {
+    encoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        encoder_ctx_, config_.num_threads);
+    decoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        decoder_ctx_, config_.num_threads);
+    joiner_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
+        joiner_ctx_, config_.num_threads);
+  }
+
  private:
   OnlineModelConfig config_;
   rknn_context encoder_ctx_ = 0;
   rknn_context decoder_ctx_ = 0;
   rknn_context joiner_ctx_ = 0;
 
+  std::unique_ptr<ContextBlockingQueueRknn> encoder_ctx_queue_;
+  std::unique_ptr<ContextBlockingQueueRknn> decoder_ctx_queue_;
+  std::unique_ptr<ContextBlockingQueueRknn> joiner_ctx_queue_;
+
   std::vector<rknn_tensor_attr> encoder_input_attrs_;
   std::vector<rknn_tensor_attr> encoder_output_attrs_;
 
diff --git a/sherpa-onnx/csrc/sherpa-onnx.cc b/sherpa-onnx/csrc/sherpa-onnx.cc
index 14783fd8..1712e833 100644
--- a/sherpa-onnx/csrc/sherpa-onnx.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx.cc
@@ -16,6 +16,7 @@
 #include "sherpa-onnx/csrc/online-stream.h"
 #include "sherpa-onnx/csrc/parse-options.h"
 #include "sherpa-onnx/csrc/symbol-table.h"
+#include "sherpa-onnx/csrc/timer.h"
 #include "sherpa-onnx/csrc/wave-reader.h"
 
 typedef struct {
@@ -96,7 +97,10 @@ for a list of pre-trained models to download.
     return -1;
   }
 
+  printf("Start to create recognizer\n");
+  sherpa_onnx::Timer timer;
   sherpa_onnx::OnlineRecognizer recognizer(config);
+  printf("Recognizer created in %.5f s\n", timer.Elapsed());
 
   std::vector<Stream> ss;
 
diff --git a/sherpa-onnx/csrc/timer.cc b/sherpa-onnx/csrc/timer.cc
new file mode 100644
index 00000000..e24a492c
--- /dev/null
+++ b/sherpa-onnx/csrc/timer.cc
@@ -0,0 +1,42 @@
+// sherpa-onnx/csrc/timer.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/timer.h"
+
+#include <chrono>
+#include <memory>
+
+namespace sherpa_onnx {
+
+// modified from https://github.com/kaldi-asr/kaldi/blob/master/src/base/timer.h
+class Timer::Impl {
+ public:
+  Impl() { Reset(); }
+
+  using high_resolution_clock = std::chrono::high_resolution_clock;
+
+  void Reset() { begin_ = high_resolution_clock::now(); }
+
+  // Return time in seconds
+  double Elapsed() {
+    auto end = high_resolution_clock::now();
+    auto diff =
+        std::chrono::duration_cast<std::chrono::microseconds>(end - begin_);
+    return diff.count() / 1000000.0;
+  }
+
+ private:
+  high_resolution_clock::time_point begin_;
+};
+
+Timer::Timer() : impl_(std::make_unique<Impl>()) {}
+
+Timer::~Timer() = default;
+
+void Timer::Reset() const { impl_->Reset(); }
+
+// Return time in seconds
+double Timer::Elapsed() const { return impl_->Elapsed(); }
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/timer.h b/sherpa-onnx/csrc/timer.h
new file mode 100644
index 00000000..60d87883
--- /dev/null
+++ b/sherpa-onnx/csrc/timer.h
@@ -0,0 +1,29 @@
+// sherpa-onnx/csrc/timer.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_TIMER_H_
+#define SHERPA_ONNX_CSRC_TIMER_H_
+
+#include <memory>
+
+namespace sherpa_onnx {
+
+class Timer {
+ public:
+  Timer();
+  ~Timer();
+
+  void Reset() const;
+
+  // Return time in seconds
+  double Elapsed() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_TIMER_H_

commit 0a7ec6edca222c93bbceb31d6ca3e20a262488a8
Author: Joe Cheng <joe@posit.co>
Date:   Tue Nov 25 17:21:13 2025 -0800

    Fix segfault when non-wav file is passed to ReadWave (#2821)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index c76d6859..c7873d76 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -717,6 +717,7 @@ if(SHERPA_ONNX_ENABLE_TESTS)
     transpose-test.cc
     unbind-test.cc
     utfcpp-test.cc
+    wave-reader-test.cc
   )
   if(SHERPA_ONNX_ENABLE_TTS)
     list(APPEND sherpa_onnx_test_srcs
diff --git a/sherpa-onnx/csrc/wave-reader-test.cc b/sherpa-onnx/csrc/wave-reader-test.cc
new file mode 100644
index 00000000..286aab06
--- /dev/null
+++ b/sherpa-onnx/csrc/wave-reader-test.cc
@@ -0,0 +1,134 @@
+// sherpa-onnx/csrc/wave-reader-test.cc
+//
+// Copyright (c)  2025  Posit Software, PBC
+
+#include "sherpa-onnx/csrc/wave-reader.h"
+
+#include <cstdio>
+#include <fstream>
+#include <string>
+
+#if defined(_WIN32)
+#include <windows.h>
+#else
+#include <unistd.h>
+#endif
+
+#include "gtest/gtest.h"
+
+namespace sherpa_onnx {
+
+// RAII helper class for managing temporary test files
+class TempFile {
+ public:
+  TempFile() : TempFile("") {}
+
+  explicit TempFile(const std::string& suffix) {
+#if defined(_WIN32)
+    char temp_path[MAX_PATH];
+    char temp_file[MAX_PATH];
+    GetTempPathA(MAX_PATH, temp_path);
+    GetTempFileNameA(temp_path, "sot", 0, temp_file);
+    path_ = temp_file;
+    if (!suffix.empty()) {
+      path_ += suffix;
+      std::remove(temp_file);  // Remove the file without suffix
+    }
+#else
+    char temp_template[] = "/tmp/sherpa_onnx_test_XXXXXX";
+    int fd = mkstemp(temp_template);
+    if (fd != -1) {
+      close(fd);
+      path_ = temp_template;
+      if (!suffix.empty()) {
+        path_ += suffix;
+        std::remove(temp_template);  // Remove the file without suffix
+      }
+    }
+#endif
+  }
+
+  ~TempFile() {
+    if (!path_.empty()) {
+      std::remove(path_.c_str());
+    }
+  }
+
+  const char* path() const { return path_.c_str(); }
+
+ private:
+  std::string path_;
+};
+
+TEST(WaveReader, TestNonWavFile) {
+  // Create a temporary file with non-WAV content (e.g., webm-like header)
+  TempFile temp_file(".webm");
+
+  {
+    std::ofstream out(temp_file.path(), std::ios::binary);
+    // Write some content that doesn't start with RIFF
+    // (webm files typically start with EBML header: 0x1a45dfa3)
+    const unsigned char webm_header[] = {
+        0x1a, 0x45, 0xdf, 0xa3,  // EBML header signature (NOT RIFF)
+        0x01, 0x00, 0x00, 0x00,
+        0x00, 0x00, 0x00, 0x1f,
+        0x42, 0x86, 0x81, 0x01,
+        // Add some more bytes to make it look like a real file
+        0x42, 0xf7, 0x81, 0x01,
+        0x42, 0xf2, 0x81, 0x04,
+        'w', 'e', 'b', 'm'
+    };
+    out.write(reinterpret_cast<const char*>(webm_header), sizeof(webm_header));
+  }
+
+  // Test C++ API - should not segfault
+  int32_t sample_rate = -1;
+  bool is_ok = false;
+  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
+
+  EXPECT_FALSE(is_ok);
+  EXPECT_TRUE(samples.empty());
+  EXPECT_EQ(sample_rate, -1);
+}
+
+TEST(WaveReader, TestNonExistentFile) {
+  // Generate a unique path but don't create the file
+  TempFile temp_file(".wav");
+
+  // Test C++ API - should not segfault
+  int32_t sample_rate = -1;
+  bool is_ok = false;
+  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
+
+  EXPECT_FALSE(is_ok);
+  EXPECT_TRUE(samples.empty());
+  EXPECT_EQ(sample_rate, -1);
+}
+
+TEST(WaveReader, TestTruncatedWaveFile) {
+  // Create a temporary file with truncated WAV header
+  TempFile temp_file(".wav");
+
+  {
+    std::ofstream out(temp_file.path(), std::ios::binary);
+    // Write only partial WAV header (less than 44 bytes required)
+    const unsigned char partial_wav[] = {
+        'R', 'I', 'F', 'F',  // chunk_id
+        0x00, 0x00, 0x00, 0x00,  // chunk_size
+        'W', 'A', 'V', 'E'  // format
+        // Missing the rest of the header
+    };
+    out.write(reinterpret_cast<const char*>(partial_wav), sizeof(partial_wav));
+  }
+
+  // Test C++ API - should not segfault
+  int32_t sample_rate = -1;
+  bool is_ok = false;
+  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
+
+  EXPECT_FALSE(is_ok);
+  EXPECT_TRUE(samples.empty());
+  EXPECT_EQ(sample_rate, -1);
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/wave-reader.cc b/sherpa-onnx/csrc/wave-reader.cc
index 56fa2718..4f2932a0 100644
--- a/sherpa-onnx/csrc/wave-reader.cc
+++ b/sherpa-onnx/csrc/wave-reader.cc
@@ -344,6 +344,10 @@ std::vector<float> ReadWave(std::istream &is, int32_t *sampling_rate,
                             bool *is_ok) {
   auto samples = ReadWaveImpl(is, sampling_rate, is_ok);
 
+  if (!*is_ok || samples.empty()) {
+    return {};
+  }
+
   if (samples.size() > 1) {
     SHERPA_ONNX_LOGE(
         "Warning: %d channels are found. We only use the first channel.\n",

commit ca160f14d6ad7171a43586a8a0644e4e98cba0f8
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 25 18:29:56 2025 +0800

    Add C++ support for Zipformer CTC on Ascend NPU (#2826)
    
    This pull request significantly enhances the sherpa-onnx library by integrating C++ support for Zipformer CTC models on Huawei Ascend NPUs. This allows for efficient, hardware-accelerated inference of Zipformer CTC-based automatic speech recognition tasks, providing users with more options for deploying high-performance ASR solutions on Ascend platforms. The changes involve adding new model and recognizer implementations, updating the build system, and extending the recognizer factory to handle the new model type.

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index ef480bdd..c76d6859 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -202,6 +202,7 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   list(APPEND sources
     ./ascend/offline-paraformer-model-ascend.cc
     ./ascend/offline-sense-voice-model-ascend.cc
+    ./ascend/offline-zipformer-ctc-model-ascend.cc
     ./ascend/utils.cc
   )
 endif()
diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
new file mode 100644
index 00000000..b289d0e9
--- /dev/null
+++ b/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
@@ -0,0 +1,130 @@
+// sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
+#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
+
+#include <ios>
+#include <memory>
+#include <sstream>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../offline-recognizer-ctc-impl.h
+OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+                                 const SymbolTable &sym_table,
+                                 int32_t frame_shift_ms,
+                                 int32_t subsampling_factor);
+
+class OfflineRecognizerZipformerCtcAscendImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerZipformerCtcAscendImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(std::make_unique<OfflineZipformerCtcModelAscend>(
+            config.model_config)) {
+    Init();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerZipformerCtcAscendImpl(Manager *mgr,
+                                          const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineZipformerCtcModelAscend>(
+            mgr, config.model_config)) {
+    Init();
+  }
+
+  void Init() {
+    if (config_.decoding_method == "greedy_search") {
+      if (!symbol_table_.Contains("<blk>") &&
+          !symbol_table_.Contains("<eps>") &&
+          !symbol_table_.Contains("<blank>") &&
+          config_.model_config.omnilingual.model.empty()) {
+        // for omnilingual asr, its blank id is 0
+        SHERPA_ONNX_LOGE(
+            "We expect that tokens.txt contains "
+            "the symbol <blk> or <eps> or <blank> and its ID.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      int32_t blank_id = 0;
+      if (symbol_table_.Contains("<blk>")) {
+        blank_id = symbol_table_["<blk>"];
+      } else if (symbol_table_.Contains("<eps>")) {
+        // for tdnn models of the yesno recipe from icefall
+        blank_id = symbol_table_["<eps>"];
+      } else if (symbol_table_.Contains("<blank>")) {
+        // for Wenet CTC models
+        blank_id = symbol_table_["<blank>"];
+      }
+
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config_.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i != n; ++i) {
+      DecodeStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  // Decode a single stream.
+  // Some models do not support batch size > 1, e.g., WeNet CTC models.
+  void DecodeStream(OfflineStream *s) const {
+    std::vector<float> f = s->GetFrames();
+
+    int32_t vocab_size = model_->VocabSize();
+
+    std::vector<float> log_probs = model_->Run(std::move(f));
+    int32_t num_out_frames = log_probs.size() / vocab_size;
+
+    auto result =
+        decoder_->Decode(log_probs.data(), num_out_frames, vocab_size);
+
+    int32_t frame_shift_ms = 10;
+
+    auto r = Convert(result, symbol_table_, frame_shift_ms,
+                     model_->SubsamplingFactor());
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineZipformerCtcModelAscend> model_;
+  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
diff --git a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
new file mode 100644
index 00000000..44547d3b
--- /dev/null
+++ b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
@@ -0,0 +1,216 @@
+// sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+// References:
+// https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/83RC1alpha003/API/appdevgapi/aclcppdevg_03_0298.html
+#include "sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <mutex>  // NOLINT
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/ascend/macros.h"
+#include "sherpa-onnx/csrc/ascend/utils.h"
+#include "sherpa-onnx/csrc/file-utils.h"
+
+namespace sherpa_onnx {
+
+class OfflineZipformerCtcModelAscend::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    PreInit();
+    InitModel(config_.zipformer_ctc.model);
+    PostInit();
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    PreInit();
+    {
+      auto buf = ReadFile(mgr, config_.zipformer_ctc.model);
+      InitModel(buf.data(), buf.size());
+    }
+    PostInit();
+  }
+
+  std::vector<float> Run(std::vector<float> features) {
+    // TODO(fangjun): Support multi clients
+    std::lock_guard<std::mutex> lock(mutex_);
+
+    int32_t num_frames = features.size() / feat_dim_;
+
+    if (num_frames != max_num_frames_) {
+      if (num_frames > max_num_frames_) {
+        SHERPA_ONNX_LOGE(
+            "Number of input frames %d is too large. Truncate it to %d frames.",
+            num_frames, max_num_frames_);
+
+        SHERPA_ONNX_LOGE(
+            "Recognition result may be truncated/incomplete. Please select a "
+            "model accepting longer audios.");
+      }
+
+      features.resize(max_num_frames_ * feat_dim_);
+
+      num_frames = max_num_frames_;
+    }
+
+    aclError ret =
+        aclrtMemcpy(*x_ptr_, features.size() * sizeof(float), features.data(),
+                    features.size() * sizeof(float), ACL_MEMCPY_HOST_TO_DEVICE);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    AclMdlDataset input_dataset;
+    AclDataBuffer x_buf(*x_ptr_, features.size() * sizeof(float));
+    input_dataset.AddBuffer(x_buf);
+
+    AclMdlDataset output_dataset;
+
+    AclDataBuffer logits_buf(*log_probs_ptr_,
+                             num_output_frames_ * vocab_size_ * sizeof(float));
+    output_dataset.AddBuffer(logits_buf);
+
+    ret = aclmdlExecute(*model_, input_dataset, output_dataset);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclmdlExecute");
+
+    std::vector<float> log_probs(num_output_frames_ * vocab_size_);
+    ret = aclrtMemcpy(
+        log_probs.data(), num_output_frames_ * vocab_size_ * sizeof(float),
+        *log_probs_ptr_, num_output_frames_ * vocab_size_ * sizeof(float),
+        ACL_MEMCPY_DEVICE_TO_HOST);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    return log_probs;
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+
+  int32_t SubsamplingFactor() const { return subsampling_factor_; }
+
+ private:
+  void InitModel(const std::string &filename) {
+    model_ = std::make_unique<AclModel>(filename);
+    if (config_.debug) {
+      auto s = model_->GetInfo();
+      SHERPA_ONNX_LOGE("%s", s.c_str());
+    }
+  }
+
+  void InitModel(void *data, size_t size) {
+    model_ = std::make_unique<AclModel>(data, size);
+    if (config_.debug) {
+      auto s = model_->GetInfo();
+      SHERPA_ONNX_LOGE("%s", s.c_str());
+    }
+  }
+
+  void PreInit() {
+    int32_t device_id = 0;
+    aclError ret = aclrtSetDevice(device_id);
+    SHERPA_ONNX_ASCEND_CHECK(
+        ret, "Failed to call aclrtSetDevice with device id: %d", device_id);
+
+    context_ = std::make_unique<AclContext>(device_id);
+
+    ret = aclrtSetCurrentContext(*context_);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtSetCurrentContext");
+  }
+
+  void PostInit() {
+    auto in_shape = model_->GetInputShapes()[0];
+
+    max_num_frames_ = in_shape[1];
+    feat_dim_ = in_shape[2];
+
+    auto out_shape = model_->GetOutputShapes()[0];
+
+    num_output_frames_ = out_shape[1];
+    vocab_size_ = out_shape[2];
+
+    subsampling_factor_ = max_num_frames_ / out_shape[1];
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("max_num_frames: %d", max_num_frames_);
+      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
+      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
+      SHERPA_ONNX_LOGE("subsampling_factor: %d", subsampling_factor_);
+    }
+
+    Preallocate();
+  }
+
+  void Preallocate() {
+    x_ptr_ = std::make_unique<AclDevicePtr>(max_num_frames_ * feat_dim_ *
+                                            sizeof(float));
+
+    log_probs_ptr_ = std::make_unique<AclDevicePtr>(
+        num_output_frames_ * vocab_size_ * sizeof(float));
+  }
+
+ private:
+  std::mutex mutex_;
+  Acl acl_;
+
+  std::unique_ptr<AclContext> context_;
+
+  OfflineModelConfig config_;
+
+  std::unique_ptr<AclModel> model_;
+  int32_t vocab_size_ = 0;
+  int32_t max_num_frames_ = 0;
+  int32_t num_output_frames_ = 0;
+  int32_t feat_dim_ = 0;
+  int32_t subsampling_factor_ = 0;
+
+  std::unique_ptr<AclDevicePtr> x_ptr_;
+  std::unique_ptr<AclDevicePtr> log_probs_ptr_;
+};
+
+OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineZipformerCtcModelAscend::~OfflineZipformerCtcModelAscend() = default;
+
+std::vector<float> OfflineZipformerCtcModelAscend::Run(
+    std::vector<float> features) const {
+  return impl_->Run(std::move(features));
+}
+
+int32_t OfflineZipformerCtcModelAscend::VocabSize() const {
+  return impl_->VocabSize();
+}
+
+int32_t OfflineZipformerCtcModelAscend::SubsamplingFactor() const {
+  return impl_->SubsamplingFactor();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
new file mode 100644
index 00000000..0c643f38
--- /dev/null
+++ b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
@@ -0,0 +1,40 @@
+// sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
+#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
+
+#include <memory>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineZipformerCtcModelAscend {
+ public:
+  ~OfflineZipformerCtcModelAscend();
+
+  explicit OfflineZipformerCtcModelAscend(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineZipformerCtcModelAscend(Manager *mgr,
+                                 const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (num_frames, feature_dim)
+   * @returns Return a tensor of shape (num_output_frames, vocab_size)
+   */
+  std::vector<float> Run(std::vector<float> features) const;
+
+  int32_t VocabSize() const;
+  int32_t SubsamplingFactor() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
diff --git a/sherpa-onnx/csrc/ascend/utils.cc b/sherpa-onnx/csrc/ascend/utils.cc
index 2977c769..f424d1f2 100644
--- a/sherpa-onnx/csrc/ascend/utils.cc
+++ b/sherpa-onnx/csrc/ascend/utils.cc
@@ -50,8 +50,10 @@ static const char *AclDataTypeToString(aclDataType data_type) {
       return "ACL_COMPLEX128";
     case ACL_BF16:
       return "ACL_BF16";
+#if defined(ACL_INT4)
     case ACL_INT4:
       return "ACL_INT4";
+#endif
     case ACL_UINT1:
       return "ACL_UINT1";
     case ACL_COMPLEX32:
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 1046f9e1..318be0e0 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -43,6 +43,7 @@
 #if SHERPA_ONNX_ENABLE_ASCEND_NPU
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
+#include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_QNN
@@ -81,10 +82,12 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(config);
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
+    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+      return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice and Paraformer models are currently supported "
-          "by Ascend NPU for non-streaming ASR.");
+          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
+          "supported by Ascend NPU for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
@@ -115,7 +118,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     SHERPA_ONNX_LOGE(
         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
         "you want to use qnn. See also "
-        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
+        "https://k2-fsa.github.io/sherpa/onnx/qnn/build.html");
     SHERPA_ONNX_EXIT(-1);
     return nullptr;
 #endif
@@ -339,10 +342,13 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     } else if (!config.model_config.paraformer.model.empty()) {
       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
                                                                      config);
+    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+      return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(mgr,
+                                                                       config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice and Paraformer models are currently supported "
-          "by Ascend NPU for non-streaming ASR. Fallback to CPU");
+          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
+          "supported by Ascend NPU for non-streaming ASR. Fallback to CPU");
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -372,7 +378,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     SHERPA_ONNX_LOGE(
         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
         "you want to use qnn. See also "
-        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
+        "https://k2-fsa.github.io/sherpa/onnx/qnn/build.html");
     SHERPA_ONNX_EXIT(-1);
     return nullptr;
 #endif

commit 589877c95ff224a31011577a6d59781c5aca5820
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 25 15:12:43 2025 +0800

    Refactor scripts for exporting models to Ascend NPU. (#2825)
    
    This pull request refactors and expands the model export infrastructure for Ascend NPUs by introducing new configuration generation capabilities for Paraformer and SenseVoice models. It also enhances existing scripts with improved documentation for Docker image sources, streamlining the process of preparing diverse models for deployment on Ascend hardware across various configurations.

diff --git a/.github/scripts/export-ascend/__init__.py b/.github/scripts/export-ascend/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/.github/scripts/export-ascend/generate_paraformer.py b/.github/scripts/export-ascend/generate_paraformer.py
new file mode 100755
index 00000000..fa22d64b
--- /dev/null
+++ b/.github/scripts/export-ascend/generate_paraformer.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import itertools
+import json
+from dataclasses import asdict, dataclass
+
+from generate_zipformer_ctc_20250703 import get_image
+
+
+@dataclass
+class Config:
+    # 7.0, 8.0, 8.2
+    cann: str
+
+    # 910B, 910B2, 910B3, 310P3
+    soc_version: str
+
+    # FunASR, WSChuan-ASR
+    framework: str
+
+    image: str = ""
+
+    def __post_init__(self):
+        self.image = get_image(self.cann, soc_version=self.soc_version)
+
+
+def main():
+    cann_version = ["7.0", "8.0", "8.2"]
+    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    framework_list = ["FunASR", "WSChuan-ASR"]
+
+    configs = [
+        Config(cann=cann, soc_version=soc, framework=framework)
+        for cann, soc, framework in itertools.product(
+            cann_version, soc_version, framework_list
+        )
+    ]
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/scripts/export-ascend/generate_sense_voice.py b/.github/scripts/export-ascend/generate_sense_voice.py
new file mode 100755
index 00000000..f1ecbf39
--- /dev/null
+++ b/.github/scripts/export-ascend/generate_sense_voice.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import itertools
+import json
+from dataclasses import asdict, dataclass
+
+from generate_zipformer_ctc_20250703 import get_image
+
+
+@dataclass
+class Config:
+    # 7.0, 8.0, 8.2
+    cann: str
+
+    # 910B, 910B2, 910B3, 310P3
+    soc_version: str
+
+    # FunASR, WSYue-ASR
+    framework: str
+
+    image: str = ""
+
+    def __post_init__(self):
+        self.image = get_image(self.cann, soc_version=self.soc_version)
+
+
+def main():
+    cann_version = ["7.0", "8.0", "8.2"]
+    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    framework_list = ["FunASR", "WSYue-ASR"]
+
+    configs = [
+        Config(cann=cann, soc_version=soc, framework=framework)
+        for cann, soc, framework in itertools.product(
+            cann_version, soc_version, framework_list
+        )
+    ]
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
index bea3127c..511bfee1 100755
--- a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
@@ -6,6 +6,12 @@ import json
 from dataclasses import asdict, dataclass
 
 
+# image: ascendai/cann:latest
+# image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
+# see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
+# see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
+# and
+# https://quay.io/repository/ascend/cann?tab=tags
 def get_image(cann: str, soc_version: str):
     cann2image_910 = {
         "7.0": "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8",
diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
index 2fbd89f1..63587194 100644
--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
+++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-910b3
+      - refactor-ascend-export-script
   workflow_dispatch:
 
 concurrency:
@@ -11,7 +11,30 @@ concurrency:
   cancel-in-progress: true
 
 jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-ascend/generate_sense_voice.py
+          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
   export-paraformer-to-rknn:
+    needs: generate_build_matrix
     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
     name: ${{ matrix.framework }} ${{ matrix.soc_version }} ${{ matrix.cann }}
     runs-on: ubuntu-latest
@@ -19,99 +42,9 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        include:
-          # ===== Ascend 910B =====
-          - soc_version: "910B"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSChuan-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSChuan-ASR"
-            cann: "8.2"
-
-          # ===== Ascend 910B2 =====
-          - soc_version: "910B2"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B2"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSChuan-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B2"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B2"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSChuan-ASR"
-            cann: "8.2"
-
-          # ===== Ascend 910B3 =====
-          - soc_version: "910B3"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B3"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSChuan-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSChuan-ASR"
-            cann: "8.2"
-
-          # ===== Ascend 310 =====
-          - soc_version: "310P3"
-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "310P3"
-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
-            framework: "WSChuan-ASR"
-            cann: "8.0"
-
-          - soc_version: "310P3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "310P3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
-            framework: "WSChuan-ASR"
-            cann: "8.2"
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
 
     container:
-      # image: ascendai/cann:latest
-      # image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
-      # see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
-      # see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
-      # and
-      # https://quay.io/repository/ascend/cann?tab=tags
       image: ${{ matrix.image }}
 
     steps:
@@ -139,7 +72,6 @@ jobs:
 
           find /usr/local/Ascend -name "libascend*.so" 2>/dev/null
 
-
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
index f71752b0..0be4e277 100644
--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
 on:
   push:
     branches:
-      - cann-7.0
+      - refactor-ascend-export-script
   workflow_dispatch:
 
 concurrency:
@@ -11,149 +11,39 @@ concurrency:
   cancel-in-progress: true
 
 jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-ascend/generate_sense_voice.py
+          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
   export-sense-voice-to-ascend-npu:
+    needs: generate_build_matrix
     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
     name: ${{ matrix.framework }} ${{ matrix.soc_version }} ${{ matrix.cann }}
     runs-on: ubuntu-latest
-
     strategy:
       fail-fast: false
       matrix:
-        include:
-          # ===== Ascend 910B =====
-          - soc_version: "910B"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "FunASR"
-            cann: "7.0"
-
-          - soc_version: "910B"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "WSYue-ASR"
-            cann: "7.0"
-
-          - soc_version: "910B"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSYue-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSYue-ASR"
-            cann: "8.2"
-
-          # ===== Ascend 910B2 =====
-          - soc_version: "910B2"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "FunASR"
-            cann: "7.0"
-
-          - soc_version: "910B2"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "WSYue-ASR"
-            cann: "7.0"
-
-          - soc_version: "910B2"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B2"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSYue-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B2"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B2"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSYue-ASR"
-            cann: "8.2"
-
-          # ===== Ascend 910B3 =====
-          - soc_version: "910B3"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "FunASR"
-            cann: "7.0"
-
-          - soc_version: "910B3"
-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
-            framework: "WSYue-ASR"
-            cann: "7.0"
-
-          - soc_version: "910B3"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "910B3"
-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
-            framework: "WSYue-ASR"
-            cann: "8.0"
-
-          - soc_version: "910B3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "910B3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
-            framework: "WSYue-ASR"
-            cann: "8.2"
-
-
-          # ===== Ascend 310 =====
-          - soc_version: "310P3"
-            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
-            framework: "FunASR"
-            cann: "7.0"
-
-          - soc_version: "310P3"
-            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
-            framework: "WSYue-ASR"
-            cann: "7.0"
-
-          - soc_version: "310P3"
-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.0"
-
-          - soc_version: "310P3"
-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
-            framework: "WSYue-ASR"
-            cann: "8.0"
-
-          - soc_version: "310P3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
-            framework: "FunASR"
-            cann: "8.2"
-
-          - soc_version: "310P3"
-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
-            framework: "WSYue-ASR"
-            cann: "8.2"
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
 
     container:
-      # image: ascendai/cann:latest
-      # image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
-      # see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
-      # see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
-      # and
-      # https://quay.io/repository/ascend/cann?tab=tags
       image: ${{ matrix.image }}
 
     steps:

commit 10c6bd421640e254622cccc215ae9cff33daa35e
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 25 14:56:35 2025 +0800

    Export zipformer ctc models to Ascend NPU (#2824)
    
    This pull request significantly enhances the support for Zipformer CTC models on Ascend NPU. It introduces automated configuration generation for model exports and provides robust testing utilities for both ONNX and Ascend-specific .om model formats. These additions streamline the deployment and validation process of speech recognition models on Ascend hardware, ensuring compatibility and performance across different NPU environments.

diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
new file mode 100755
index 00000000..bea3127c
--- /dev/null
+++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
@@ -0,0 +1,63 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import itertools
+import json
+from dataclasses import asdict, dataclass
+
+
+def get_image(cann: str, soc_version: str):
+    cann2image_910 = {
+        "7.0": "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8",
+        "8.0": "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9",
+        "8.2": "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2",
+    }
+
+    cann2image_310 = {
+        "7.0": "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9",
+        "8.0": "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2",
+        "8.2": "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2",
+    }
+    if "910" in soc_version:
+        return cann2image_910[cann]
+    elif "310" in soc_version:
+        return cann2image_310[cann]
+    else:
+        raise ValueError(f"Unsupported soc_version {soc_version}")
+
+
+@dataclass
+class Config:
+    # 7.0, 8.0, 8.2
+    cann: str
+
+    # 910B, 910B2, 910B3, 310P3
+    soc_version: str
+
+    num_seconds: str
+
+    image: str = ""
+
+    def __post_init__(self):
+        self.image = get_image(self.cann, soc_version=self.soc_version)
+
+
+def main():
+    cann_version = ["7.0", "8.0", "8.2"]
+    soc_version = ["910B", "910B2", "910B3", "310P3"]
+    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+
+    configs = [
+        Config(cann=cann, soc_version=soc, num_seconds=sec)
+        for cann, soc, sec in itertools.product(
+            cann_version, soc_version, input_in_seconds
+        )
+    ]
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
new file mode 100644
index 00000000..4169f7c0
--- /dev/null
+++ b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
@@ -0,0 +1,177 @@
+name: export-zipformer-ctc-to-ascend-npu-20250703
+
+on:
+  push:
+    branches:
+      - export-zipformer-ctc-ascend
+  workflow_dispatch:
+
+concurrency:
+  group: export-zipformer-ctc-to-ascend-npu-20250703-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+          MATRIX=$(python3 .github/scripts/export-ascend/generate_zipformer_ctc_20250703.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
+  export-zipformer-ctc-to-ascend-npu-20250703:
+    needs: generate_build_matrix
+    name: ${{ matrix.soc_version }} ${{ matrix.cann }} ${{ matrix.num_seconds }}
+    runs-on: ubuntu-latest
+    strategy:
+      fail-fast: false
+      matrix:
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+
+    container:
+      image: ${{ matrix.image }}
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python 3.8
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.8"
+
+      - name: Show Python
+        shell: bash
+        run: |
+          python3 --version
+          which python3
+
+      - name: Install curl
+        shell: bash
+        run: apt-get update && apt-get install -y curl bzip2
+
+      - name: Verify environment
+        shell: bash
+        run: |
+          ls -lh /usr/local/Ascend/ascend-toolkit/set_env.sh
+
+          find /usr/local/Ascend -name "libascend*.so" 2>/dev/null
+
+
+          source /usr/local/Ascend/ascend-toolkit/set_env.sh
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
+          echo "CANN environment:"
+          which atc || echo "atc not found"
+          atc --help
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          python3 -m pip install "numpy<2" \
+                  onnx==1.17.0 \
+                  torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+                  attrs psutil scipy decorator cloudpickle ml-dtypes tornado \
+                  sentencepiece \
+                  pyyaml
+
+      - name: Export ${{ matrix.num_seconds }}
+        shell: bash
+        run: |
+          mkdir tmp
+          cd tmp
+
+          t=${{ matrix.num_seconds }}
+          num_frames=$(($t*100))
+
+          echo "num_frames: $num_frames"
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
+          chmod +x generate_test_data.py
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
+
+
+          source /usr/local/Ascend/ascend-toolkit/set_env.sh
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
+          soc_version=${{ matrix.soc_version }}
+          cann=${{ matrix.cann }}
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/model-$t-seconds.onnx
+          mv model-$t-seconds.onnx model.onnx
+
+          atc --model=./model.onnx \
+            --framework=5 \
+            --host_env_os=linux \
+            --host_env_cpu=aarch64 \
+            --output=model \
+            --input_format=ND \
+            --input_shape="x:1,${num_frames},80" \
+            --soc_version="Ascend${soc_version}"
+
+          ls -lh *.om
+
+          echo "collect results"
+          d=sherpa-onnx-ascend-${soc_version}-cann-${cann}-$t-seconds-zipformer-ctc-zh-2025-07-03
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          cp -v model_linux_aarch64.om $d/model.om || cp -v model.om $d/model.om
+          cp -v tokens.txt $d
+          cp -v ../scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py $d
+          cp -v ../scripts/zipformer-ctc/ascend/2025-07-03/test_om.py $d
+          cp -v *.wav $d/test_wavs
+          ls -lh $d
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-ascend
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          tag: asr-models-ascend
diff --git a/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py b/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py
new file mode 100755
index 00000000..8cf52247
--- /dev/null
+++ b/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py
@@ -0,0 +1,173 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from typing import Tuple
+
+import kaldi_native_fbank as knf
+import numpy as np
+import onnxruntime as ort
+import soundfile as sf
+
+BPE_UNK = chr(8263)
+PRINTABLE_BASE_CHARS = (
+    list(range(256, 287 + 1))
+    + list(range(32, 126 + 1))
+    + list(range(288, 305 + 1))
+    + list(range(308, 318 + 1))
+    + list(range(321, 328 + 1))
+    + list(range(330, 382 + 1))
+    + list(range(384, 422 + 1))
+)
+
+
+BYTE_TO_BCHAR = {b: chr(PRINTABLE_BASE_CHARS[b]) for b in range(256)}
+BCHAR_TO_BYTE = {bc: b for b, bc in BYTE_TO_BCHAR.items()}
+BCHAR_TO_BYTE[BPE_UNK] = 32  # map unk to space
+
+
+def load_tokens(filename):
+    ans = dict()
+    i = 0
+    with open(filename, encoding="utf-8") as f:
+        for line in f:
+            ans[i] = line.strip().split()[0]
+            i += 1
+    return ans
+
+
+def load_audio(filename: str) -> Tuple[np.ndarray, int]:
+    data, sample_rate = sf.read(
+        filename,
+        always_2d=True,
+        dtype="float32",
+    )
+    data = data[:, 0]  # use only the first channel
+
+    if sample_rate != 16000:
+        import librosa
+
+        data = librosa.resample(data, orig_sr=sample_rate, target_sr=16000)
+        sample_rate = 16000
+
+    samples = np.ascontiguousarray(data)
+    return samples, sample_rate
+
+
+def compute_feat(
+    samples: np.ndarray,
+    sample_rate: int,
+    max_len: int,
+):
+    opts = knf.FbankOptions()
+    opts.frame_opts.dither = 0
+    opts.frame_opts.snip_edges = False
+    opts.frame_opts.window_type = "povey"
+    opts.frame_opts.samp_freq = sample_rate
+    opts.mel_opts.num_bins = 80
+
+    online_fbank = knf.OnlineFbank(opts)
+    online_fbank.accept_waveform(sample_rate, samples.tolist())
+    online_fbank.input_finished()
+
+    features = np.stack(
+        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
+    )
+
+    if features.shape[0] > max_len:
+        features = features[:max_len]
+    elif features.shape[0] < max_len:
+        features = np.pad(
+            features,
+            ((0, max_len - features.shape[0]), (0, 0)),
+            mode="constant",
+            constant_values=0,
+        )
+
+    features = np.ascontiguousarray(features)
+
+    assert features.data.contiguous is True
+    assert features.dtype == np.float32, features.dtype
+
+    return features
+
+
+class OnnxModel:
+    def __init__(self, filename):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 1
+
+        self.session_opts = session_opts
+
+        self.model = ort.InferenceSession(
+            filename,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+        shape = self.model.get_inputs()[0].shape
+        self.max_len = shape[1]
+
+        for i in self.model.get_inputs():
+            print(i)
+
+        print("-----")
+
+        for i in self.model.get_outputs():
+            print(i)
+
+    def __call__(self, x):
+        log_probs = self.model.run(
+            [
+                self.model.get_outputs()[0].name,
+            ],
+            {self.model.get_inputs()[0].name: x[None]},
+        )[0]
+
+        return log_probs
+
+
+def main():
+    wave = "./0.wav"
+    wave = "./1.wav"
+    samples, sample_rate = load_audio(wave)
+
+    model = OnnxModel("./model.onnx")
+
+    features = compute_feat(
+        samples=samples,
+        sample_rate=sample_rate,
+        max_len=model.max_len,
+    )
+    print("features", features.shape)
+
+    log_probs = model(features)
+
+    idx = log_probs[0].argmax(axis=-1)
+    print("idx", idx)
+    print(len(idx))
+    prev = -1
+    ids = []
+    for i in idx:
+        if i != prev:
+            ids.append(i)
+        prev = i
+    ids = [i for i in ids if i != 0]
+    print(ids)
+
+    tokens = load_tokens("./tokens.txt")
+    text = "".join([tokens[i] for i in ids])
+
+    s = b""
+    for t in text:
+        if t == "":
+            continue
+        elif t in BCHAR_TO_BYTE:
+            s += bytes([BCHAR_TO_BYTE[t]])
+        else:
+            print("skip OOV", t)
+
+    print(s.decode())
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py b/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py
new file mode 100755
index 00000000..1d4ebee2
--- /dev/null
+++ b/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py
@@ -0,0 +1,73 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from ais_bench.infer.interface import InferSession
+
+from onnx_test import BCHAR_TO_BYTE, compute_feat, load_audio, load_tokens
+
+
+class OmModel:
+    def __init__(self):
+        self.model = InferSession(device_id=0, model_path="./model.om", debug=False)
+
+        self.max_len = self.model.get_inputs()[0].shape[1]
+        print("---model---")
+        for i in self.model.get_inputs():
+            print(i.name, i.datatype, i.shape)
+
+        print("-----")
+
+        for i in self.model.get_outputs():
+            print(i.name, i.datatype, i.shape)
+
+    def __call__(self, x):
+        """
+        Args:
+          x: (N, T, C)
+        Returns:
+          log_probs: (N, T, vocab_size)
+        """
+        return self.model.infer([x], mode="static", custom_sizes=10000000)[0]
+
+
+def main():
+    samples, sample_rate = load_audio("./test_wavs/0.wav")
+    model = OmModel()
+
+    features = compute_feat(
+        samples=samples, sample_rate=sample_rate, max_len=model.max_len
+    )
+    print("features.shape", features.shape)
+
+    log_probs = model(x=features[None])
+    print("log_probs.shape", log_probs.shape, type(log_probs))
+
+    idx = log_probs[0].argmax(axis=-1)
+    print("idx", idx)
+    print(len(idx))
+    prev = -1
+    ids = []
+    for i in idx:
+        if i != prev:
+            ids.append(i)
+        prev = i
+    ids = [i for i in ids if i != 0]
+    print(ids)
+
+    tokens = load_tokens("./tokens.txt")
+    text = "".join([tokens[i] for i in ids])
+
+    s = b""
+    for t in text:
+        if t == "":
+            continue
+        elif t in BCHAR_TO_BYTE:
+            s += bytes([BCHAR_TO_BYTE[t]])
+        else:
+            print("skip OOV", t)
+
+    print(s.decode())
+
+
+if __name__ == "__main__":
+    main()

commit 3c79bda15625e5756eac648e3fde641522e004c1
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 25 14:50:51 2025 +0800

    Fix warnings for initializing tts lexicon. (#2823)
    
    This pull request focuses on improving the robustness of the TTS lexicon initialization process by eliminating a recurring warning. The change ensures that the lexicon parser correctly handles and ignores empty or whitespace-only lines, leading to cleaner log output and more reliable lexicon loading.

diff --git a/sherpa-onnx/csrc/character-lexicon.cc b/sherpa-onnx/csrc/character-lexicon.cc
index 60d48b55..483828bb 100644
--- a/sherpa-onnx/csrc/character-lexicon.cc
+++ b/sherpa-onnx/csrc/character-lexicon.cc
@@ -266,6 +266,10 @@ class CharacterLexicon::Impl {
 
     while (std::getline(is, line)) {
       ++line_num;
+      if (line.find_first_not_of(" \t\n\v\f\r") == std::string::npos) {
+        // Line is empty or only spaces/tabs, skip it
+        continue;
+      }
 
       std::istringstream iss(line);
 

commit 98713062411d3921482232c3cf363ad46ad5048b
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 25 14:34:09 2025 +0800

    Limit symbol visibility in the shared libraries (#2822)
    
    This pull request refines the build process for the sherpa-onnx shared libraries by introducing explicit symbol visibility control. This change aims to improve library encapsulation, prevent symbol conflicts with other loaded libraries, and potentially optimize library size and load performance by only exposing necessary public API symbols.

diff --git a/sherpa-onnx/c-api/CMakeLists.txt b/sherpa-onnx/c-api/CMakeLists.txt
index 03909373..cbb81dfb 100644
--- a/sherpa-onnx/c-api/CMakeLists.txt
+++ b/sherpa-onnx/c-api/CMakeLists.txt
@@ -12,6 +12,16 @@ add_library(sherpa-onnx-cxx-api cxx-api.cc)
 target_link_libraries(sherpa-onnx-cxx-api sherpa-onnx-c-api)
 target_include_directories(sherpa-onnx-cxx-api PUBLIC ${PROJECT_SOURCE_DIR})
 
+if(ANDROID OR (UNIX AND NOT APPLE))
+  set_target_properties(sherpa-onnx-c-api PROPERTIES
+    LINK_FLAGS "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols-c.lds"
+  )
+elseif(APPLE)
+  set_target_properties(sherpa-onnx-c-api PROPERTIES
+    LINK_FLAGS "-Wl,-exported_symbols_list,${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols-c.exp"
+  )
+endif()
+
 install(
   TARGETS
     sherpa-onnx-c-api
diff --git a/sherpa-onnx/c-api/generate.sh b/sherpa-onnx/c-api/generate.sh
new file mode 100755
index 00000000..6ba93bbd
--- /dev/null
+++ b/sherpa-onnx/c-api/generate.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+set -ex
+
+nm -g ../../build/lib/libsherpa-onnx-c-api.dylib | awk '$2=="T" && $3 ~ /^_Sherpa/ {print $3}' | sort  > ./sherpa-onnx-symbols-c.exp
+
diff --git a/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp
new file mode 100644
index 00000000..aedffcde
--- /dev/null
+++ b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp
@@ -0,0 +1,149 @@
+_SherpaOfflinePunctuationAddPunct
+_SherpaOfflinePunctuationFreeText
+_SherpaOnnxAcceptWaveformOffline
+_SherpaOnnxAudioTaggingCompute
+_SherpaOnnxAudioTaggingCreateOfflineStream
+_SherpaOnnxAudioTaggingFreeResults
+_SherpaOnnxCircularBufferFree
+_SherpaOnnxCircularBufferGet
+_SherpaOnnxCircularBufferHead
+_SherpaOnnxCircularBufferPop
+_SherpaOnnxCircularBufferPush
+_SherpaOnnxCircularBufferReset
+_SherpaOnnxCircularBufferSize
+_SherpaOnnxCreateAudioTagging
+_SherpaOnnxCreateCircularBuffer
+_SherpaOnnxCreateDisplay
+_SherpaOnnxCreateKeywordSpotter
+_SherpaOnnxCreateKeywordStream
+_SherpaOnnxCreateKeywordStreamWithKeywords
+_SherpaOnnxCreateLinearResampler
+_SherpaOnnxCreateOfflinePunctuation
+_SherpaOnnxCreateOfflineRecognizer
+_SherpaOnnxCreateOfflineSpeakerDiarization
+_SherpaOnnxCreateOfflineSpeechDenoiser
+_SherpaOnnxCreateOfflineStream
+_SherpaOnnxCreateOfflineStreamWithHotwords
+_SherpaOnnxCreateOfflineTts
+_SherpaOnnxCreateOnlinePunctuation
+_SherpaOnnxCreateOnlineRecognizer
+_SherpaOnnxCreateOnlineStream
+_SherpaOnnxCreateOnlineStreamWithHotwords
+_SherpaOnnxCreateSpeakerEmbeddingExtractor
+_SherpaOnnxCreateSpeakerEmbeddingManager
+_SherpaOnnxCreateSpokenLanguageIdentification
+_SherpaOnnxCreateVoiceActivityDetector
+_SherpaOnnxDecodeKeywordStream
+_SherpaOnnxDecodeMultipleKeywordStreams
+_SherpaOnnxDecodeMultipleOfflineStreams
+_SherpaOnnxDecodeMultipleOnlineStreams
+_SherpaOnnxDecodeOfflineStream
+_SherpaOnnxDecodeOnlineStream
+_SherpaOnnxDestroyAudioTagging
+_SherpaOnnxDestroyCircularBuffer
+_SherpaOnnxDestroyDenoisedAudio
+_SherpaOnnxDestroyDisplay
+_SherpaOnnxDestroyKeywordResult
+_SherpaOnnxDestroyKeywordSpotter
+_SherpaOnnxDestroyLinearResampler
+_SherpaOnnxDestroyOfflinePunctuation
+_SherpaOnnxDestroyOfflineRecognizer
+_SherpaOnnxDestroyOfflineRecognizerResult
+_SherpaOnnxDestroyOfflineSpeakerDiarization
+_SherpaOnnxDestroyOfflineSpeechDenoiser
+_SherpaOnnxDestroyOfflineStream
+_SherpaOnnxDestroyOfflineStreamResultJson
+_SherpaOnnxDestroyOfflineTts
+_SherpaOnnxDestroyOfflineTtsGeneratedAudio
+_SherpaOnnxDestroyOnlinePunctuation
+_SherpaOnnxDestroyOnlineRecognizer
+_SherpaOnnxDestroyOnlineRecognizerResult
+_SherpaOnnxDestroyOnlineStream
+_SherpaOnnxDestroyOnlineStreamResultJson
+_SherpaOnnxDestroySpeakerEmbeddingExtractor
+_SherpaOnnxDestroySpeakerEmbeddingManager
+_SherpaOnnxDestroySpeechSegment
+_SherpaOnnxDestroySpokenLanguageIdentification
+_SherpaOnnxDestroySpokenLanguageIdentificationResult
+_SherpaOnnxDestroyVoiceActivityDetector
+_SherpaOnnxFileExists
+_SherpaOnnxFreeKeywordResultJson
+_SherpaOnnxFreeWave
+_SherpaOnnxGetGitDate
+_SherpaOnnxGetGitSha1
+_SherpaOnnxGetKeywordResult
+_SherpaOnnxGetKeywordResultAsJson
+_SherpaOnnxGetOfflineStreamResult
+_SherpaOnnxGetOfflineStreamResultAsJson
+_SherpaOnnxGetOnlineStreamResult
+_SherpaOnnxGetOnlineStreamResultAsJson
+_SherpaOnnxGetVersionStr
+_SherpaOnnxIsKeywordStreamReady
+_SherpaOnnxIsOnlineStreamReady
+_SherpaOnnxLinearResamplerResample
+_SherpaOnnxLinearResamplerResampleFree
+_SherpaOnnxLinearResamplerResampleGetInputSampleRate
+_SherpaOnnxLinearResamplerResampleGetOutputSampleRate
+_SherpaOnnxLinearResamplerReset
+_SherpaOnnxOfflineRecognizerSetConfig
+_SherpaOnnxOfflineSpeakerDiarizationDestroyResult
+_SherpaOnnxOfflineSpeakerDiarizationDestroySegment
+_SherpaOnnxOfflineSpeakerDiarizationGetSampleRate
+_SherpaOnnxOfflineSpeakerDiarizationProcess
+_SherpaOnnxOfflineSpeakerDiarizationProcessWithCallback
+_SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg
+_SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments
+_SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers
+_SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime
+_SherpaOnnxOfflineSpeakerDiarizationSetConfig
+_SherpaOnnxOfflineSpeechDenoiserGetSampleRate
+_SherpaOnnxOfflineSpeechDenoiserRun
+_SherpaOnnxOfflineTtsGenerate
+_SherpaOnnxOfflineTtsGenerateWithCallback
+_SherpaOnnxOfflineTtsGenerateWithCallbackWithArg
+_SherpaOnnxOfflineTtsGenerateWithProgressCallback
+_SherpaOnnxOfflineTtsGenerateWithProgressCallbackWithArg
+_SherpaOnnxOfflineTtsGenerateWithZipvoice
+_SherpaOnnxOfflineTtsNumSpeakers
+_SherpaOnnxOfflineTtsSampleRate
+_SherpaOnnxOnlinePunctuationAddPunct
+_SherpaOnnxOnlinePunctuationFreeText
+_SherpaOnnxOnlineStreamAcceptWaveform
+_SherpaOnnxOnlineStreamInputFinished
+_SherpaOnnxOnlineStreamIsEndpoint
+_SherpaOnnxOnlineStreamReset
+_SherpaOnnxPrint
+_SherpaOnnxReadWave
+_SherpaOnnxReadWaveFromBinaryData
+_SherpaOnnxResetKeywordStream
+_SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding
+_SherpaOnnxSpeakerEmbeddingExtractorCreateStream
+_SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding
+_SherpaOnnxSpeakerEmbeddingExtractorDim
+_SherpaOnnxSpeakerEmbeddingExtractorIsReady
+_SherpaOnnxSpeakerEmbeddingManagerAdd
+_SherpaOnnxSpeakerEmbeddingManagerAddList
+_SherpaOnnxSpeakerEmbeddingManagerAddListFlattened
+_SherpaOnnxSpeakerEmbeddingManagerContains
+_SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers
+_SherpaOnnxSpeakerEmbeddingManagerFreeBestMatches
+_SherpaOnnxSpeakerEmbeddingManagerFreeSearch
+_SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers
+_SherpaOnnxSpeakerEmbeddingManagerGetBestMatches
+_SherpaOnnxSpeakerEmbeddingManagerNumSpeakers
+_SherpaOnnxSpeakerEmbeddingManagerRemove
+_SherpaOnnxSpeakerEmbeddingManagerSearch
+_SherpaOnnxSpeakerEmbeddingManagerVerify
+_SherpaOnnxSpokenLanguageIdentificationCompute
+_SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream
+_SherpaOnnxVoiceActivityDetectorAcceptWaveform
+_SherpaOnnxVoiceActivityDetectorClear
+_SherpaOnnxVoiceActivityDetectorDetected
+_SherpaOnnxVoiceActivityDetectorEmpty
+_SherpaOnnxVoiceActivityDetectorFlush
+_SherpaOnnxVoiceActivityDetectorFront
+_SherpaOnnxVoiceActivityDetectorPop
+_SherpaOnnxVoiceActivityDetectorReset
+_SherpaOnnxWaveFileSize
+_SherpaOnnxWriteWave
+_SherpaOnnxWriteWaveToBuffer
diff --git a/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds
new file mode 100644
index 00000000..5805cafc
--- /dev/null
+++ b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds
@@ -0,0 +1,8 @@
+{
+  global:
+    SherpaOnnx*;
+    # For offline punctuation.
+    SherpaOffline*;
+  local:
+    *;
+};
diff --git a/sherpa-onnx/jni/CMakeLists.txt b/sherpa-onnx/jni/CMakeLists.txt
index 56cf7bd9..6ba5fcda 100644
--- a/sherpa-onnx/jni/CMakeLists.txt
+++ b/sherpa-onnx/jni/CMakeLists.txt
@@ -48,5 +48,15 @@ add_library(sherpa-onnx-jni SHARED ${sources})
 target_compile_definitions(sherpa-onnx-jni PRIVATE SHERPA_ONNX_BUILD_SHARED_LIBS=1)
 target_compile_definitions(sherpa-onnx-jni PRIVATE SHERPA_ONNX_BUILD_MAIN_LIB=1)
 
+if(ANDROID OR (UNIX AND NOT APPLE))
+  set_target_properties(sherpa-onnx-jni PROPERTIES
+    LINK_FLAGS "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols.lds"
+  )
+elseif(APPLE)
+  set_target_properties(sherpa-onnx-jni PROPERTIES
+    LINK_FLAGS "-Wl,-exported_symbols_list,${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols.exp"
+  )
+endif()
+
 target_link_libraries(sherpa-onnx-jni sherpa-onnx-core)
 install(TARGETS sherpa-onnx-jni DESTINATION lib)
diff --git a/sherpa-onnx/jni/generate.sh b/sherpa-onnx/jni/generate.sh
new file mode 100755
index 00000000..aa1781ba
--- /dev/null
+++ b/sherpa-onnx/jni/generate.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+set -ex
+
+nm -g ../../build/lib/libsherpa-onnx-jni.dylib | awk '$2=="T" && $3 ~ /^_Java_com_k2fsa/ {print $3}' | sort  > ./sherpa-onnx-symbols.exp
+
diff --git a/sherpa-onnx/jni/sherpa-onnx-symbols.exp b/sherpa-onnx/jni/sherpa-onnx-symbols.exp
new file mode 100644
index 00000000..af18d292
--- /dev/null
+++ b/sherpa-onnx/jni/sherpa-onnx-symbols.exp
@@ -0,0 +1,110 @@
+_Java_com_k2fsa_sherpa_onnx_AudioTagging_compute
+_Java_com_k2fsa_sherpa_onnx_AudioTagging_createStream
+_Java_com_k2fsa_sherpa_onnx_AudioTagging_delete
+_Java_com_k2fsa_sherpa_onnx_AudioTagging_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_AudioTagging_newFromFile
+_Java_com_k2fsa_sherpa_onnx_DenoisedAudio_saveImpl
+_Java_com_k2fsa_sherpa_onnx_GeneratedAudio_saveImpl
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_createStream
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_decode
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_delete
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_getResult
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_isReady
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_newFromFile
+_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_reset
+_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_addPunctuation
+_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_delete
+_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_createStream
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_decode
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_decodeStreams
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_delete
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_getResult
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_prependAdspLibraryPath
+_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_setConfig
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_delete
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_getSampleRate
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_process
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_processWithCallback
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_setConfig
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_delete
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_getSampleRate
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_run
+_Java_com_k2fsa_sherpa_onnx_OfflineStream_acceptWaveform
+_Java_com_k2fsa_sherpa_onnx_OfflineStream_delete
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_delete
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_generateImpl
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_generateWithCallbackImpl
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_getNumSpeakers
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_getSampleRate
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OfflineTts_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_addPunctuation
+_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_delete
+_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_createStream
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_decode
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_decodeStreams
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_delete
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_getResult
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_isEndpoint
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_isReady
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_newFromFile
+_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_reset
+_Java_com_k2fsa_sherpa_onnx_OnlineStream_acceptWaveform
+_Java_com_k2fsa_sherpa_onnx_OnlineStream_delete
+_Java_com_k2fsa_sherpa_onnx_OnlineStream_inputFinished
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_compute
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_createStream
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_delete
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_dim
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_isReady
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_newFromFile
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_add
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_addList
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_allSpeakerNames
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_contains
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_create
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_delete
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_numSpeakers
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_remove
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_search
+_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_verify
+_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_compute
+_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_createStream
+_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_delete
+_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_newFromFile
+_Java_com_k2fsa_sherpa_onnx_Vad_acceptWaveform
+_Java_com_k2fsa_sherpa_onnx_Vad_clear
+_Java_com_k2fsa_sherpa_onnx_Vad_compute
+_Java_com_k2fsa_sherpa_onnx_Vad_delete
+_Java_com_k2fsa_sherpa_onnx_Vad_empty
+_Java_com_k2fsa_sherpa_onnx_Vad_flush
+_Java_com_k2fsa_sherpa_onnx_Vad_front
+_Java_com_k2fsa_sherpa_onnx_Vad_isSpeechDetected
+_Java_com_k2fsa_sherpa_onnx_Vad_newFromAsset
+_Java_com_k2fsa_sherpa_onnx_Vad_newFromFile
+_Java_com_k2fsa_sherpa_onnx_Vad_pop
+_Java_com_k2fsa_sherpa_onnx_Vad_reset
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getGitDate2
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getGitSha12
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getVersionStr2
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_getGitDate2
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_getGitSha12
+_Java_com_k2fsa_sherpa_onnx_VersionInfo_getVersionStr2
+_Java_com_k2fsa_sherpa_onnx_WaveReader_00024Companion_readWaveFromAsset
+_Java_com_k2fsa_sherpa_onnx_WaveReader_00024Companion_readWaveFromFile
+_Java_com_k2fsa_sherpa_onnx_WaveReader_readWaveFromFile
+_Java_com_k2fsa_sherpa_onnx_WaveWriter_writeWaveToFile
diff --git a/sherpa-onnx/jni/sherpa-onnx-symbols.lds b/sherpa-onnx/jni/sherpa-onnx-symbols.lds
new file mode 100644
index 00000000..4931fa00
--- /dev/null
+++ b/sherpa-onnx/jni/sherpa-onnx-symbols.lds
@@ -0,0 +1,6 @@
+{
+  global:
+    Java_com_k2fsa_sherpa_onnx*;
+  local:
+    *;
+};

commit d1c458b95d92c74ed7339f9ac24c27dae93bcdd4
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 24 18:14:22 2025 +0800

    Add C++ QNN support for Zipformer CTC models. (#2809)

diff --git a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
index 60191019..8289439d 100644
--- a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
+++ b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
@@ -4,7 +4,7 @@ on:
   push:
     branches:
       - apk
-      - android-qnn-2
+      - zipformer-ctc-qnn-2
 
   workflow_dispatch:
 
@@ -24,8 +24,8 @@ jobs:
       fail-fast: false
       matrix:
         os: [ubuntu-latest]
-        total: ["5"]
-        index: ["0", "1", "2", "3", "4"]
+        total: ["10"]
+        index: ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
 
     steps:
       - uses: actions/checkout@v4
@@ -47,7 +47,7 @@ jobs:
       - name: ccache
         uses: hendrikmuhs/ccache-action@v1.2
         with:
-          key: ${{ matrix.os }}-android
+          key: ${{ matrix.os }}-android-qnn
 
       - name: Display NDK HOME
         shell: bash
diff --git a/.gitignore b/.gitignore
index 060ce16b..6ff6eb29 100644
--- a/.gitignore
+++ b/.gitignore
@@ -159,3 +159,5 @@ configuration.json
 sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
 sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
 sherpa-onnx-paraformer-zh-int8-2025-10-07
+sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
index 1afcf2a9..470850c9 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
@@ -125,20 +125,32 @@ object SimulateStreamingAsr {
                 OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
 
                 // for qnn, we need to copy *.so files from assets folder to sd card
-                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()) {
+                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty() && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()) {
                     Log.e(TAG, "You should provide libQnnHtp.so for qnn")
                     throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
                 }
                 config.modelConfig.tokens =
                     copyAssetToInternalStorage(config.modelConfig.tokens, context)
 
-                config.modelConfig.senseVoice.model =
-                    copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
-
-                config.modelConfig.senseVoice.qnnConfig.contextBinary = copyAssetToInternalStorage(
-                    config.modelConfig.senseVoice.qnnConfig.contextBinary,
-                    context
-                )
+                if (config.modelConfig.senseVoice.model.isNotEmpty()) {
+                    config.modelConfig.senseVoice.model =
+                        copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
+
+                    config.modelConfig.senseVoice.qnnConfig.contextBinary =
+                        copyAssetToInternalStorage(
+                            config.modelConfig.senseVoice.qnnConfig.contextBinary,
+                            context
+                        )
+                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
+                    config.modelConfig.zipformerCtc.model =
+                        copyAssetToInternalStorage(config.modelConfig.zipformerCtc.model, context)
+
+                    config.modelConfig.zipformerCtc.qnnConfig.contextBinary =
+                        copyAssetToInternalStorage(
+                            config.modelConfig.zipformerCtc.qnnConfig.contextBinary,
+                            context
+                        )
+                }
 
                 if (config.hr.lexicon.isNotEmpty()) {
                     config.hr.lexicon = copyAssetToInternalStorage(config.hr.lexicon, context)
diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
index 28f31b1a..d70bff23 100755
--- a/scripts/apk/generate-qnn-vad-asr-apk-script.py
+++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
@@ -35,7 +35,6 @@ class Model:
 
     # e.g., zh, en, zh_en
     lang: str
-    lang2: str
 
     # e.g., whisper, paraformer, zipformer
     short_name: str = ""
@@ -55,7 +54,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9000,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="5-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -72,7 +70,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9001,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="8-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -89,7 +86,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9002,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="10-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -106,7 +102,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9003,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="13-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -123,7 +118,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9004,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="15-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -140,7 +134,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9005,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="18-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -157,7 +150,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9006,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="20-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -174,7 +166,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9007,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="23-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -191,7 +182,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9008,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="25-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -208,7 +198,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9009,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="28-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -225,7 +214,6 @@ def get_models():
             model_name="sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
             idx=9010,
             lang="zh_en_ko_ja_yue",
-            lang2="",
             short_name="30-seconds-sense_voice_2024_07_17_int8",
             use_hr=True,
             cmd="""
@@ -235,6 +223,182 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9011,
+            lang="zh",
+            short_name="5-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9012,
+            lang="zh",
+            short_name="8-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9013,
+            lang="zh",
+            short_name="10-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9014,
+            lang="zh",
+            short_name="13-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9015,
+            lang="zh",
+            short_name="15-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9016,
+            lang="zh",
+            short_name="18-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9017,
+            lang="zh",
+            short_name="20-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9018,
+            lang="zh",
+            short_name="23-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9019,
+            lang="zh",
+            short_name="25-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9020,
+            lang="zh",
+            short_name="28-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8",
+            idx=9021,
+            lang="zh",
+            short_name="30-seconds-zipformer_ctc_2025_07_03_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 58bafbab..ef480bdd 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -208,10 +208,11 @@ endif()
 
 if(SHERPA_ONNX_ENABLE_QNN)
   list(APPEND sources
+    ./qnn/offline-sense-voice-model-qnn.cc
+    ./qnn/offline-zipformer-ctc-model-qnn.cc
     ./qnn/qnn-backend.cc
     ./qnn/qnn-model.cc
     ./qnn/utils.cc
-    ./qnn/offline-sense-voice-model-qnn.cc
   )
 endif()
 
diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
index 78a72fc1..11f7b81d 100644
--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
@@ -23,10 +23,10 @@
 
 namespace sherpa_onnx {
 
-static OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
-                                        const SymbolTable &sym_table,
-                                        int32_t frame_shift_ms,
-                                        int32_t subsampling_factor) {
+OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+                                 const SymbolTable &sym_table,
+                                 int32_t frame_shift_ms,
+                                 int32_t subsampling_factor) {
   OfflineRecognitionResult r;
   r.tokens.reserve(src.tokens.size());
   r.timestamps.reserve(src.timestamps.size());
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index ff4e055d..1046f9e1 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -47,6 +47,7 @@
 
 #if SHERPA_ONNX_ENABLE_QNN
 #include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
+#include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
 #endif
 
 namespace sherpa_onnx {
@@ -101,10 +102,12 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #if SHERPA_ONNX_ENABLE_QNN
     if (!config.model_config.sense_voice.model.empty()) {
       return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
+    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+      return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
-          "by qnn for non-streaming ASR.");
+          "Only SenseVoice models and Zipformer CTC models are currently "
+          "supported by qnn for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
@@ -355,10 +358,13 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #if SHERPA_ONNX_ENABLE_QNN
     if (!config.model_config.sense_voice.model.empty()) {
       return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
+    } else if (!config.model_config.zipformer_ctc.model.empty()) {
+      return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
+                                                                    config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice models are currently supported "
-          "by qnn for non-streaming ASR.");
+          "Only SenseVoice models and Zipformer CTC models are currently "
+          "supported by qnn for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
index e38dfcbb..ddadbfb9 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
@@ -22,7 +22,10 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
       "sense-voice-use-itn", &use_itn,
       "True to enable inverse text normalization. False to disable it.");
 
-  qnn_config.Register(po);
+  std::string prefix = "sense-voice";
+  ParseOptions p(prefix, po);
+
+  qnn_config.Register(&p);
 }
 
 bool OfflineSenseVoiceModelConfig::Validate() const {
diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
index fd5e0321..0aaaacbb 100644
--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
@@ -8,11 +8,17 @@
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/text-utils.h"
 
 namespace sherpa_onnx {
 
 void OfflineZipformerCtcModelConfig::Register(ParseOptions *po) {
   po->Register("zipformer-ctc-model", &model, "Path to zipformer CTC model");
+
+  std::string prefix = "zipformer-ctc";
+  ParseOptions p(prefix, po);
+
+  qnn_config.Register(&p);
 }
 
 bool OfflineZipformerCtcModelConfig::Validate() const {
@@ -22,6 +28,10 @@ bool OfflineZipformerCtcModelConfig::Validate() const {
     return false;
   }
 
+  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
+    return qnn_config.Validate();
+  }
+
   return true;
 }
 
@@ -29,7 +39,13 @@ std::string OfflineZipformerCtcModelConfig::ToString() const {
   std::ostringstream os;
 
   os << "OfflineZipformerCtcModelConfig(";
-  os << "model=\"" << model << "\")";
+  os << "model=\"" << model << "\"";
+
+  if (!qnn_config.backend_lib.empty()) {
+    os << ", qnn_config=" << qnn_config.ToString() << ", ";
+  }
+
+  os << ")";
 
   return os.str();
 }
diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
index 702575e7..44ff0fff 100644
--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
+++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
@@ -7,6 +7,7 @@
 #include <string>
 
 #include "sherpa-onnx/csrc/parse-options.h"
+#include "sherpa-onnx/csrc/qnn-config.h"
 
 namespace sherpa_onnx {
 
@@ -14,6 +15,7 @@ namespace sherpa_onnx {
 // https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/zipformer/export-onnx-ctc.py
 struct OfflineZipformerCtcModelConfig {
   std::string model;
+  QnnConfig qnn_config;
 
   OfflineZipformerCtcModelConfig() = default;
 
diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
index 2276cd79..4db59535 100644
--- a/sherpa-onnx/csrc/qnn-config.cc
+++ b/sherpa-onnx/csrc/qnn-config.cc
@@ -37,11 +37,8 @@ bool QnnConfig::Validate() const {
     return false;
   }
 
-  if (!FileExists(backend_lib)) {
-    SHERPA_ONNX_LOGE("--qnn-backend-lib: '%s' does not exist",
-                     backend_lib.c_str());
-    return false;
-  }
+  // we don't check whether backend_lib and system_lib exist or not since
+  // dlopen() will find them by searching predefined paths
 
   if (!context_binary.empty() && FileExists(context_binary)) {
     if (system_lib.empty()) {
@@ -50,12 +47,6 @@ bool QnnConfig::Validate() const {
           "--qnn-context-binary");
       return false;
     }
-
-    if (!FileExists(system_lib)) {
-      SHERPA_ONNX_LOGE("--qnn-system-lib: '%s' does not exist",
-                       system_lib.c_str());
-      return false;
-    }
   }
 
   return true;
diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h b/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
new file mode 100644
index 00000000..a7877c8b
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
@@ -0,0 +1,130 @@
+// sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
+#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
+
+#include <ios>
+#include <memory>
+#include <sstream>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h"
+#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../offline-recognizer-ctc-impl.h
+OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+                                 const SymbolTable &sym_table,
+                                 int32_t frame_shift_ms,
+                                 int32_t subsampling_factor);
+
+class OfflineRecognizerZipformerCtcQnnImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerZipformerCtcQnnImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(std::make_unique<OfflineZipformerCtcModelQnn>(
+            config.model_config)) {
+    Init();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerZipformerCtcQnnImpl(Manager *mgr,
+                                       const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineZipformerCtcModelQnn>(
+            mgr, config.model_config)) {
+    Init();
+  }
+
+  void Init() {
+    if (config_.decoding_method == "greedy_search") {
+      if (!symbol_table_.Contains("<blk>") &&
+          !symbol_table_.Contains("<eps>") &&
+          !symbol_table_.Contains("<blank>") &&
+          config_.model_config.omnilingual.model.empty()) {
+        // for omnilingual asr, its blank id is 0
+        SHERPA_ONNX_LOGE(
+            "We expect that tokens.txt contains "
+            "the symbol <blk> or <eps> or <blank> and its ID.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      int32_t blank_id = 0;
+      if (symbol_table_.Contains("<blk>")) {
+        blank_id = symbol_table_["<blk>"];
+      } else if (symbol_table_.Contains("<eps>")) {
+        // for tdnn models of the yesno recipe from icefall
+        blank_id = symbol_table_["<eps>"];
+      } else if (symbol_table_.Contains("<blank>")) {
+        // for Wenet CTC models
+        blank_id = symbol_table_["<blank>"];
+      }
+
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config_.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i != n; ++i) {
+      DecodeStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  // Decode a single stream.
+  // Some models do not support batch size > 1, e.g., WeNet CTC models.
+  void DecodeStream(OfflineStream *s) const {
+    std::vector<float> f = s->GetFrames();
+
+    int32_t vocab_size = model_->VocabSize();
+
+    std::vector<float> log_probs = model_->Run(std::move(f));
+    int32_t num_out_frames = log_probs.size() / vocab_size;
+
+    auto result =
+        decoder_->Decode(log_probs.data(), num_out_frames, vocab_size);
+
+    int32_t frame_shift_ms = 10;
+
+    auto r = Convert(result, symbol_table_, frame_shift_ms,
+                     model_->SubsamplingFactor());
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineZipformerCtcModelQnn> model_;
+  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
diff --git a/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
new file mode 100644
index 00000000..6af8a49e
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
@@ -0,0 +1,247 @@
+// sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <mutex>  // NOLINT
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/qnn/macros.h"
+#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
+#include "sherpa-onnx/csrc/qnn/qnn-model.h"
+
+namespace sherpa_onnx {
+
+class OfflineZipformerCtcModelQnn::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    backend_ = std::make_unique<QnnBackend>(
+        config.zipformer_ctc.qnn_config.backend_lib, config_.debug);
+
+    const auto &context_binary =
+        config_.zipformer_ctc.qnn_config.context_binary;
+
+    if (context_binary.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from model lib since context binary is not given");
+      }
+
+      InitFromModelLib();
+
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Skip generating context binary since you don't provide a path to "
+            "save it");
+      }
+    } else if (!FileExists(context_binary)) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from model lib since context binary '%s' does not exist",
+            context_binary.c_str());
+      }
+
+      InitFromModelLib();
+
+      CreateContextBinary();
+    } else {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("Init from context binary '%s'",
+                         context_binary.c_str());
+      }
+      InitFromContextBinary();
+    }
+
+    PostInit();
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    SHERPA_ONNX_LOGE(
+        "Please copy all files from assets to SD card and set assetManager to "
+        "null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  std::vector<float> Run(std::vector<float> features) {
+    int32_t num_frames = features.size() / feat_dim_;
+
+    if (num_frames != max_num_frames_) {
+      if (num_frames > max_num_frames_) {
+        SHERPA_ONNX_LOGE(
+            "Number of input frames %d is too large. Truncate it to %d frames.",
+            num_frames, max_num_frames_);
+
+        SHERPA_ONNX_LOGE(
+            "Recognition result may be truncated/incomplete. Please select a "
+            "model accepting longer audios.");
+      }
+
+      features.resize(max_num_frames_ * feat_dim_);
+
+      num_frames = max_num_frames_;
+    }
+
+    std::lock_guard<std::mutex> lock(mutex_);
+
+    model_->SetInputTensorData("x", features.data(), features.size());
+
+    model_->Run();
+
+    return model_->GetOutputTensorData("log_probs");
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+  int32_t SubsamplingFactor() const { return subsampling_factor_; }
+
+ private:
+  void InitFromModelLib() {
+    backend_->InitContext();
+
+    model_ = std::make_unique<QnnModel>(config_.zipformer_ctc.model,
+                                        backend_.get(), config_.debug);
+  }
+
+  void InitFromContextBinary() {
+    model_ = std::make_unique<QnnModel>(
+        config_.zipformer_ctc.qnn_config.context_binary,
+        config_.zipformer_ctc.qnn_config.system_lib, backend_.get(),
+        BinaryContextTag{}, config_.debug);
+  }
+
+  void CreateContextBinary() {
+    const auto &context_binary =
+        config_.zipformer_ctc.qnn_config.context_binary;
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
+    }
+
+    bool ok = model_->SaveBinaryContext(context_binary);
+
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
+                       context_binary.c_str());
+    }
+
+    if (config_.debug && ok) {
+      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
+      SHERPA_ONNX_LOGE(
+          "It should be super fast the next time you init the system.");
+      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
+    }
+  }
+
+  void PostInit() { CheckModel(); }
+
+  void CheckModel() {
+    const auto &input_tensor_names = model_->InputTensorNames();
+    if (input_tensor_names.size() != 1) {
+      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
+                       static_cast<int32_t>(input_tensor_names.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[0] != "x") {
+      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
+                       input_tensor_names[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> x_shape = model_->TensorShape(input_tensor_names[0]);
+    if (x_shape.size() != 3) {
+      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
+                       static_cast<int32_t>(x_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    max_num_frames_ = x_shape[1];
+    feat_dim_ = x_shape[2];
+
+    if (!model_->HasTensor("log_probs")) {
+      SHERPA_ONNX_LOGE("Model does not have output node 'log_probs'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    auto out_shape = model_->TensorShape("log_probs");
+    vocab_size_ = out_shape[2];
+
+    subsampling_factor_ = max_num_frames_ / out_shape[1];
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("max_num_frames: %d", max_num_frames_);
+      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
+      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
+      SHERPA_ONNX_LOGE("subsampling_factor: %d", subsampling_factor_);
+    }
+  }
+
+ private:
+  std::mutex mutex_;
+
+  OfflineModelConfig config_;
+
+  std::unique_ptr<QnnBackend> backend_;
+  std::unique_ptr<QnnModel> model_;
+
+  int32_t max_num_frames_ = 0;
+  int32_t feat_dim_ = 0;
+  int32_t vocab_size_ = 0;
+  int32_t subsampling_factor_ = 1;
+};
+
+OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineZipformerCtcModelQnn::~OfflineZipformerCtcModelQnn() = default;
+
+std::vector<float> OfflineZipformerCtcModelQnn::Run(
+    std::vector<float> features) const {
+  return impl_->Run(std::move(features));
+}
+
+int32_t OfflineZipformerCtcModelQnn::VocabSize() const {
+  return impl_->VocabSize();
+}
+
+int32_t OfflineZipformerCtcModelQnn::SubsamplingFactor() const {
+  return impl_->SubsamplingFactor();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
new file mode 100644
index 00000000..f11cd2d5
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
@@ -0,0 +1,39 @@
+// sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
+#define SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
+
+#include <memory>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineZipformerCtcModelQnn {
+ public:
+  ~OfflineZipformerCtcModelQnn();
+
+  explicit OfflineZipformerCtcModelQnn(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineZipformerCtcModelQnn(Manager *mgr, const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (num_frames, feature_dim)
+   * @returns Return a tensor of shape (num_output_frames, vocab_size)
+   */
+  std::vector<float> Run(std::vector<float> features) const;
+
+  int32_t VocabSize() const;
+  int32_t SubsamplingFactor() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
index 441820da..4e90950d 100644
--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
@@ -134,7 +134,7 @@ class QnnModel::Impl {
     if (ret != QNN_SUCCESS) {
       SHERPA_ONNX_LOGE(
           "Failed to get context binary info from '%s'. ret code is %d",
-          binary_context_file.c_str(), ret);
+          binary_context_file.c_str(), static_cast<int32_t>(ret));
 
       qnn_system_interface_.systemContextFree(sys_ctx_handle);
       return false;
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
index 115f0c2d..a1624da8 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
@@ -4,9 +4,11 @@ package com.k2fsa.sherpa.onnx;
 
 public class OfflineZipformerCtcModelConfig {
     private final String model;
+    private final QnnConfig qnnConfig;
 
     private OfflineZipformerCtcModelConfig(Builder builder) {
         this.model = builder.model;
+        this.qnnConfig = builder.qnnConfig;
     }
 
     public static Builder builder() {
@@ -17,8 +19,13 @@ public class OfflineZipformerCtcModelConfig {
         return model;
     }
 
+    public QnnConfig getQnnConfig() {
+        return qnnConfig;
+    }
+
     public static class Builder {
         private String model = "";
+        private QnnConfig qnnConfig = QnnConfig.builder().build();
 
         public OfflineZipformerCtcModelConfig build() {
             return new OfflineZipformerCtcModelConfig(this);
@@ -28,5 +35,10 @@ public class OfflineZipformerCtcModelConfig {
             this.model = model;
             return this;
         }
+
+        public Builder setQnnConfig(QnnConfig qnnConfig) {
+            this.qnnConfig = qnnConfig;
+            return this;
+        }
     }
 }
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index ed940cbf..3995d40a 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -203,6 +203,24 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.zipformer_ctc.model, model,
                               zipformer_ctc_config_cls, zipformer_ctc_config);
 
+  fid = env->GetFieldID(zipformer_ctc_config_cls, "qnnConfig",
+                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
+
+  qnn_config = env->GetObjectField(zipformer_ctc_config, fid);
+  qnn_config_cls = env->GetObjectClass(qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.zipformer_ctc.qnn_config.backend_lib, backendLib,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.zipformer_ctc.qnn_config.context_binary, contextBinary,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.zipformer_ctc.qnn_config.system_lib, systemLib,
+      qnn_config_cls, qnn_config);
+
   // wenet ctc
   fid = env->GetFieldID(model_config_cls, "wenetCtc",
                         "Lcom/k2fsa/sherpa/onnx/OfflineWenetCtcModelConfig;");
@@ -332,7 +350,7 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile(JNIEnv *env,
     }
   }
 
-  if (config.model_config.provider != "qnn" && !config.Validate()) {
+  if (!config.Validate()) {
     SHERPA_ONNX_LOGE("Errors found in config!");
     return 0;
   }
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index d564f9d3..b1b4bd8e 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -34,6 +34,7 @@ data class OfflineDolphinModelConfig(
 
 data class OfflineZipformerCtcModelConfig(
     var model: String = "",
+    var qnnConfig: QnnConfig = QnnConfig(),
 )
 
 data class OfflineWenetCtcModelConfig(
@@ -768,6 +769,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                     ),
                 ),
                 tokens = "$modelDir/tokens.txt",
+                debug = true,
             )
         }
 
@@ -784,6 +786,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                     ),
                 ),
                 tokens = "$modelDir/tokens.txt",
+                debug = true,
             )
         }
 
@@ -800,6 +803,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                     ),
                 ),
                 tokens = "$modelDir/tokens.txt",
+                debug = true,
             )
         }
 
@@ -930,6 +934,185 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                 tokens = "$modelDir/tokens.txt",
             )
         }
+
+        9011 -> {
+            val modelDir = "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
+
+        9012 -> {
+            val modelDir = "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
+
+        9013 -> {
+            val modelDir = "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+                debug = true,
+            )
+        }
+
+        9014 -> {
+            val modelDir = "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9015 -> {
+            val modelDir = "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9016 -> {
+            val modelDir = "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9017 -> {
+            val modelDir = "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9018 -> {
+            val modelDir = "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9019 -> {
+            val modelDir = "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9020 -> {
+            val modelDir = "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9021 -> {
+            val modelDir = "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                zipformerCtc = OfflineZipformerCtcModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
     }
     return null
 }

commit b4ab72e51bde1cb5bdf2fa0996a2aa21dbf9c634
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 24 16:00:44 2025 +0800

    Add spaces between English words for Homophone replacer. (#2817)
    
    This pull request refines the output formatting of the Homophone Replacer by implementing more robust spacing logic. It ensures that English words are properly separated and that the final output does not contain any unnecessary trailing spaces, leading to a more natural and correct text presentation.

diff --git a/sherpa-onnx/csrc/homophone-replacer.cc b/sherpa-onnx/csrc/homophone-replacer.cc
index 9dd223d5..db3325eb 100644
--- a/sherpa-onnx/csrc/homophone-replacer.cc
+++ b/sherpa-onnx/csrc/homophone-replacer.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/homophone-replacer.h"
 
+#include <cctype>
 #include <fstream>
 #include <memory>
 #include <sstream>
@@ -176,6 +177,9 @@ class HomophoneReplacer::Impl {
           current_pronunciations.clear();
         }
         ans += w;
+        if (isalpha(w[0])) {
+          ans.push_back(' ');
+        }
         continue;
       }
 
@@ -196,6 +200,10 @@ class HomophoneReplacer::Impl {
       SHERPA_ONNX_LOGE("Output text: '%s'", ans.c_str());
     }
 
+    if (!ans.empty() && ans.back() == ' ') {
+      ans.pop_back();
+    }
+
     return ans;
   }
 

commit 28c27bd4275f2d0e4231663b25babf6b1bd908e6
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 24 15:57:48 2025 +0800

    Export zipformer ctc models to QNN (#2815)

diff --git a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
new file mode 100644
index 00000000..4b411384
--- /dev/null
+++ b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
@@ -0,0 +1,303 @@
+name: export-zipformer-ctc-to-qnn-20250703
+
+on:
+  push:
+    branches:
+      - qnn-zipformer-ctc-models
+  workflow_dispatch:
+
+concurrency:
+  group: export-zipformer-ctc-to-qnn-20250703-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-zipformer-ctc-to-qnn-20250703:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: ${{ matrix.input_in_seconds }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-22.04]
+        python-version: ["3.10"]
+        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Display NDK HOME
+        shell: bash
+        run: |
+          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
+          ls -lh ${ANDROID_NDK_LATEST_HOME}
+
+      - name: Create Python virtual environment
+        shell: bash
+        run: |
+          python3 -m venv py310
+          which python3
+          source py310/bin/activate
+          which python3
+
+      - name: Show ndk-build help
+        shell: bash
+        run: |
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          ndk-build --help
+
+      - name: Download toolkit
+        shell: bash
+        run: |
+          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
+          ls -lh v2.33.0.250327.zip
+
+      - name: Unzip toolkit
+        shell: bash
+        run: |
+          unzip v2.33.0.250327.zip
+
+      - name: Show
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---ls -lh qairt---"
+
+          ls -lh qairt
+
+          echo "---"
+
+      - name: Install linux dependencies
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---"
+
+          ls -lh qairt
+
+          cd qairt/2.33.0.250327/bin
+          source envsetup.sh
+
+          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          cd qairt/2.33.0.250327/bin
+          source envsetup.sh
+
+          python3 -m pip install \
+            mock \
+            numpy \
+            opencv-python \
+            optuna \
+            packaging \
+            pandas \
+            paramiko \
+            pathlib2 \
+            pillow \
+            plotly \
+            protobuf \
+            psutil \
+            pydantic \
+            pytest \
+            pyyaml \
+            rich \
+            scikit-optimize \
+            scipy \
+            six \
+            tabulate \
+            typing-extensions \
+            xlsxwriter
+
+          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
+
+          which python3
+
+      - name: Install onnx dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+          python3 -m pip install --upgrade \
+            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+            kaldi_native_fbank \
+            pip \
+            "numpy<2" \
+            onnx==1.17.0 \
+            onnxruntime==1.17.1 \
+            soundfile \
+            librosa \
+            onnxsim \
+            sentencepiece \
+            pyyaml
+
+          which python3
+
+      - name: Show qnn-onnx-converter help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-onnx-converter --help
+
+      - name: Show qnn-model-lib-generator help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-model-lib-generator --help
+
+      - name: Show qnn-net-run help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-net-run --help
+
+      - name: Run ${{ matrix.input_in_seconds }}
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+
+          mkdir tmp
+
+          cd tmp
+
+          t=${{ matrix.input_in_seconds }}
+          num_frames=$(($t*100))
+
+          echo "num_frames: $num_frames"
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
+          chmod +x generate_test_data.py
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
+
+          ./generate_test_data.py --num-frames $num_frames --wav 0.wav
+          ./generate_test_data.py --num-frames $num_frames --wav 1.wav
+          ./generate_test_data.py --num-frames $num_frames --wav 8k.wav
+
+          echo -e "0.raw\n1.raw\n8k.raw" > input_list.txt
+
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/model-$t-seconds.onnx
+
+          python3 ../scripts/pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
+
+
+          echo "export to qnn"
+          echo "----------$t----------"
+
+          qnn-onnx-converter \
+            --input_network model-$t-seconds.onnx \
+            --output_path ./model-$t-seconds-quantized \
+            --out_node log_probs \
+            --input_list ./input_list.txt \
+            --use_native_input_files  \
+            --input_dtype x float32 \
+            --act_bitwidth 16 \
+            --bias_bitwidth 32 \
+            --input_layout x NTF
+
+          ls -lh
+          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
+          echo "----"
+          ls -lh
+
+          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+            -c "model-$t-seconds-quantized.cpp" \
+            -b "model-$t-seconds-quantized.bin" \
+            -o model_libs > /dev/null 2>&1
+
+          ls -lh model_libs/*/
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "collect results"
+
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v model_libs/$p/lib*.so $d/libmodel.so
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+
+            echo "num_frames=$num_frames" > $d/info.txt
+            echo "target=$p" >> $d/info.txt
+
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: ${{ matrix.input_in_seconds }}-seconds
+          path: ./tmp/*.json
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          tag: asr-models-qnn

commit 16d62b6a08f617c2bd6d21d411911c6462607f08
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 20 17:22:07 2025 +0800

    Add Android demo with QNN (Qualcomm NPU) for SenseVoice ASR (#2803)

diff --git a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
new file mode 100644
index 00000000..60191019
--- /dev/null
+++ b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
@@ -0,0 +1,192 @@
+name: apk-qnn-vad-asr-simulated-streaming
+
+on:
+  push:
+    branches:
+      - apk
+      - android-qnn-2
+
+  workflow_dispatch:
+
+concurrency:
+  group: apk-qnn-vad-asr-simulated-streaming-${{ github.ref }}
+  cancel-in-progress: true
+
+permissions:
+  contents: write
+
+jobs:
+  simulated_streaming_asr:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    runs-on: ${{ matrix.os }}
+    name: ${{ matrix.index }}/${{ matrix.total }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        total: ["5"]
+        index: ["0", "1", "2", "3", "4"]
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      # https://github.com/actions/setup-java
+      - uses: actions/setup-java@v4
+        with:
+          distribution: 'temurin' # See 'Supported distributions' for available options
+          java-version: '21'
+
+      - name: ccache
+        uses: hendrikmuhs/ccache-action@v1.2
+        with:
+          key: ${{ matrix.os }}-android
+
+      - name: Display NDK HOME
+        shell: bash
+        run: |
+          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
+          ls -lh ${ANDROID_NDK_LATEST_HOME}
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          python3 -m pip install --upgrade pip jinja2
+
+      - name: Setup build tool version variable
+        shell: bash
+        run: |
+          echo "---"
+          ls -lh /usr/local/lib/android/
+          echo "---"
+
+          ls -lh /usr/local/lib/android/sdk
+          echo "---"
+
+          ls -lh /usr/local/lib/android/sdk/build-tools
+          echo "---"
+
+          BUILD_TOOL_VERSION=$(ls /usr/local/lib/android/sdk/build-tools/ | tail -n 1)
+          echo "BUILD_TOOL_VERSION=$BUILD_TOOL_VERSION" >> $GITHUB_ENV
+          echo "Last build tool version is: $BUILD_TOOL_VERSION"
+
+      - name: Generate build script
+        shell: bash
+        run: |
+          cd scripts/apk
+
+          total=${{ matrix.total }}
+          index=${{ matrix.index }}
+
+          ./generate-qnn-vad-asr-apk-script.py --total $total --index $index
+
+          chmod +x build-apk-qnn-vad-asr-simulate-streaming.sh
+          mv -v ./build-apk-qnn-vad-asr-simulate-streaming.sh ../..
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: build-script-${{ matrix.total }}-${{ matrix.index }}
+          path: ./build-apk-qnn-vad-asr-simulate-streaming.sh
+
+      - name: build APK
+        shell: bash
+        run: |
+          export CMAKE_CXX_COMPILER_LAUNCHER=ccache
+          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
+          cmake --version
+
+          export ANDROID_NDK=$ANDROID_NDK_LATEST_HOME
+          ./build-apk-qnn-vad-asr-simulate-streaming.sh
+
+      - name: Display APK
+        shell: bash
+        run: |
+          ls -lh ./apks/
+          du -h -d1 .
+
+      # https://github.com/marketplace/actions/sign-android-release
+      - uses: r0adkll/sign-android-release@v1
+        name: Sign app APK
+        with:
+          releaseDirectory: ./apks
+          signingKeyBase64: ${{ secrets.ANDROID_SIGNING_KEY }}
+          alias: ${{ secrets.ANDROID_SIGNING_KEY_ALIAS }}
+          keyStorePassword: ${{ secrets.ANDROID_SIGNING_KEY_STORE_PASSWORD }}
+        env:
+          BUILD_TOOLS_VERSION: ${{ env.BUILD_TOOL_VERSION }}
+
+      - name: Display APK after signing
+        shell: bash
+        run: |
+          ls -lh ./apks/
+          du -h -d1 .
+
+      - name: Rename APK after signing
+        shell: bash
+        run: |
+          cd apks
+          rm -fv signingKey.jks
+          rm -fv *.apk.idsig
+          rm -fv *-aligned.apk
+
+          all_apks=$(ls -1 *-signed.apk)
+          echo "----"
+          echo $all_apks
+          echo "----"
+          for apk in ${all_apks[@]}; do
+            n=$(echo $apk | sed -e s/-signed//)
+            mv -v $apk $n
+          done
+
+          cd ..
+
+          ls -lh ./apks/
+          du -h -d1 .
+
+      - name: Display APK after rename
+        shell: bash
+        run: |
+          ls -lh ./apks/
+          du -h -d1 .
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_LFS_SKIP_SMUDGE=1
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+
+            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+            echo "SHERPA_ONNX_VERSION $SHERPA_ONNX_VERSION"
+
+            git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-apk huggingface
+            cd huggingface
+            du -h -d1 .
+            git fetch
+            git pull
+            git merge -m "merge remote" --ff origin main
+
+            d=qnn-vad-asr-simulated-streaming/$SHERPA_ONNX_VERSION
+            mkdir -p $d
+            cp -v ../apks/*.apk $d/
+            git status
+            git lfs track "*.apk"
+            git add .
+            git commit -m "add more apks for qnn"
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-apk main
diff --git a/.gitignore b/.gitignore
index 768fc466..060ce16b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -157,4 +157,5 @@ am.mvn
 config.yaml
 configuration.json
 sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
 sherpa-onnx-paraformer-zh-int8-2025-10-07
diff --git a/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
new file mode 120000
index 00000000..cfd1fc18
--- /dev/null
+++ b/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
@@ -0,0 +1 @@
+../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
\ No newline at end of file
diff --git a/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
new file mode 120000
index 00000000..cfd1fc18
--- /dev/null
+++ b/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
@@ -0,0 +1 @@
+../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
\ No newline at end of file
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
index c97f7eb2..0c69c725 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
@@ -14,6 +14,19 @@
         android:supportsRtl="true"
         android:theme="@style/Theme.SimulateStreamingAsr"
         tools:targetApi="31">
+
+        <!--
+        required by qnn
+
+        If you don't add it, you would get an error from the deviceCreate() API
+        and the error code is 14001
+
+        It is located at /vendor/lib64/libcdsprpc.so on your Phone
+        -->
+        <uses-native-library
+            android:name="libcdsprpc.so"
+            android:required="false"/>
+
         <activity
             android:name=".MainActivity"
             android:exported="true"
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
new file mode 120000
index 00000000..cfd1fc18
--- /dev/null
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
@@ -0,0 +1 @@
+../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
\ No newline at end of file
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
index 76282453..8663e4ac 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
@@ -57,9 +57,6 @@ class MainActivity : ComponentActivity() {
             }
         }
         ActivityCompat.requestPermissions(this, permissions, REQUEST_RECORD_AUDIO_PERMISSION)
-
-        SimulateStreamingAsr.initOfflineRecognizer(this.assets, this.application)
-        SimulateStreamingAsr.initVad(this.assets)
     }
 
     @Deprecated("Deprecated in Java")
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
index 3dd22b47..1afcf2a9 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
@@ -1,6 +1,6 @@
 package com.k2fsa.sherpa.onnx.simulate.streaming.asr
 
-import android.app.Application
+import android.content.Context
 import android.content.res.AssetManager
 import android.util.Log
 import com.k2fsa.sherpa.onnx.HomophoneReplacerConfig
@@ -11,7 +11,51 @@ import com.k2fsa.sherpa.onnx.getOfflineModelConfig
 import com.k2fsa.sherpa.onnx.getVadModelConfig
 import java.io.File
 import java.io.FileOutputStream
-import java.io.IOException
+import java.io.InputStream
+import java.io.OutputStream
+
+
+fun assetExists(assetManager: AssetManager, path: String): Boolean {
+    val dir = path.substringBeforeLast('/', "")
+    val fileName = path.substringAfterLast('/')
+
+    val files = assetManager.list(dir) ?: return false
+    return files.contains(fileName)
+}
+
+fun copyAssetToInternalStorage(path: String, context: Context): String {
+    val targetRoot = context.filesDir
+    val outFile = File(targetRoot, path)
+
+    if (!assetExists(context.assets, path = path)) {
+        // for context binary, if it is does not exist, we return a path
+        // that can be written to
+        outFile.parentFile?.mkdirs()
+        Log.i(TAG, "$path does not exist, return ${outFile.absolutePath}")
+        return outFile.absolutePath
+    }
+
+    if (outFile.exists()) {
+        val assetSize = context.assets.open(path).use { it.available() }
+        if (outFile.length() == assetSize.toLong()) {
+            Log.i(TAG, "$targetRoot/$path already exists, skip copying, return $targetRoot/$path")
+
+            return "$targetRoot/$path"
+        }
+    }
+
+    outFile.parentFile?.mkdirs()
+
+    context.assets.open(path).use { input: InputStream ->
+        FileOutputStream(outFile).use { output: OutputStream ->
+            input.copyTo(output)
+        }
+    }
+    Log.i(TAG, "Copied $path to $targetRoot/$path")
+
+    return outFile.absolutePath
+}
+
 
 object SimulateStreamingAsr {
     private var _recognizer: OfflineRecognizer? = null
@@ -26,7 +70,7 @@ object SimulateStreamingAsr {
             return _vad!!
         }
 
-    fun initOfflineRecognizer(assetManager: AssetManager? = null, application: Application) {
+    fun initOfflineRecognizer(context: Context, asrModelType: Int) {
         synchronized(this) {
             if (_recognizer != null) {
                 return
@@ -35,13 +79,12 @@ object SimulateStreamingAsr {
             // Please change getOfflineModelConfig() to add new models
             // See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html
             // for a list of available models
-            val asrModelType = 15
             val asrRuleFsts: String?
             asrRuleFsts = null
             Log.i(TAG, "Select model type $asrModelType for ASR")
 
             val useHr = false
-            val hr =  HomophoneReplacerConfig(
+            val hr = HomophoneReplacerConfig(
                 // Used only when useHr is true
                 // Please download the following 2 files from
                 // https://github.com/k2-fsa/sherpa-onnx/releases/tag/hr-files
@@ -69,6 +112,46 @@ object SimulateStreamingAsr {
                 config.hr = hr
             }
 
+            var assetManager: AssetManager? = context.assets
+
+            if (config.modelConfig.provider == "qnn") {
+                // We assume you have copied files like libQnnHtpV81Skel.so to jniLibs/arm64-v8a
+                Log.i(TAG, "nativelibdir: ${context.applicationInfo.nativeLibraryDir}")
+
+                // If we don't set the environment variable for ADSP_LIBRARY_PATH, we will see
+                // the error code 1008 from qnn_interface.deviceCreate()
+                // See also
+                // https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
+                OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
+
+                // for qnn, we need to copy *.so files from assets folder to sd card
+                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()) {
+                    Log.e(TAG, "You should provide libQnnHtp.so for qnn")
+                    throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
+                }
+                config.modelConfig.tokens =
+                    copyAssetToInternalStorage(config.modelConfig.tokens, context)
+
+                config.modelConfig.senseVoice.model =
+                    copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
+
+                config.modelConfig.senseVoice.qnnConfig.contextBinary = copyAssetToInternalStorage(
+                    config.modelConfig.senseVoice.qnnConfig.contextBinary,
+                    context
+                )
+
+                if (config.hr.lexicon.isNotEmpty()) {
+                    config.hr.lexicon = copyAssetToInternalStorage(config.hr.lexicon, context)
+                }
+
+                if (config.hr.ruleFsts.isNotEmpty()) {
+                    // it assumes there is only one fst. otherwise, you need to copy each fst separately
+                    config.hr.ruleFsts = copyAssetToInternalStorage(config.hr.ruleFsts, context)
+                }
+
+                assetManager = null
+            }
+
             _recognizer = OfflineRecognizer(
                 assetManager = assetManager,
                 config = config,
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
index 36c6a3be..e6d5cb3d 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
@@ -25,6 +25,7 @@ import androidx.compose.foundation.lazy.rememberLazyListState
 import androidx.compose.material3.Button
 import androidx.compose.material3.Text
 import androidx.compose.runtime.Composable
+import androidx.compose.runtime.LaunchedEffect
 import androidx.compose.runtime.getValue
 import androidx.compose.runtime.mutableStateListOf
 import androidx.compose.runtime.mutableStateOf
@@ -46,6 +47,7 @@ import kotlinx.coroutines.CoroutineScope
 import kotlinx.coroutines.Dispatchers
 import kotlinx.coroutines.channels.Channel
 import kotlinx.coroutines.launch
+import kotlinx.coroutines.withContext
 
 private var audioRecord: AudioRecord? = null
 
@@ -63,6 +65,29 @@ fun HomeScreen() {
     val lazyColumnListState = rememberLazyListState()
     val coroutineScope = rememberCoroutineScope()
 
+    var isInitialized by remember { mutableStateOf(false) }
+
+    // we change asrModelType in github actions
+    val asrModelType = 15
+
+    LaunchedEffect(Unit) {
+        if (asrModelType >= 9000) {
+            resultList.add("Using QNN for Qualcomm NPU (HTP backend)")
+            resultList.add("It takes about 10s for the first run to start")
+            resultList.add("Later runs require less than 1 second")
+        }
+
+        withContext(Dispatchers.Default) {
+            // Call your heavy initialization off the main thread
+            SimulateStreamingAsr.initOfflineRecognizer(activity, asrModelType)
+            SimulateStreamingAsr.initVad(activity.assets)
+        }
+
+        // Back on the Main thread: update UI state
+        isInitialized = true
+        resultList.clear()
+    }
+
     val onRecordingButtonClick: () -> Unit = {
         isStarted = !isStarted
         if (isStarted) {
@@ -211,13 +236,32 @@ fun HomeScreen() {
             audioRecord = null
         }
     }
+
     Box(
         modifier = Modifier.fillMaxSize(),
         contentAlignment = Alignment.TopCenter,
     ) {
         Column(modifier = Modifier) {
+            if (!isInitialized) {
+                Row(
+                    modifier = Modifier.fillMaxWidth(),
+                    horizontalArrangement = Arrangement.Center,
+                ) {
+                    Text(text = "Initializing... Please wait")
+                }
+            }
+            if (asrModelType >= 9000) {
+                Row(
+                    modifier = Modifier.fillMaxWidth(),
+                    horizontalArrangement = Arrangement.Center,
+                ) {
+                    Text(text = "Qualcomm NPU (HTP backend with QNN)")
+                }
+            }
+
             HomeButtonRow(
                 isStarted = isStarted,
+                isInitialized = isInitialized,
                 onRecordingButtonClick = onRecordingButtonClick,
                 onCopyButtonClick = {
                     if (resultList.isNotEmpty()) {
@@ -255,7 +299,7 @@ fun HomeScreen() {
                     state = lazyColumnListState
                 ) {
                     itemsIndexed(resultList) { index, line ->
-                        Text(text = "${index+1}: $line")
+                        Text(text = "${index + 1}: $line")
                     }
                 }
             }
@@ -269,6 +313,7 @@ fun HomeScreen() {
 private fun HomeButtonRow(
     modifier: Modifier = Modifier,
     isStarted: Boolean,
+    isInitialized: Boolean,
     onRecordingButtonClick: () -> Unit,
     onCopyButtonClick: () -> Unit,
     onClearButtonClick: () -> Unit,
@@ -278,20 +323,27 @@ private fun HomeButtonRow(
         horizontalArrangement = Arrangement.Center,
     ) {
         Button(
-            onClick = onRecordingButtonClick
+            onClick = onRecordingButtonClick,
+            enabled = isInitialized,
         ) {
             Text(text = stringResource(if (isStarted) R.string.stop else R.string.start))
         }
 
         Spacer(modifier = Modifier.width(24.dp))
 
-        Button(onClick = onCopyButtonClick) {
+        Button(
+            onClick = onCopyButtonClick,
+            enabled = isInitialized,
+        ) {
             Text(text = stringResource(id = R.string.copy))
         }
 
         Spacer(modifier = Modifier.width(24.dp))
 
-        Button(onClick = onClearButtonClick) {
+        Button(
+            onClick = onClearButtonClick,
+            enabled = isInitialized,
+        ) {
             Text(text = stringResource(id = R.string.clear))
         }
     }
diff --git a/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
new file mode 120000
index 00000000..cfd1fc18
--- /dev/null
+++ b/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
@@ -0,0 +1 @@
+../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
\ No newline at end of file
diff --git a/kotlin-api-examples/QnnConfig.kt b/kotlin-api-examples/QnnConfig.kt
new file mode 120000
index 00000000..951cd308
--- /dev/null
+++ b/kotlin-api-examples/QnnConfig.kt
@@ -0,0 +1 @@
+../sherpa-onnx/kotlin-api/QnnConfig.kt
\ No newline at end of file
diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
index 6dc1d164..427d19fb 100755
--- a/kotlin-api-examples/run.sh
+++ b/kotlin-api-examples/run.sh
@@ -275,6 +275,7 @@ function testOfflineAsr() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_offline_asr.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
@@ -304,6 +305,7 @@ function testInverseTextNormalizationOfflineAsr() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_itn_offline_asr.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
@@ -457,6 +459,7 @@ function testOfflineSenseVoiceWithHr() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_offline_sense_voice_with_hr.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
@@ -478,6 +481,7 @@ function testOfflineNeMoCanary() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_offline_nemo_canary.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
@@ -500,6 +504,7 @@ function testOfflineOmnilingualAsrCtc() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_offline_omnilingual_asr_ctc.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
@@ -521,6 +526,7 @@ function testOfflineWenetCtc() {
   kotlinc-jvm -include-runtime -d $out_filename \
     test_offline_wenet_ctc.kt \
     FeatureConfig.kt \
+    QnnConfig.kt \
     HomophoneReplacerConfig.kt \
     OfflineRecognizer.kt \
     OfflineStream.kt \
diff --git a/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in b/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in
new file mode 100644
index 00000000..6a9398f4
--- /dev/null
+++ b/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in
@@ -0,0 +1,132 @@
+#!/usr/bin/env bash
+#
+# Auto generated! Please DO NOT EDIT!
+
+# Please set the environment variable ANDROID_NDK
+# before running this script
+
+# Inside the $ANDROID_NDK directory, you can find a binary ndk-build
+# and some other files like the file "build/cmake/android.toolchain.cmake"
+
+set -ex
+
+log() {
+  # This function is from espnet
+  local fname=${BASH_SOURCE[1]##*/}
+  echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
+}
+
+SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+log "Building simulated-streaming VAD + ASR APK + QNN for sherpa-onnx v${SHERPA_ONNX_VERSION}"
+
+export SHERPA_ONNX_ENABLE_TTS=OFF
+
+export SHERPA_ONNX_ENABLE_QNN=ON
+
+log "Download qnn header files"
+
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/qnn-include-2.40.0.251030.tar.bz2
+tar xf qnn-include-2.40.0.251030.tar.bz2
+rm qnn-include-2.40.0.251030.tar.bz2
+ls -lh qnn-include-2.40.0.251030
+
+export QNN_SDK_ROOT=$PWD/qnn-include-2.40.0.251030
+
+log "====================arm64-v8a================="
+./build-android-arm64-v8a.sh
+
+cp -v ./build-android-arm64-v8a/install/lib/*.so ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
+
+log "=======Download qnn libs============"
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/qnn-libs-2.40.0.251030.tar.bz2
+tar xvf qnn-libs-2.40.0.251030.tar.bz2
+rm qnn-libs-2.40.0.251030.tar.bz2
+cp -v qnn-libs-2.40.0.251030/*.so ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
+
+rm -rf qnn-libs-2.40.0.251030
+
+ls -lh ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
+
+mkdir -p apks
+
+{% for model in model_list %}
+pushd ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/
+model_name={{ model.model_name }}-android-aarch64
+type={{ model.idx }}
+lang={{ model.lang }}
+short_name={{ model.short_name }}
+
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/${model_name}.tar.bz2
+tar xvf ${model_name}.tar.bz2
+
+{% if model.use_hr %}
+  if [ ! -f lexicon.txt ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lexicon.txt
+  fi
+
+  if [ ! -f replace.fst ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/replace.fst
+  fi
+{% endif %}
+
+{{ model.cmd }}
+
+rm -rf  *.tar.bz2
+ls -lh $model_name
+
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx
+
+popd
+# Now we are at the project root directory
+
+git checkout .
+
+pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens
+sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./Home.kt
+popd
+
+pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr
+
+{% if model.use_hr %}
+  sed -i.bak s/"useHr = false/useHr = true/" ./SimulateStreamingAsr.kt
+{% endif %}
+
+{% if model.rule_fsts %}
+  rule_fsts={{ model.rule_fsts }}
+  sed -i.bak s%"asrRuleFsts = null"%"asrRuleFsts = \"$rule_fsts\""% ./MainActivity.kt
+{% endif %}
+
+git diff
+popd
+
+for arch in arm64-v8a; do
+  log "------------------------------------------------------------"
+  log "build simulated-streaming ASR apk for $arch"
+  log "------------------------------------------------------------"
+  src_arch=$arch
+  if [ $arch == "armeabi-v7a" ]; then
+    src_arch=armv7-eabi
+  elif [ $arch == "x86_64" ]; then
+    src_arch=x86-64
+  fi
+
+  pushd ./android/SherpaOnnxSimulateStreamingAsr
+  sed -i.bak s/2048/9012/g ./gradle.properties
+  git diff ./gradle.properties
+  ./gradlew assembleRelease
+  popd
+
+  mv android/SherpaOnnxSimulateStreamingAsr/app/build/outputs/apk/release/app-release-unsigned.apk ./apks/sherpa-onnx-${SHERPA_ONNX_VERSION}-qnn-$arch-simulated_streaming_asr-$lang-$short_name.apk
+  ls -lh apks
+done
+
+rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/$model_name
+rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/lexicon.txt
+rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/replace.fst
+
+{% endfor %}
+
+git checkout .
+
+ls -lh apks/
diff --git a/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in b/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
index d315c973..7051883d 100644
--- a/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
+++ b/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
@@ -44,12 +44,6 @@ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/$
 tar xvf ${model_name}.tar.bz2
 
 {% if model.use_hr %}
-  if [ ! -d dict ]; then
-    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/dict.tar.bz2
-    tar xvf dict.tar.bz2
-    rm dict.tar.bz2
-  fi
-
   if [ ! -f lexicon.txt ]; then
     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lexicon.txt
   fi
@@ -70,8 +64,12 @@ popd
 # Now we are at the project root directory
 
 git checkout .
+
+pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens
+sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./Home.kt
+popd
+
 pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr
-sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./SimulateStreamingAsr.kt
 
 {% if model.use_hr %}
   sed -i.bak s/"useHr = false/useHr = true/" ./SimulateStreamingAsr.kt
@@ -112,7 +110,6 @@ for arch in arm64-v8a armeabi-v7a x86_64 x86; do
 done
 
 rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/$model_name
-rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/dict
 rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/lexicon.txt
 rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/replace.fst
 
diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
new file mode 100755
index 00000000..28f31b1a
--- /dev/null
+++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
@@ -0,0 +1,292 @@
+#!/usr/bin/env python3
+
+import argparse
+from dataclasses import dataclass
+from pathlib import Path
+
+import jinja2
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--total",
+        type=int,
+        default=1,
+        help="Number of runners",
+    )
+    parser.add_argument(
+        "--index",
+        type=int,
+        default=0,
+        help="Index of the current runner",
+    )
+    return parser.parse_args()
+
+
+@dataclass
+class Model:
+    # We will download
+    # https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/{model_name}.tar.bz2
+    model_name: str
+
+    # The type of the model, e..g, 0, 1, 2. It is hardcoded in the kotlin code
+    idx: int
+
+    # e.g., zh, en, zh_en
+    lang: str
+    lang2: str
+
+    # e.g., whisper, paraformer, zipformer
+    short_name: str = ""
+
+    # cmd is used to remove extra file from the model directory
+    cmd: str = ""
+
+    rule_fsts: str = ""
+
+    use_hr: bool = False
+
+
+# See get_2nd_models() in ./generate-asr-2pass-apk-script.py
+def get_models():
+    models = [
+        Model(
+            model_name="sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9000,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="5-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9001,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="8-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9002,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="10-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9003,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="13-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9004,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="15-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9005,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="18-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9006,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="20-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9007,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="23-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9008,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="25-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9009,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="28-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+            idx=9010,
+            lang="zh_en_ko_ja_yue",
+            lang2="",
+            short_name="30-seconds-sense_voice_2024_07_17_int8",
+            use_hr=True,
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
+            popd
+            """,
+        ),
+    ]
+    return models
+
+
+def main():
+    args = get_args()
+    index = args.index
+    total = args.total
+    assert 0 <= index < total, (index, total)
+
+    all_model_list = get_models()
+
+    num_models = len(all_model_list)
+
+    num_per_runner = num_models // total
+    if num_per_runner <= 0:
+        raise ValueError(f"num_models: {num_models}, num_runners: {total}")
+
+    start = index * num_per_runner
+    end = start + num_per_runner
+
+    remaining = num_models - args.total * num_per_runner
+
+    print(f"{index}/{total}: {start}-{end}/{num_models}")
+
+    d = dict()
+    d["model_list"] = all_model_list[start:end]
+    if index < remaining:
+        s = args.total * num_per_runner + index
+        d["model_list"].append(all_model_list[s])
+        print(f"{s}/{num_models}")
+
+    filename_list = [
+        "./build-apk-qnn-vad-asr-simulate-streaming.sh",
+    ]
+    for filename in filename_list:
+        environment = jinja2.Environment()
+        if not Path(f"{filename}.in").is_file():
+            print(f"skip {filename}")
+            continue
+
+        with open(f"{filename}.in") as f:
+            s = f.read()
+        template = environment.from_string(s)
+
+        s = template.render(**d)
+        with open(filename, "w") as f:
+            print(s, file=f)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/sherpa-onnx/csrc/macros.h b/sherpa-onnx/csrc/macros.h
index 2788292d..0a0e1b2a 100644
--- a/sherpa-onnx/csrc/macros.h
+++ b/sherpa-onnx/csrc/macros.h
@@ -21,13 +21,15 @@
 
 #if __ANDROID_API__ >= 8
 #include "android/log.h"
-#define SHERPA_ONNX_LOGE(...)                                            \
-  do {                                                                   \
-    fprintf(stderr, "%s:%s:%d ", __FILE__, __func__,                     \
-            static_cast<int>(__LINE__));                                 \
-    fprintf(stderr, ##__VA_ARGS__);                                      \
-    fprintf(stderr, "\n");                                               \
-    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", ##__VA_ARGS__); \
+#define SHERPA_ONNX_LOGE(...)                                                  \
+  do {                                                                         \
+    fprintf(stderr, "%s:%s:%d ", __FILE__, __func__,                           \
+            static_cast<int32_t>(__LINE__));                                   \
+    fprintf(stderr, ##__VA_ARGS__);                                            \
+    fprintf(stderr, "\n");                                                     \
+    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", "%s:%s:%d", __FILE__, \
+                        __func__, static_cast<int32_t>(__LINE__));             \
+    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", ##__VA_ARGS__);       \
   } while (0)
 #elif defined(__OHOS__)
 #define SHERPA_ONNX_LOGE(...) OH_LOG_INFO(LOG_APP, ##__VA_ARGS__)
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
index ec1c9838..441820da 100644
--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
@@ -132,8 +132,9 @@ class QnnModel::Impl {
         sys_ctx_handle, static_cast<void *>(buffer.data()), buffer.size(),
         &binary_info, &binary_info_size);
     if (ret != QNN_SUCCESS) {
-      SHERPA_ONNX_LOGE("Failed to get context binary info from '%s'",
-                       binary_context_file.c_str());
+      SHERPA_ONNX_LOGE(
+          "Failed to get context binary info from '%s'. ret code is %d",
+          binary_context_file.c_str(), ret);
 
       qnn_system_interface_.systemContextFree(sys_ctx_handle);
       return false;
diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
index 6bc3c054..3206d7b9 100644
--- a/sherpa-onnx/java-api/Makefile
+++ b/sherpa-onnx/java-api/Makefile
@@ -15,6 +15,7 @@ java_files += WaveWriter.java
 java_files += EndpointRule.java
 java_files += EndpointConfig.java
 java_files += FeatureConfig.java
+java_files += QnnConfig.java
 java_files += HomophoneReplacerConfig.java
 java_files += OnlineLMConfig.java
 java_files += OnlineParaformerModelConfig.java
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
index b14de19b..79529401 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
@@ -6,11 +6,13 @@ public class OfflineSenseVoiceModelConfig {
     private final String model;
     private final String language;
     private final boolean useInverseTextNormalization;
+    private final QnnConfig qnnConfig;
 
     private OfflineSenseVoiceModelConfig(Builder builder) {
         this.model = builder.model;
         this.language = builder.language;
         this.useInverseTextNormalization = builder.useInverseTextNormalization;
+        this.qnnConfig = builder.qnnConfig;
     }
 
     public static Builder builder() {
@@ -29,10 +31,15 @@ public class OfflineSenseVoiceModelConfig {
         return useInverseTextNormalization;
     }
 
+    public QnnConfig getQnnConfig() {
+        return qnnConfig;
+    }
+
     public static class Builder {
         private String model = "";
         private String language = "";
         private boolean useInverseTextNormalization = true;
+        private QnnConfig qnnConfig = QnnConfig.builder().build();
 
         public OfflineSenseVoiceModelConfig build() {
             return new OfflineSenseVoiceModelConfig(this);
@@ -52,5 +59,10 @@ public class OfflineSenseVoiceModelConfig {
             this.useInverseTextNormalization = useInverseTextNormalization;
             return this;
         }
+
+        public Builder setQnnConfig(QnnConfig qnnConfig) {
+            this.qnnConfig = qnnConfig;
+            return this;
+        }
     }
 }
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java
new file mode 100644
index 00000000..c46958fe
--- /dev/null
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java
@@ -0,0 +1,56 @@
+// Copyright 2025 Xiaomi Corporation
+
+package com.k2fsa.sherpa.onnx;
+
+public class QnnConfig {
+    private final String backendLib;
+    private final String contextBinary;
+    private final String systemLib;
+
+    private QnnConfig(Builder builder) {
+        this.backendLib = builder.backendLib;
+        this.contextBinary = builder.contextBinary;
+        this.systemLib = builder.systemLib;
+    }
+
+    public static Builder builder() {
+        return new Builder();
+    }
+
+    public String getBackendLib() {
+        return backendLib;
+    }
+
+    public String getContextBinary() {
+        return contextBinary;
+    }
+
+    public String getSystemLib() {
+        return systemLib;
+    }
+
+    public static class Builder {
+        private String backendLib = "";
+        private String contextBinary = "";
+        private String systemLib = "";
+
+        public QnnConfig build() {
+            return new QnnConfig(this);
+        }
+
+        public Builder setBackendLib(String backendLib) {
+            this.backendLib = backendLib;
+            return this;
+        }
+
+        public Builder setContextBinary(String contextBinary) {
+            this.contextBinary = contextBinary;
+            return this;
+        }
+
+        public Builder setSystemLib(String systemLib) {
+            this.systemLib = systemLib;
+            return this;
+        }
+    }
+}
diff --git a/sherpa-onnx/jni/CMakeLists.txt b/sherpa-onnx/jni/CMakeLists.txt
index cff5ea6e..56cf7bd9 100644
--- a/sherpa-onnx/jni/CMakeLists.txt
+++ b/sherpa-onnx/jni/CMakeLists.txt
@@ -12,6 +12,7 @@ endif()
 
 set(sources
   audio-tagging.cc
+  common.cc
   jni.cc
   keyword-spotter.cc
   offline-punctuation.cc
diff --git a/sherpa-onnx/jni/common.cc b/sherpa-onnx/jni/common.cc
new file mode 100644
index 00000000..2de20727
--- /dev/null
+++ b/sherpa-onnx/jni/common.cc
@@ -0,0 +1,47 @@
+// sherpa-onnx/jni/common.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#include "sherpa-onnx/jni/common.h"
+
+#include <stdlib.h>
+
+#include <string>
+
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+/* For qnn to load libQnnHtpVxxSkel.so, e.g., libQnnHtpV81Skel.so file
+
+https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
+ */
+void PrependAdspLibraryPath(const std::string &new_path) {
+  const char *old_path = getenv("ADSP_LIBRARY_PATH");
+  std::string updated_path;
+
+  if (old_path && !std::string(old_path).empty()) {
+    // Caution(fangjun):
+    // 1. Must use ; here, not :
+    // 2. Must use prepend, not append
+    updated_path = new_path + ";" + std::string(old_path);
+  } else {
+    updated_path = new_path;  // no old path
+  }
+
+  if (setenv("ADSP_LIBRARY_PATH", updated_path.c_str(), 1) != 0) {
+    SHERPA_ONNX_LOGE("Failed to set ADSP_LIBRARY_PATH to '%s'",
+                     updated_path.c_str());
+  } else {
+    SHERPA_ONNX_LOGE("Successfully set ADSP_LIBRARY_PATH to '%s'",
+                     updated_path.c_str());
+  }
+  /*
+You will see something like the following:
+
+Successfully set ADSP_LIBRARY_PATH to
+'/data/app/~~pHS2-9SwVjl9ma3cIKtj-g==/com.k2fsa.sherpa.onnx.simulate.streaming.asr-ejCDb8LodsnyK5cr3SvGjA==/lib/arm64;/odm/lib/rfsa/adsp;/vendor/lib/rfsa/adsp/;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp'
+
+   */
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/jni/common.h b/sherpa-onnx/jni/common.h
index f9ae2e52..4c478515 100644
--- a/sherpa-onnx/jni/common.h
+++ b/sherpa-onnx/jni/common.h
@@ -193,4 +193,8 @@ inline bool ValidatePointer(JNIEnv *env, jlong ptr, const char *functionName,
   return true;
 }
 
+namespace sherpa_onnx {
+void PrependAdspLibraryPath(const std::string &new_path);
+}
+
 #endif  // SHERPA_ONNX_JNI_COMMON_H_
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index b091670e..ed940cbf 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-recognizer.h"
 
+#include <stdlib.h>
+
 #include <memory>
 
 #include "sherpa-onnx/csrc/macros.h"
@@ -164,6 +166,23 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
                             useInverseTextNormalization, sense_voice_config_cls,
                             sense_voice_config);
 
+  fid = env->GetFieldID(sense_voice_config_cls, "qnnConfig",
+                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
+  jobject qnn_config = env->GetObjectField(sense_voice_config, fid);
+  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.sense_voice.qnn_config.backend_lib, backendLib,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.sense_voice.qnn_config.context_binary, contextBinary,
+      qnn_config_cls, qnn_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(
+      ans.model_config.sense_voice.qnn_config.system_lib, systemLib,
+      qnn_config_cls, qnn_config);
+
   // nemo
   fid = env->GetFieldID(
       model_config_cls, "nemo",
@@ -313,7 +332,7 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile(JNIEnv *env,
     }
   }
 
-  if (!config.Validate()) {
+  if (config.model_config.provider != "qnn" && !config.Validate()) {
     SHERPA_ONNX_LOGE("Errors found in config!");
     return 0;
   }
@@ -461,3 +480,13 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_getResult(JNIEnv *env,
 
   return obj_arr;
 }
+
+SHERPA_ONNX_EXTERN_C
+JNIEXPORT void JNICALL
+Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_prependAdspLibraryPath(
+    JNIEnv *env, jclass /*cls*/, jstring new_path) {
+  const char *p = env->GetStringUTFChars(new_path, nullptr);
+  sherpa_onnx::PrependAdspLibraryPath(p);
+
+  env->ReleaseStringUTFChars(new_path, p);
+}
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index 1e94f4f9..d564f9d3 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -76,6 +76,7 @@ data class OfflineSenseVoiceModelConfig(
     var model: String = "",
     var language: String = "",
     var useInverseTextNormalization: Boolean = true,
+    var qnnConfig: QnnConfig = QnnConfig(),
 )
 
 data class OfflineModelConfig(
@@ -191,6 +192,9 @@ class OfflineRecognizer(
         init {
             System.loadLibrary("sherpa-onnx-jni")
         }
+
+        @JvmStatic
+        external fun prependAdspLibraryPath(newPath: String) // for qnn
     }
 }
 
@@ -746,6 +750,186 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                 tokens = "$modelDir/tokens.txt",
             )
         }
+
+        9000 -> {
+            val modelDir = "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                      // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
+                      //
+                      // model.bin is created in the first run and is used from the second run
+                      // to speed up the initialization
+                      backendLib = "libQnnHtp.so",
+                      systemLib = "libQnnSystem.so",
+                      contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9001 -> {
+            val modelDir = "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9002 -> {
+            val modelDir = "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9003 -> {
+            val modelDir = "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9004 -> {
+            val modelDir = "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9005 -> {
+            val modelDir = "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9006 -> {
+            val modelDir = "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9007 -> {
+            val modelDir = "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9008 -> {
+            val modelDir = "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9009 -> {
+            val modelDir = "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
+        9010 -> {
+            val modelDir = "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+            return OfflineModelConfig(
+                provider = "qnn",
+                senseVoice = OfflineSenseVoiceModelConfig(
+                    model = "$modelDir/libmodel.so",
+                    qnnConfig = QnnConfig(
+                        backendLib = "libQnnHtp.so",
+                        systemLib = "libQnnSystem.so",
+                        contextBinary = "$modelDir/model.bin",
+                    ),
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
     }
     return null
 }
diff --git a/sherpa-onnx/kotlin-api/QnnConfig.kt b/sherpa-onnx/kotlin-api/QnnConfig.kt
new file mode 100644
index 00000000..ea7aa511
--- /dev/null
+++ b/sherpa-onnx/kotlin-api/QnnConfig.kt
@@ -0,0 +1,7 @@
+package com.k2fsa.sherpa.onnx
+
+data class QnnConfig(
+    var backendLib: String = "",
+    var contextBinary: String = "",
+    var systemLib: String = "",
+)

commit 2fcde7d3c63f1df2d8b59d70cd5dd268c14f7a97
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Nov 19 18:21:16 2025 +0800

    Support hotwords with byte level bpe (#2802)

diff --git a/.gitignore b/.gitignore
index 37337ac3..768fc466 100644
--- a/.gitignore
+++ b/.gitignore
@@ -157,3 +157,4 @@ am.mvn
 config.yaml
 configuration.json
 sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+sherpa-onnx-paraformer-zh-int8-2025-10-07
diff --git a/scripts/bbpe/generate_bbpe_table.py b/scripts/bbpe/generate_bbpe_table.py
index 0b520424..e164c420 100755
--- a/scripts/bbpe/generate_bbpe_table.py
+++ b/scripts/bbpe/generate_bbpe_table.py
@@ -46,8 +46,10 @@ def main():
     s += "      "
     for i, (k, v) in enumerate(BCHAR_TO_BYTE.items()):
         s += "{"
-        if k in ["\\", '"']:
-            s += f'"\{k}", {v}'
+        if k == "\\":
+            s += f'"\\\\", {v}'
+        elif k == '"':
+            s += f'"\\"", {v}'
         else:
             s += f'"{k}", {v}'
         s += "}, "
@@ -59,6 +61,29 @@ def main():
     s += "  return table\n;"
     s += "}\n"
 
+    s += "\n"
+    s += "const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token() {\n"
+    s += "  static const std::unordered_map<uint8_t, std::string> table = {\n"
+
+    s += "      "
+    for i, (k, v) in enumerate(BCHAR_TO_BYTE.items()):
+        s += "{"
+        if k == "\\":
+            s += f'{v}, "\\\\"'
+        elif k == '"':
+            s += f'{v}, "\\""'
+        else:
+            s += f'{v}, "{k}"'
+
+        s += "}, "
+        if i > 0 and i % 7 == 0:
+            s += "\n"
+            s += "      "
+    s += "};\n"
+    s += "\n"
+    s += "  return table\n;"
+    s += "}\n"
+
     with open("bbpe.cc", "w", encoding="utf-8") as f:
         f.write(s)
 
diff --git a/sherpa-onnx/csrc/bbpe.cc b/sherpa-onnx/csrc/bbpe.cc
index 1aa67ffa..de523bea 100644
--- a/sherpa-onnx/csrc/bbpe.cc
+++ b/sherpa-onnx/csrc/bbpe.cc
@@ -1,6 +1,6 @@
 // sherpa-onnx/csrc/bbpe.cc
 //
-// Copyright (c)  2024  Xiaomi Corporation
+// Copyright (c)  2024 Xiaomi Corporation
 
 // Auto-generated! DO NOT EDIT
 
@@ -59,3 +59,53 @@ const std::unordered_map<std::string, uint8_t> &GetByteBpeTable() {
 
   return table;
 }
+
+const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token() {
+  static const std::unordered_map<uint8_t, std::string> table = {
+      {0, ""},   {1, ""},   {2, ""},   {3, ""},   {4, ""},   {5, ""},
+      {6, ""},   {7, ""},   {8, ""},   {9, ""},   {10, ""},  {11, ""},
+      {12, ""},  {13, ""},  {14, ""},  {15, ""},  {16, ""},  {17, ""},
+      {18, ""},  {19, ""},  {20, ""},  {21, ""},  {22, ""},  {23, ""},
+      {24, ""},  {25, ""},  {26, ""},  {27, ""},  {28, ""},  {29, ""},
+      {30, ""},  {31, ""},  {32, " "},  {33, "!"},  {34, "\""}, {35, "#"},
+      {36, "$"},  {37, "%"},  {38, "&"},  {39, "'"},  {40, "("},  {41, ")"},
+      {42, "*"},  {43, "+"},  {44, ","},  {45, "-"},  {46, "."},  {47, "/"},
+      {48, "0"},  {49, "1"},  {50, "2"},  {51, "3"},  {52, "4"},  {53, "5"},
+      {54, "6"},  {55, "7"},  {56, "8"},  {57, "9"},  {58, ":"},  {59, ";"},
+      {60, "<"},  {61, "="},  {62, ">"},  {63, "?"},  {64, "@"},  {65, "A"},
+      {66, "B"},  {67, "C"},  {68, "D"},  {69, "E"},  {70, "F"},  {71, "G"},
+      {72, "H"},  {73, "I"},  {74, "J"},  {75, "K"},  {76, "L"},  {77, "M"},
+      {78, "N"},  {79, "O"},  {80, "P"},  {81, "Q"},  {82, "R"},  {83, "S"},
+      {84, "T"},  {85, "U"},  {86, "V"},  {87, "W"},  {88, "X"},  {89, "Y"},
+      {90, "Z"},  {91, "["},  {92, "\\"}, {93, "]"},  {94, "^"},  {95, "_"},
+      {96, "`"},  {97, "a"},  {98, "b"},  {99, "c"},  {100, "d"}, {101, "e"},
+      {102, "f"}, {103, "g"}, {104, "h"}, {105, "i"}, {106, "j"}, {107, "k"},
+      {108, "l"}, {109, "m"}, {110, "n"}, {111, "o"}, {112, "p"}, {113, "q"},
+      {114, "r"}, {115, "s"}, {116, "t"}, {117, "u"}, {118, "v"}, {119, "w"},
+      {120, "x"}, {121, "y"}, {122, "z"}, {123, "{"}, {124, "|"}, {125, "}"},
+      {126, "~"}, {127, ""}, {128, ""}, {129, ""}, {130, ""}, {131, ""},
+      {132, ""}, {133, ""}, {134, ""}, {135, ""}, {136, ""}, {137, ""},
+      {138, ""}, {139, ""}, {140, ""}, {141, ""}, {142, ""}, {143, ""},
+      {144, ""}, {145, ""}, {146, ""}, {147, ""}, {148, ""}, {149, ""},
+      {150, ""}, {151, ""}, {152, ""}, {153, ""}, {154, ""}, {155, ""},
+      {156, ""}, {157, ""}, {158, ""}, {159, ""}, {160, ""}, {161, ""},
+      {162, ""}, {163, ""}, {164, ""}, {165, ""}, {166, ""}, {167, ""},
+      {168, ""}, {169, ""}, {170, ""}, {171, ""}, {172, ""}, {173, ""},
+      {174, ""}, {175, ""}, {176, ""}, {177, ""}, {178, ""}, {179, ""},
+      {180, ""}, {181, ""}, {182, ""}, {183, ""}, {184, ""}, {185, ""},
+      {186, ""}, {187, ""}, {188, ""}, {189, ""}, {190, ""}, {191, ""},
+      {192, ""}, {193, ""}, {194, ""}, {195, ""}, {196, ""}, {197, ""},
+      {198, ""}, {199, ""}, {200, ""}, {201, ""}, {202, ""}, {203, ""},
+      {204, ""}, {205, ""}, {206, ""}, {207, ""}, {208, ""}, {209, ""},
+      {210, ""}, {211, ""}, {212, ""}, {213, ""}, {214, ""}, {215, ""},
+      {216, ""}, {217, ""}, {218, ""}, {219, ""}, {220, ""}, {221, ""},
+      {222, ""}, {223, ""}, {224, ""}, {225, ""}, {226, ""}, {227, ""},
+      {228, ""}, {229, ""}, {230, ""}, {231, ""}, {232, ""}, {233, ""},
+      {234, ""}, {235, ""}, {236, ""}, {237, ""}, {238, ""}, {239, ""},
+      {240, ""}, {241, ""}, {242, ""}, {243, ""}, {244, ""}, {245, ""},
+      {246, ""}, {247, ""}, {248, ""}, {249, ""}, {250, ""}, {251, ""},
+      {252, ""}, {253, ""}, {254, ""}, {255, ""},
+  };
+
+  return table;
+}
diff --git a/sherpa-onnx/csrc/bbpe.h b/sherpa-onnx/csrc/bbpe.h
index 0b6a4ecf..7325077c 100644
--- a/sherpa-onnx/csrc/bbpe.h
+++ b/sherpa-onnx/csrc/bbpe.h
@@ -13,4 +13,6 @@
 // https://github.com/k2-fsa/icefall/blob/master/icefall/byte_utils.py#L280
 const std::unordered_map<std::string, uint8_t> &GetByteBpeTable();
 
+const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token();
+
 #endif  // SHERPA_ONNX_CSRC_BBPE_H_
diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
index 0171c379..7d536750 100644
--- a/sherpa-onnx/csrc/offline-model-config.cc
+++ b/sherpa-onnx/csrc/offline-model-config.cc
@@ -45,17 +45,18 @@ void OfflineModelConfig::Register(ParseOptions *po) {
                "Valid values are: transducer, paraformer, nemo_ctc, whisper, "
                "tdnn, zipformer2_ctc, telespeech_ctc, fire_red_asr."
                "All other values lead to loading the model twice.");
-  po->Register("modeling-unit", &modeling_unit,
-               "The modeling unit of the model, commonly used units are bpe, "
-               "cjkchar, cjkchar+bpe, etc. Currently, it is needed only when "
-               "hotwords are provided, we need it to encode the hotwords into "
-               "token sequence.");
+  po->Register(
+      "modeling-unit", &modeling_unit,
+      "The modeling unit of the model, commonly used units are bpe, "
+      "bbpe, cjkchar, cjkchar+bpe, etc. Currently, it is needed only when "
+      "hotwords are provided, we need it to encode the hotwords into "
+      "token sequence.");
   po->Register("bpe-vocab", &bpe_vocab,
                "The vocabulary generated by google's sentencepiece program. "
                "It is a file has two columns, one is the token, the other is "
                "the log probability, you can get it from the directory where "
                "your bpe model is generated. Only used when hotwords provided "
-               "and the modeling unit is bpe or cjkchar+bpe");
+               "and the modeling unit is bpe, bbpe, or cjkchar+bpe");
 }
 
 bool OfflineModelConfig::Validate() const {
@@ -98,7 +99,8 @@ bool OfflineModelConfig::Validate() const {
   }
 
   if (!modeling_unit.empty() &&
-      (modeling_unit == "bpe" || modeling_unit == "cjkchar+bpe")) {
+      (modeling_unit == "bpe" || modeling_unit == "cjkchar+bpe" ||
+       modeling_unit == "bbpe")) {
     if (!FileExists(bpe_vocab)) {
       SHERPA_ONNX_LOGE("bpe_vocab: '%s' does not exist", bpe_vocab.c_str());
       return false;
diff --git a/sherpa-onnx/csrc/utils.cc b/sherpa-onnx/csrc/utils.cc
index f40b6769..63190697 100644
--- a/sherpa-onnx/csrc/utils.cc
+++ b/sherpa-onnx/csrc/utils.cc
@@ -11,6 +11,7 @@
 #include <utility>
 #include <vector>
 
+#include "sherpa-onnx/csrc/bbpe.h"
 #include "sherpa-onnx/csrc/log.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/text-utils.h"
@@ -152,6 +153,23 @@ bool EncodeHotwords(std::istream &is, const std::string &modeling_unit,
         for (const auto &bpe : bpes) {
           oss << " " << bpe;
         }
+      } else if (modeling_unit == "bbpe") {
+        std::vector<std::string> bpes;
+
+        const auto &id2token = GetByteBpeTableId2Token();
+        std::string tokens;
+        for (size_t i = 0; i < word.length(); ++i) {
+          uint8_t byte = static_cast<uint8_t>(word[i]);
+          tokens += id2token.at(byte);
+          if ((i + 1) % 3 == 0 && (i + 1) < word.length()) {
+            tokens += " ";
+          }
+        }
+
+        bpe_encoder->Encode(tokens, &bpes);
+        for (const auto &bpe : bpes) {
+          oss << " " << bpe;
+        }
       } else {
         if (modeling_unit != "cjkchar+bpe") {
           SHERPA_ONNX_LOGE(
diff --git a/sherpa-onnx/csrc/utils.h b/sherpa-onnx/csrc/utils.h
index a9d59e8a..a2d4dbdd 100644
--- a/sherpa-onnx/csrc/utils.h
+++ b/sherpa-onnx/csrc/utils.h
@@ -28,7 +28,7 @@ namespace sherpa_onnx {
  */
 bool EncodeHotwords(std::istream &is, const std::string &modeling_unit,
                     const SymbolTable &symbol_table,
-                    const ssentencepiece::Ssentencepiece *bpe_encoder_,
+                    const ssentencepiece::Ssentencepiece *bpe_encoder,
                     std::vector<std::vector<int32_t>> *hotwords_id,
                     std::vector<float> *boost_scores);
 

commit 3f13f7a3c00719a75c50593d5d3942ffba6e6552
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 18 14:06:50 2025 +0800

    Export models for CANN toolkit 7.0 (#2795)

diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
index 4f10df79..f71752b0 100644
--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-910b3
+      - cann-7.0
   workflow_dispatch:
 
 concurrency:
@@ -21,6 +21,16 @@ jobs:
       matrix:
         include:
           # ===== Ascend 910B =====
+          - soc_version: "910B"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "FunASR"
+            cann: "7.0"
+
+          - soc_version: "910B"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "WSYue-ASR"
+            cann: "7.0"
+
           - soc_version: "910B"
             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
             framework: "FunASR"
@@ -42,6 +52,16 @@ jobs:
             cann: "8.2"
 
           # ===== Ascend 910B2 =====
+          - soc_version: "910B2"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "FunASR"
+            cann: "7.0"
+
+          - soc_version: "910B2"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "WSYue-ASR"
+            cann: "7.0"
+
           - soc_version: "910B2"
             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
             framework: "FunASR"
@@ -63,6 +83,16 @@ jobs:
             cann: "8.2"
 
           # ===== Ascend 910B3 =====
+          - soc_version: "910B3"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "FunASR"
+            cann: "7.0"
+
+          - soc_version: "910B3"
+            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+            framework: "WSYue-ASR"
+            cann: "7.0"
+
           - soc_version: "910B3"
             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
             framework: "FunASR"
@@ -85,6 +115,16 @@ jobs:
 
 
           # ===== Ascend 310 =====
+          - soc_version: "310P3"
+            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
+            framework: "FunASR"
+            cann: "7.0"
+
+          - soc_version: "310P3"
+            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
+            framework: "WSYue-ASR"
+            cann: "7.0"
+
           - soc_version: "310P3"
             # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
             image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
@@ -145,6 +185,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           echo "CANN environment:"
           which atc || echo "atc not found"
           atc --help
@@ -189,6 +232,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           soc_version=${{ matrix.soc_version }}
           cann=${{ matrix.cann }}
 
@@ -256,6 +302,9 @@ jobs:
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
 
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
           soc_version=${{ matrix.soc_version }}
           cann=${{ matrix.cann }}
 

commit 6659d47d05ccb246ba1ea340360c096664403cde
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 17 18:56:13 2025 +0800

    Add C++ QNN support for SenseVoice (#2793)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 20190dc7..4bc8cb40 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -304,6 +304,10 @@ if(SHERPA_ONNX_ENABLE_RKNN)
   add_definitions(-DSHERPA_ONNX_ENABLE_RKNN=1)
 endif()
 
+if(SHERPA_ONNX_ENABLE_QNN)
+  add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
+endif()
+
 if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   set(ASCEND_TOOLKIT_HOME)
   if(NOT DEFINED ENV{ASCEND_TOOLKIT_HOME})
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
index 7cd62e59..0d80f483 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
@@ -3,6 +3,7 @@
 #include <memory>
 #include <sstream>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "macros.h"  // NOLINT
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index a9a09286..58bafbab 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -192,7 +192,7 @@ if(SHERPA_ONNX_ENABLE_RKNN)
 
 endif()
 
-if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU)
+if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN)
   list(APPEND sources
     ./rknn/offline-ctc-greedy-search-decoder-rknn.cc
   )
@@ -211,6 +211,7 @@ if(SHERPA_ONNX_ENABLE_QNN)
     ./qnn/qnn-backend.cc
     ./qnn/qnn-model.cc
     ./qnn/utils.cc
+    ./qnn/offline-sense-voice-model-qnn.cc
   )
 endif()
 
diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
index 54da97b3..739b752b 100644
--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
@@ -426,6 +426,11 @@ OfflineParaformerModelAscend::OfflineParaformerModelAscend(
     const OfflineModelConfig &config)
     : impl_(std::make_unique<Impl>(config)) {}
 
+template <typename Manager>
+OfflineParaformerModelAscend::OfflineParaformerModelAscend(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
 OfflineParaformerModelAscend::~OfflineParaformerModelAscend() = default;
 
 std::vector<float> OfflineParaformerModelAscend::Run(
diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
index 9192f0ae..a7409206 100644
--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
@@ -216,6 +216,11 @@ OfflineSenseVoiceModelAscend::OfflineSenseVoiceModelAscend(
     const OfflineModelConfig &config)
     : impl_(std::make_unique<Impl>(config)) {}
 
+template <typename Manager>
+OfflineSenseVoiceModelAscend::OfflineSenseVoiceModelAscend(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
 OfflineSenseVoiceModelAscend::~OfflineSenseVoiceModelAscend() = default;
 
 std::vector<float> OfflineSenseVoiceModelAscend::Run(
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 3a675fc1..ff4e055d 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -45,6 +45,10 @@
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
 #endif
 
+#if SHERPA_ONNX_ENABLE_QNN
+#include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
+#endif
+
 namespace sherpa_onnx {
 
 std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
@@ -54,7 +58,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     if (config.model_config.sense_voice.model.empty()) {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice models are currently supported "
-          "by rknn for non-streaming ASR. Fallback to CPU");
+          "by rknn for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
     } else if (!config.model_config.sense_voice.model.empty()) {
       return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
     }
@@ -77,7 +83,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     } else {
       SHERPA_ONNX_LOGE(
           "Only SenseVoice and Paraformer models are currently supported "
-          "by Ascend NPU for non-streaming ASR. Fallback to CPU");
+          "by Ascend NPU for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
     }
 #else
     SHERPA_ONNX_LOGE(
@@ -89,6 +97,27 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #endif
   }
 
+  if (config.model_config.provider == "qnn") {
+#if SHERPA_ONNX_ENABLE_QNN
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported "
+          "by qnn for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    }
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
+        "you want to use qnn. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
   if (!config.model_config.sense_voice.model.empty()) {
     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(config);
   }
@@ -175,7 +204,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     model_filename = config.model_config.whisper.encoder;
   } else {
     SHERPA_ONNX_LOGE("Please provide a model");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
 
   auto buf = ReadFile(model_filename);
@@ -228,7 +257,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
         "https://github.com/Tele-AI/TeleSpeech-ASR"
         "\n"
         "\n");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
 
   if (model_type == "conformer" || model_type == "zipformer" ||
@@ -274,7 +303,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       " - TeleSpeech CTC models\n",
       model_type.c_str());
 
-  exit(-1);
+  SHERPA_ONNX_EXIT(-1);
 }
 
 template <typename Manager>
@@ -322,6 +351,27 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
 #endif
   }
 
+  if (config.model_config.provider == "qnn") {
+#if SHERPA_ONNX_ENABLE_QNN
+    if (!config.model_config.sense_voice.model.empty()) {
+      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only SenseVoice models are currently supported "
+          "by qnn for non-streaming ASR.");
+      SHERPA_ONNX_EXIT(-1);
+      return nullptr;
+    }
+#else
+    SHERPA_ONNX_LOGE(
+        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
+        "you want to use qnn. See also "
+        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
+    SHERPA_ONNX_EXIT(-1);
+    return nullptr;
+#endif
+  }
+
   if (!config.model_config.sense_voice.model.empty()) {
     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(mgr, config);
   }
@@ -406,7 +456,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     model_filename = config.model_config.whisper.encoder;
   } else {
     SHERPA_ONNX_LOGE("Please provide a model");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
 
   auto buf = ReadFile(mgr, model_filename);
@@ -459,7 +509,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
         "https://github.com/Tele-AI/TeleSpeech-ASR"
         "\n"
         "\n");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
 
   if (model_type == "conformer" || model_type == "zipformer" ||
@@ -505,7 +555,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       " - TeleSpeech CTC models\n",
       model_type.c_str());
 
-  exit(-1);
+  SHERPA_ONNX_EXIT(-1);
 }
 
 OfflineRecognizerImpl::OfflineRecognizerImpl(
diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
index 8a02fc16..cc819bf4 100644
--- a/sherpa-onnx/csrc/offline-recognizer.cc
+++ b/sherpa-onnx/csrc/offline-recognizer.cc
@@ -139,10 +139,6 @@ std::string OfflineRecognizerConfig::ToString() const {
   os << "lm_config=" << lm_config.ToString() << ", ";
   os << "ctc_fst_decoder_config=" << ctc_fst_decoder_config.ToString() << ", ";
 
-  if (!qnn_config.backend_lib.empty()) {
-    os << "qnn_config=" << qnn_config.ToString() << ", ";
-  }
-
   os << "decoding_method=\"" << decoding_method << "\", ";
   os << "max_active_paths=" << max_active_paths << ", ";
   os << "hotwords_file=\"" << hotwords_file << "\", ";
diff --git a/sherpa-onnx/csrc/offline-recognizer.h b/sherpa-onnx/csrc/offline-recognizer.h
index ae2c23a4..1fcc1016 100644
--- a/sherpa-onnx/csrc/offline-recognizer.h
+++ b/sherpa-onnx/csrc/offline-recognizer.h
@@ -17,7 +17,6 @@
 #include "sherpa-onnx/csrc/offline-stream.h"
 #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
 #include "sherpa-onnx/csrc/parse-options.h"
-#include "sherpa-onnx/csrc/qnn-config.h"
 
 namespace sherpa_onnx {
 
@@ -28,7 +27,6 @@ struct OfflineRecognizerConfig {
   OfflineModelConfig model_config;
   OfflineLMConfig lm_config;
   OfflineCtcFstDecoderConfig ctc_fst_decoder_config;
-  QnnConfig qnn_config;
 
   std::string decoding_method = "greedy_search";
   int32_t max_active_paths = 4;
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
index cc18a11a..e38dfcbb 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
@@ -8,6 +8,7 @@
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/text-utils.h"
 
 namespace sherpa_onnx {
 
@@ -20,6 +21,8 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
   po->Register(
       "sense-voice-use-itn", &use_itn,
       "True to enable inverse text normalization. False to disable it.");
+
+  qnn_config.Register(po);
 }
 
 bool OfflineSenseVoiceModelConfig::Validate() const {
@@ -40,6 +43,10 @@ bool OfflineSenseVoiceModelConfig::Validate() const {
     }
   }
 
+  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
+    return qnn_config.Validate();
+  }
+
   return true;
 }
 
@@ -48,6 +55,11 @@ std::string OfflineSenseVoiceModelConfig::ToString() const {
 
   os << "OfflineSenseVoiceModelConfig(";
   os << "model=\"" << model << "\", ";
+
+  if (!qnn_config.backend_lib.empty()) {
+    os << "qnn_config=" << qnn_config.ToString() << ", ";
+  }
+
   os << "language=\"" << language << "\", ";
   os << "use_itn=" << (use_itn ? "True" : "False") << ")";
 
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.h b/sherpa-onnx/csrc/offline-sense-voice-model-config.h
index f19e959e..6f5fcf2f 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.h
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.h
@@ -7,6 +7,7 @@
 #include <string>
 
 #include "sherpa-onnx/csrc/parse-options.h"
+#include "sherpa-onnx/csrc/qnn-config.h"
 
 namespace sherpa_onnx {
 
@@ -22,6 +23,8 @@ struct OfflineSenseVoiceModelConfig {
   // false to not use inverse text normalization
   bool use_itn = false;
 
+  QnnConfig qnn_config;
+
   OfflineSenseVoiceModelConfig() = default;
   OfflineSenseVoiceModelConfig(const std::string &model,
                                const std::string &language, bool use_itn)
diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h b/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
new file mode 100644
index 00000000..7c793ed9
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
@@ -0,0 +1,138 @@
+// sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
+#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+// defined in ../offline-recognizer-sense-voice-impl.h
+OfflineRecognitionResult ConvertSenseVoiceResult(
+    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+    int32_t frame_shift_ms, int32_t subsampling_factor);
+
+class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerSenseVoiceQnnImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(
+            std::make_unique<OfflineSenseVoiceModelQnn>(config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerSenseVoiceQnnImpl(Manager *mgr,
+                                     const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<OfflineSenseVoiceModelQnn>(
+            mgr, config.model_config)) {
+    const auto &meta_data = model_->GetModelMetadata();
+    if (config.decoding_method == "greedy_search") {
+      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+          meta_data.blank_id);
+    } else {
+      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                       config.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitFeatConfig();
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    return std::make_unique<OfflineStream>(config_.feat_config);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    for (int32_t i = 0; i < n; ++i) {
+      DecodeOneStream(ss[i]);
+    }
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void InitFeatConfig() {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+    config_.feat_config.window_type = "hamming";
+    config_.feat_config.high_freq = 0;
+    config_.feat_config.snip_edges = true;
+  }
+
+  void DecodeOneStream(OfflineStream *s) const {
+    const auto &meta_data = model_->GetModelMetadata();
+
+    std::vector<float> f = s->GetFrames();
+
+    int32_t language = 0;
+    if (config_.model_config.sense_voice.language.empty()) {
+      language = 0;
+    } else if (meta_data.lang2id.count(
+                   config_.model_config.sense_voice.language)) {
+      language =
+          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+    } else {
+      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+                       config_.model_config.sense_voice.language.c_str());
+    }
+
+    int32_t text_norm = config_.model_config.sense_voice.use_itn
+                            ? meta_data.with_itn_id
+                            : meta_data.without_itn_id;
+
+    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+
+    auto result =
+        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+
+    int32_t frame_shift_ms = 10;
+    int32_t subsampling_factor = meta_data.window_shift;
+    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+                                     subsampling_factor);
+
+    r.text = ApplyInverseTextNormalization(std::move(r.text));
+    r.text = ApplyHomophoneReplacer(std::move(r.text));
+    s->SetResult(r);
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<OfflineSenseVoiceModelQnn> model_;
+  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
new file mode 100644
index 00000000..b628dc20
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
@@ -0,0 +1,288 @@
+// sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <mutex>  // NOLINT
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/qnn/macros.h"
+#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
+#include "sherpa-onnx/csrc/qnn/qnn-model.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelQnn::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    backend_ = std::make_unique<QnnBackend>(
+        config.sense_voice.qnn_config.backend_lib, config_.debug);
+
+    const auto &context_binary = config_.sense_voice.qnn_config.context_binary;
+
+    if (context_binary.empty()) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from model lib since context binary is not given");
+      }
+
+      InitFromModelLib();
+
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Skip generating context binary since you don't provide a path to "
+            "save it");
+      }
+
+    } else if (!FileExists(context_binary)) {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE(
+            "Init from model lib since context binary '%s' does not exist",
+            context_binary.c_str());
+      }
+
+      InitFromModelLib();
+
+      CreateContextBinary();
+    } else {
+      if (config_.debug) {
+        SHERPA_ONNX_LOGE("Init from context binary '%s'",
+                         context_binary.c_str());
+      }
+      InitFromContextBinary();
+    }
+
+    PostInit();
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    SHERPA_ONNX_LOGE(
+        "Please copy all files from assets to SD card and set assetManager to "
+        "null");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+    return meta_data_;
+  }
+
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) {
+    std::lock_guard<std::mutex> lock(mutex_);
+
+    features = ApplyLFR(std::move(features));
+
+    int32_t num_frames = features.size() / feat_dim_;
+
+    model_->SetInputTensorData("x", features.data(), features.size());
+
+    std::array<int32_t, 4> prompt = {language, 1, 2, text_norm};
+    model_->SetInputTensorData("prompt", prompt.data(), prompt.size());
+
+    model_->Run();
+
+    return model_->GetOutputTensorData("logits");
+  }
+
+ private:
+  void InitFromModelLib() {
+    backend_->InitContext();
+
+    model_ = std::make_unique<QnnModel>(config_.sense_voice.model,
+                                        backend_.get(), config_.debug);
+  }
+
+  void InitFromContextBinary() {
+    model_ = std::make_unique<QnnModel>(
+        config_.sense_voice.qnn_config.context_binary,
+        config_.sense_voice.qnn_config.system_lib, backend_.get(),
+        BinaryContextTag{}, config_.debug);
+  }
+
+  void CreateContextBinary() {
+    const auto &context_binary = config_.sense_voice.qnn_config.context_binary;
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
+    }
+
+    bool ok = model_->SaveBinaryContext(context_binary);
+
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
+                       context_binary.c_str());
+    }
+
+    if (config_.debug && ok) {
+      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
+      SHERPA_ONNX_LOGE(
+          "It should be super fast the next time you init the system.");
+      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
+    }
+  }
+
+  void PostInit() { CheckModel(); }
+
+  void CheckModel() {
+    const auto &input_tensor_names = model_->InputTensorNames();
+    if (input_tensor_names.size() != 2) {
+      SHERPA_ONNX_LOGE("Expect two input tensors. Actual %d",
+                       static_cast<int32_t>(input_tensor_names.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[0] != "x") {
+      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
+                       input_tensor_names[0].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (input_tensor_names[1] != "prompt") {
+      SHERPA_ONNX_LOGE("The 2nd input should be prompt, actual '%s'",
+                       input_tensor_names[1].c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> x_shape = model_->TensorShape(input_tensor_names[0]);
+    if (x_shape.size() != 3) {
+      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
+                       static_cast<int32_t>(x_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (x_shape[2] != feat_dim_) {
+      SHERPA_ONNX_LOGE("The x.shape[2] should be %d, actual '%d'", feat_dim_,
+                       x_shape[2]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<int32_t> prompt_shape =
+        model_->TensorShape(input_tensor_names[1]);
+
+    if (prompt_shape.size() != 1) {
+      SHERPA_ONNX_LOGE("The 2nd input should be 1-d, actual '%d'",
+                       static_cast<int32_t>(prompt_shape.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (prompt_shape[0] != 4) {
+      SHERPA_ONNX_LOGE("The prompt.shape[0] should be 4, actual '%d'",
+                       prompt_shape[0]);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (!model_->HasTensor("logits")) {
+      SHERPA_ONNX_LOGE("Model does not have output node 'logits'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    expected_num_frames_ = x_shape[1];
+  }
+
+  std::vector<float> ApplyLFR(std::vector<float> in) const {
+    int32_t lfr_window_size = meta_data_.window_size;
+    int32_t lfr_window_shift = meta_data_.window_shift;
+    int32_t in_feat_dim = 80;
+
+    int32_t in_num_frames = in.size() / in_feat_dim;
+    int32_t out_num_frames =
+        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+
+    if (out_num_frames > expected_num_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          out_num_frames, expected_num_frames_);
+
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios.");
+
+      out_num_frames = expected_num_frames_;
+    }
+
+    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+
+    // if out_num_frames < expected_num_frames_, it uses 0 padding
+    std::vector<float> out(expected_num_frames_ * out_feat_dim, 0);
+
+    const float *p_in = in.data();
+    float *p_out = out.data();
+
+    for (int32_t i = 0; i != out_num_frames; ++i) {
+      std::copy(p_in, p_in + out_feat_dim, p_out);
+
+      p_out += out_feat_dim;
+      p_in += lfr_window_shift * in_feat_dim;
+    }
+
+    return out;
+  }
+
+ private:
+  std::mutex mutex_;
+
+  OfflineModelConfig config_;
+  OfflineSenseVoiceModelMetaData meta_data_;
+
+  std::unique_ptr<QnnBackend> backend_;
+  std::unique_ptr<QnnModel> model_;
+
+  int32_t expected_num_frames_ = 0;
+  int32_t feat_dim_ = 560;
+};
+
+OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineSenseVoiceModelQnn::~OfflineSenseVoiceModelQnn() = default;
+
+std::vector<float> OfflineSenseVoiceModelQnn::Run(std::vector<float> features,
+                                                  int32_t language,
+                                                  int32_t text_norm) const {
+  return impl_->Run(std::move(features), language, text_norm);
+}
+
+const OfflineSenseVoiceModelMetaData &
+OfflineSenseVoiceModelQnn::GetModelMetadata() const {
+  return impl_->GetModelMetadata();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
new file mode 100644
index 00000000..0a9342e0
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
@@ -0,0 +1,43 @@
+// sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
+#define SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
+
+#include <memory>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+
+namespace sherpa_onnx {
+
+class OfflineSenseVoiceModelQnn {
+ public:
+  ~OfflineSenseVoiceModelQnn();
+
+  explicit OfflineSenseVoiceModelQnn(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineSenseVoiceModelQnn(Manager *mgr, const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (num_frames, feature_dim)
+   *                 before applying LFR.
+   * @param language
+   * @param text_norm
+   * @returns Return a tensor of shape (num_output_frames, vocab_size)
+   */
+  std::vector<float> Run(std::vector<float> features, int32_t language,
+                         int32_t text_norm) const;
+
+  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
index df4299ac..b482acf4 100644
--- a/sherpa-onnx/csrc/qnn/qnn-backend.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
@@ -22,7 +22,7 @@ namespace sherpa_onnx {
 
 class QnnBackend::Impl {
  public:
-  explicit Impl(const std::string &backend_lib) {
+  explicit Impl(const std::string &backend_lib, bool debug) : debug_(debug) {
     bool ok = InitQnnInterface(backend_lib);
     if (!ok) {
       SHERPA_ONNX_LOGE("Failed to init qnn interface from '%s'",
@@ -95,7 +95,10 @@ class QnnBackend::Impl {
                        backend_lib.c_str(), dlerror());
       return false;
     }
-    SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
+    }
 
     const char *symbol = "QnnInterface_getProviders";
     auto get_interface_providers =
@@ -106,7 +109,10 @@ class QnnBackend::Impl {
                        dlerror());
       return false;
     }
-    SHERPA_ONNX_LOGE("Got %s", symbol);
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("Got %s", symbol);
+    }
 
     const QnnInterface_t **interface_providers = nullptr;
     uint32_t num_providers = 0;
@@ -218,8 +224,8 @@ class QnnBackend::Impl {
 
 QnnBackend::~QnnBackend() = default;
 
-QnnBackend::QnnBackend(const std::string &backend_lib)
-    : impl_(std::make_unique<Impl>(backend_lib)) {}
+QnnBackend::QnnBackend(const std::string &backend_lib, bool debug)
+    : impl_(std::make_unique<Impl>(backend_lib, debug)) {}
 
 void QnnBackend::InitContext() const { impl_->InitContext(); }
 
diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.h b/sherpa-onnx/csrc/qnn/qnn-backend.h
index 62049a69..07a236a8 100644
--- a/sherpa-onnx/csrc/qnn/qnn-backend.h
+++ b/sherpa-onnx/csrc/qnn/qnn-backend.h
@@ -13,7 +13,7 @@ namespace sherpa_onnx {
 
 class QnnBackend {
  public:
-  explicit QnnBackend(const std::string &backend_lib);
+  explicit QnnBackend(const std::string &backend_lib, bool debug);
   ~QnnBackend();
 
   void InitContext() const;
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
index 735995ea..ec1c9838 100644
--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
@@ -21,8 +21,8 @@ namespace sherpa_onnx {
 
 class QnnModel::Impl {
  public:
-  Impl(const std::string &model_so, const QnnBackend *backend)
-      : backend_(backend) {
+  Impl(const std::string &model_so, const QnnBackend *backend, bool debug)
+      : debug_(debug), backend_(backend) {
     bool ok = InitModel(model_so);
     if (!ok) {
       SHERPA_ONNX_LOGE("Failed to load '%s'", model_so.c_str());
@@ -42,8 +42,8 @@ class QnnModel::Impl {
   }
 
   Impl(const std::string &binary_context_file, const std::string &system_lib,
-       const QnnBackend *backend, BinaryContextTag)
-      : backend_(backend) {
+       const QnnBackend *backend, BinaryContextTag, bool debug)
+      : debug_(debug), backend_(backend) {
     bool ok = LoadSystemLib(binary_context_file, system_lib);
     if (!ok) {
       return;
@@ -61,7 +61,9 @@ class QnnModel::Impl {
                        system_lib.c_str(), dlerror());
       return false;
     }
-    SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
+    if (debug_) {
+      SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
+    }
 
     auto get_system_interface_providers =
         reinterpret_cast<QnnSystemInterfaceGetProvidersFnType>(
@@ -247,6 +249,11 @@ class QnnModel::Impl {
       return false;
     }
     std::ofstream ofs(filename, std::ios::binary | std::ios::trunc);
+    if (!ofs) {
+      SHERPA_ONNX_LOGE("Failed to create '%s'", filename.c_str());
+      return false;
+    }
+
     ofs.write(reinterpret_cast<const char *>(saveBuffer.data()),
               saveBuffer.size());
 
@@ -329,7 +336,6 @@ class QnnModel::Impl {
     }
 
     FillData(t, p, n);
-    SHERPA_ONNX_LOGE("set %s", name.c_str());
 
     return true;
   }
@@ -359,7 +365,6 @@ class QnnModel::Impl {
     }
 
     FillData(t, p, n);
-    SHERPA_ONNX_LOGE("set %s", name.c_str());
 
     return true;
   }
@@ -440,7 +445,10 @@ class QnnModel::Impl {
                        model_so.c_str(), dlerror());
       return false;
     }
-    SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
+    }
 
     return true;
   }
@@ -480,7 +488,9 @@ class QnnModel::Impl {
         &graphs_info, &graphs_count, debug_, LogCallback, backend_->LogLevel());
     SHERPA_ONNX_QNN_CHECK(ret, "Failed to call compose_graphs_fn_handle_");
 
-    SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
+    if (debug_) {
+      SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
+    }
 
     for (uint32_t i = 0; i < graphs_count; ++i) {
       if (debug_) {
@@ -509,11 +519,14 @@ class QnnModel::Impl {
     input_tensor_names_.reserve(graph.num_input_tensors);
 
     for (uint32_t i = 0; i < graph.num_input_tensors; ++i) {
-      SHERPA_ONNX_LOGE("input %d", (int)i);
       auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
 
       CopyTensorInfo(graph.input_tensors[i], *p);
-      PrintTensor(p->v2);
+
+      if (debug_) {
+        SHERPA_ONNX_LOGE("input %d", (int)i);
+        PrintTensor(p->v2);
+      }
 
       std::string name = p->v1.name;
       name2tensor_[name] = p.get();
@@ -531,8 +544,11 @@ class QnnModel::Impl {
 
       CopyTensorInfo(graph.output_tensors[i], *p);
 
-      SHERPA_ONNX_LOGE("output %d", (int)i);
-      PrintTensor(p->v2);
+      if (debug_ && (i + 3 > graph.num_output_tensors)) {
+        SHERPA_ONNX_LOGE("output %d", (int)i);
+
+        PrintTensor(p->v2);
+      }
 
       std::string name = p->v1.name;
       name2tensor_[name] = p.get();
@@ -610,14 +626,15 @@ class QnnModel::Impl {
 
 QnnModel::~QnnModel() = default;
 
-QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend)
-    : impl_(std::make_unique<Impl>(model_so, backend)) {}
+QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend,
+                   bool debug)
+    : impl_(std::make_unique<Impl>(model_so, backend, debug)) {}
 
 QnnModel::QnnModel(const std::string &binary_context_file,
                    const std::string &system_lib, const QnnBackend *backend,
-                   BinaryContextTag tag)
+                   BinaryContextTag tag, bool debug)
     : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
-                                   tag)) {}  // NOLINT
+                                   tag, debug)) {}  // NOLINT
 
 bool QnnModel::SaveBinaryContext(const std::string &filename) const {
   return impl_->SaveBinaryContext(filename);
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.h b/sherpa-onnx/csrc/qnn/qnn-model.h
index d4cbfa3d..a1be4df4 100644
--- a/sherpa-onnx/csrc/qnn/qnn-model.h
+++ b/sherpa-onnx/csrc/qnn/qnn-model.h
@@ -18,10 +18,10 @@ struct BinaryContextTag {};
 
 class QnnModel {
  public:
-  QnnModel(const std::string &model_so, const QnnBackend *backend);
+  QnnModel(const std::string &model_so, const QnnBackend *backend, bool debug);
   QnnModel(const std::string &binary_context_file,
            const std::string &system_lib, const QnnBackend *backend,
-           BinaryContextTag tag);
+           BinaryContextTag tag, bool debug);
   ~QnnModel();
 
   bool SaveBinaryContext(const std::string &filename) const;

commit e7ef78be11fed077be12eb2dbab0dd79ce3da351
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Fri Nov 14 15:47:41 2025 +0800

    export omniASR_CTC_1B (#2788)

diff --git a/.github/workflows/android-rknn.yaml b/.github/workflows/android-rknn.yaml
index cd333e5a..265fd318 100644
--- a/.github/workflows/android-rknn.yaml
+++ b/.github/workflows/android-rknn.yaml
@@ -139,7 +139,7 @@ jobs:
           file: sherpa-onnx-*-android-rknn.tar.bz2
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.12.13
+          # tag: v1.12.17
 
   build-android-aar-rknn:
     needs: [build-android-rknn-libs]
@@ -275,7 +275,7 @@ jobs:
           file: ./*.aar
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.12.13
+          # tag: v1.12.17
 
       - name: Release android aar
         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
diff --git a/.github/workflows/android-static.yaml b/.github/workflows/android-static.yaml
index e6ed40ab..90fa5360 100644
--- a/.github/workflows/android-static.yaml
+++ b/.github/workflows/android-static.yaml
@@ -171,7 +171,7 @@ jobs:
           file: sherpa-onnx-*-android*.tar.bz2
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.11.3
+          # tag: v1.12.17
 
   build-android-aar-static:
     needs: [build-android-static-libs]
@@ -306,4 +306,4 @@ jobs:
           file: ./*.aar
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.11.3
+          # tag: v1.12.17
diff --git a/.github/workflows/android.yaml b/.github/workflows/android.yaml
index 46147bad..20eed0c7 100644
--- a/.github/workflows/android.yaml
+++ b/.github/workflows/android.yaml
@@ -175,9 +175,9 @@ jobs:
           file_glob: true
           overwrite: true
           file: sherpa-onnx-*-android.tar.bz2
-          repo_name: k2-fsa/sherpa-onnx
-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: v1.12.14
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.17
 
       - name: Release android libs
         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
@@ -318,9 +318,9 @@ jobs:
           file_glob: true
           overwrite: true
           file: ./*.aar
-          repo_name: k2-fsa/sherpa-onnx
-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: v1.12.14
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.17
 
       - name: Release android aar
         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
index 51a6f280..c11d7091 100644
--- a/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
@@ -3,7 +3,7 @@ name: export-omnilingual-asr-to-onnx
 on:
   push:
     branches:
-      - export-omnilingual-asr
+      - omnilingual-1b
   workflow_dispatch:
 
 concurrency:
@@ -13,13 +13,14 @@ concurrency:
 jobs:
   export-omnilingual-asr-to-onnx:
     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
-    name: export omnilingual-asr
+    name: ${{ matrix.model_card }}
     runs-on: ${{ matrix.os }}
     strategy:
       fail-fast: false
       matrix:
         os: [ubuntu-latest]
         python-version: ["3.10"]
+        model_card: ["omniASR_CTC_300M", "omniASR_CTC_1B"]
 
     steps:
       - uses: actions/checkout@v4
@@ -58,9 +59,11 @@ jobs:
         shell: bash
         run: |
           cd scripts/omnilingual-asr
-          python3 ./export-onnx.py
+          model_card=${{ matrix.model_card }}
+          python3 ./export-onnx.py --model-card $model_card
 
           ls -lh *.onnx
+          ls -lh *.weights || true
 
           rm README.md
 
@@ -78,11 +81,15 @@ jobs:
           echo "---collect files----"
 
           d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
+          if [[ $model_card == omniASR_CTC_1B ]]; then
+            d=sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-2025-11-12
+          fi
 
           mkdir -p $d
           mkdir -p $d/test_wavs
 
           mv -v model.onnx $d
+          mv -v model.weights $d || true
           cp -v tokens.txt $d
           cp -v README.md $d
           cp -v LICENSE* $d
@@ -94,6 +101,9 @@ jobs:
           mv $d ../..
 
           d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+          if [[ $model_card == omniASR_CTC_1B ]]; then
+            d=sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-int8-2025-11-12
+          fi
 
           mkdir -p $d
           mkdir -p $d/test_wavs
@@ -115,6 +125,16 @@ jobs:
 
           ls -lh *.tar.bz2
 
+          df -h
+          rm -fv onnx_* model.encoder* model.final*
+
+          ls -lh ~/.cache/fairseq2/assets/*
+
+          rm -rf ~/.cache/fairseq2/assets/
+          rm -rf ~/.cache
+
+          df -h
+
       - name: Publish to huggingface
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
@@ -130,10 +150,18 @@ jobs:
             export GIT_LFS_SKIP_SMUDGE=1
             export GIT_CLONE_PROTECTION_ACTIVE=false
 
+            model_card=${{ matrix.model_card }}
+
             dirs=(
               sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
               sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
             )
+            if [[ $model_card == omniASR_CTC_1B ]]; then
+              dirs=(
+                sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-2025-11-12
+                sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-int8-2025-11-12
+              )
+            fi
 
             for d in ${dirs[@]}; do
               rm -rf huggingface
@@ -143,9 +171,11 @@ jobs:
               git fetch
               git pull
               echo "pwd: $PWD"
-              cp -a ../$d/* .
+              rm -fv ./*.weights
+              mv -v ../$d/* .
 
               git lfs track "*.onnx"
+              git lfs track "*.weights"
               git lfs track "*.wav"
               ls -lh
               git add .
@@ -159,6 +189,20 @@ jobs:
               popd
             done
 
+      # List large files first (safe)
+      - name: List .tar.bz2 files larger than 2GB
+        run: |
+          ls -lh *.tar.bz2
+          echo "----"
+          find . -type f -name "*.tar.bz2" -size +2G -print
+
+      # Delete large files
+      - name: Delete .tar.bz2 files larger than 2GB
+        run: |
+          find . -type f -name "*.tar.bz2" -size +2G -delete
+
+          ls -lh *.tar.bz2
+
       - name: Release
         uses: svenstaro/upload-release-action@v2
         with:
diff --git a/scripts/omnilingual-asr/export-onnx.py b/scripts/omnilingual-asr/export-onnx.py
index deaf556d..e8f765bb 100755
--- a/scripts/omnilingual-asr/export-onnx.py
+++ b/scripts/omnilingual-asr/export-onnx.py
@@ -1,6 +1,7 @@
 #!/usr/bin/env python3
 # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
 
+import argparse
 from typing import Dict
 
 import onnx
@@ -10,7 +11,20 @@ from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline
 from onnxruntime.quantization import QuantType, quantize_dynamic
 
 
-def add_meta_data(filename: str, meta_data: Dict[str, str]):
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
+    )
+    parser.add_argument(
+        "--model-card",
+        type=str,
+        required=True,
+        help="omniASR_CTC_300M, or omniASR_CTC_1B",
+    )
+    return parser.parse_args()
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, str], model_card: str):
     """Add meta data to an ONNX model. It is changed in-place.
 
     Args:
@@ -28,6 +42,18 @@ def add_meta_data(filename: str, meta_data: Dict[str, str]):
         meta.key = key
         meta.value = str(value)
 
+    if "300M" in model_card:
+        onnx.save(model, filename)
+    else:
+        external_filename = filename.split(".onnx")[0]
+        onnx.save(
+            model,
+            filename,
+            save_as_external_data=True,
+            all_tensors_to_one_file=True,
+            location=external_filename + ".weights",
+        )
+
 
 class ModelWrapper(torch.nn.Module):
     def __init__(self, model):
@@ -46,8 +72,10 @@ class ModelWrapper(torch.nn.Module):
 
 @torch.no_grad()
 def main():
+    args = get_args()
+    print(vars(args))
     pipeline = ASRInferencePipeline(
-        model_card="omniASR_CTC_300M",
+        model_card=args.model_card,
         device="cpu",
         dtype=torch.float32,
     )
@@ -87,7 +115,7 @@ def main():
         "comment": "300M-CTC",
     }
 
-    add_meta_data("model.onnx", meta_data)
+    add_meta_data("model.onnx", meta_data, args.model_card)
     print("saved to model.onnx")
 
     quantize_dynamic(
diff --git a/scripts/omnilingual-asr/test.py b/scripts/omnilingual-asr/test.py
index 9415902a..7257cd18 100755
--- a/scripts/omnilingual-asr/test.py
+++ b/scripts/omnilingual-asr/test.py
@@ -76,7 +76,7 @@ def load_audio(filename):
     return (samples - mean) / np.sqrt(var + eps)
 
 
-def test(filename, wav_file_list, num_iter=10):
+def test(filename, wav_file_list, num_iter=1):
     id2token = load_tokens()
     model = OnnxModel(filename)
 
diff --git a/sherpa-onnx/csrc/file-utils.cc b/sherpa-onnx/csrc/file-utils.cc
index b8361445..6ad16166 100644
--- a/sherpa-onnx/csrc/file-utils.cc
+++ b/sherpa-onnx/csrc/file-utils.cc
@@ -26,8 +26,19 @@ void AssertFileExists(const std::string &filename) {
 }
 
 std::vector<char> ReadFile(const std::string &filename) {
-  std::ifstream input(filename, std::ios::binary);
-  std::vector<char> buffer(std::istreambuf_iterator<char>(input), {});
+  std::ifstream file(filename, std::ios::binary | std::ios::ate);
+  if (!file.is_open()) {
+    return {};
+  }
+
+  std::streamsize size = file.tellg();
+  file.seekg(0, std::ios::beg);
+
+  std::vector<char> buffer(size);
+  if (!file.read(buffer.data(), size)) {
+    return {};
+  }
+
   return buffer;
 }
 
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
index 7df94402..d5bbc176 100644
--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
@@ -36,8 +36,9 @@ class OfflineOmnilingualAsrCtcModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    auto buf = ReadFile(config_.omnilingual.model);
-    Init(buf.data(), buf.size());
+    sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config_.omnilingual.model), sess_opts_);
+    Init(nullptr, 0);
   }
 
   template <typename Manager>
@@ -101,8 +102,18 @@ class OfflineOmnilingualAsrCtcModel::Impl {
 
  private:
   void Init(void *model_data, size_t model_data_length) {
-    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
-                                           sess_opts_);
+    // For models with 1B parameters, weights are saved externally
+    // in model.weights
+    // We cannot create session from buffer in this case.
+    if (model_data) {
+      sess_ = std::make_unique<Ort::Session>(env_, model_data,
+                                             model_data_length, sess_opts_);
+    } else if (!sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize session outside of this "
+          "function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
 
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 37279c37..3a675fc1 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -334,6 +334,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       !config.model_config.zipformer_ctc.model.empty() ||
       !config.model_config.tdnn.model.empty() ||
       !config.model_config.wenet_ctc.model.empty() ||
+      !config.model_config.omnilingual.model.empty() ||
       !config.model_config.dolphin.model.empty()) {
     return std::make_unique<OfflineRecognizerCtcImpl>(mgr, config);
   }
diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
index a33594f0..24a63bc4 100644
--- a/sherpa-onnx/csrc/session.cc
+++ b/sherpa-onnx/csrc/session.cc
@@ -56,6 +56,9 @@ Ort::SessionOptions GetSessionOptionsImpl(
   // sess_opts.SetLogSeverityLevel(ORT_LOGGING_LEVEL_VERBOSE);
   // sess_opts.EnableProfiling("profile");
 
+  // If you want to speed up initialization, please uncomment the following line
+  // sess_opts.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_DISABLE_ALL);
+
   switch (p) {
     case Provider::kCPU:
       break;  // nothing to do for the CPU provider
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
index 41fb3a4f..8f4d1934 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
@@ -123,8 +123,18 @@ for a list of pre-trained models to download.
   }
 
   fprintf(stderr, "Creating recognizer ...\n");
+  const auto begin_init = std::chrono::steady_clock::now();
+
   sherpa_onnx::OfflineRecognizer recognizer(config);
 
+  const auto end_init = std::chrono::steady_clock::now();
+  float elapsed_seconds_init =
+      std::chrono::duration_cast<std::chrono::milliseconds>(end_init -
+                                                            begin_init)
+          .count() /
+      1000.;
+  fprintf(stderr, "recognizer created in %.3f s\n", elapsed_seconds_init);
+
   fprintf(stderr, "Started\n");
   const auto begin = std::chrono::steady_clock::now();
 
diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
index 5a3fff09..3f0f80d5 100644
--- a/sherpa-onnx/csrc/text-utils.h
+++ b/sherpa-onnx/csrc/text-utils.h
@@ -182,6 +182,12 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
 
 bool IsPunct(const std::string &s);
 
+#if defined(_WIN32)
+#define SHERPA_ONNX_TO_ORT_PATH(s) (ToWideString(s).c_str())
+#else
+#define SHERPA_ONNX_TO_ORT_PATH(s) ((s).c_str())
+#endif
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_TEXT_UTILS_H_

commit bb96ea34bbf7f97ffd076bee69814b4c68b67558
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Fri Nov 14 07:30:24 2025 +0800

    Fix building wheels (#2786)

diff --git a/.github/workflows/aarch64-linux-gnu-shared.yaml b/.github/workflows/aarch64-linux-gnu-shared.yaml
index d5f2b9c8..1c6fdf3f 100644
--- a/.github/workflows/aarch64-linux-gnu-shared.yaml
+++ b/.github/workflows/aarch64-linux-gnu-shared.yaml
@@ -255,7 +255,7 @@ jobs:
           file: sherpa-onnx-*linux-aarch64*.tar.bz2
           # repo_name: k2-fsa/sherpa-onnx
           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          # tag: v1.12.13
+          # tag: v1.12.17
 
       - name: Test offline Moonshine
         if: matrix.build_type != 'Debug'
diff --git a/.github/workflows/ascend.yaml b/.github/workflows/ascend.yaml
index 05543985..280ece7a 100644
--- a/.github/workflows/ascend.yaml
+++ b/.github/workflows/ascend.yaml
@@ -2,10 +2,6 @@ name: ascend
 
 on:
   push:
-    branches:
-      - master
-      - ci-ascend
-  pull_request:
     branches:
       - master
 
diff --git a/.github/workflows/build-wheels-armv7l.yaml b/.github/workflows/build-wheels-armv7l.yaml
index 5f861283..accd3784 100644
--- a/.github/workflows/build-wheels-armv7l.yaml
+++ b/.github/workflows/build-wheels-armv7l.yaml
@@ -21,7 +21,7 @@ jobs:
       fail-fast: false
       matrix:
         os: [ubuntu-latest]
-        python-version: ["3.7", "3.8", "3.9", "3.10", "3.11"]
+        python-version: ["3.8", "3.9", "3.10", "3.11"]
 
     steps:
       - uses: actions/checkout@v4
diff --git a/.github/workflows/build-wheels-linux.yaml b/.github/workflows/build-wheels-linux.yaml
index 0f71b6c7..d9757a6a 100644
--- a/.github/workflows/build-wheels-linux.yaml
+++ b/.github/workflows/build-wheels-linux.yaml
@@ -427,4 +427,4 @@ jobs:
           TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}
         shell: bash
         run: |
-          twine upload dist/sherpa-onnx-*.tar.gz
+          twine upload dist/sherpa*.tar.gz
diff --git a/.github/workflows/build-wheels-win32.yaml b/.github/workflows/build-wheels-win32.yaml
index 3dca70c1..d76d9d8f 100644
--- a/.github/workflows/build-wheels-win32.yaml
+++ b/.github/workflows/build-wheels-win32.yaml
@@ -26,7 +26,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [windows-latest]
+        os: [windows-2022]
 
     steps:
       - uses: actions/checkout@v4
@@ -263,7 +263,7 @@ jobs:
     strategy:
       fail-fast: false
       matrix:
-        os: [windows-latest]
+        os: [windows-2022]
         python-version: ["cp38", "cp39", "cp310", "cp311", "cp312", "cp313", "cp314"]
 
     steps:
diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
index 332d8d96..51a6f280 100644
--- a/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
@@ -66,7 +66,6 @@ jobs:
 
           curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/README.md
           curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE
-          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE-CC-BY-4.0.md
 
           curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en.wav
           curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/es.wav
diff --git a/.github/workflows/mfc.yaml b/.github/workflows/mfc.yaml
index dd18942a..5911f7a0 100644
--- a/.github/workflows/mfc.yaml
+++ b/.github/workflows/mfc.yaml
@@ -125,9 +125,9 @@ jobs:
           file_glob: true
           overwrite: true
           file: ./mfc-examples/${{ matrix.arch }}/Release/sherpa-onnx-streaming-*.exe
-          repo_name: k2-fsa/sherpa-onnx
-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: v1.12.15
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.17
 
       - name: Release pre-compiled binaries and libs for Windows ${{ matrix.arch }}
         if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
@@ -136,9 +136,9 @@ jobs:
           file_glob: true
           overwrite: true
           file: ./mfc-examples/${{ matrix.arch }}/Release/sherpa-onnx-non-streaming-*.exe
-          repo_name: k2-fsa/sherpa-onnx
-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: v1.12.15
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.17
 
       - name: Release pre-compiled binaries and libs for Windows ${{ matrix.arch }}
         if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
@@ -147,6 +147,6 @@ jobs:
           file_glob: true
           overwrite: true
           file: ./mfc-examples/${{ matrix.arch }}/sherpa-onnx-non-streaming-*.exe
-          repo_name: k2-fsa/sherpa-onnx
-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: v1.12.15
+          # repo_name: k2-fsa/sherpa-onnx
+          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          # tag: v1.12.17
diff --git a/scripts/go/release.sh b/scripts/go/release.sh
index c9334e56..c0dbeb83 100755
--- a/scripts/go/release.sh
+++ b/scripts/go/release.sh
@@ -31,7 +31,8 @@ function linux() {
   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_x86_64.whl
   unzip sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_x86_64.whl
 
-  cp -v sherpa_onnx/lib/*.so* $dst
+  rm -fv $dst/_sherpa*.so
+  cp -v sherpa_onnx/lib/lib*.so* $dst
 
   cd ..
   rm -rf t
@@ -43,7 +44,8 @@ function linux() {
   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_aarch64.whl
   unzip ./sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_aarch64.whl
 
-  cp -v sherpa_onnx/lib/*.so* $dst
+  rm -fv $dst/_sherpa*.so
+  cp -v sherpa_onnx/lib/lib*.so* $dst
 
   cd ..
   rm -rf t
@@ -55,7 +57,8 @@ function linux() {
   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
   unzip ./sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
 
-  cp -v sherpa_onnx/lib/*.so* $dst
+  rm -fv $dst/_sherpa*.so
+  cp -v sherpa_onnx/lib/lib*.so* $dst
 
   cd ..
   rm -rf t
diff --git a/setup.py b/setup.py
index 648d118c..061a2f03 100644
--- a/setup.py
+++ b/setup.py
@@ -43,7 +43,7 @@ def get_package_version():
     return latest_version
 
 
-package_name = "sherpa-onnx"
+package_name = "sherpa_onnx"
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "a") as f:
     f.write(f"__version__ = '{get_package_version()}'\n")
@@ -81,13 +81,17 @@ setuptools.setup(
         "sherpa_onnx": "sherpa-onnx/python/sherpa_onnx",
     },
     packages=["sherpa_onnx"],
-    data_files=[
-        ("Scripts", get_binaries_to_install())
-        if is_windows()
-        else ("bin", get_binaries_to_install())
-    ]
-    if get_binaries_to_install()
-    else None,
+    data_files=(
+        [
+            (
+                ("Scripts", get_binaries_to_install())
+                if is_windows()
+                else ("bin", get_binaries_to_install())
+            )
+        ]
+        if get_binaries_to_install()
+        else None
+    ),
     url="https://github.com/k2-fsa/sherpa-onnx",
     long_description=read_long_description(),
     long_description_content_type="text/markdown",
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
index 0ed69a58..7df94402 100644
--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
 
 #include <algorithm>
+#include <cmath>
 #include <memory>
 #include <string>
 #include <utility>

commit fb2b2f0d7b13a1e9d5516016635667c5e254b8ad
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 19:24:04 2025 +0800

    Release v1.12.17 (#2785)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 7efbb6b3..8c42ad5e 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,7 @@
+## 1.12.17
+
+* Fix releasing
+
 ## 1.12.16
 
 * Support exporting SenseVoice and Paraformer to Ascend 310P3 NPU. (#2716)
@@ -33,7 +37,7 @@
 * Add Dart API for Omnilingual ASR CTC models (#2779)
 * Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)
 * Add Pascal API for Omnilingual ASR CTC models (#2782)
-* Add Koltin and Java API for Omnilingual ASR CTC models (#2783)
+* Add Kotlin and Java API for Omnilingual ASR CTC models (#2783)
 
 ## 1.12.15
 
@@ -504,7 +508,7 @@
 * Fix: Prepend 0 to tokenization to prevent word skipping for Kokoro. (#1787)
 * Export Kokoro 1.0 to sherpa-onnx (#1788)
 * Add C++ and Python API for Kokoro 1.0 multilingual TTS model (#1795)
-* Add Java and Koltin API for Kokoro TTS 1.0 (#1798)
+* Add Java and Kotlin API for Kokoro TTS 1.0 (#1798)
 * Add Android demo for Kokoro TTS 1.0 (#1799)
 * Add C API for Kokoro TTS 1.0 (#1801)
 * Add CXX API for Kokoro TTS 1.0 (#1802)
@@ -544,7 +548,7 @@
 * Add Pascal API for Kokoro TTS models (#1724)
 * Add JavaScript API (node-addon) for Kokoro TTS models (#1725)
 * Add JavaScript (WebAssembly) API for Kokoro TTS models. (#1726)
-* Add Koltin and Java API for Kokoro TTS models (#1728)
+* Add Kotlin and Java API for Kokoro TTS models (#1728)
 * Update README.md for KWS to not use git lfs. (#1729)
 
 
diff --git a/CMakeLists.txt b/CMakeLists.txt
index ac0be33b..20190dc7 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.16")
+set(SHERPA_ONNX_VERSION "1.12.17")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index 1d59d8cb..76024de6 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 32
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index 1d59d8cb..76024de6 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 32
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index 3b1a5392..c3c5ac3d 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-v1.12.16-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.16-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-v1.12.17-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.17-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.16.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.17.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index b4407bbe..bd6e098c 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index 2b95c60a..12638bee 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 26
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index 039efe01..93848bc6 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -10,7 +10,7 @@ android {
         minSdk 28
         targetSdk 34
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.16'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.17'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index 1d59d8cb..76024de6 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 32
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index 5cd52b3c..082ea5c8 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index 39ef31e9..ef491de9 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 28
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.16")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.17")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index e538009a..a5ce9201 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index e49b49c2..ddb6f444 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index 98f11773..a7617b14 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 2636a9b6..034325ac 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 32
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 1b1d0bc1..53af0d02 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -12,7 +12,7 @@ android {
         minSdk = 21
         targetSdk = 34
         versionCode = 20251113
-        versionName = "1.12.16"
+        versionName = "1.12.17"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index 566e93d2..15784a18 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 33
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index 566e93d2..15784a18 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 33
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 23547587..7f10eb5d 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -12,7 +12,7 @@ android {
         minSdk 21
         targetSdk 32
         versionCode 20251113
-        versionName "1.12.16"
+        versionName "1.12.17"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index 80ca7b1f..c1477eb7 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.16</string>
+	<string>1.12.17</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index 4b1994a0..3fde5ed0 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index 752eec75..909c3751 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index 727da49f..762f3493 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index 42b1c2b0..187661b5 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index f52042bb..c2893c4c 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index bbe878f8..2d68710a 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index 0161e730..32ec2161 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index 3248a361..b9e2166d 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index 630ddd72..577b8efd 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index ee263496..54c9abf7 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index 005c49e1..b544cba8 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 7206336a..6c124a31 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index 103aa7ee..1fe5515f 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.16
+version: 1.12.17
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index 23aab88d..455b6f1f 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.16
+version: 1.12.17
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index 2c393901..dd4d2433 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.16
+version: 1.12.17
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.16
+  sherpa_onnx: ^1.12.17
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index a04d6a71..825f975f 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.16
+version: 1.12.17
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.16
+  sherpa_onnx_android: ^1.12.17
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.16
+  sherpa_onnx_macos: ^1.12.17
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.16
+  sherpa_onnx_linux: ^1.12.17
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.16
+  sherpa_onnx_windows: ^1.12.17
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.16
+  sherpa_onnx_ios: ^1.12.17
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index 3119aff1..3e674158 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.16'
+  s.version          = '1.12.17'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index 17def84d..dee37a00 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.16'
+  s.version          = '1.12.17'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index 0228e2c5..dd8845a8 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.16';
+export const HAR_VERSION = '1.12.17';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index d1b4325d..187519e9 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.16",
+    "sherpa_onnx": "1.12.17",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index b76d8527..1bfb0d94 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.16",
+  "version": "1.12.17",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index 19efd920..ec30333f 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.16"
+    "sherpa_onnx": "1.12.17"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 913e059e..992dd9d0 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.16",
+    "sherpa_onnx": "1.12.17",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 913e059e..992dd9d0 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.16",
+    "sherpa_onnx": "1.12.17",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 913e059e..992dd9d0 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.16",
+    "sherpa_onnx": "1.12.17",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index 3d6da438..acc4feb5 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.16.har
+Please download ./sherpa_onnx-v1.12.17.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index a1215a29..27182c9b 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.16",
+    "sherpa_onnx": "1.12.17",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index aefbd313..9e83e15c 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-1.12.16.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-1.12.17.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.16.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.16 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.17.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.17 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index 89f9292b..2c7edb25 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x86-v1.12.16.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x86-v1.12.16.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x86-v1.12.16.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x86-v1.12.17.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x86-v1.12.17.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x86-v1.12.17.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index d4784850..3758063c 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -5,8 +5,8 @@ set -ex
 old_version_code=20251022
 new_version_code=20251113
 
-old_version="1\.12\.15"
-new_version="1\.12\.16"
+old_version="1\.12\.16"
+new_version="1\.12\.17"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index c4f6a333..bf6beed9 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.16"
+    "sherpa-onnx-node": "^1.12.17"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index f10c5745..ab2c37d2 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.16",
+    "sherpa-onnx": "^1.12.17",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index 51eb50c8..e8191a24 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.16</version>
+    <version>1.12.17</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index 02dc7925..b7a22878 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.16",
+    version="1.12.17",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.16",
+        "sherpa-onnx-core==1.12.17",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index a71b3815..071319fb 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.16",
+    version="1.12.17",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 6ffc01dc..648d118c 100644
--- a/setup.py
+++ b/setup.py
@@ -105,7 +105,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.16"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.17"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index a7fc063a..02c48a40 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Thu Nov 13 19:03:01 2025";
+  static const char *date = "Thu Nov 13 19:08:29 2025";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "0f647d32";
+  static const char *sha1 = "6d199b18";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.16";
+  static const char *version = "1.12.17";
   return version;
 }
 

commit 6d199b18558b0f0ce383fcef83b3105d0439f85d
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 19:08:29 2025 +0800

    Release v1.12.16 (#2784)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 6e28b458..7efbb6b3 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,40 @@
+## 1.12.16
+
+* Support exporting SenseVoice and Paraformer to Ascend 310P3 NPU. (#2716)
+* Demo for no stream vad asr with flutter (#2705)
+* Fix crashing in Android KWS demo (#2719)
+* Add C++ API with ACL C API for SenseVoice ASR on Ascend NPU (#2728)
+* Allow up to 30 seconds ASR for sense-voice on Ascend NPU (#2729)
+* Fix compilation error for Ascend NPU (#2731)
+* docs: fix Flutter TTS macOS mirror link targets; fix speech-enhancement link typo (#2723)
+* Export models for Ascend910B2 (#2740)
+* Add C++ runtime for Paraformer on Ascend NPU. (#2741)
+* Expose ys probs to JNI, Kotlin and Java API (#2736)
+* Add CI for Ascend NPU (#2743)
+* Export models for CANN 8.2 (#2745)
+* Fix validating model config for Paraformer. (#2749)
+* Add cxx API for online punctuation models (#2759)
+* Export sense voice to qnn (#2760)
+* Export models to Ascend 910B3 (#2761)
+* Support MatchaTTS models for Chinese+English. (#2763)
+* Fix zipvoice. (#2764)
+* Support passing multiple lexicon files for matcha tts models. (#2765)
+* Begin to add qnn C API (#2766)
+* Add QnnConfig. (#2768)
+* Fix missing includes. (#2769)
+* Begin to export omnilingual-asr to sherpa-onnx (#2770)
+* Add C++ and Python API for Omnilingual ASR models. (#2772)
+* Add C API for Omnilingual ASR CTC models (#2773)
+* Add CXX API for Omnilingual ASR CTC models (#2774)
+* Add C# API for Omnilingual ASR CTC models (#2775)
+* Add Swift API for Omnilingual ASR CTC models (#2776)
+* Add Go API for Omnilingual ASR CTC models (#2778)
+* Add JavaScript (node-addon) API for Omnilingual ASR CTC models (#2780)
+* Add Dart API for Omnilingual ASR CTC models (#2779)
+* Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)
+* Add Pascal API for Omnilingual ASR CTC models (#2782)
+* Add Koltin and Java API for Omnilingual ASR CTC models (#2783)
+
 ## 1.12.15
 
 * Exposing online punctuation model support in node-addon-api (#2609)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 92a3ff01..ac0be33b 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.15")
+set(SHERPA_ONNX_VERSION "1.12.16")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index 1e20e730..1d59d8cb 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index 1e20e730..1d59d8cb 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index d8a67dc6..3b1a5392 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-v1.12.15-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.15-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-v1.12.16-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.16-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.15.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.16.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index 66925f8b..b4407bbe 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index bfd5023d..2b95c60a 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index c59ab06e..039efe01 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.15'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.16'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index 1e20e730..1d59d8cb 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index 59fe4c9b..5cd52b3c 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index 103c683f..39ef31e9 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.15")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.16")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index e0128ee0..e538009a 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index fcd5f076..e49b49c2 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index 889c1efa..98f11773 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 33d8e55f..2636a9b6 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 3172dd30..1b1d0bc1 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251022
-        versionName = "1.12.15"
+        versionCode = 20251113
+        versionName = "1.12.16"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index c13e6886..566e93d2 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index c13e6886..566e93d2 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 1ab62085..23547587 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251022
-        versionName "1.12.15"
+        versionCode 20251113
+        versionName "1.12.16"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index 8159dad9..80ca7b1f 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.15</string>
+	<string>1.12.16</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index 93da5a5e..4b1994a0 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index c76d38c7..752eec75 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index 9dba9dba..727da49f 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index 63793658..42b1c2b0 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index 632e39a1..f52042bb 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index 97742512..bbe878f8 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index 39b5ed56..0161e730 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index af5273f0..3248a361 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index 56c07e66..630ddd72 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index f98e8326..ee263496 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index 79a643c2..005c49e1 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 9b7de990..7206336a 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index 3d4aaf90..103aa7ee 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.15
+version: 1.12.16
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index aa986f78..23aab88d 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.15
+version: 1.12.16
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index 0e45ba3e..2c393901 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.15
+version: 1.12.16
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.15
+  sherpa_onnx: ^1.12.16
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index 56b678b1..a04d6a71 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.15
+version: 1.12.16
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.15
+  sherpa_onnx_android: ^1.12.16
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.15
+  sherpa_onnx_macos: ^1.12.16
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.15
+  sherpa_onnx_linux: ^1.12.16
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.15
+  sherpa_onnx_windows: ^1.12.16
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.15
+  sherpa_onnx_ios: ^1.12.16
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index ab87f3c6..3119aff1 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.15'
+  s.version          = '1.12.16'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index 894c4470..17def84d 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.15'
+  s.version          = '1.12.16'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index 06ac52e5..0228e2c5 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.15';
+export const HAR_VERSION = '1.12.16';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index 8f92e35e..d1b4325d 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.15",
+    "sherpa_onnx": "1.12.16",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index da1ec3e2..b76d8527 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.15",
+  "version": "1.12.16",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index 3de6c277..19efd920 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.15"
+    "sherpa_onnx": "1.12.16"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index f6633742..913e059e 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.15",
+    "sherpa_onnx": "1.12.16",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index f6633742..913e059e 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.15",
+    "sherpa_onnx": "1.12.16",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index f6633742..913e059e 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.15",
+    "sherpa_onnx": "1.12.16",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index 6ba991dd..3d6da438 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.15.har
+Please download ./sherpa_onnx-v1.12.16.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index f72068ab..a1215a29 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.15",
+    "sherpa_onnx": "1.12.16",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index 63f70826..aefbd313 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-1.12.15.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-1.12.16.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.15.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.15 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.16.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.16 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index a296b953..89f9292b 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-asr-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-asr-x86-v1.12.15.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-streaming-asr-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-streaming-asr-x86-v1.12.15.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-tts-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-tts-x86-v1.12.15.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x86-v1.12.16.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x86-v1.12.16.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x86-v1.12.16.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 6570f2f3..d4784850 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20250918
-new_version_code=20251022
+old_version_code=20251022
+new_version_code=20251113
 
-old_version="1\.12\.14"
-new_version="1\.12\.15"
+old_version="1\.12\.15"
+new_version="1\.12\.16"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index 6e4d9016..c4f6a333 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.15"
+    "sherpa-onnx-node": "^1.12.16"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index a4374ef2..f10c5745 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.15",
+    "sherpa-onnx": "^1.12.16",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index 22b9fbd4..51eb50c8 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.15</version>
+    <version>1.12.16</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index 4821b69f..02dc7925 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.15",
+    version="1.12.16",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.15",
+        "sherpa-onnx-core==1.12.16",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index 0cfb2b15..a71b3815 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.15",
+    version="1.12.16",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 22e9fec5..6ffc01dc 100644
--- a/setup.py
+++ b/setup.py
@@ -105,7 +105,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.15"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.16"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index ed8c542c..a7fc063a 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Wed Oct 22 12:00:34 2025";
+  static const char *date = "Thu Nov 13 19:03:01 2025";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "8a6dec20";
+  static const char *sha1 = "0f647d32";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.15";
+  static const char *version = "1.12.16";
   return version;
 }
 

commit 0f647d32423d243aa108a43d78b67d87195e1e1e
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 19:03:01 2025 +0800

    Add Koltin and Java API for Omnilingual ASR CTC models (#2783)

diff --git a/.github/workflows/run-java-test.yaml b/.github/workflows/run-java-test.yaml
index 3f6fda2e..b2167ff0 100644
--- a/.github/workflows/run-java-test.yaml
+++ b/.github/workflows/run-java-test.yaml
@@ -108,6 +108,20 @@ jobs:
           cd ./java-api-examples
           ./run-version-test.sh
 
+      - name:  Run java test (Omnilingual ASR CTC)
+        shell: bash
+        run: |
+          cd ./java-api-examples
+          ./run-non-streaming-decode-file-omnilingual-asr-ctc.sh
+          rm -rf sherpa-onnx-omnilingual-*
+
+      - name:  Run java test (WeNet CTC)
+        shell: bash
+        run: |
+          cd ./java-api-examples
+          ./run-non-streaming-decode-file-wenet-ctc.sh
+          rm -rf sherpa-onnx-wenet*
+
       - name:  Run java test (Streaming T-one)
         shell: bash
         run: |
diff --git a/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
new file mode 100644
index 00000000..cee8bb77
--- /dev/null
+++ b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
@@ -0,0 +1,54 @@
+// Copyright 2025 Xiaomi Corporation
+
+// This file shows how to use an offline Omnilingual ASR CTC model,
+// i.e., non-streaming Omnilingual ASR CTC model,
+// to decode files.
+import com.k2fsa.sherpa.onnx.*;
+
+public class NonStreamingDecodeFileOmnilingualAsrCtc {
+  public static void main(String[] args) {
+    // please refer to
+    // https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html
+    // to download model files
+    String model =
+        "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
+
+    String tokens =
+        "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
+
+    String waveFilename =
+        "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
+
+    WaveReader reader = new WaveReader(waveFilename);
+
+    OfflineOmnilingualAsrCtcModelConfig omnilingual =
+        OfflineOmnilingualAsrCtcModelConfig.builder().setModel(model).build();
+
+    OfflineModelConfig modelConfig =
+        OfflineModelConfig.builder()
+            .setOmnilingual(omnilingual)
+            .setTokens(tokens)
+            .setNumThreads(1)
+            .setDebug(true)
+            .build();
+
+    OfflineRecognizerConfig config =
+        OfflineRecognizerConfig.builder()
+            .setOfflineModelConfig(modelConfig)
+            .setDecodingMethod("greedy_search")
+            .build();
+
+    OfflineRecognizer recognizer = new OfflineRecognizer(config);
+    OfflineStream stream = recognizer.createStream();
+    stream.acceptWaveform(reader.getSamples(), reader.getSampleRate());
+
+    recognizer.decode(stream);
+
+    String text = recognizer.getResult(stream).getText();
+
+    System.out.printf("filename:%s\nresult:%s\n", waveFilename, text);
+
+    stream.release();
+    recognizer.release();
+  }
+}
diff --git a/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh b/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh
new file mode 100755
index 00000000..025880ba
--- /dev/null
+++ b/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh
@@ -0,0 +1,38 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [[ ! -f ../build/lib/libsherpa-onnx-jni.dylib  && ! -f ../build/lib/libsherpa-onnx-jni.so ]]; then
+  mkdir -p ../build
+  pushd ../build
+  cmake \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    -DSHERPA_ONNX_ENABLE_JNI=ON \
+    ..
+
+  make -j4
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f ../sherpa-onnx/java-api/build/sherpa-onnx.jar ]; then
+  pushd ../sherpa-onnx/java-api
+  make
+  popd
+fi
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+
+java \
+  -Djava.library.path=$PWD/../build/lib \
+  -cp ../sherpa-onnx/java-api/build/sherpa-onnx.jar \
+  NonStreamingDecodeFileOmnilingualAsrCtc.java
diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
index e338f4d0..6dc1d164 100755
--- a/kotlin-api-examples/run.sh
+++ b/kotlin-api-examples/run.sh
@@ -488,6 +488,28 @@ function testOfflineNeMoCanary() {
   java -Djava.library.path=../build/lib -jar $out_filename
 }
 
+
+function testOfflineOmnilingualAsrCtc() {
+  if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+    tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+    rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  fi
+
+  out_filename=test_offline_omnilingual_asr_ctc.jar
+  kotlinc-jvm -include-runtime -d $out_filename \
+    test_offline_omnilingual_asr_ctc.kt \
+    FeatureConfig.kt \
+    HomophoneReplacerConfig.kt \
+    OfflineRecognizer.kt \
+    OfflineStream.kt \
+    WaveReader.kt \
+    faked-asset-manager.kt
+
+  ls -lh $out_filename
+  java -Djava.library.path=../build/lib -jar $out_filename
+}
+
 function testOfflineWenetCtc() {
   if [ ! -f sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10/model.int8.onnx ]; then
     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
@@ -511,6 +533,7 @@ function testOfflineWenetCtc() {
 
 testVersion
 
+testOfflineOmnilingualAsrCtc
 testOfflineWenetCtc
 testOfflineNeMoCanary
 testOfflineSenseVoiceWithHr
diff --git a/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt b/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt
new file mode 100644
index 00000000..a69908c4
--- /dev/null
+++ b/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt
@@ -0,0 +1,31 @@
+package com.k2fsa.sherpa.onnx
+
+fun main() {
+  val recognizer = createOfflineRecognizer()
+  val waveFilename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
+
+  val objArray = WaveReader.readWaveFromFile(
+      filename = waveFilename,
+  )
+  val samples: FloatArray = objArray[0] as FloatArray
+  val sampleRate: Int = objArray[1] as Int
+
+  var stream = recognizer.createStream()
+  stream.acceptWaveform(samples, sampleRate=sampleRate)
+  recognizer.decode(stream)
+
+  var result = recognizer.getResult(stream)
+  println(result)
+
+  stream.release()
+  recognizer.release()
+}
+
+
+fun createOfflineRecognizer(): OfflineRecognizer {
+  val config = OfflineRecognizerConfig(
+      modelConfig = getOfflineModelConfig(type = 44)!!,
+  )
+
+  return OfflineRecognizer(config = config)
+}
diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
index d1dee201..c84ef99c 100755
--- a/scripts/apk/generate-vad-asr-apk-script.py
+++ b/scripts/apk/generate-vad-asr-apk-script.py
@@ -743,6 +743,22 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12",
+            idx=44,
+            lang="1600",
+            lang2="1600_languages",
+            short_name="omnilingual_asr_300M_ctc_int8",
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
index 597173e4..6bc3c054 100644
--- a/sherpa-onnx/java-api/Makefile
+++ b/sherpa-onnx/java-api/Makefile
@@ -37,6 +37,7 @@ java_files += OfflineMoonshineModelConfig.java
 java_files += OfflineNemoEncDecCtcModelConfig.java
 java_files += OfflineZipformerCtcModelConfig.java
 java_files += OfflineWenetCtcModelConfig.java
+java_files += OfflineOmnilingualAsrCtcModelConfig.java
 java_files += OfflineCanaryModelConfig.java
 java_files += OfflineSenseVoiceModelConfig.java
 java_files += OfflineDolphinModelConfig.java
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
index 3e8c5bf5..19450ead 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
@@ -13,6 +13,7 @@ public class OfflineModelConfig {
     private final OfflineDolphinModelConfig dolphin;
     private final OfflineZipformerCtcModelConfig zipformerCtc;
     private final OfflineWenetCtcModelConfig wenetCtc;
+    private final OfflineOmnilingualAsrCtcModelConfig omnilingual;
     private final OfflineCanaryModelConfig canary;
     private final String teleSpeech;
     private final String tokens;
@@ -34,6 +35,7 @@ public class OfflineModelConfig {
         this.zipformerCtc = builder.zipformerCtc;
         this.canary = builder.canary;
         this.wenetCtc = builder.wenetCtc;
+        this.omnilingual = builder.omnilingual;
         this.senseVoice = builder.senseVoice;
         this.dolphin = builder.dolphin;
         this.teleSpeech = builder.teleSpeech;
@@ -86,6 +88,10 @@ public class OfflineModelConfig {
         return wenetCtc;
     }
 
+    public OfflineOmnilingualAsrCtcModelConfig getOmnilingual() {
+        return omnilingual;
+    }
+
     public OfflineCanaryModelConfig getCanary() {
         return canary;
     }
@@ -133,6 +139,7 @@ public class OfflineModelConfig {
         private OfflineDolphinModelConfig dolphin = OfflineDolphinModelConfig.builder().build();
         private OfflineZipformerCtcModelConfig zipformerCtc = OfflineZipformerCtcModelConfig.builder().build();
         private OfflineWenetCtcModelConfig wenetCtc = OfflineWenetCtcModelConfig.builder().build();
+        private OfflineOmnilingualAsrCtcModelConfig omnilingual = OfflineOmnilingualAsrCtcModelConfig.builder().build();
         private OfflineCanaryModelConfig canary = OfflineCanaryModelConfig.builder().build();
         private String teleSpeech = "";
         private String tokens = "";
@@ -177,6 +184,11 @@ public class OfflineModelConfig {
             return this;
         }
 
+        public Builder setOmnilingual(OfflineOmnilingualAsrCtcModelConfig omnilingual) {
+            this.omnilingual = omnilingual;
+            return this;
+        }
+
         public Builder setCanary(OfflineCanaryModelConfig canary) {
             this.canary = canary;
             return this;
@@ -242,4 +254,4 @@ public class OfflineModelConfig {
             return this;
         }
     }
-}
\ No newline at end of file
+}
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java
new file mode 100644
index 00000000..2603ce9a
--- /dev/null
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java
@@ -0,0 +1,30 @@
+package com.k2fsa.sherpa.onnx;
+
+public class OfflineOmnilingualAsrCtcModelConfig {
+    private final String model;
+
+    private OfflineOmnilingualAsrCtcModelConfig(Builder builder) {
+        this.model = builder.model;
+    }
+
+    public static Builder builder() {
+        return new Builder();
+    }
+
+    public String getModel() {
+        return model;
+    }
+
+    public static class Builder {
+        private String model = "";
+
+        public OfflineOmnilingualAsrCtcModelConfig build() {
+            return new OfflineOmnilingualAsrCtcModelConfig(this);
+        }
+
+        public Builder setModel(String model) {
+            this.model = model;
+            return this;
+        }
+    }
+}
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index baa58813..b091670e 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -193,6 +193,18 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.wenet_ctc.model, model,
                               wenet_ctc_config_cls, wenet_ctc_config);
 
+  // omnilingual asr ctc
+  fid = env->GetFieldID(
+      model_config_cls, "omnilingual",
+      "Lcom/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig;");
+  jobject omnilingual_ctc_config = env->GetObjectField(model_config, fid);
+  jclass omnilingual_ctc_config_cls =
+      env->GetObjectClass(omnilingual_ctc_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.omnilingual.model, model,
+                              omnilingual_ctc_config_cls,
+                              omnilingual_ctc_config);
+
   // canary
   fid = env->GetFieldID(model_config_cls, "canary",
                         "Lcom/k2fsa/sherpa/onnx/OfflineCanaryModelConfig;");
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index 57287d76..1e94f4f9 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -40,6 +40,10 @@ data class OfflineWenetCtcModelConfig(
     var model: String = "",
 )
 
+data class OfflineOmnilingualAsrCtcModelConfig(
+    var model: String = "",
+)
+
 data class OfflineWhisperModelConfig(
     var encoder: String = "",
     var decoder: String = "",
@@ -85,6 +89,7 @@ data class OfflineModelConfig(
     var dolphin: OfflineDolphinModelConfig = OfflineDolphinModelConfig(),
     var zipformerCtc: OfflineZipformerCtcModelConfig = OfflineZipformerCtcModelConfig(),
     var wenetCtc: OfflineWenetCtcModelConfig = OfflineWenetCtcModelConfig(),
+    var omnilingual: OfflineOmnilingualAsrCtcModelConfig = OfflineOmnilingualAsrCtcModelConfig(),
     var canary: OfflineCanaryModelConfig = OfflineCanaryModelConfig(),
     var teleSpeech: String = "",
     var numThreads: Int = 1,
@@ -731,6 +736,16 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
                 modelType = "paraformer",
             )
         }
+
+        44 -> {
+            val modelDir = "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12"
+            return OfflineModelConfig(
+                omnilingual = OfflineOmnilingualAsrCtcModelConfig(
+                    model = "$modelDir/model.int8.onnx",
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
     }
     return null
 }

commit c218342218e23fccc87fd333a04426dd79e1dd11
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 18:13:15 2025 +0800

    Add Pascal API for Omnilingual ASR CTC models (#2782)

diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
index df3cf75a..6f4c55e4 100644
--- a/.github/workflows/pascal.yaml
+++ b/.github/workflows/pascal.yaml
@@ -126,6 +126,77 @@ jobs:
             cp -v ../sherpa-onnx/pascal-api/*.pas ../pascal-api-examples/vad-with-non-streaming-asr
           fi
 
+      - name:  Run Pascal test (Non Streaming ASR)
+        shell: bash
+        run: |
+          export PATH=/c/lazarus/fpc/3.2.2/bin/x86_64-win64:$PATH
+
+          cd ./pascal-api-examples
+
+          pushd non-streaming-asr
+
+          ./run-wenet-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-omnilingual-asr-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-nemo-canary.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-zipformer-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-dolphin-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-zipformer-transducer.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-moonshine.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-fire-red-asr.sh
+          rm -rf sherpa-onnx-fire-red-asr*
+          echo "---"
+
+          ./run-whisper.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-nemo-transducer.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-nemo-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-sense-voice.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-telespeech-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ./run-paraformer.sh
+
+          ./run-paraformer-itn.sh
+
+          rm -rf sherpa-onnx-*
+          echo "---"
+
+          ls -lh
+          popd
+
       - name:  Run Pascal test (Streaming ASR)
         shell: bash
         run: |
@@ -195,69 +266,6 @@ jobs:
           ./run-gtcrn.sh
           ls -lh
 
-      - name:  Run Pascal test (Non Streaming ASR)
-        shell: bash
-        run: |
-          export PATH=/c/lazarus/fpc/3.2.2/bin/x86_64-win64:$PATH
-
-          cd ./pascal-api-examples
-
-          pushd non-streaming-asr
-
-          ./run-nemo-canary.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-zipformer-ctc.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-dolphin-ctc.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-zipformer-transducer.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-moonshine.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-fire-red-asr.sh
-          rm -rf sherpa-onnx-fire-red-asr*
-          echo "---"
-
-          ./run-whisper.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-nemo-transducer.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-nemo-ctc.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-sense-voice.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-telespeech-ctc.sh
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ./run-paraformer.sh
-
-          ./run-paraformer-itn.sh
-
-          rm -rf sherpa-onnx-*
-          echo "---"
-
-          ls -lh
-          popd
-
       - name:  Run Pascal test (Speaker diarization)
         shell: bash
         run: |
diff --git a/pascal-api-examples/non-streaming-asr/.gitignore b/pascal-api-examples/non-streaming-asr/.gitignore
index 635e4bff..d1917ed1 100644
--- a/pascal-api-examples/non-streaming-asr/.gitignore
+++ b/pascal-api-examples/non-streaming-asr/.gitignore
@@ -12,3 +12,4 @@ dolphin_ctc
 zipformer_ctc
 wenet_ctc
 nemo_canary
+omnilingual_asr_ctc
diff --git a/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas b/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas
new file mode 100644
index 00000000..00d7e35b
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas
@@ -0,0 +1,76 @@
+{ Copyright (c)  2025  Xiaomi Corporation }
+
+{
+This file shows how to use a non-streaming Omnilingual ASR CTC model
+to decode files.
+
+You can download the model files from
+https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+}
+
+program omnilingual_asr_ctc;
+
+{$mode objfpc}
+
+uses
+  sherpa_onnx,
+  DateUtils,
+  SysUtils;
+
+var
+  Wave: TSherpaOnnxWave;
+  WaveFilename: AnsiString;
+
+  Config: TSherpaOnnxOfflineRecognizerConfig;
+  Recognizer: TSherpaOnnxOfflineRecognizer;
+  Stream: TSherpaOnnxOfflineStream;
+  RecognitionResult: TSherpaOnnxOfflineRecognizerResult;
+
+  Start: TDateTime;
+  Stop: TDateTime;
+
+  Elapsed: Single;
+  Duration: Single;
+  RealTimeFactor: Single;
+begin
+  Initialize(Config);
+
+  Config.ModelConfig.Omnilingual.Model := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx';
+  Config.ModelConfig.Tokens := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt';
+  Config.ModelConfig.Provider := 'cpu';
+  Config.ModelConfig.NumThreads := 1;
+  Config.ModelConfig.Debug := False;
+
+  WaveFilename := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
+
+  Wave := SherpaOnnxReadWave(WaveFilename);
+
+  Recognizer := TSherpaOnnxOfflineRecognizer.Create(Config);
+  Stream := Recognizer.CreateStream();
+  Start := Now;
+
+  Stream.AcceptWaveform(Wave.Samples, Wave.SampleRate);
+  Recognizer.Decode(Stream);
+
+  RecognitionResult := Recognizer.GetResult(Stream);
+
+  Stop := Now;
+
+  Elapsed := MilliSecondsBetween(Stop, Start) / 1000;
+  Duration := Length(Wave.Samples) / Wave.SampleRate;
+  RealTimeFactor := Elapsed / Duration;
+
+  WriteLn(RecognitionResult.ToString);
+  WriteLn(Format('NumThreads %d', [Config.ModelConfig.NumThreads]));
+  WriteLn(Format('Elapsed %.3f s', [Elapsed]));
+  WriteLn(Format('Wave duration %.3f s', [Duration]));
+  WriteLn(Format('RTF = %.3f/%.3f = %.3f', [Elapsed, Duration, RealTimeFactor]));
+
+  {Free resources to avoid memory leak.
+
+  Note: You don't need to invoke them for this simple script.
+  However, you have to invoke them in your own large/complex project.
+  }
+  FreeAndNil(Stream);
+  FreeAndNil(Recognizer);
+end.
diff --git a/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh b/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
new file mode 100755
index 00000000..8a21a06f
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
@@ -0,0 +1,42 @@
+#!/usr/bin/env bash
+
+set -ex
+
+SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
+SHERPA_ONNX_DIR=$(cd $SCRIPT_DIR/../.. && pwd)
+
+echo "SHERPA_ONNX_DIR: $SHERPA_ONNX_DIR"
+
+if [[ ! -f ../../build/install/lib/libsherpa-onnx-c-api.dylib  && ! -f ../../build/install/lib/libsherpa-onnx-c-api.so && ! -f ../../build/install/lib/sherpa-onnx-c-api.dll ]]; then
+  mkdir -p ../../build
+  pushd ../../build
+  cmake \
+    -DCMAKE_INSTALL_PREFIX=./install \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    ..
+
+  cmake --build . --target install --config Release
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+fpc \
+  -dSHERPA_ONNX_USE_SHARED_LIBS \
+  -Fu$SHERPA_ONNX_DIR/sherpa-onnx/pascal-api \
+  -Fl$SHERPA_ONNX_DIR/build/install/lib \
+  ./omnilingual_asr_ctc.pas
+
+export LD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$LD_LIBRARY_PATH
+export DYLD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$DYLD_LIBRARY_PATH
+
+./omnilingual_asr_ctc
diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
index 00c6a068..2357724d 100644
--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
+++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
@@ -335,6 +335,11 @@ type
     function ToString: AnsiString;
   end;
 
+  TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig = record
+    Model: AnsiString;
+    function ToString: AnsiString;
+  end;
+
   TSherpaOnnxOfflineWhisperModelConfig = record
     Encoder: AnsiString;
     Decoder: AnsiString;
@@ -410,6 +415,7 @@ type
     ZipformerCtc: TSherpaOnnxOfflineZipformerCtcModelConfig;
     Canary: TSherpaOnnxOfflineCanaryModelConfig;
     WenetCtc: TSherpaOnnxOfflineWenetCtcModelConfig;
+    Omnilingual: TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
     class operator Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineModelConfig);
     function ToString: AnsiString;
   end;
@@ -820,6 +826,9 @@ type
   SherpaOnnxOfflineWenetCtcModelConfig = record
     Model: PAnsiChar;
   end;
+  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig = record
+    Model: PAnsiChar;
+  end;
   SherpaOnnxOfflineWhisperModelConfig = record
     Encoder: PAnsiChar;
     Decoder: PAnsiChar;
@@ -877,6 +886,7 @@ type
     ZipformerCtc: SherpaOnnxOfflineZipformerCtcModelConfig;
     Canary: SherpaOnnxOfflineCanaryModelConfig;
     WenetCtc: SherpaOnnxOfflineWenetCtcModelConfig;
+    Omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
   end;
 
   SherpaOnnxOfflineRecognizerConfig = record
@@ -1704,6 +1714,12 @@ begin
     [Self.Model]);
 end;
 
+function TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig.ToString: AnsiString;
+begin
+  Result := Format('TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(Model := %s)',
+    [Self.Model]);
+end;
+
 function TSherpaOnnxOfflineWhisperModelConfig.ToString: AnsiString;
 begin
   Result := Format('TSherpaOnnxOfflineWhisperModelConfig(' +
@@ -1794,7 +1810,8 @@ begin
     'Dolphin := %s, ' +
     'ZipformerCtc := %s, ' +
     'Canary := %s, ' +
-    'WenetCtc := %s' +
+    'WenetCtc := %s, ' +
+    'Omnilingual := %s' +
     ')',
     [Self.Transducer.ToString, Self.Paraformer.ToString,
      Self.NeMoCtc.ToString, Self.Whisper.ToString, Self.Tdnn.ToString,
@@ -1802,7 +1819,8 @@ begin
      Self.ModelType, Self.ModelingUnit, Self.BpeVocab,
      Self.TeleSpeechCtc, Self.SenseVoice.ToString, Self.Moonshine.ToString,
      Self.FireRedAsr.ToString, Self.Dolphin.ToString,
-     Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString
+     Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString,
+     Self.Omnilingual.ToString
      ]);
 end;
 
@@ -1882,6 +1900,7 @@ begin
   C.ModelConfig.Canary.UsePnc := Ord(Config.ModelConfig.Canary.UsePnc);
 
   C.ModelConfig.WenetCtc.Model := PAnsiChar(Config.ModelConfig.WenetCtc.Model);
+  C.ModelConfig.Omnilingual.Model := PAnsiChar(Config.ModelConfig.Omnilingual.Model);
 
   C.LMConfig.Model := PAnsiChar(Config.LMConfig.Model);
   C.LMConfig.Scale := Config.LMConfig.Scale;

commit 7e71363fbf2213835a013882981cde576c607a4f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 18:12:33 2025 +0800

    Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)

diff --git a/.github/scripts/test-nodejs-npm.sh b/.github/scripts/test-nodejs-npm.sh
index 87d7ffab..16f21b5f 100755
--- a/.github/scripts/test-nodejs-npm.sh
+++ b/.github/scripts/test-nodejs-npm.sh
@@ -9,6 +9,14 @@ git status
 ls -lh
 ls -lh node_modules
 
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+node ./test-offline-omnilingual-asr-ctc.js
+
+rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
 tar xvf sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
 rm sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
diff --git a/nodejs-examples/README.md b/nodejs-examples/README.md
index 46f79b0c..bc8c6832 100644
--- a/nodejs-examples/README.md
+++ b/nodejs-examples/README.md
@@ -203,10 +203,26 @@ rm sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03.tar.bz2
 node ./test-offline-zipformer-ctc.js
 ```
 
+## ./test-offline-omnilingual-asr-ctc.js
+
+[./test-offline-omnilingual-asr-ctc.js](./test-offline-omnilingual-asr-ctc.js) demonstrates
+how to decode a file with a Omnilingual ASR CTC model. In the code we use
+[sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2).
+
+You can use the following command to run it:
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+node ./test-offline-omnilingual-asr-ctc.js
+```
+
 ## ./test-offline-wenet-ctc.js
 
 [./test-offline-wenet-ctc.js](./test-offline-wenet-ctc.js) demonstrates
-how to decode a file with a Wenet CTC model. In the code we use
+how to decode a file with a WeNet CTC model. In the code we use
 [sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2).
 
 You can use the following command to run it:
diff --git a/nodejs-examples/test-offline-omnilingual-asr-ctc.js b/nodejs-examples/test-offline-omnilingual-asr-ctc.js
new file mode 100644
index 00000000..ffa3d604
--- /dev/null
+++ b/nodejs-examples/test-offline-omnilingual-asr-ctc.js
@@ -0,0 +1,37 @@
+// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+//
+const fs = require('fs');
+const {Readable} = require('stream');
+const wav = require('wav');
+
+const sherpa_onnx = require('sherpa-onnx');
+
+function createOfflineRecognizer() {
+  let config = {
+    modelConfig: {
+      omnilingual: {
+        model:
+            './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx',
+      },
+      tokens:
+          './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt',
+    }
+  };
+
+  return sherpa_onnx.createOfflineRecognizer(config);
+}
+
+const recognizer = createOfflineRecognizer();
+const stream = recognizer.createStream();
+
+const waveFilename =
+    './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform(wave.sampleRate, wave.samples);
+
+recognizer.decode(stream);
+const text = recognizer.getResult(stream).text;
+console.log(text);
+
+stream.free();
+recognizer.free();
diff --git a/wasm/asr/sherpa-onnx-asr.js b/wasm/asr/sherpa-onnx-asr.js
index 3ffb35ba..a77d61c1 100644
--- a/wasm/asr/sherpa-onnx-asr.js
+++ b/wasm/asr/sherpa-onnx-asr.js
@@ -55,6 +55,10 @@ function freeConfig(config, Module) {
     freeConfig(config.wenetCtc, Module)
   }
 
+  if ('omnilingual' in config) {
+    freeConfig(config.omnilingual, Module)
+  }
+
   if ('moonshine' in config) {
     freeConfig(config.moonshine, Module)
   }
@@ -755,6 +759,23 @@ function initSherpaOnnxOfflineWenetCtcModelConfig(config, Module) {
   }
 }
 
+function initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(config, Module) {
+  const n = Module.lengthBytesUTF8(config.model || '') + 1;
+
+  const buffer = Module._malloc(n);
+
+  const len = 1 * 4;  // 1 pointer
+  const ptr = Module._malloc(len);
+
+  Module.stringToUTF8(config.model || '', buffer, n);
+
+  Module.setValue(ptr, buffer, 'i8*');
+
+  return {
+    buffer: buffer, ptr: ptr, len: len,
+  }
+}
+
 function initSherpaOnnxOfflineWhisperModelConfig(config, Module) {
   const encoderLen = Module.lengthBytesUTF8(config.encoder || '') + 1;
   const decoderLen = Module.lengthBytesUTF8(config.decoder || '') + 1;
@@ -1025,6 +1046,12 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
     };
   }
 
+  if (!('omnilingual' in config)) {
+    config.omnilingual = {
+      model: '',
+    };
+  }
+
   if (!('whisper' in config)) {
     config.whisper = {
       encoder: '',
@@ -1109,9 +1136,13 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   const wenetCtc =
       initSherpaOnnxOfflineWenetCtcModelConfig(config.wenetCtc, Module);
 
+  const omnilingual = initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
+      config.omnilingual, Module);
+
   const len = transducer.len + paraformer.len + nemoCtc.len + whisper.len +
       tdnn.len + 8 * 4 + senseVoice.len + moonshine.len + fireRedAsr.len +
-      dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len;
+      dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len +
+      omnilingual.len;
 
   const ptr = Module._malloc(len);
 
@@ -1222,12 +1253,15 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   Module._CopyHeap(wenetCtc.ptr, wenetCtc.len, ptr + offset);
   offset += wenetCtc.len;
 
+  Module._CopyHeap(omnilingual.ptr, omnilingual.len, ptr + offset);
+  offset += omnilingual.len;
+
   return {
     buffer: buffer, ptr: ptr, len: len, transducer: transducer,
         paraformer: paraformer, nemoCtc: nemoCtc, whisper: whisper, tdnn: tdnn,
         senseVoice: senseVoice, moonshine: moonshine, fireRedAsr: fireRedAsr,
         dolphin: dolphin, zipformerCtc: zipformerCtc, canary: canary,
-        wenetCtc: wenetCtc,
+        wenetCtc: wenetCtc, omnilingual: omnilingual
   }
 }
 
diff --git a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
index ac8fbfe1..f5ded701 100644
--- a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
+++ b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
@@ -15,6 +15,7 @@ static_assert(sizeof(SherpaOnnxOfflineParaformerModelConfig) == 4, "");
 
 static_assert(sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWenetCtcModelConfig) == 4, "");
+static_assert(sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineDolphinModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineNemoEncDecCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWhisperModelConfig) == 5 * 4, "");
@@ -37,7 +38,8 @@ static_assert(sizeof(SherpaOnnxOfflineModelConfig) ==
                       sizeof(SherpaOnnxOfflineDolphinModelConfig) +
                       sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) +
                       sizeof(SherpaOnnxOfflineCanaryModelConfig) +
-                      sizeof(SherpaOnnxOfflineWenetCtcModelConfig),
+                      sizeof(SherpaOnnxOfflineWenetCtcModelConfig) +
+                      sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig),
 
               "");
 static_assert(sizeof(SherpaOnnxFeatureConfig) == 2 * 4, "");
@@ -86,6 +88,7 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   auto zipformer_ctc = &model_config->zipformer_ctc;
   auto canary = &model_config->canary;
   auto wenet_ctc = &model_config->wenet_ctc;
+  auto omnilingual = &model_config->omnilingual;
 
   fprintf(stdout, "----------offline transducer model config----------\n");
   fprintf(stdout, "encoder: %s\n", transducer->encoder);
@@ -139,6 +142,9 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   fprintf(stdout, "----------offline wenet ctc model config----------\n");
   fprintf(stdout, "model: %s\n", wenet_ctc->model);
 
+  fprintf(stdout, "----------offline Omnilingual ASR model config----------\n");
+  fprintf(stdout, "model: %s\n", omnilingual->model);
+
   fprintf(stdout, "tokens: %s\n", model_config->tokens);
   fprintf(stdout, "num_threads: %d\n", model_config->num_threads);
   fprintf(stdout, "provider: %s\n", model_config->provider);

commit 1ab1104a456e17cb51078cfe15e8d980c8c8c722
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 18:11:25 2025 +0800

    Add Dart API for Omnilingual ASR CTC models (#2779)

diff --git a/.github/scripts/test-dart.sh b/.github/scripts/test-dart.sh
index 863cfe20..a1e1e615 100755
--- a/.github/scripts/test-dart.sh
+++ b/.github/scripts/test-dart.sh
@@ -74,6 +74,10 @@ popd
 
 pushd non-streaming-asr
 
+echo '----------Omnilingual ASR CTC----------'
+./run-omnilingual-asr-ctc.sh
+rm -rf sherpa-onnx-*
+
 echo '----------Wenet CTC----------'
 ./run-wenet-ctc.sh
 rm -rf sherpa-onnx-*
diff --git a/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart b/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart
new file mode 100644
index 00000000..b1dda8c4
--- /dev/null
+++ b/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart
@@ -0,0 +1,52 @@
+// Copyright (c)  2025  Xiaomi Corporation
+import 'dart:io';
+
+import 'package:args/args.dart';
+import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
+
+import './init.dart';
+
+void main(List<String> arguments) async {
+  await initSherpaOnnx();
+
+  final parser = ArgParser()
+    ..addOption('model', help: 'Path to the Omnilingual ASR CTC model')
+    ..addOption('tokens', help: 'Path to tokens.txt')
+    ..addOption('input-wav', help: 'Path to input.wav to transcribe');
+
+  final res = parser.parse(arguments);
+  if (res['model'] == null ||
+      res['tokens'] == null ||
+      res['input-wav'] == null) {
+    print(parser.usage);
+    exit(1);
+  }
+
+  final model = res['model'] as String;
+  final tokens = res['tokens'] as String;
+  final inputWav = res['input-wav'] as String;
+
+  final omnilingual = sherpa_onnx.OfflineOmnilingualAsrCtcModelConfig(model: model);
+
+  final modelConfig = sherpa_onnx.OfflineModelConfig(
+    omnilingual: omnilingual,
+    tokens: tokens,
+    debug: true,
+    numThreads: 1,
+  );
+  final config = sherpa_onnx.OfflineRecognizerConfig(model: modelConfig);
+  final recognizer = sherpa_onnx.OfflineRecognizer(config);
+
+  final waveData = sherpa_onnx.readWave(inputWav);
+  final stream = recognizer.createStream();
+
+  stream.acceptWaveform(
+      samples: waveData.samples, sampleRate: waveData.sampleRate);
+  recognizer.decode(stream);
+
+  final result = recognizer.getResult(stream);
+  print(result.text);
+
+  stream.free();
+  recognizer.free();
+}
diff --git a/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh b/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
new file mode 100755
index 00000000..d0e46e50
--- /dev/null
+++ b/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
@@ -0,0 +1,17 @@
+#!/usr/bin/env bash
+
+set -ex
+
+dart pub get
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+dart run \
+  ./bin/omnilingual-asr-ctc.dart \
+  --model ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx \
+  --tokens ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt \
+  --input-wav ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav
diff --git a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
index 55126f9b..4fba68bf 100644
--- a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
+++ b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
@@ -146,6 +146,27 @@ class OfflineWenetCtcModelConfig {
   final String model;
 }
 
+class OfflineOmnilingualAsrCtcModelConfig {
+  const OfflineOmnilingualAsrCtcModelConfig({this.model = ''});
+
+  factory OfflineOmnilingualAsrCtcModelConfig.fromJson(Map<String, dynamic> json) {
+    return OfflineOmnilingualAsrCtcModelConfig(
+      model: json['model'] as String? ?? '',
+    );
+  }
+
+  @override
+  String toString() {
+    return 'OfflineOmnilingualAsrCtcModelConfig(model: $model)';
+  }
+
+  Map<String, dynamic> toJson() => {
+        'model': model,
+      };
+
+  final String model;
+}
+
 class OfflineWhisperModelConfig {
   const OfflineWhisperModelConfig(
       {this.encoder = '',
@@ -371,6 +392,7 @@ class OfflineModelConfig {
     this.zipformerCtc = const OfflineZipformerCtcModelConfig(),
     this.canary = const OfflineCanaryModelConfig(),
     this.wenetCtc = const OfflineWenetCtcModelConfig(),
+    this.omnilingual = const OfflineOmnilingualAsrCtcModelConfig(),
     required this.tokens,
     this.numThreads = 1,
     this.debug = true,
@@ -431,6 +453,10 @@ class OfflineModelConfig {
           ? OfflineWenetCtcModelConfig.fromJson(
               json['wenetCtc'] as Map<String, dynamic>)
           : const OfflineWenetCtcModelConfig(),
+      omnilingual: json['omnilingual'] != null
+          ? OfflineOmnilingualAsrCtcModelConfig.fromJson(
+              json['omnilingual'] as Map<String, dynamic>)
+          : const OfflineOmnilingualAsrCtcModelConfig(),
       tokens: json['tokens'] as String,
       numThreads: json['numThreads'] as int? ?? 1,
       debug: json['debug'] as bool? ?? true,
@@ -444,7 +470,7 @@ class OfflineModelConfig {
 
   @override
   String toString() {
-    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
+    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, omnilingual: $omnilingual, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
   }
 
   Map<String, dynamic> toJson() => {
@@ -460,6 +486,7 @@ class OfflineModelConfig {
         'zipformerCtc': zipformerCtc.toJson(),
         'canary': canary.toJson(),
         'wenetCtc': wenetCtc.toJson(),
+        'omnilingual': omnilingual.toJson(),
         'tokens': tokens,
         'numThreads': numThreads,
         'debug': debug,
@@ -482,6 +509,7 @@ class OfflineModelConfig {
   final OfflineZipformerCtcModelConfig zipformerCtc;
   final OfflineCanaryModelConfig canary;
   final OfflineWenetCtcModelConfig wenetCtc;
+  final OfflineOmnilingualAsrCtcModelConfig omnilingual;
 
   final String tokens;
   final int numThreads;
@@ -719,6 +747,7 @@ class OfflineRecognizer {
     c.ref.model.canary.usePnc = config.model.canary.usePnc ? 1 : 0;
 
     c.ref.model.wenetCtc.model = config.model.wenetCtc.model.toNativeUtf8();
+    c.ref.model.omnilingual.model = config.model.omnilingual.model.toNativeUtf8();
 
     c.ref.model.tokens = config.model.tokens.toNativeUtf8();
 
@@ -764,6 +793,7 @@ class OfflineRecognizer {
     calloc.free(c.ref.model.modelType);
     calloc.free(c.ref.model.provider);
     calloc.free(c.ref.model.tokens);
+    calloc.free(c.ref.model.omnilingual.model);
     calloc.free(c.ref.model.wenetCtc.model);
     calloc.free(c.ref.model.canary.tgtLang);
     calloc.free(c.ref.model.canary.srcLang);
diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
index b9147c58..8ce77797 100644
--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
@@ -307,6 +307,10 @@ final class SherpaOnnxOfflineWenetCtcModelConfig extends Struct {
   external Pointer<Utf8> model;
 }
 
+final class SherpaOnnxOfflineOmnilingualAsrCtcModelConfig extends Struct {
+  external Pointer<Utf8> model;
+}
+
 final class SherpaOnnxOfflineWhisperModelConfig extends Struct {
   external Pointer<Utf8> encoder;
   external Pointer<Utf8> decoder;
@@ -387,6 +391,7 @@ final class SherpaOnnxOfflineModelConfig extends Struct {
   external SherpaOnnxOfflineZipformerCtcModelConfig zipformerCtc;
   external SherpaOnnxOfflineCanaryModelConfig canary;
   external SherpaOnnxOfflineWenetCtcModelConfig wenetCtc;
+  external SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
 }
 
 final class SherpaOnnxOfflineRecognizerConfig extends Struct {

commit 4aee828b2974b000e51bc7aa3c0302673fe93d68
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 18:08:39 2025 +0800

    Add JavaScript (node-addon) API for Omnilingual ASR CTC models (#2780)

diff --git a/.github/scripts/test-nodejs-addon-npm.sh b/.github/scripts/test-nodejs-addon-npm.sh
index 113d8137..397935c6 100755
--- a/.github/scripts/test-nodejs-addon-npm.sh
+++ b/.github/scripts/test-nodejs-addon-npm.sh
@@ -10,7 +10,17 @@ arch=$(node -p "require('os').arch()")
 platform=$(node -p "require('os').platform()")
 node_version=$(node -p "process.versions.node.split('.')[0]")
 
-echo "----------non-streaming ASR Wenet CTC----------"
+echo "----------non-streaming ASR Omnilingual ASR CTC----------"
+
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+node ./test_asr_non_streaming_omnilingual_asr_ctc.js
+
+rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+
+echo "----------non-streaming ASR WeNet CTC----------"
 
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
 tar xvf sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
index 8ef59c4d..0e40f99e 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
@@ -78,6 +78,22 @@ static SherpaOnnxOfflineWenetCtcModelConfig GetOfflineWenetCtcModelConfig(
   return c;
 }
 
+static SherpaOnnxOfflineOmnilingualAsrCtcModelConfig
+GetOfflineOmnilingualAsrCtcModelConfig(Napi::Object obj) {
+  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig c;
+  memset(&c, 0, sizeof(c));
+
+  if (!obj.Has("omnilingual") || !obj.Get("omnilingual").IsObject()) {
+    return c;
+  }
+
+  Napi::Object o = obj.Get("omnilingual").As<Napi::Object>();
+
+  SHERPA_ONNX_ASSIGN_ATTR_STR(model, model);
+
+  return c;
+}
+
 static SherpaOnnxOfflineDolphinModelConfig GetOfflineDolphinModelConfig(
     Napi::Object obj) {
   SherpaOnnxOfflineDolphinModelConfig c;
@@ -243,6 +259,7 @@ static SherpaOnnxOfflineModelConfig GetOfflineModelConfig(Napi::Object obj) {
   c.zipformer_ctc = GetOfflineZipformerCtcModelConfig(o);
   c.canary = GetOfflineCanaryModelConfig(o);
   c.wenet_ctc = GetOfflineWenetCtcModelConfig(o);
+  c.omnilingual = GetOfflineOmnilingualAsrCtcModelConfig(o);
 
   SHERPA_ONNX_ASSIGN_ATTR_STR(tokens, tokens);
   SHERPA_ONNX_ASSIGN_ATTR_INT32(num_threads, numThreads);
@@ -336,6 +353,7 @@ static void FreeConfig(const SherpaOnnxOfflineRecognizerConfig &c) {
   SHERPA_ONNX_DELETE_C_STR(c.model_config.canary.tgt_lang);
 
   SHERPA_ONNX_DELETE_C_STR(c.model_config.wenet_ctc.model);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.omnilingual.model);
 
   SHERPA_ONNX_DELETE_C_STR(c.model_config.tokens);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.provider);
diff --git a/nodejs-addon-examples/README.md b/nodejs-addon-examples/README.md
index 20bc5b6e..de4d6ce7 100644
--- a/nodejs-addon-examples/README.md
+++ b/nodejs-addon-examples/README.md
@@ -125,6 +125,7 @@ The following tables list the examples in this folder.
 |[./test_vad_with_non_streaming_asr_moonshine.js](./test_vad_with_non_streaming_asr_moonshine.js)| Non-streaming speech recognition from a file using [Moonshine](https://github.com/usefulsensors/moonshine) + [Silero VAD](https://github.com/snakers4/silero-vad)|
 |[./test_asr_non_streaming_nemo_ctc.js](./test_asr_non_streaming_nemo_ctc.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) CTC model with greedy search|
 |[./test_asr_non_streaming_wenet_ctc.js](./test_asr_non_streaming_wenet_ctc.js)|Non-streaming speech recognition from a file using a [u2pp_conformer_yue](https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/u2pp_conformer_yue) CTC model with greedy search|
+|[./test_asr_non_streaming_omnilingual_asr_ctc.js](./test_asr_non_streaming_omnilingual_asr_ctc.js)|Non-streaming speech recognition from a file using a [Omnilingual-ASR](https://github.com/facebookresearch/omnilingual-asr) CTC model with greedy search|
 |[./test_asr_non_streaming_nemo_canary.js](./test_asr_non_streaming_nemo_canary.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [Canary](https://k2-fsa.github.io/sherpa/onnx/nemo/canary.html#sherpa-onnx-nemo-canary-180m-flash-en-es-de-fr-int8-english-spanish-german-french) model|
 |[./test_asr_non_streaming_zipformer_ctc.js](./test_asr_non_streaming_zipformer_ctc.js)|Non-streaming speech recognition from a file using a Zipformer CTC model with greedy search|
 |[./test_asr_non_streaming_nemo_parakeet_tdt_v2.js](./test_asr_non_streaming_nemo_parakeet_tdt_v2.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [parakeet-tdt-0.6b-v2](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-parakeet-tdt-0-6b-v2-int8-english) model with greedy search|
@@ -427,7 +428,17 @@ npm install naudiodon2
 node ./test_vad_asr_non_streaming_nemo_ctc_microphone.js
 ```
 
-### Non-streaming speech recognition with Wenet CTC models
+### Non-streaming speech recognition with Omnilingual ASR CTC models
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+node ./test_asr_non_streaming_omnilingual_asr_ctc.js
+```
+
+### Non-streaming speech recognition with WeNet CTC models
 
 ```bash
 wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
diff --git a/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
index 42f25511..88f89e06 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js b/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
index 1b64e2d1..c9f6e928 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
@@ -33,7 +33,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_moonshine.js b/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
index a0792a24..ebdcf949 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
@@ -34,7 +34,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
index 05668fba..5a106e10 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
@@ -37,7 +37,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-let result = recognizer.getResult(stream)
+let result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
index 5130a39f..aced80e3 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
index ac3517c3..20b7784e 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
@@ -35,7 +35,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js
new file mode 100644
index 00000000..e55d19c9
--- /dev/null
+++ b/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js
@@ -0,0 +1,48 @@
+// Copyright (c)  2025  Xiaomi Corporation
+const sherpa_onnx = require('sherpa-onnx-node');
+
+// Please download test files from
+// https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+const config = {
+  'featConfig': {
+    'sampleRate': 16000,
+    'featureDim': 80,
+  },
+  'modelConfig': {
+    'omnilingual': {
+      'model':
+          './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx',
+    },
+    'tokens':
+        './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt',
+    'numThreads': 2,
+    'provider': 'cpu',
+    'debug': 1,
+  }
+};
+
+const waveFilename =
+    './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
+
+const recognizer = new sherpa_onnx.OfflineRecognizer(config);
+console.log('Started')
+let start = Date.now();
+const stream = recognizer.createStream();
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+
+recognizer.decode(stream);
+const result = recognizer.getResult(stream);
+let stop = Date.now();
+console.log('Done')
+
+const elapsed_seconds = (stop - start) / 1000;
+const duration = wave.samples.length / wave.sampleRate;
+const real_time_factor = elapsed_seconds / duration;
+console.log('Wave duration', duration.toFixed(3), 'seconds')
+console.log('Elapsed', elapsed_seconds.toFixed(3), 'seconds')
+console.log(
+    `RTF = ${elapsed_seconds.toFixed(3)}/${duration.toFixed(3)} =`,
+    real_time_factor.toFixed(3))
+console.log(waveFilename)
+console.log('result\n', result)
diff --git a/nodejs-addon-examples/test_asr_non_streaming_paraformer.js b/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
index 157ccdea..4f1ecd2e 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
@@ -30,7 +30,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js b/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
index b4a693f5..9e0859c7 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js b/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
index b573cf29..dd79d011 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
@@ -47,7 +47,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js b/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
index ab2484f7..280c82f2 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
@@ -53,7 +53,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_transducer.js b/nodejs-addon-examples/test_asr_non_streaming_transducer.js
index 3d6dd2ac..e3f22549 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_transducer.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_transducer.js
@@ -34,7 +34,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
index d59328ff..e1aa7f6e 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_whisper.js b/nodejs-addon-examples/test_asr_non_streaming_whisper.js
index da8a32bf..a811a2c0 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_whisper.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_whisper.js
@@ -33,7 +33,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
index 3e5b25e9..0dfc61cf 100644
--- a/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
+++ b/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
@@ -30,7 +30,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
 
 recognizer.decode(stream);
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_ctc.js b/nodejs-addon-examples/test_asr_streaming_ctc.js
index e5936a31..2f69ea3d 100644
--- a/nodejs-addon-examples/test_asr_streaming_ctc.js
+++ b/nodejs-addon-examples/test_asr_streaming_ctc.js
@@ -37,7 +37,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js b/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
index 3537663c..93940d0e 100644
--- a/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
+++ b/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
@@ -40,7 +40,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_paraformer.js b/nodejs-addon-examples/test_asr_streaming_paraformer.js
index a03453dd..a87d94b1 100644
--- a/nodejs-addon-examples/test_asr_streaming_paraformer.js
+++ b/nodejs-addon-examples/test_asr_streaming_paraformer.js
@@ -38,7 +38,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js b/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
index 2e7fcf71..c8bd1660 100644
--- a/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
+++ b/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
@@ -34,7 +34,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_transducer.js b/nodejs-addon-examples/test_asr_streaming_transducer.js
index 3bb3de7c..61898212 100644
--- a/nodejs-addon-examples/test_asr_streaming_transducer.js
+++ b/nodejs-addon-examples/test_asr_streaming_transducer.js
@@ -41,7 +41,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_transducer_itn.js b/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
index 713db644..fce49460 100644
--- a/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
+++ b/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
@@ -43,7 +43,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js b/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
index 471d3084..56a06d52 100644
--- a/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
+++ b/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
@@ -44,7 +44,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
 while (recognizer.isReady(stream)) {
   recognizer.decode(stream);
 }
-result = recognizer.getResult(stream)
+const result = recognizer.getResult(stream);
 let stop = Date.now();
 console.log('Done')
 
diff --git a/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js b/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
index adf4f4ea..3b3efbb6 100644
--- a/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
+++ b/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
@@ -62,7 +62,7 @@ ai.on('data', data => {
     kws.decode(stream);
   }
 
-  const keyword = kws.getResult(stream).keyword
+  const keyword = kws.getResult(stream).keyword;
   if (keyword != '') {
     display.print(segmentIndex, keyword);
     segmentIndex += 1;
diff --git a/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js b/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
index 91914f72..e772e6ba 100644
--- a/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
+++ b/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
@@ -10,7 +10,6 @@ function createOfflineTts() {
       matcha: {
         acousticModel: './matcha-icefall-en_US-ljspeech/model-steps-3.onnx',
         vocoder: './vocos-22khz-univ.onnx',
-        lexicon: './matcha-icefall-en_US-ljspeech/lexicon.txt',
         tokens: './matcha-icefall-en_US-ljspeech/tokens.txt',
         dataDir: './matcha-icefall-en_US-ljspeech/espeak-ng-data',
       },

commit 634f4fdb1da37a45c20fe81a163697b801d7cf6a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 16:23:38 2025 +0800

    Add Go API for Omnilingual ASR CTC models (#2778)

diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
index cb18b4b3..e362418e 100644
--- a/.github/workflows/test-go.yaml
+++ b/.github/workflows/test-go.yaml
@@ -101,6 +101,7 @@ jobs:
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/keyword-spotting-from-file/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-canary-decode-files/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-decode-files/
+            cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-speaker-diarization/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-tts/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/speaker-identification/
@@ -140,6 +141,19 @@ jobs:
           name: ${{ matrix.os }}-libs
           path: to-upload/
 
+      - name: Test non-streaming decoding files with Omnilingual ASR
+        shell: bash
+        run: |
+          cd scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
+          ls -lh
+          go mod tidy
+          cat go.mod
+          go build
+          ls -lh
+
+          ./run.sh
+          rm -rf sherpa-onnx-omnilingual-*
+
       - name: Test non-streaming TTS
         shell: bash
         run: |
diff --git a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
index fe764d1b..154f956a 100755
--- a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
+++ b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
@@ -2,7 +2,7 @@
 
 set -ex
 
-if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12 ]; then
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
   curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
   tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
   rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
new file mode 100644
index 00000000..7f0a0a3e
--- /dev/null
+++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
@@ -0,0 +1,3 @@
+module non-streaming-omnilingual-asr-ctc-decode-files
+
+go 1.17
diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
new file mode 100644
index 00000000..e910ee80
--- /dev/null
+++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
@@ -0,0 +1,97 @@
+package main
+
+import (
+	"bytes"
+	"encoding/binary"
+	"log"
+	"os"
+	"strings"
+
+	sherpa "github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx"
+	"github.com/youpy/go-wav"
+)
+
+func main() {
+	log.SetFlags(log.LstdFlags | log.Lmicroseconds)
+
+	config := sherpa.OfflineRecognizerConfig{}
+
+	config.ModelConfig.Omnilingual.Model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
+	config.ModelConfig.Tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
+
+	waveFilename := "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
+
+	samples, sampleRate := readWave(waveFilename)
+
+	log.Println("Initializing recognizer (may take several seconds)")
+	recognizer := sherpa.NewOfflineRecognizer(&config)
+	log.Println("Recognizer created!")
+	defer sherpa.DeleteOfflineRecognizer(recognizer)
+
+	log.Println("Start decoding!")
+	stream := sherpa.NewOfflineStream(recognizer)
+	defer sherpa.DeleteOfflineStream(stream)
+
+	stream.AcceptWaveform(sampleRate, samples)
+
+	recognizer.Decode(stream)
+	log.Println("Decoding done!")
+	result := stream.GetResult()
+
+	log.Println("Text: " + strings.ToLower(result.Text))
+}
+
+func readWave(filename string) (samples []float32, sampleRate int) {
+	file, _ := os.Open(filename)
+	defer file.Close()
+
+	reader := wav.NewReader(file)
+	format, err := reader.Format()
+	if err != nil {
+		log.Fatalf("Failed to read wave format")
+	}
+
+	if format.AudioFormat != 1 {
+		log.Fatalf("Support only PCM format. Given: %v\n", format.AudioFormat)
+	}
+
+	if format.NumChannels != 1 {
+		log.Fatalf("Support only 1 channel wave file. Given: %v\n", format.NumChannels)
+	}
+
+	if format.BitsPerSample != 16 {
+		log.Fatalf("Support only 16-bit per sample. Given: %v\n", format.BitsPerSample)
+	}
+
+	reader.Duration() // so that it initializes reader.Size
+
+	buf := make([]byte, reader.Size)
+	n, err := reader.Read(buf)
+	if n != int(reader.Size) {
+		log.Fatalf("Failed to read %v bytes. Returned %v bytes\n", reader.Size, n)
+	}
+
+	samples = samplesInt16ToFloat(buf)
+	sampleRate = int(format.SampleRate)
+
+	return
+}
+
+func samplesInt16ToFloat(inSamples []byte) []float32 {
+	numSamples := len(inSamples) / 2
+	outSamples := make([]float32, numSamples)
+
+	for i := 0; i != numSamples; i++ {
+		s := inSamples[i*2 : (i+1)*2]
+
+		var s16 int16
+		buf := bytes.NewReader(s)
+		err := binary.Read(buf, binary.LittleEndian, &s16)
+		if err != nil {
+			log.Fatal("Failed to parse 16-bit sample")
+		}
+		outSamples[i] = float32(s16) / 32768
+	}
+
+	return outSamples
+}
diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
new file mode 100755
index 00000000..f996d7cc
--- /dev/null
+++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
@@ -0,0 +1,13 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+go mod tidy
+go build
+./non-streaming-omnilingual-asr-ctc-decode-files
diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
new file mode 100644
index 00000000..0fac61a7
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
@@ -0,0 +1,5 @@
+module non-streaming-omnilingual-asr-ctc-decode-files
+
+go 1.17
+
+replace github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx => ../
diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go
new file mode 120000
index 00000000..68aebe63
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
\ No newline at end of file
diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
new file mode 120000
index 00000000..82b1c19b
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
\ No newline at end of file
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 5fe44e22..721db857 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -419,6 +419,10 @@ type OfflineWenetCtcModelConfig struct {
 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
 }
 
+type OfflineOmnilingualAsrCtcModelConfig struct {
+	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
+}
+
 type OfflineDolphinModelConfig struct {
 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
 }
@@ -480,6 +484,7 @@ type OfflineModelConfig struct {
 	ZipformerCtc OfflineZipformerCtcModelConfig
 	Canary       OfflineCanaryModelConfig
 	WenetCtc     OfflineWenetCtcModelConfig
+	Omnilingual     OfflineOmnilingualAsrCtcModelConfig
 	Tokens       string // Path to tokens.txt
 
 	// Number of threads to use for neural network computation
@@ -583,6 +588,8 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 
 	c.model_config.wenet_ctc.model = C.CString(config.ModelConfig.WenetCtc.Model)
 
+	c.model_config.omnilingual.model = C.CString(config.ModelConfig.Omnilingual.Model)
+
 	c.model_config.tokens = C.CString(config.ModelConfig.Tokens)
 
 	c.model_config.num_threads = C.int(config.ModelConfig.NumThreads)
@@ -735,6 +742,11 @@ func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig)
 		c.model_config.wenet_ctc.model = nil
 	}
 
+	if c.model_config.omnilingual.model != nil {
+		C.free(unsafe.Pointer(c.model_config.omnilingual.model))
+		c.model_config.omnilingual.model = nil
+	}
+
 	if c.model_config.tokens != nil {
 		C.free(unsafe.Pointer(c.model_config.tokens))
 		c.model_config.tokens = nil

commit e6062c9cca2cdc2afeebac9de4e90f4c1d702603
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 15:41:32 2025 +0800

    Add Swift API for Omnilingual ASR CTC models (#2776)

diff --git a/.github/scripts/test-swift.sh b/.github/scripts/test-swift.sh
index 59af61c7..9944d861 100755
--- a/.github/scripts/test-swift.sh
+++ b/.github/scripts/test-swift.sh
@@ -9,6 +9,9 @@ ls -lh
 
 ./run-test-version.sh
 
+./run-omnilingual-asr-ctc-asr.sh
+rm -rf sherpa-onnx-omnilingual-*
+
 ./run-decode-file-t-one-streaming.sh
 rm -rf sherpa-onnx-streaming-*
 
diff --git a/swift-api-examples/.gitignore b/swift-api-examples/.gitignore
index 2eeba0bb..52857338 100644
--- a/swift-api-examples/.gitignore
+++ b/swift-api-examples/.gitignore
@@ -24,3 +24,4 @@ dolphin-ctc-asr
 tts-kitten-en
 compute-speaker-embeddings
 decode-file-t-one-streaming
+omnilingual-asr-ctc
diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index 6c8454c8..b18b8e1a 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -368,6 +368,14 @@ func sherpaOnnxOfflineWenetCtcModelConfig(
   )
 }
 
+func sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
+  model: String = ""
+) -> SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
+  return SherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
+    model: toCPointer(model)
+  )
+}
+
 func sherpaOnnxOfflineNemoEncDecCtcModelConfig(
   model: String = ""
 ) -> SherpaOnnxOfflineNemoEncDecCtcModelConfig {
@@ -492,7 +500,9 @@ func sherpaOnnxOfflineModelConfig(
     sherpaOnnxOfflineZipformerCtcModelConfig(),
   canary: SherpaOnnxOfflineCanaryModelConfig = sherpaOnnxOfflineCanaryModelConfig(),
   wenetCtc: SherpaOnnxOfflineWenetCtcModelConfig =
-    sherpaOnnxOfflineWenetCtcModelConfig()
+    sherpaOnnxOfflineWenetCtcModelConfig(),
+  omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig =
+    sherpaOnnxOfflineOmnilingualAsrCtcModelConfig()
 ) -> SherpaOnnxOfflineModelConfig {
   return SherpaOnnxOfflineModelConfig(
     transducer: transducer,
@@ -514,7 +524,8 @@ func sherpaOnnxOfflineModelConfig(
     dolphin: dolphin,
     zipformer_ctc: zipformerCtc,
     canary: canary,
-    wenet_ctc: wenetCtc
+    wenet_ctc: wenetCtc,
+    omnilingual: omnilingual
   )
 }
 
diff --git a/swift-api-examples/omnilingual-asr-ctc.swift b/swift-api-examples/omnilingual-asr-ctc.swift
new file mode 100644
index 00000000..0384e6f6
--- /dev/null
+++ b/swift-api-examples/omnilingual-asr-ctc.swift
@@ -0,0 +1,41 @@
+func run() {
+  let model =
+    "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
+  let tokens =
+    "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
+
+  let omnilingual = sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
+    model: model
+  )
+
+  let modelConfig = sherpaOnnxOfflineModelConfig(
+    tokens: tokens,
+    debug: 0,
+    omnilingual: omnilingual
+  )
+
+  let featConfig = sherpaOnnxFeatureConfig()
+  var config = sherpaOnnxOfflineRecognizerConfig(
+    featConfig: featConfig,
+    modelConfig: modelConfig
+  )
+
+  let recognizer = SherpaOnnxOfflineRecognizer(config: &config)
+
+  let filePath = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
+  let audio = SherpaOnnxWaveWrapper.readWave(filename: filePath)
+
+  let result = recognizer.decode(samples: audio.samples, sampleRate: audio.sampleRate)
+
+  print("\nresult is:\n\(result.text)")
+  if result.timestamps.count != 0 {
+    print("\ntimestamps is:\n\(result.timestamps)")
+  }
+}
+
+@main
+struct App {
+  static func main() {
+    run()
+  }
+}
diff --git a/swift-api-examples/run-omnilingual-asr-ctc-asr.sh b/swift-api-examples/run-omnilingual-asr-ctc-asr.sh
new file mode 100755
index 00000000..65870027
--- /dev/null
+++ b/swift-api-examples/run-omnilingual-asr-ctc-asr.sh
@@ -0,0 +1,34 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -d ../build-swift-macos ]; then
+  echo "Please run ../build-swift-macos.sh first!"
+  exit 1
+fi
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+if [ ! -e ./omnilingual-asr-ctc ]; then
+  # Note: We use -lc++ to link against libc++ instead of libstdc++
+  swiftc \
+    -lc++ \
+    -I ../build-swift-macos/install/include \
+    -import-objc-header ./SherpaOnnx-Bridging-Header.h \
+    ./omnilingual-asr-ctc.swift  ./SherpaOnnx.swift \
+    -L ../build-swift-macos/install/lib/ \
+    -l sherpa-onnx \
+    -l onnxruntime \
+    -o omnilingual-asr-ctc
+
+  strip omnilingual-asr-ctc
+else
+  echo "./omnilingual-asr-ctc exists - skip building"
+fi
+
+export DYLD_LIBRARY_PATH=$PWD/../build-swift-macos/install/lib:$DYLD_LIBRARY_PATH
+./omnilingual-asr-ctc

commit 1832b35070cd8f5df0cf1bf01cc658b0b6adf8c1
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 15:12:20 2025 +0800

    Add C# API for Omnilingual ASR CTC models (#2775)

diff --git a/.github/scripts/test-dot-net.sh b/.github/scripts/test-dot-net.sh
index 2679d9bb..f65c0f82 100755
--- a/.github/scripts/test-dot-net.sh
+++ b/.github/scripts/test-dot-net.sh
@@ -32,6 +32,9 @@ rm -rf sherpa-onnx-nemo-*
 
 cd ../offline-decode-files
 
+./run-omnilingual-asr-ctc.sh
+rm -rf sherpa-onnx-*
+
 ./run-wenet-ctc.sh
 rm -rf sherpa-onnx-*
 
diff --git a/.gitignore b/.gitignore
index f0a4c52e..37337ac3 100644
--- a/.gitignore
+++ b/.gitignore
@@ -156,3 +156,4 @@ am.mvn
 *bpe.model
 config.yaml
 configuration.json
+sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
diff --git a/dotnet-examples/offline-decode-files/Program.cs b/dotnet-examples/offline-decode-files/Program.cs
index a133ef0c..4b3d1893 100644
--- a/dotnet-examples/offline-decode-files/Program.cs
+++ b/dotnet-examples/offline-decode-files/Program.cs
@@ -87,6 +87,9 @@ class OfflineDecodeFiles
     [Option("wenet-ctc", Required = false, HelpText = "Path to model.onnx. Used only for Wenet CTC models")]
     public string WenetCtc { get; set; } = string.Empty;
 
+    [Option("omnilingual-asr-ctc", Required = false, HelpText = "Path to model.onnx. Used only for Omnilingual ASR CTC models")]
+    public string Omnilingual { get; set; } = string.Empty;
+
     [Option("sense-voice-model", Required = false, HelpText = "Path to model.onnx. Used only for SenseVoice CTC models")]
     public string SenseVoiceModel { get; set; } = string.Empty;
 
@@ -258,6 +261,10 @@ to download pre-trained Tdnn models.
     {
       config.ModelConfig.WenetCtc.Model = options.WenetCtc;
     }
+    else if (!string.IsNullOrEmpty(options.Omnilingual))
+    {
+      config.ModelConfig.Omnilingual.Model = options.Omnilingual;
+    }
     else if (!string.IsNullOrEmpty(options.WhisperEncoder))
     {
       config.ModelConfig.Whisper.Encoder = options.WhisperEncoder;
diff --git a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
new file mode 100755
index 00000000..fe764d1b
--- /dev/null
+++ b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
@@ -0,0 +1,14 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12 ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+fi
+
+dotnet run \
+  --omnilingual-asr-ctc=./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx \
+  --tokens=./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt \
+  --files ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav
diff --git a/scripts/dotnet/OfflineModelConfig.cs b/scripts/dotnet/OfflineModelConfig.cs
index 6d6a9e90..47205cfe 100644
--- a/scripts/dotnet/OfflineModelConfig.cs
+++ b/scripts/dotnet/OfflineModelConfig.cs
@@ -30,6 +30,7 @@ namespace SherpaOnnx
             ZipformerCtc = new OfflineZipformerCtcModelConfig();
             Canary = new OfflineCanaryModelConfig();
             WenetCtc = new OfflineWenetCtcModelConfig();
+            Omnilingual = new OfflineOmnilingualAsrCtcModelConfig();
         }
         public OfflineTransducerModelConfig Transducer;
         public OfflineParaformerModelConfig Paraformer;
@@ -66,5 +67,6 @@ namespace SherpaOnnx
         public OfflineZipformerCtcModelConfig ZipformerCtc;
         public OfflineCanaryModelConfig Canary;
         public OfflineWenetCtcModelConfig WenetCtc;
+        public OfflineOmnilingualAsrCtcModelConfig Omnilingual;
     }
 }
diff --git a/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs b/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs
new file mode 100644
index 00000000..d1dad64e
--- /dev/null
+++ b/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs
@@ -0,0 +1,18 @@
+/// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+
+using System.Runtime.InteropServices;
+
+namespace SherpaOnnx
+{
+
+    [StructLayout(LayoutKind.Sequential)]
+    public struct OfflineOmnilingualAsrCtcModelConfig
+    {
+        public OfflineOmnilingualAsrCtcModelConfig()
+        {
+            Model = "";
+        }
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string Model;
+    }
+}

commit a0d3e5ea4ce770537166454eef4f71c2c04c4db6
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 14:10:15 2025 +0800

    Add CXX API for Omnilingual ASR CTC models (#2774)

diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
index c35a92cf..81866989 100644
--- a/.github/workflows/cxx-api.yaml
+++ b/.github/workflows/cxx-api.yaml
@@ -78,6 +78,39 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
           fi
 
+      - name: Test Omnilingual ASR CTC
+        shell: bash
+        run: |
+          name=omnilingual-asr-ctc-cxx-api
+          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-cxx-api \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ls -lh ./$name
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+          tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+          rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+          echo "---"
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm -rf sherpa-onnx-omnilingual-*
+          rm -v ./$name
 
       - name: Test Online punctuation
         shell: bash
diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
index 6c47e0a1..2dafd987 100644
--- a/cxx-api-examples/CMakeLists.txt
+++ b/cxx-api-examples/CMakeLists.txt
@@ -39,6 +39,9 @@ target_link_libraries(sense-voice-cxx-api sherpa-onnx-cxx-api)
 add_executable(wenet-ctc-cxx-api ./wenet-ctc-cxx-api.cc)
 target_link_libraries(wenet-ctc-cxx-api sherpa-onnx-cxx-api)
 
+add_executable(omnilingual-asr-ctc-cxx-api ./omnilingual-asr-ctc-cxx-api.cc)
+target_link_libraries(omnilingual-asr-ctc-cxx-api sherpa-onnx-cxx-api)
+
 add_executable(nemo-canary-cxx-api ./nemo-canary-cxx-api.cc)
 target_link_libraries(nemo-canary-cxx-api sherpa-onnx-cxx-api)
 
diff --git a/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc b/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
new file mode 100644
index 00000000..819d1620
--- /dev/null
+++ b/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
@@ -0,0 +1,75 @@
+// cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
+// Copyright (c)  2025  Xiaomi Corporation
+
+//
+// This file demonstrates how to use Omnilingual ASR with sherpa-onnx's C++ API.
+// clang-format off
+/*
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+*/
+//
+// clang-format on
+
+#include <chrono>  // NOLINT
+#include <iostream>
+#include <string>
+
+#include "sherpa-onnx/c-api/cxx-api.h"
+
+int32_t main() {
+  using namespace sherpa_onnx::cxx;  // NOLINT
+  OfflineRecognizerConfig config;
+
+  // clang-format off
+  config.model_config.omnilingual.model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
+  config.model_config.tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
+
+  config.model_config.num_threads = 1;
+
+  std::cout << "Loading model\n";
+  OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
+  if (!recognizer.Get()) {
+    std::cerr << "Please check your config\n";
+    return -1;
+  }
+  std::cout << "Loading model done\n";
+
+  std::string wave_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
+  // clang-format on
+
+  Wave wave = ReadWave(wave_filename);
+  if (wave.samples.empty()) {
+    std::cerr << "Failed to read: '" << wave_filename << "'\n";
+    return -1;
+  }
+
+  std::cout << "Start recognition\n";
+  const auto begin = std::chrono::steady_clock::now();
+
+  OfflineStream stream = recognizer.CreateStream();
+  stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
+                        wave.samples.size());
+
+  recognizer.Decode(&stream);
+
+  OfflineRecognizerResult result = recognizer.GetResult(&stream);
+
+  const auto end = std::chrono::steady_clock::now();
+  const float elapsed_seconds =
+      std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
+          .count() /
+      1000.;
+  float duration = wave.samples.size() / static_cast<float>(wave.sample_rate);
+  float rtf = elapsed_seconds / duration;
+
+  std::cout << "text: " << result.text << "\n";
+  printf("Number of threads: %d\n", config.model_config.num_threads);
+  printf("Duration: %.3fs\n", duration);
+  printf("Elapsed seconds: %.3fs\n", elapsed_seconds);
+  printf("(Real time factor) RTF = %.3f / %.3f = %.3f\n", elapsed_seconds,
+         duration, rtf);
+
+  return 0;
+}
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index 7e42902c..a7e35c55 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -269,6 +269,9 @@ static SherpaOnnxOfflineRecognizerConfig Convert(
 
   c.model_config.wenet_ctc.model = config.model_config.wenet_ctc.model.c_str();
 
+  c.model_config.omnilingual.model =
+      config.model_config.omnilingual.model.c_str();
+
   c.lm_config.model = config.lm_config.model.c_str();
   c.lm_config.scale = config.lm_config.scale;
 
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index 1cf0eef7..77401e67 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -268,6 +268,10 @@ struct SHERPA_ONNX_API OfflineWenetCtcModelConfig {
   std::string model;
 };
 
+struct SHERPA_ONNX_API OfflineOmnilingualAsrCtcModelConfig {
+  std::string model;
+};
+
 struct SHERPA_ONNX_API OfflineMoonshineModelConfig {
   std::string preprocessor;
   std::string encoder;
@@ -297,6 +301,7 @@ struct SHERPA_ONNX_API OfflineModelConfig {
   OfflineZipformerCtcModelConfig zipformer_ctc;
   OfflineCanaryModelConfig canary;
   OfflineWenetCtcModelConfig wenet_ctc;
+  OfflineOmnilingualAsrCtcModelConfig omnilingual;
 };
 
 struct SHERPA_ONNX_API OfflineLMConfig {

commit 867d0445f7493de0121a067498cddd8aba3e9fd7
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 12:45:23 2025 +0800

    Add C API for Omnilingual ASR CTC models (#2773)

diff --git a/.github/workflows/c-api.yaml b/.github/workflows/c-api.yaml
index 561d1d20..b8e1adea 100644
--- a/.github/workflows/c-api.yaml
+++ b/.github/workflows/c-api.yaml
@@ -75,6 +75,36 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-c-api.dylib
           fi
 
+      - name: Test Omnilingual ASR CTC
+        shell: bash
+        run: |
+          name=omnilingual-asr-ctc-c-api
+          gcc -o $name ./c-api-examples/$name.c \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+          tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+          rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm $name
+          rm -rf sherpa-onnx-omnilingual-*
+
       - name: Test Wenet CTC
         shell: bash
         run: |
diff --git a/c-api-examples/CMakeLists.txt b/c-api-examples/CMakeLists.txt
index cd3d9d4a..b3baed3a 100644
--- a/c-api-examples/CMakeLists.txt
+++ b/c-api-examples/CMakeLists.txt
@@ -83,6 +83,9 @@ target_link_libraries(zipformer-c-api sherpa-onnx-c-api)
 add_executable(wenet-ctc-c-api wenet-ctc-c-api.c)
 target_link_libraries(wenet-ctc-c-api sherpa-onnx-c-api)
 
+add_executable(omnilingual-asr-ctc-c-api omnilingual-asr-ctc-c-api.c)
+target_link_libraries(omnilingual-asr-ctc-c-api sherpa-onnx-c-api)
+
 add_executable(streaming-zipformer-c-api streaming-zipformer-c-api.c)
 target_link_libraries(streaming-zipformer-c-api sherpa-onnx-c-api)
 
diff --git a/c-api-examples/omnilingual-asr-ctc-c-api.c b/c-api-examples/omnilingual-asr-ctc-c-api.c
new file mode 100644
index 00000000..eabd3c2f
--- /dev/null
+++ b/c-api-examples/omnilingual-asr-ctc-c-api.c
@@ -0,0 +1,82 @@
+// c-api-examples/omnilingual-asr-ctc-c-api.c
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+//
+// This file demonstrates how to use Omnilingual ASR with sherpa-onnx's C API.
+// clang-format off
+/*
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+*/
+//
+// clang-format on
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include "sherpa-onnx/c-api/c-api.h"
+
+int32_t main() {
+  // clang-format off
+  const char *wav_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
+  const char *model_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
+  const char *tokens_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
+  // clang-format on
+
+  const char *provider = "cpu";
+
+  const SherpaOnnxWave *wave = SherpaOnnxReadWave(wav_filename);
+  if (wave == NULL) {
+    fprintf(stderr, "Failed to read %s\n", wav_filename);
+    return -1;
+  }
+
+  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+  memset(&omnilingual, 0, sizeof(omnilingual));
+  omnilingual.model = model_filename;
+
+  // Offline model config
+  SherpaOnnxOfflineModelConfig offline_model_config;
+  memset(&offline_model_config, 0, sizeof(offline_model_config));
+  offline_model_config.debug = 1;
+  offline_model_config.num_threads = 1;
+  offline_model_config.provider = provider;
+  offline_model_config.tokens = tokens_filename;
+  offline_model_config.omnilingual = omnilingual;
+
+  // Recognizer config
+  SherpaOnnxOfflineRecognizerConfig recognizer_config;
+  memset(&recognizer_config, 0, sizeof(recognizer_config));
+  recognizer_config.decoding_method = "greedy_search";
+  recognizer_config.model_config = offline_model_config;
+
+  const SherpaOnnxOfflineRecognizer *recognizer =
+      SherpaOnnxCreateOfflineRecognizer(&recognizer_config);
+
+  if (recognizer == NULL) {
+    fprintf(stderr, "Please check your config!\n");
+    SherpaOnnxFreeWave(wave);
+    return -1;
+  }
+
+  const SherpaOnnxOfflineStream *stream =
+      SherpaOnnxCreateOfflineStream(recognizer);
+
+  SherpaOnnxAcceptWaveformOffline(stream, wave->sample_rate, wave->samples,
+                                  wave->num_samples);
+  SherpaOnnxDecodeOfflineStream(recognizer, stream);
+  const SherpaOnnxOfflineRecognizerResult *result =
+      SherpaOnnxGetOfflineStreamResult(stream);
+
+  fprintf(stderr, "Decoded text: %s\n", result->text);
+
+  SherpaOnnxDestroyOfflineRecognizerResult(result);
+  SherpaOnnxDestroyOfflineStream(stream);
+  SherpaOnnxDestroyOfflineRecognizer(recognizer);
+  SherpaOnnxFreeWave(wave);
+
+  return 0;
+}
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 75d63569..4d612ae1 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -508,6 +508,9 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.wenet_ctc.model =
       SHERPA_ONNX_OR(config->model_config.wenet_ctc.model, "");
 
+  recognizer_config.model_config.omnilingual.model =
+      SHERPA_ONNX_OR(config->model_config.omnilingual.model, "");
+
   recognizer_config.lm_config.model =
       SHERPA_ONNX_OR(config->lm_config.model, "");
   recognizer_config.lm_config.scale =
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 2f22a449..36731e66 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -480,6 +480,10 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineWenetCtcModelConfig {
   const char *model;
 } SherpaOnnxOfflineWenetCtcModelConfig;
 
+SHERPA_ONNX_API typedef struct SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
+  const char *model;
+} SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
+
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineTransducerModelConfig transducer;
   SherpaOnnxOfflineParaformerModelConfig paraformer;
@@ -506,6 +510,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineZipformerCtcModelConfig zipformer_ctc;
   SherpaOnnxOfflineCanaryModelConfig canary;
   SherpaOnnxOfflineWenetCtcModelConfig wenet_ctc;
+  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
 } SherpaOnnxOfflineModelConfig;
 
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerConfig {

commit 36ef8d1fbc2b757dcb2274361ba7482da1c23d56
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Thu Nov 13 12:25:30 2025 +0800

    Add C++ and Python API for Omnilingual ASR models. (#2772)
    
    This PR adds C++ and Python API support for Omnilingual ASR models with CTC decoding, enabling speech recognition across 1600+ languages using models from Facebook Research's omnilingual-asr project.
    
    Key changes:
    
    - Implements OfflineOmnilingualAsrCtcModel class with feature normalization and model inference
    - Adds Python bindings through from_omnilingual_asr_ctc() method on OfflineRecognizer
    - Integrates omnilingual models into existing CTC recognition pipeline with special handling for input shapes and frame shifts

diff --git a/.github/scripts/test-python.sh b/.github/scripts/test-python.sh
index 9ea01c0c..49523aaf 100755
--- a/.github/scripts/test-python.sh
+++ b/.github/scripts/test-python.sh
@@ -8,6 +8,16 @@ log() {
   echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
 }
 
+log "test omnilingual ASR"
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+ls -lh sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+
+python3 ./python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
+
+rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+
 log "test T-one"
 
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-t-one-russian-2025-09-08.tar.bz2
diff --git a/python-api-examples/offline-fire-red-asr-decode-files.py b/python-api-examples/offline-fire-red-asr-decode-files.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/offline-moonshine-decode-files.py b/python-api-examples/offline-moonshine-decode-files.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/offline-nemo-canary-decode-files.py b/python-api-examples/offline-nemo-canary-decode-files.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/offline-nemo-parakeet-decode-file.py b/python-api-examples/offline-nemo-parakeet-decode-file.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py b/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
new file mode 100755
index 00000000..1354ef8d
--- /dev/null
+++ b/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
@@ -0,0 +1,133 @@
+#!/usr/bin/env python3
+
+"""
+This file shows how to use a non-streaming Omnilingual ASR CTC model from
+https://github.com/facebookresearch/omnilingual-asr
+to decode files.
+
+Please download model files from
+https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+
+For instance,
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+"""
+
+from pathlib import Path
+
+import numpy as np
+import time
+import sherpa_onnx
+import soundfile as sf
+
+
+def create_recognizer():
+    model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
+    tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
+    test_wav_en = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
+    test_wav_de = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/de.wav"
+    test_wav_fr = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/fr.wav"
+    test_wav_es = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/es.wav"
+
+    for f in [model, tokens, test_wav_en, test_wav_de, test_wav_fr, test_wav_es]:
+        if not Path(f).is_file():
+            print(f"{f} does not exist")
+
+            raise ValueError(
+                """Please download model files from
+                https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+                """
+            )
+    return (
+        sherpa_onnx.OfflineRecognizer.from_omnilingual_asr_ctc(
+            model=model,
+            tokens=tokens,
+            num_threads=1,
+        ),
+        test_wav_en,
+        test_wav_de,
+        test_wav_fr,
+        test_wav_es,
+    )
+
+
+def load_audio(filename):
+    audio, sample_rate = sf.read(filename, dtype="float32", always_2d=True)
+    audio = audio[:, 0]  # only use the first channel
+    if sample_rate != 16000:
+        import librosa
+
+        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)
+
+    return np.ascontiguousarray(audio)
+
+
+def decode_single_file(recognizer, filename):
+    samples = load_audio(filename)
+
+    start_time = time.time()
+
+    stream = recognizer.create_stream()
+    stream.accept_waveform(sample_rate=16000, waveform=samples)
+    recognizer.decode_stream(stream)
+
+    end_time = time.time()
+    elapsed_seconds = end_time - start_time
+    audio_duration = len(samples) / 16000
+    real_time_factor = elapsed_seconds / audio_duration
+
+    print("---")
+    print(filename)
+    print(stream.result)
+    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
+    print(f"Audio duration in seconds: {audio_duration:.3f}")
+    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
+    print()
+
+
+def decode_multiple_files(recognizer, filenames):
+    streams = []
+
+    start_time = time.time()
+
+    audio_duration = 0
+
+    for filename in filenames:
+        samples = load_audio(filename)
+        audio_duration += len(samples) / 16000
+
+        stream = recognizer.create_stream()
+        stream.accept_waveform(sample_rate=16000, waveform=samples)
+        streams.append(stream)
+
+    recognizer.decode_streams(streams)
+
+    end_time = time.time()
+    elapsed_seconds = end_time - start_time
+    real_time_factor = elapsed_seconds / audio_duration
+
+    for name, stream in zip(filenames, streams):
+        print("---")
+        print(name)
+        print(stream.result)
+        print()
+
+    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
+    print(f"Audio duration in seconds: {audio_duration:.3f}")
+    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
+    print()
+    print()
+
+
+def main():
+    recognizer, *filenames = create_recognizer()
+
+    decode_single_file(recognizer, filenames[0])
+    decode_single_file(recognizer, filenames[1])
+    decode_multiple_files(recognizer, filenames[2:])
+
+
+if __name__ == "__main__":
+    main()
diff --git a/python-api-examples/offline-sense-voice-ctc-decode-files.py b/python-api-examples/offline-sense-voice-ctc-decode-files.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/offline-whisper-decode-files.py b/python-api-examples/offline-whisper-decode-files.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/speaker-identification-with-vad-non-streaming-asr-alsa.py b/python-api-examples/speaker-identification-with-vad-non-streaming-asr-alsa.py
old mode 100644
new mode 100755
diff --git a/python-api-examples/two-pass-wss.py b/python-api-examples/two-pass-wss.py
old mode 100644
new mode 100755
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 10e90fd8..a9a09286 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -44,6 +44,8 @@ set(sources
   offline-moonshine-model.cc
   offline-nemo-enc-dec-ctc-model-config.cc
   offline-nemo-enc-dec-ctc-model.cc
+  offline-omnilingual-asr-ctc-model-config.cc
+  offline-omnilingual-asr-ctc-model.cc
   offline-paraformer-greedy-search-decoder.cc
   offline-paraformer-model-config.cc
   offline-paraformer-model.cc
diff --git a/sherpa-onnx/csrc/offline-ctc-model.cc b/sherpa-onnx/csrc/offline-ctc-model.cc
index f115af90..4becf1af 100644
--- a/sherpa-onnx/csrc/offline-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-ctc-model.cc
@@ -22,6 +22,7 @@
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-dolphin-model.h"
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
+#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-telespeech-ctc-model.h"
 #include "sherpa-onnx/csrc/offline-wenet-ctc-model.h"
@@ -123,6 +124,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
     return std::make_unique<OfflineWenetCtcModel>(config);
   } else if (!config.telespeech_ctc.empty()) {
     return std::make_unique<OfflineTeleSpeechCtcModel>(config);
+  } else if (!config.omnilingual.model.empty()) {
+    return std::make_unique<OfflineOmnilingualAsrCtcModel>(config);
   }
 
   // TODO(fangjun): Refactor it. We don't need to use model_type here
@@ -187,6 +190,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
     return std::make_unique<OfflineWenetCtcModel>(mgr, config);
   } else if (!config.telespeech_ctc.empty()) {
     return std::make_unique<OfflineTeleSpeechCtcModel>(mgr, config);
+  } else if (!config.omnilingual.model.empty()) {
+    return std::make_unique<OfflineOmnilingualAsrCtcModel>(mgr, config);
   }
 
   // TODO(fangjun): Refactor it. We don't need to use model_type here
diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
index 68f98dd2..0171c379 100644
--- a/sherpa-onnx/csrc/offline-model-config.cc
+++ b/sherpa-onnx/csrc/offline-model-config.cc
@@ -24,6 +24,7 @@ void OfflineModelConfig::Register(ParseOptions *po) {
   moonshine.Register(po);
   dolphin.Register(po);
   canary.Register(po);
+  omnilingual.Register(po);
 
   po->Register("telespeech-ctc", &telespeech_ctc,
                "Path to model.onnx for telespeech ctc");
@@ -148,6 +149,10 @@ bool OfflineModelConfig::Validate() const {
     return canary.Validate();
   }
 
+  if (!omnilingual.model.empty()) {
+    return omnilingual.Validate();
+  }
+
   if (!telespeech_ctc.empty() && !FileExists(telespeech_ctc)) {
     SHERPA_ONNX_LOGE("telespeech_ctc: '%s' does not exist",
                      telespeech_ctc.c_str());
@@ -177,6 +182,7 @@ std::string OfflineModelConfig::ToString() const {
   os << "moonshine=" << moonshine.ToString() << ", ";
   os << "dolphin=" << dolphin.ToString() << ", ";
   os << "canary=" << canary.ToString() << ", ";
+  os << "omnilingual=" << omnilingual.ToString() << ", ";
   os << "telespeech_ctc=\"" << telespeech_ctc << "\", ";
   os << "tokens=\"" << tokens << "\", ";
   os << "num_threads=" << num_threads << ", ";
diff --git a/sherpa-onnx/csrc/offline-model-config.h b/sherpa-onnx/csrc/offline-model-config.h
index 8164c7f7..6ef84edc 100644
--- a/sherpa-onnx/csrc/offline-model-config.h
+++ b/sherpa-onnx/csrc/offline-model-config.h
@@ -11,6 +11,7 @@
 #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
 #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
+#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
 #include "sherpa-onnx/csrc/offline-paraformer-model-config.h"
 #include "sherpa-onnx/csrc/offline-sense-voice-model-config.h"
 #include "sherpa-onnx/csrc/offline-tdnn-model-config.h"
@@ -34,6 +35,7 @@ struct OfflineModelConfig {
   OfflineMoonshineModelConfig moonshine;
   OfflineDolphinModelConfig dolphin;
   OfflineCanaryModelConfig canary;
+  OfflineOmnilingualAsrCtcModelConfig omnilingual;
   std::string telespeech_ctc;
 
   std::string tokens;
@@ -68,6 +70,7 @@ struct OfflineModelConfig {
                      const OfflineMoonshineModelConfig &moonshine,
                      const OfflineDolphinModelConfig &dolphin,
                      const OfflineCanaryModelConfig &canary,
+                     const OfflineOmnilingualAsrCtcModelConfig &omnilingual,
                      const std::string &telespeech_ctc,
                      const std::string &tokens, int32_t num_threads, bool debug,
                      const std::string &provider, const std::string &model_type,
@@ -85,6 +88,7 @@ struct OfflineModelConfig {
         moonshine(moonshine),
         dolphin(dolphin),
         canary(canary),
+        omnilingual(omnilingual),
         telespeech_ctc(telespeech_ctc),
         tokens(tokens),
         num_threads(num_threads),
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
new file mode 100644
index 00000000..50ae999c
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
@@ -0,0 +1,38 @@
+// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
+
+#include <string>
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+void OfflineOmnilingualAsrCtcModelConfig::Register(ParseOptions *po) {
+  po->Register("omnilingual-asr-model", &model,
+               "Path to Omnilingual ASR CTC model");
+}
+
+bool OfflineOmnilingualAsrCtcModelConfig::Validate() const {
+  if (!FileExists(model)) {
+    SHERPA_ONNX_LOGE("Omnilingual ASR CTC model file '%s' does not exist",
+                     model.c_str());
+    return false;
+  }
+
+  return true;
+}
+
+std::string OfflineOmnilingualAsrCtcModelConfig::ToString() const {
+  std::ostringstream os;
+
+  os << "OfflineOmnilingualAsrCtcModelConfig(";
+  os << "model=\"" << model << "\")";
+
+  return os.str();
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
new file mode 100644
index 00000000..cfd63a3b
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
@@ -0,0 +1,32 @@
+// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+
+#include <string>
+
+#include "sherpa-onnx/csrc/parse-options.h"
+
+namespace sherpa_onnx {
+
+// for
+// https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/omnilingual-asr/test.py
+struct OfflineOmnilingualAsrCtcModelConfig {
+  std::string model;
+
+  OfflineOmnilingualAsrCtcModelConfig() = default;
+
+  explicit OfflineOmnilingualAsrCtcModelConfig(const std::string &model)
+      : model(model) {}
+
+  void Register(ParseOptions *po);
+
+  bool Validate() const;
+
+  std::string ToString() const;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
new file mode 100644
index 00000000..0ed69a58
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
@@ -0,0 +1,184 @@
+// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
+
+#include <algorithm>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/onnx-utils.h"
+#include "sherpa-onnx/csrc/session.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+#include "sherpa-onnx/csrc/transpose.h"
+
+namespace sherpa_onnx {
+
+class OfflineOmnilingualAsrCtcModel::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR),
+        sess_opts_(GetSessionOptions(config)),
+        allocator_{} {
+    auto buf = ReadFile(config_.omnilingual.model);
+    Init(buf.data(), buf.size());
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR),
+        sess_opts_(GetSessionOptions(config)),
+        allocator_{} {
+    auto buf = ReadFile(mgr, config_.omnilingual.model);
+    Init(buf.data(), buf.size());
+  }
+
+  std::vector<Ort::Value> Forward(Ort::Value features,
+                                  Ort::Value /*/features_length*/) {
+    auto out_vec =
+        sess_->Run({}, input_names_ptr_.data(), &features, 1,
+                   output_names_ptr_.data(), output_names_ptr_.size());
+    std::vector<int64_t> logits_shape =
+        out_vec[0].GetTensorTypeAndShapeInfo().GetShape();
+
+    std::vector<int64_t> num_frames(logits_shape[0], logits_shape[1]);
+
+    int64_t shape = logits_shape[0];
+
+    Ort::Value logits_len =
+        Ort::Value::CreateTensor<int64_t>(allocator_, &shape, 1);
+    std::copy(num_frames.begin(), num_frames.end(),
+              logits_len.GetTensorMutableData<int64_t>());
+
+    out_vec.push_back(std::move(logits_len));
+
+    return out_vec;
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+
+  OrtAllocator *Allocator() { return allocator_; }
+
+  static void NormalizeFeatures(float *features, int32_t num_frames,
+                                int32_t feat_dim) {
+    if (num_frames != 1) {
+      SHERPA_ONNX_LOGE(
+          "Unexpected error in collecting samples for Omnilingual ASR models!");
+      return;
+    }
+
+    double s = 0;
+    double sq = 0;
+    for (int32_t i = 0; i < feat_dim; ++i) {
+      s += features[i];
+      sq += features[i] * features[i];
+    }
+
+    double mean = s / feat_dim;
+    double inv_stddev = 1 / std::sqrt(sq / feat_dim - mean * mean + 1e-5);
+
+    for (int32_t i = 0; i < feat_dim; ++i) {
+      features[i] = (features[i] - mean) * inv_stddev;
+    }
+  }
+
+ private:
+  void Init(void *model_data, size_t model_data_length) {
+    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
+                                           sess_opts_);
+
+    GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
+
+    GetOutputNames(sess_.get(), &output_names_, &output_names_ptr_);
+
+    // get meta data
+    Ort::ModelMetadata meta_data = sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+
+    // get vocab size from the output[0].shape, which is (N, T, vocab_size)
+    vocab_size_ =
+        sess_->GetOutputTypeInfo(0).GetTensorTypeAndShapeInfo().GetShape()[2];
+  }
+
+ private:
+  OfflineModelConfig config_;
+  Ort::Env env_;
+  Ort::SessionOptions sess_opts_;
+  Ort::AllocatorWithDefaultOptions allocator_;
+
+  std::unique_ptr<Ort::Session> sess_;
+
+  std::vector<std::string> input_names_;
+  std::vector<const char *> input_names_ptr_;
+
+  std::vector<std::string> output_names_;
+  std::vector<const char *> output_names_ptr_;
+
+  int32_t vocab_size_ = 0;
+};
+
+OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineOmnilingualAsrCtcModel::~OfflineOmnilingualAsrCtcModel() = default;
+
+std::vector<Ort::Value> OfflineOmnilingualAsrCtcModel::Forward(
+    Ort::Value features, Ort::Value features_length) {
+  return impl_->Forward(std::move(features), std::move(features_length));
+}
+
+int32_t OfflineOmnilingualAsrCtcModel::VocabSize() const {
+  return impl_->VocabSize();
+}
+
+OrtAllocator *OfflineOmnilingualAsrCtcModel::Allocator() const {
+  return impl_->Allocator();
+}
+
+void OfflineOmnilingualAsrCtcModel::NormalizeFeatures(float *features,
+                                                      int32_t num_frames,
+                                                      int32_t feat_dim) const {
+  return impl_->NormalizeFeatures(features, num_frames, feat_dim);
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
new file mode 100644
index 00000000..c3653d4d
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
@@ -0,0 +1,64 @@
+// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "onnxruntime_cxx_api.h"  // NOLINT
+#include "sherpa-onnx/csrc/offline-ctc-model.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+/** This class implements the Omnilingual ASR CTC model
+ * from
+ * https://github.com/facebookresearch/omnilingual-asr
+ *
+ * See
+ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/omnilingual-asr/export-onnx.py
+ */
+class OfflineOmnilingualAsrCtcModel : public OfflineCtcModel {
+ public:
+  explicit OfflineOmnilingualAsrCtcModel(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineOmnilingualAsrCtcModel(Manager *mgr, const OfflineModelConfig &config);
+
+  ~OfflineOmnilingualAsrCtcModel() override;
+
+  /** Run the forward method of the model.
+   *
+   * @param features  A tensor of shape (N, T, C).
+   * @param features_length  A 1-D tensor of shape (N,) containing number of
+   *                         valid frames in `features` before padding.
+   *                         Its dtype is int64_t.
+   *
+   * @return Return a vector containing:
+   *  - log_probs: A 3-D tensor of shape (N, T', vocab_size).
+   *  - log_probs_length A 1-D tensor of shape (N,). Its dtype is int64_t
+   */
+  std::vector<Ort::Value> Forward(Ort::Value features,
+                                  Ort::Value features_length) override;
+
+  /** Return the vocabulary size of the model
+   */
+  int32_t VocabSize() const override;
+
+  /** Return an allocator for allocating memory
+   */
+  OrtAllocator *Allocator() const override;
+
+  void NormalizeFeatures(float *features, int32_t num_frames,
+                         int32_t feat_dim) const override;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
index 491ac27b..78a72fc1 100644
--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
@@ -12,6 +12,7 @@
 #include <utility>
 #include <vector>
 
+#include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-ctc-decoder.h"
 #include "sherpa-onnx/csrc/offline-ctc-fst-decoder.h"
 #include "sherpa-onnx/csrc/offline-ctc-greedy-search-decoder.h"
@@ -154,11 +155,13 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
     } else if (config_.decoding_method == "greedy_search") {
       if (!symbol_table_.Contains("<blk>") &&
           !symbol_table_.Contains("<eps>") &&
-          !symbol_table_.Contains("<blank>")) {
+          !symbol_table_.Contains("<blank>") &&
+          config_.model_config.omnilingual.model.empty()) {
+        // for omnilingual asr, its blank id is 0
         SHERPA_ONNX_LOGE(
             "We expect that tokens.txt contains "
             "the symbol <blk> or <eps> or <blank> and its ID.");
-        exit(-1);
+        SHERPA_ONNX_EXIT(-1);
       }
 
       int32_t blank_id = 0;
@@ -181,23 +184,33 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
   }
 
   std::unique_ptr<OfflineStream> CreateStream() const override {
-    return std::make_unique<OfflineStream>(config_.feat_config);
+    if (config_.model_config.omnilingual.model.empty()) {
+      return std::make_unique<OfflineStream>(config_.feat_config);
+    } else {
+      return std::make_unique<OfflineStream>(OmnilingualAsrTag{});
+    }
   }
 
   void DecodeStreams(OfflineStream **ss, int32_t n) const override {
-    if (!model_->SupportBatchProcessing() || (n == 1)) {
-      // If the model does not support batch process,
+    if (!model_->SupportBatchProcessing() || (n == 1) ||
+        !config_.model_config.omnilingual.model.empty()) {
+      // If the model does not support batch processing,
       // we process each stream independently.
+      //
+      // omnilingual asr is disabled for batch processing at present
       for (int32_t i = 0; i != n; ++i) {
         DecodeStream(ss[i]);
       }
       return;
     }
 
+    // Even if the omnilingual asr model can process batch input, the following
+    // code does not support batching raw audio samples.
+
     auto memory_info =
         Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
 
-    int32_t feat_dim = config_.feat_config.feature_dim;
+    int32_t feat_dim = ss[0]->FeatureDim();
 
     std::vector<Ort::Value> features;
     features.reserve(n);
@@ -259,14 +272,17 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
     auto memory_info =
         Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
 
-    int32_t feat_dim = config_.feat_config.feature_dim;
+    int32_t feat_dim = s->FeatureDim();
     std::vector<float> f = s->GetFrames();
 
     int32_t num_frames = f.size() / feat_dim;
 
     model_->NormalizeFeatures(f.data(), num_frames, feat_dim);
 
-    std::array<int64_t, 3> shape = {1, num_frames, feat_dim};
+    std::vector<int64_t> shape = {1, num_frames, feat_dim};
+    if (!config_.model_config.omnilingual.model.empty()) {
+      shape = {1, feat_dim};
+    }
 
     Ort::Value x = Ort::Value::CreateTensor(memory_info, f.data(), f.size(),
                                             shape.data(), shape.size());
@@ -281,6 +297,10 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
     auto results = decoder_->Decode(std::move(t[0]), std::move(t[1]));
     int32_t frame_shift_ms = 10;
 
+    if (!config_.model_config.omnilingual.model.empty()) {
+      frame_shift_ms = 20;
+    }
+
     auto r = Convert(results[0], symbol_table_, frame_shift_ms,
                      model_->SubsamplingFactor());
     r.text = ApplyInverseTextNormalization(std::move(r.text));
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index c283c4ae..37279c37 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -101,6 +101,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
       !config.model_config.zipformer_ctc.model.empty() ||
       !config.model_config.tdnn.model.empty() ||
       !config.model_config.wenet_ctc.model.empty() ||
+      !config.model_config.omnilingual.model.empty() ||
       !config.model_config.dolphin.model.empty()) {
     return std::make_unique<OfflineRecognizerCtcImpl>(config);
   }
diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
index 9c5d1088..8f757e6a 100644
--- a/sherpa-onnx/csrc/offline-stream.cc
+++ b/sherpa-onnx/csrc/offline-stream.cc
@@ -145,6 +145,10 @@ class OfflineStream::Impl {
     config_.sampling_rate = 16000;
   }
 
+  explicit Impl(OmnilingualAsrTag /*tag*/) : is_omnilingual_asr_(true) {
+    config_.sampling_rate = 16000;
+  }
+
   void AcceptWaveform(int32_t sampling_rate, const float *waveform, int32_t n) {
     if (config_.normalize_samples) {
       AcceptWaveformImpl(sampling_rate, waveform, n);
@@ -176,7 +180,7 @@ class OfflineStream::Impl {
       std::vector<float> samples;
       resampler->Resample(waveform, n, true, &samples);
 
-      if (is_moonshine_) {
+      if (is_moonshine_ || is_omnilingual_asr_) {
         samples_.insert(samples_.end(), samples.begin(), samples.end());
       } else if (fbank_) {
         fbank_->AcceptWaveform(config_.sampling_rate, samples.data(),
@@ -195,7 +199,7 @@ class OfflineStream::Impl {
       return;
     }  // if (sampling_rate != config_.sampling_rate)
 
-    if (is_moonshine_) {
+    if (is_moonshine_ || is_omnilingual_asr_) {
       samples_.insert(samples_.end(), waveform, waveform + n);
     } else if (fbank_) {
       fbank_->AcceptWaveform(sampling_rate, waveform, n);
@@ -210,7 +214,7 @@ class OfflineStream::Impl {
   }
 
   int32_t FeatureDim() const {
-    if (is_moonshine_) {
+    if (is_moonshine_ || is_omnilingual_asr_) {
       return samples_.size();
     }
 
@@ -218,7 +222,7 @@ class OfflineStream::Impl {
   }
 
   std::vector<float> GetFrames() const {
-    if (is_moonshine_) {
+    if (is_moonshine_ || is_omnilingual_asr_) {
       return samples_;
     }
 
@@ -325,8 +329,9 @@ class OfflineStream::Impl {
   ContextGraphPtr context_graph_;
   bool is_ced_ = false;
   bool is_moonshine_ = false;
+  bool is_omnilingual_asr_ = false;
 
-  // used only when is_moonshine_== true
+  // used only when (is_moonshine_ || is_omnilingual_asr_) == true
   std::vector<float> samples_;
 };
 
@@ -342,6 +347,9 @@ OfflineStream::OfflineStream(CEDTag tag) : impl_(std::make_unique<Impl>(tag)) {}
 OfflineStream::OfflineStream(MoonshineTag tag)
     : impl_(std::make_unique<Impl>(tag)) {}
 
+OfflineStream::OfflineStream(OmnilingualAsrTag tag)
+    : impl_(std::make_unique<Impl>(tag)) {}
+
 OfflineStream::~OfflineStream() = default;
 
 void OfflineStream::AcceptWaveform(int32_t sampling_rate, const float *waveform,
diff --git a/sherpa-onnx/csrc/offline-stream.h b/sherpa-onnx/csrc/offline-stream.h
index 1039935d..5e2a514b 100644
--- a/sherpa-onnx/csrc/offline-stream.h
+++ b/sherpa-onnx/csrc/offline-stream.h
@@ -57,6 +57,9 @@ struct CEDTag {};
 // audio samples to features
 struct MoonshineTag {};
 
+// It is based on Wav2Vec, accepting raw audio samples as input
+struct OmnilingualAsrTag {};
+
 class OfflineStream {
  public:
   explicit OfflineStream(const FeatureExtractorConfig &config = {},
@@ -65,6 +68,7 @@ class OfflineStream {
   explicit OfflineStream(WhisperTag tag);
   explicit OfflineStream(CEDTag tag);
   explicit OfflineStream(MoonshineTag tag);
+  explicit OfflineStream(OmnilingualAsrTag tag);
   ~OfflineStream();
 
   /**
diff --git a/sherpa-onnx/python/csrc/CMakeLists.txt b/sherpa-onnx/python/csrc/CMakeLists.txt
index 4603aab8..0b55ae91 100644
--- a/sherpa-onnx/python/csrc/CMakeLists.txt
+++ b/sherpa-onnx/python/csrc/CMakeLists.txt
@@ -17,6 +17,7 @@ set(srcs
   offline-model-config.cc
   offline-moonshine-model-config.cc
   offline-nemo-enc-dec-ctc-model-config.cc
+  offline-omnilingual-asr-ctc-model-config.cc
   offline-paraformer-model-config.cc
   offline-punctuation.cc
   offline-recognizer.cc
diff --git a/sherpa-onnx/python/csrc/offline-model-config.cc b/sherpa-onnx/python/csrc/offline-model-config.cc
index dc3c65dc..6c1286b8 100644
--- a/sherpa-onnx/python/csrc/offline-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-model-config.cc
@@ -13,6 +13,7 @@
 #include "sherpa-onnx/python/csrc/offline-fire-red-asr-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-nemo-enc-dec-ctc-model-config.h"
+#include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-paraformer-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-sense-voice-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-tdnn-model-config.h"
@@ -36,6 +37,7 @@ void PybindOfflineModelConfig(py::module *m) {
   PybindOfflineMoonshineModelConfig(m);
   PybindOfflineDolphinModelConfig(m);
   PybindOfflineCanaryModelConfig(m);
+  PybindOfflineOmnilingualAsrCtcModelConfig(m);
 
   using PyClass = OfflineModelConfig;
   py::class_<PyClass>(*m, "OfflineModelConfig")
@@ -50,10 +52,11 @@ void PybindOfflineModelConfig(py::module *m) {
                     const OfflineSenseVoiceModelConfig &,
                     const OfflineMoonshineModelConfig &,
                     const OfflineDolphinModelConfig &,
-                    const OfflineCanaryModelConfig &, const std::string &,
-                    const std::string &, int32_t, bool, const std::string &,
+                    const OfflineCanaryModelConfig &,
+                    const OfflineOmnilingualAsrCtcModelConfig &,
+                    const std::string &, const std::string &, int32_t, bool,
                     const std::string &, const std::string &,
-                    const std::string &>(),
+                    const std::string &, const std::string &>(),
            py::arg("transducer") = OfflineTransducerModelConfig(),
            py::arg("paraformer") = OfflineParaformerModelConfig(),
            py::arg("nemo_ctc") = OfflineNemoEncDecCtcModelConfig(),
@@ -66,6 +69,7 @@ void PybindOfflineModelConfig(py::module *m) {
            py::arg("moonshine") = OfflineMoonshineModelConfig(),
            py::arg("dolphin") = OfflineDolphinModelConfig(),
            py::arg("canary") = OfflineCanaryModelConfig(),
+           py::arg("omnilingual") = OfflineOmnilingualAsrCtcModelConfig(),
            py::arg("telespeech_ctc") = "", py::arg("tokens") = "",
            py::arg("num_threads") = 1, py::arg("debug") = false,
            py::arg("provider") = "cpu", py::arg("model_type") = "",
@@ -82,6 +86,7 @@ void PybindOfflineModelConfig(py::module *m) {
       .def_readwrite("moonshine", &PyClass::moonshine)
       .def_readwrite("dolphin", &PyClass::dolphin)
       .def_readwrite("canary", &PyClass::canary)
+      .def_readwrite("omnilingual", &PyClass::omnilingual)
       .def_readwrite("telespeech_ctc", &PyClass::telespeech_ctc)
       .def_readwrite("tokens", &PyClass::tokens)
       .def_readwrite("num_threads", &PyClass::num_threads)
diff --git a/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
new file mode 100644
index 00000000..0ba3cff0
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
@@ -0,0 +1,22 @@
+// sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
+
+#include <string>
+
+#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineOmnilingualAsrCtcModelConfig(py::module *m) {
+  using PyClass = OfflineOmnilingualAsrCtcModelConfig;
+  py::class_<PyClass>(*m, "OfflineOmnilingualAsrCtcModelConfig")
+      .def(py::init<>())
+      .def(py::init<const std::string &>(), py::arg("model"))
+      .def_readwrite("model", &PyClass::model)
+      .def("__str__", &PyClass::ToString);
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
new file mode 100644
index 00000000..d3770622
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
@@ -0,0 +1,16 @@
+// sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+#define SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+
+#include "sherpa-onnx/python/csrc/sherpa-onnx.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineOmnilingualAsrCtcModelConfig(py::module *m);
+
+}
+
+#endif  // SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
index 74eb54c2..1194c664 100644
--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
@@ -7,6 +7,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
     FeatureExtractorConfig,
     HomophoneReplacerConfig,
     OfflineCanaryModelConfig,
+    OfflineOmnilingualAsrCtcModelConfig,
     OfflineCtcFstDecoderConfig,
     OfflineDolphinModelConfig,
     OfflineFireRedAsrModelConfig,
@@ -535,6 +536,56 @@ class OfflineRecognizer(object):
         self.config = recognizer_config
         return self
 
+    @classmethod
+    def from_omnilingual_asr_ctc(
+        cls,
+        model: str,
+        tokens: str,
+        num_threads: int = 1,
+        decoding_method: str = "greedy_search",
+        debug: bool = False,
+        provider: str = "cpu",
+    ):
+        """
+        Please refer to
+        `<https://k2-fsa.github.io/sherpa/onnx/omnilingual-asr/index.html>`_
+        to download pre-trained models.
+
+        Args:
+          model:
+            Path to ``model.onnx``.
+          tokens:
+            Path to ``tokens.txt``. Each line in ``tokens.txt`` contains two
+            columns::
+
+                symbol integer_id
+
+          num_threads:
+            Number of threads for neural network computation.
+          decoding_method:
+            The only supported decoding method is greedy_search.
+          debug:
+            True to show debug messages.
+          provider:
+            onnxruntime execution providers. Valid values are: cpu, cuda, coreml.
+        """
+        self = cls.__new__(cls)
+        model_config = OfflineModelConfig(
+            omnilingual=OfflineOmnilingualAsrCtcModelConfig(model=model),
+            tokens=tokens,
+            num_threads=num_threads,
+            debug=debug,
+            provider=provider,
+        )
+
+        recognizer_config = OfflineRecognizerConfig(
+            model_config=model_config,
+            decoding_method=decoding_method,
+        )
+        self.recognizer = _Recognizer(recognizer_config)
+        self.config = recognizer_config
+        return self
+
     @classmethod
     def from_zipformer_ctc(
         cls,

commit db77fe69993719f3968a4a5b7f0227b0429b909f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Nov 12 20:24:27 2025 +0800

    Begin to export omnilingual-asr to sherpa-onnx (#2770)

diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
new file mode 100644
index 00000000..332d8d96
--- /dev/null
+++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
@@ -0,0 +1,171 @@
+name: export-omnilingual-asr-to-onnx
+
+on:
+  push:
+    branches:
+      - export-omnilingual-asr
+  workflow_dispatch:
+
+concurrency:
+  group: export-omnilingual-asr-to-onnx-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-omnilingual-asr-to-onnx:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: export omnilingual-asr
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+        python-version: ["3.10"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Install dependencies
+        shell: bash
+        run: |
+          sudo apt install libsndfile1
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          pip install fairseq2 \
+            --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.8.0/cpu \
+            torch==2.8.0+cpu -f https://download.pytorch.org/whl/torch \
+            torchaudio==2.8.0+cpu -f https://download.pytorch.org/whl/torchaudio \
+            onnx==1.17.0 \
+            onnxruntime==1.17.1 \
+            soundfile \
+            librosa
+
+          pip install --no-deps omnilingual_asr
+
+          pip install retrying pandas polars pyarrow xxhash
+
+      - name: Setup tmate session
+        if: false
+        uses: mxschmitt/action-tmate@v3
+
+      - name: Run
+        shell: bash
+        run: |
+          cd scripts/omnilingual-asr
+          python3 ./export-onnx.py
+
+          ls -lh *.onnx
+
+          rm README.md
+
+          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/README.md
+          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE
+          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE-CC-BY-4.0.md
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/es.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/fr.wav
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/de.wav
+
+          echo "---test----"
+          python3 ./test.py
+
+          echo "---collect files----"
+
+          d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          mv -v model.onnx $d
+          cp -v tokens.txt $d
+          cp -v README.md $d
+          cp -v LICENSE* $d
+          cp -v *.wav $d/test_wavs
+
+          ls -lh $d
+
+          tar cjfv $d.tar.bz2 $d
+          mv $d ../..
+
+          d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          mv -v model.int8.onnx $d
+          cp -v tokens.txt $d
+          cp -v README.md $d
+          cp -v LICENSE* $d
+          cp -v *.wav $d/test_wavs
+          ls -lh $d
+
+          tar cjfv $d.tar.bz2 $d
+
+          mv $d ../..
+
+          mv *.tar.bz2 ../../
+
+          cd ../..
+
+          ls -lh *.tar.bz2
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            export GIT_LFS_SKIP_SMUDGE=1
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+
+            dirs=(
+              sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
+              sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+            )
+
+            for d in ${dirs[@]}; do
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
+              pushd huggingface
+
+              git fetch
+              git pull
+              echo "pwd: $PWD"
+              cp -a ../$d/* .
+
+              git lfs track "*.onnx"
+              git lfs track "*.wav"
+              ls -lh
+              git add .
+
+              ls -lh
+
+              git status
+
+              git commit -m "add models"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main || true
+              popd
+            done
+
+      - name: Release
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models
diff --git a/scripts/omnilingual-asr/README.md b/scripts/omnilingual-asr/README.md
new file mode 100644
index 00000000..342bb4a8
--- /dev/null
+++ b/scripts/omnilingual-asr/README.md
@@ -0,0 +1,23 @@
+# Introduction
+
+This folder contains script to export
+https://github.com/facebookresearch/omnilingual-asr
+to sherpa-onnx
+
+See
+https://github.com/k2-fsa/sherpa-onnx/blob/master/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+for usage.
+
+```
+num_frames = round(num_samples / 318 - 1.5)
+num_samples = round(318 * num_frames + 477)
+
+or
+num_frames = round(num_samples / 320)
+
+```
+
+20ms per frame
+
+
+
diff --git a/scripts/omnilingual-asr/export-onnx.py b/scripts/omnilingual-asr/export-onnx.py
new file mode 100755
index 00000000..deaf556d
--- /dev/null
+++ b/scripts/omnilingual-asr/export-onnx.py
@@ -0,0 +1,103 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from typing import Dict
+
+import onnx
+import torch
+from fairseq2.nn.batch_layout import BatchLayout
+from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline
+from onnxruntime.quantization import QuantType, quantize_dynamic
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+
+class ModelWrapper(torch.nn.Module):
+    def __init__(self, model):
+        super().__init__()
+        self.model = model
+
+    def forward(self, x):
+        """
+        Args:
+          x: (N, num_samples), float32
+        """
+        batch_layout = BatchLayout(shape=x.shape, seq_lens=[x.shape[1]])
+        logits, _ = self.model(x, batch_layout)
+        return logits
+
+
+@torch.no_grad()
+def main():
+    pipeline = ASRInferencePipeline(
+        model_card="omniASR_CTC_300M",
+        device="cpu",
+        dtype=torch.float32,
+    )
+
+    vocab_size = pipeline.tokenizer._model.vocabulary_size
+
+    with open("tokens.txt", "w") as f:
+        for i in range(pipeline.tokenizer._model.vocabulary_size):
+            f.write(f"{pipeline.tokenizer._model.index_to_token(i)} {i}\n")
+
+    print("saved to tokens.txt")
+
+    wrapper = ModelWrapper(pipeline.model)
+    wrapper.eval()
+
+    x = torch.rand(1, 16000 * 10)
+    torch.onnx.export(
+        wrapper,
+        x,
+        "model.onnx",
+        opset_version=14,
+        input_names=["x"],
+        output_names=["logits"],
+        dynamic_axes={
+            "x": {0: "N", 1: "num_samples"},
+            "logits": {0: "N", 1: "num_frames"},
+        },
+    )
+
+    meta_data = {
+        "vocab_size": vocab_size,
+        "model_type": "omnilingual-asr",
+        "version": "1",
+        "sample_rate": 16000,
+        "model_author": "facebookresearch",
+        "url": "https://github.com/facebookresearch/omnilingual-asr",
+        "comment": "300M-CTC",
+    }
+
+    add_meta_data("model.onnx", meta_data)
+    print("saved to model.onnx")
+
+    quantize_dynamic(
+        model_input="./model.onnx",
+        model_output="./model.int8.onnx",
+        op_types_to_quantize=["MatMul"],
+        weight_type=QuantType.QUInt8,
+    )
+    print("saved to model.int8.onnx")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/omnilingual-asr/test.py b/scripts/omnilingual-asr/test.py
new file mode 100755
index 00000000..9415902a
--- /dev/null
+++ b/scripts/omnilingual-asr/test.py
@@ -0,0 +1,118 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import time
+
+import numpy as np
+import onnxruntime as ort
+import soundfile as sf
+
+
+def display(sess):
+    print("==========Input==========")
+    for i in sess.get_inputs():
+        print(i)
+    print("==========Output==========")
+    for i in sess.get_outputs():
+        print(i)
+
+
+class OnnxModel:
+    def __init__(
+        self,
+        filename: str,
+    ):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 1
+
+        self.model = ort.InferenceSession(
+            filename,
+            sess_options=session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+        display(self.model)
+
+    def __call__(self, x: np.ndarray):
+        logits = self.model.run(
+            [
+                self.model.get_outputs()[0].name,
+            ],
+            {
+                self.model.get_inputs()[0].name: x,
+            },
+        )[0]
+        # [batch_size, T, vocab_size]
+        return logits
+
+
+def load_tokens():
+    id2token = dict()
+    with open("./tokens.txt", encoding="utf-8") as f:
+        for line in f:
+            fields = line.split()
+            if len(fields) == 1:
+                id2token[int(fields[0])] = " "
+            else:
+                t, idx = fields
+                id2token[int(idx)] = t
+    return id2token
+
+
+def load_audio(filename):
+    samples, sr = sf.read(filename, always_2d=True, dtype="float32")
+    samples = samples[:, 0]  # only use the first channel
+    if sr != 16000:
+        import librosa
+
+        samples = librosa.resample(samples, orig_sr=sr, target_sr=16000)
+    if len(samples) / 16000 > 40:
+        raise ValueError(f"{filename} is too long. Support at most 40 seconds")
+
+    mean = np.mean(samples, axis=0, keepdims=True)
+    var = np.var(samples, axis=0, keepdims=True)
+
+    eps = 1e-5
+    return (samples - mean) / np.sqrt(var + eps)
+
+
+def test(filename, wav_file_list, num_iter=10):
+    id2token = load_tokens()
+    model = OnnxModel(filename)
+
+    for it in range(num_iter):
+        for wav in wav_file_list:
+            print(f"---test {filename} with {wav}----iter---{it}")
+            start = time.time()
+            samples = load_audio(wav)
+
+            logits = model(samples[None])
+            ids = logits[0].argmax(axis=-1)
+            ans = []
+            prev = -1
+            blank = 0
+            for i in ids:
+                if i != blank and i != prev:
+                    ans.append(i)
+                prev = i
+
+            words = [id2token[k] for k in ans]
+            end = time.time()
+            elapsed_seconds = end - start
+            audio_duration = samples.shape[0] / 16000
+            real_time_factor = elapsed_seconds / audio_duration
+
+            print("---> text is----", "".join(words))
+            print(f"RTF: {real_time_factor}")
+            print()
+
+
+def main():
+    wav_file_list = ["./en.wav", "./de.wav", "./es.wav", "./fr.wav"]
+    test("./model.onnx", wav_file_list)
+
+    test("./model.int8.onnx", wav_file_list)
+
+
+if __name__ == "__main__":
+    main()

commit f8d6fe5f3af722a23002926cbac390e6ff44a274
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Nov 12 17:19:43 2025 +0800

    Fix missing includes. (#2769)

diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
index 6562ef5a..979e621c 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
@@ -1,7 +1,9 @@
 // scripts/node-addon-api/src/keyword-spotting.cc
 //
 // Copyright (c)  2024  Xiaomi Corporation
+#include <memory>
 #include <sstream>
+#include <string>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
index 96cb4290..8ef59c4d 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
@@ -1,6 +1,7 @@
 // scripts/node-addon-api/src/non-streaming-asr.cc
 //
 // Copyright (c)  2024  Xiaomi Corporation
+#include <memory>
 #include <sstream>
 
 #include "macros.h"  // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
index cf23fa75..07b9e068 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
@@ -3,7 +3,10 @@
 // Copyright (c)  2024  Xiaomi Corporation
 
 #include <algorithm>
+#include <memory>
 #include <sstream>
+#include <utility>
+#include <vector>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
index 5a847fec..ea3929ea 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
@@ -1,6 +1,8 @@
 // scripts/node-addon-api/src/non-streaming-speech-denoiser.cc
 //
 // Copyright (c)  2025  Xiaomi Corporation
+#include <algorithm>
+#include <memory>
 #include <sstream>
 
 #include "macros.h"  // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
index 65cb798c..f958094c 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
@@ -3,7 +3,10 @@
 // Copyright (c)  2024  Xiaomi Corporation
 
 #include <algorithm>
+#include <memory>
 #include <sstream>
+#include <string>
+#include <vector>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
index 65a27ebc..e8f760de 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
@@ -2,6 +2,7 @@
 //
 // Copyright (c)  2024  Xiaomi Corporation
 #include <sstream>
+#include <string>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
index c2965591..8e34e77e 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
@@ -2,7 +2,9 @@
 //
 // Copyright (c)  2024  Xiaomi Corporation
 #include <algorithm>
+#include <memory>
 #include <sstream>
+#include <string>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
index 0d77139f..a9e8c7e7 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
@@ -3,6 +3,7 @@
 // Copyright (c)  2024  Xiaomi Corporation
 
 #include <sstream>
+#include <string>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
index 0184975e..09808cbb 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
@@ -1,7 +1,9 @@
 // scripts/node-addon-api/src/streaming-asr.cc
 //
 // Copyright (c)  2024  Xiaomi Corporation
+#include <memory>
 #include <sstream>
+#include <string>
 
 #include "macros.h"  // NOLINT
 #include "napi.h"    // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
index 0ab9be74..e9ef8335 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
@@ -3,6 +3,7 @@
 // Copyright (c)  2024  Xiaomi Corporation
 
 #include <algorithm>
+#include <memory>
 #include <sstream>
 
 #include "macros.h"  // NOLINT
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
index 23b3a724..b08a3465 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
@@ -4,6 +4,7 @@
 
 #include <algorithm>
 #include <sstream>
+#include <string>
 
 #include "napi.h"  // NOLINT
 #include "sherpa-onnx/c-api/c-api.h"
diff --git a/scripts/check_style_cpplint.sh b/scripts/check_style_cpplint.sh
index dcadcd99..6e3f2842 100755
--- a/scripts/check_style_cpplint.sh
+++ b/scripts/check_style_cpplint.sh
@@ -26,7 +26,7 @@
 #  ./scripts/check_style_cpplint.sh 2
 
 
-cpplint_version="1.5.4"
+cpplint_version="2.0.2"
 cur_dir=$(cd $(dirname $BASH_SOURCE) && pwd)
 sherpa_onnx_dir=$(cd $cur_dir/.. && pwd)
 
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index 3b8f2e6e..7e42902c 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -6,7 +6,9 @@
 #include <algorithm>
 #include <cstring>
 #include <memory>
+#include <string>
 #include <utility>
+#include <vector>
 
 namespace sherpa_onnx::cxx {
 
diff --git a/sherpa-onnx/csrc/alsa-play.cc b/sherpa-onnx/csrc/alsa-play.cc
index 5602e389..faf49619 100644
--- a/sherpa-onnx/csrc/alsa-play.cc
+++ b/sherpa-onnx/csrc/alsa-play.cc
@@ -7,6 +7,9 @@
 #include "sherpa-onnx/csrc/alsa-play.h"
 
 #include <algorithm>
+#include <cstdio>
+#include <memory>
+#include <vector>
 
 namespace sherpa_onnx {
 
diff --git a/sherpa-onnx/csrc/alsa.cc b/sherpa-onnx/csrc/alsa.cc
index a6576109..d136253a 100644
--- a/sherpa-onnx/csrc/alsa.cc
+++ b/sherpa-onnx/csrc/alsa.cc
@@ -7,6 +7,9 @@
 #include "sherpa-onnx/csrc/alsa.h"
 
 #include <algorithm>
+#include <cstdio>
+#include <memory>
+#include <vector>
 
 #include "alsa/asoundlib.h"
 
diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
index bc081099..54da97b3 100644
--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
@@ -8,7 +8,8 @@
 
 #include <algorithm>
 #include <array>
-#include <mutex>  // NOLINT
+#include <memory>
+#include <mutex>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
index 260b6b9a..9192f0ae 100644
--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
@@ -8,6 +8,7 @@
 
 #include <algorithm>
 #include <array>
+#include <memory>
 #include <mutex>  // NOLINT
 #include <string>
 #include <utility>
diff --git a/sherpa-onnx/csrc/ascend/utils.cc b/sherpa-onnx/csrc/ascend/utils.cc
index 8efca9ba..2977c769 100644
--- a/sherpa-onnx/csrc/ascend/utils.cc
+++ b/sherpa-onnx/csrc/ascend/utils.cc
@@ -4,8 +4,11 @@
 
 #include "sherpa-onnx/csrc/ascend/utils.h"
 
+#include <memory>
 #include <sstream>
+#include <string>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/ascend/macros.h"
 
diff --git a/sherpa-onnx/csrc/audio-tagging-model-config.cc b/sherpa-onnx/csrc/audio-tagging-model-config.cc
index ba68c50e..3456df4f 100644
--- a/sherpa-onnx/csrc/audio-tagging-model-config.cc
+++ b/sherpa-onnx/csrc/audio-tagging-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/audio-tagging-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/audio-tagging.cc b/sherpa-onnx/csrc/audio-tagging.cc
index 966a1920..b86a11c8 100644
--- a/sherpa-onnx/csrc/audio-tagging.cc
+++ b/sherpa-onnx/csrc/audio-tagging.cc
@@ -4,7 +4,9 @@
 
 #include "sherpa-onnx/csrc/audio-tagging.h"
 
+#include <memory>
 #include <string>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/base64-decode.cc b/sherpa-onnx/csrc/base64-decode.cc
index 5723790f..7aa9bd35 100644
--- a/sherpa-onnx/csrc/base64-decode.cc
+++ b/sherpa-onnx/csrc/base64-decode.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/base64-decode.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/csrc/cat.cc b/sherpa-onnx/csrc/cat.cc
index a8d748ba..15cc2f34 100644
--- a/sherpa-onnx/csrc/cat.cc
+++ b/sherpa-onnx/csrc/cat.cc
@@ -9,6 +9,7 @@
 #include <numeric>
 #include <sstream>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/circular-buffer.cc b/sherpa-onnx/csrc/circular-buffer.cc
index 2ba81807..4af756c6 100644
--- a/sherpa-onnx/csrc/circular-buffer.cc
+++ b/sherpa-onnx/csrc/circular-buffer.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/circular-buffer.h"
 
 #include <algorithm>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/context-graph-test.cc b/sherpa-onnx/csrc/context-graph-test.cc
index 5c45b69e..2da44ac4 100644
--- a/sherpa-onnx/csrc/context-graph-test.cc
+++ b/sherpa-onnx/csrc/context-graph-test.cc
@@ -4,11 +4,12 @@
 
 #include "sherpa-onnx/csrc/context-graph.h"
 
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <cmath>
 #include <map>
 #include <random>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "gtest/gtest.h"
diff --git a/sherpa-onnx/csrc/context-graph.cc b/sherpa-onnx/csrc/context-graph.cc
index 336208b1..f3da0ab6 100644
--- a/sherpa-onnx/csrc/context-graph.cc
+++ b/sherpa-onnx/csrc/context-graph.cc
@@ -6,10 +6,12 @@
 
 #include <algorithm>
 #include <cassert>
+#include <memory>
 #include <queue>
 #include <string>
 #include <tuple>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/fast-clustering-test.cc b/sherpa-onnx/csrc/fast-clustering-test.cc
index aea4be55..489b0116 100644
--- a/sherpa-onnx/csrc/fast-clustering-test.cc
+++ b/sherpa-onnx/csrc/fast-clustering-test.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/fast-clustering.h"
 
+#include <iostream>
 #include <vector>
 
 #include "gtest/gtest.h"
diff --git a/sherpa-onnx/csrc/features.cc b/sherpa-onnx/csrc/features.cc
index 41848abc..47d12dad 100644
--- a/sherpa-onnx/csrc/features.cc
+++ b/sherpa-onnx/csrc/features.cc
@@ -6,8 +6,9 @@
 
 #include <algorithm>
 #include <memory>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <sstream>
+#include <string>
 #include <vector>
 
 #include "kaldi-native-fbank/csrc/online-feature.h"
diff --git a/sherpa-onnx/csrc/file-utils.cc b/sherpa-onnx/csrc/file-utils.cc
index 25be9bc9..b8361445 100644
--- a/sherpa-onnx/csrc/file-utils.cc
+++ b/sherpa-onnx/csrc/file-utils.cc
@@ -8,6 +8,7 @@
 #include <memory>
 #include <sstream>
 #include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/fst-utils.cc b/sherpa-onnx/csrc/fst-utils.cc
index 5fcf5235..cda51ffb 100644
--- a/sherpa-onnx/csrc/fst-utils.cc
+++ b/sherpa-onnx/csrc/fst-utils.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/fst-utils.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/csrc/hifigan-vocoder.cc b/sherpa-onnx/csrc/hifigan-vocoder.cc
index 6703449f..05d896f6 100644
--- a/sherpa-onnx/csrc/hifigan-vocoder.cc
+++ b/sherpa-onnx/csrc/hifigan-vocoder.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/hifigan-vocoder.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/homophone-replacer.cc b/sherpa-onnx/csrc/homophone-replacer.cc
index 550b66ec..9dd223d5 100644
--- a/sherpa-onnx/csrc/homophone-replacer.cc
+++ b/sherpa-onnx/csrc/homophone-replacer.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/homophone-replacer.h"
 
 #include <fstream>
+#include <memory>
 #include <sstream>
 #include <string>
 #include <strstream>
diff --git a/sherpa-onnx/csrc/hypothesis.cc b/sherpa-onnx/csrc/hypothesis.cc
index ea332bcb..b6463a12 100644
--- a/sherpa-onnx/csrc/hypothesis.cc
+++ b/sherpa-onnx/csrc/hypothesis.cc
@@ -7,6 +7,7 @@
 
 #include <algorithm>
 #include <utility>
+#include <vector>
 
 namespace sherpa_onnx {
 
diff --git a/sherpa-onnx/csrc/keyword-spotter-impl.cc b/sherpa-onnx/csrc/keyword-spotter-impl.cc
index b6900e30..350830a7 100644
--- a/sherpa-onnx/csrc/keyword-spotter-impl.cc
+++ b/sherpa-onnx/csrc/keyword-spotter-impl.cc
@@ -4,12 +4,7 @@
 
 #include "sherpa-onnx/csrc/keyword-spotter-impl.h"
 
-#include "sherpa-onnx/csrc/keyword-spotter-transducer-impl.h"
-#include "sherpa-onnx/csrc/macros.h"
-
-#if SHERPA_ONNX_ENABLE_RKNN
-#include "sherpa-onnx/csrc/rknn/keyword-spotter-transducer-rknn-impl.h"
-#endif
+#include <memory>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
@@ -20,6 +15,13 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
+#include "sherpa-onnx/csrc/keyword-spotter-transducer-impl.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+#if SHERPA_ONNX_ENABLE_RKNN
+#include "sherpa-onnx/csrc/rknn/keyword-spotter-transducer-rknn-impl.h"
+#endif
+
 namespace sherpa_onnx {
 
 std::unique_ptr<KeywordSpotterImpl> KeywordSpotterImpl::Create(
diff --git a/sherpa-onnx/csrc/keyword-spotter.cc b/sherpa-onnx/csrc/keyword-spotter.cc
index 615aab9c..ca8ecd11 100644
--- a/sherpa-onnx/csrc/keyword-spotter.cc
+++ b/sherpa-onnx/csrc/keyword-spotter.cc
@@ -10,6 +10,7 @@
 #include <iomanip>
 #include <memory>
 #include <sstream>
+#include <string>
 #include <utility>
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
index 77cb5123..8bf7bcac 100644
--- a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+++ b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
@@ -6,7 +6,7 @@
 
 #include <codecvt>
 #include <fstream>
-#include <regex>  // NOLINT
+#include <regex>
 #include <sstream>
 #include <string>
 #include <strstream>
@@ -25,8 +25,8 @@
 #endif
 
 #include "espeak-ng/speak_lib.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/phrase-matcher.h"
@@ -565,7 +565,7 @@ KokoroMultiLangLexicon::KokoroMultiLangLexicon(
     const std::string &data_dir, const OfflineTtsKokoroModelMetaData &meta_data,
     bool debug)
     : impl_(std::make_unique<Impl>(tokens, lexicon, data_dir, meta_data,
-                                   debug)) {}
+                                   debug)) {}  // NOLINT
 
 template <typename Manager>
 KokoroMultiLangLexicon::KokoroMultiLangLexicon(
@@ -573,7 +573,7 @@ KokoroMultiLangLexicon::KokoroMultiLangLexicon(
     const std::string &data_dir, const OfflineTtsKokoroModelMetaData &meta_data,
     bool debug)
     : impl_(std::make_unique<Impl>(mgr, tokens, lexicon, data_dir, meta_data,
-                                   debug)) {}
+                                   debug)) {}  // NOLINT
 
 std::vector<TokenIDs> KokoroMultiLangLexicon::ConvertTextToTokenIds(
     const std::string &text, const std::string &voice /*= ""*/) const {
diff --git a/sherpa-onnx/csrc/lexicon.cc b/sherpa-onnx/csrc/lexicon.cc
index e1ebaa3c..620a2a3c 100644
--- a/sherpa-onnx/csrc/lexicon.cc
+++ b/sherpa-onnx/csrc/lexicon.cc
@@ -10,8 +10,11 @@
 #include <iomanip>
 #include <memory>
 #include <sstream>
+#include <string>
 #include <strstream>
+#include <unordered_map>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/lodr-fst.cc b/sherpa-onnx/csrc/lodr-fst.cc
index a5d0d218..253352cc 100644
--- a/sherpa-onnx/csrc/lodr-fst.cc
+++ b/sherpa-onnx/csrc/lodr-fst.cc
@@ -5,13 +5,18 @@
 //
 // Copyright (c)  2025 Tilde SIA (Askars Salimbajevs)
 
+#include "sherpa-onnx/csrc/lodr-fst.h"
+
 #include <algorithm>
+#include <limits>
+#include <memory>
+#include <string>
+#include <unordered_map>
 #include <utility>
 #include <vector>
 
-#include "sherpa-onnx/csrc/lodr-fst.h"
-#include "sherpa-onnx/csrc/log.h"
 #include "sherpa-onnx/csrc/hypothesis.h"
+#include "sherpa-onnx/csrc/log.h"
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
@@ -21,8 +26,8 @@ int32_t LodrFst::FindBackoffId() {
 
   for (int32_t state = 0; state < fst_->NumStates(); ++state) {
     fst::ArcIterator<fst::StdConstFst> arc_iter(*fst_, state);
-    for ( ; !arc_iter.Done(); arc_iter.Next()) {
-      const auto& arc = arc_iter.Value();
+    for (; !arc_iter.Done(); arc_iter.Next()) {
+      const auto &arc = arc_iter.Value();
       if (arc.olabel == 0) {  // Check if the output label is epsilon (0)
         return arc.ilabel;    // Return the input label
       }
@@ -35,7 +40,7 @@ int32_t LodrFst::FindBackoffId() {
 LodrFst::LodrFst(const std::string &fst_path, int32_t backoff_id)
     : backoff_id_(backoff_id) {
   fst_ = std::unique_ptr<fst::StdConstFst>(
-    CastOrConvertToConstFst(fst::StdVectorFst::Read(fst_path)));
+      CastOrConvertToConstFst(fst::StdVectorFst::Read(fst_path)));
 
   if (backoff_id < 0) {
     // backoff_id_ is not provided, find it automatically
@@ -49,7 +54,7 @@ LodrFst::LodrFst(const std::string &fst_path, int32_t backoff_id)
 }
 
 std::vector<std::tuple<int32_t, float>> LodrFst::ProcessBackoffArcs(
-  int32_t state, float cost) {
+    int32_t state, float cost) {
   std::vector<std::tuple<int32_t, float>> ans;
   auto next = GetNextStatesCostsNoBackoff(state, backoff_id_);
   if (!next.has_value()) {
@@ -63,7 +68,7 @@ std::vector<std::tuple<int32_t, float>> LodrFst::ProcessBackoffArcs(
 }
 
 std::optional<std::tuple<int32_t, float>> LodrFst::GetNextStatesCostsNoBackoff(
-  int32_t state, int32_t label) {
+    int32_t state, int32_t label) {
   fst::ArcIterator<fst::StdConstFst> arc_iter(*fst_, state);
   int32_t num_arcs = fst_->NumArcs(state);
 
@@ -84,12 +89,12 @@ std::optional<std::tuple<int32_t, float>> LodrFst::GetNextStatesCostsNoBackoff(
 }
 
 std::pair<std::vector<int32_t>, std::vector<float>> LodrFst::GetNextStateCosts(
-  int32_t state, int32_t label) {
+    int32_t state, int32_t label) {
   std::vector<int32_t> states = {state};
   std::vector<float> costs = {0};
 
   auto extra_states_costs = ProcessBackoffArcs(state, 0);
-  for (const auto& [s, c] : extra_states_costs) {
+  for (const auto &[s, c] : extra_states_costs) {
     states.push_back(s);
     costs.push_back(c);
   }
@@ -140,7 +145,7 @@ float LodrFst::GetFinalCost(int32_t state) {
 }
 
 LodrStateCost::LodrStateCost(
-    LodrFst* fst, const std::unordered_map<int32_t, float> &state_cost)
+    LodrFst *fst, const std::unordered_map<int32_t, float> &state_cost)
     : fst_(fst) {
   if (state_cost.empty()) {
     state_cost_[0] = 0.0;
@@ -151,7 +156,7 @@ LodrStateCost::LodrStateCost(
 
 LodrStateCost LodrStateCost::ForwardOneStep(int32_t label) {
   std::unordered_map<int32_t, float> state_cost;
-  for (const auto& [s, c] : state_cost_) {
+  for (const auto &[s, c] : state_cost_) {
     auto [next_states, next_costs] = fst_->GetNextStateCosts(s, label);
     for (size_t i = 0; i < next_states.size(); ++i) {
       int32_t ns = next_states[i];
@@ -169,10 +174,9 @@ float LodrStateCost::Score() const {
   if (state_cost_.empty()) {
     return -std::numeric_limits<float>::infinity();
   }
-  auto min_cost = std::min_element(state_cost_.begin(), state_cost_.end(),
-                                   [](const auto& a, const auto& b) {
-                                     return a.second < b.second;
-                                   });
+  auto min_cost = std::min_element(
+      state_cost_.begin(), state_cost_.end(),
+      [](const auto &a, const auto &b) { return a.second < b.second; });
   return -min_cost->second;
 }
 
@@ -180,12 +184,10 @@ float LodrStateCost::FinalScore() const {
   if (state_cost_.empty()) {
     return -std::numeric_limits<float>::infinity();
   }
-  auto min_cost = std::min_element(state_cost_.begin(), state_cost_.end(),
-                                   [](const auto& a, const auto& b) {
-                                     return a.second < b.second;
-                                   });
-  return -(min_cost->second +
-           fst_->GetFinalCost(min_cost->first));
+  auto min_cost = std::min_element(
+      state_cost_.begin(), state_cost_.end(),
+      [](const auto &a, const auto &b) { return a.second < b.second; });
+  return -(min_cost->second + fst_->GetFinalCost(min_cost->first));
 }
 
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index b557fe08..b5247d6e 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -27,8 +27,8 @@
 #endif
 
 #include "espeak-ng/speak_lib.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-canary-model-config.cc b/sherpa-onnx/csrc/offline-canary-model-config.cc
index 2821c10d..2910d45e 100644
--- a/sherpa-onnx/csrc/offline-canary-model-config.cc
+++ b/sherpa-onnx/csrc/offline-canary-model-config.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-canary-model-config.h"
 
 #include <sstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/offline-canary-model.cc b/sherpa-onnx/csrc/offline-canary-model.cc
index 37471420..5ae52011 100644
--- a/sherpa-onnx/csrc/offline-canary-model.cc
+++ b/sherpa-onnx/csrc/offline-canary-model.cc
@@ -6,10 +6,12 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
 #include <tuple>
 #include <unordered_map>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/offline-canary-model-meta-data.h"
 
diff --git a/sherpa-onnx/csrc/offline-ced-model.cc b/sherpa-onnx/csrc/offline-ced-model.cc
index 241f03b3..b7d03d5a 100644
--- a/sherpa-onnx/csrc/offline-ced-model.cc
+++ b/sherpa-onnx/csrc/offline-ced-model.cc
@@ -4,7 +4,9 @@
 
 #include "sherpa-onnx/csrc/offline-ced-model.h"
 
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-ct-transformer-model.cc b/sherpa-onnx/csrc/offline-ct-transformer-model.cc
index ee016285..320a3934 100644
--- a/sherpa-onnx/csrc/offline-ct-transformer-model.cc
+++ b/sherpa-onnx/csrc/offline-ct-transformer-model.cc
@@ -4,7 +4,9 @@
 
 #include "sherpa-onnx/csrc/offline-ct-transformer-model.h"
 
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc b/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
index ca3dcaee..225eaf54 100644
--- a/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
+++ b/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
@@ -6,6 +6,7 @@
 
 #include <string>
 #include <utility>
+#include <vector>
 
 #include "fst/fstlib.h"
 #include "kaldi-decoder/csrc/decodable-ctc.h"
diff --git a/sherpa-onnx/csrc/offline-dolphin-model-config.cc b/sherpa-onnx/csrc/offline-dolphin-model-config.cc
index 03f4cb57..96ad7e91 100644
--- a/sherpa-onnx/csrc/offline-dolphin-model-config.cc
+++ b/sherpa-onnx/csrc/offline-dolphin-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-dolphin-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-dolphin-model.cc b/sherpa-onnx/csrc/offline-dolphin-model.cc
index b8abd5b3..843a0127 100644
--- a/sherpa-onnx/csrc/offline-dolphin-model.cc
+++ b/sherpa-onnx/csrc/offline-dolphin-model.cc
@@ -5,8 +5,10 @@
 #include "sherpa-onnx/csrc/offline-dolphin-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
index 07e4b875..c27ac0b1 100644
--- a/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <tuple>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc b/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
index 53eb9337..a4947077 100644
--- a/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
+++ b/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
index f2103892..b5677f06 100644
--- a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
+++ b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
@@ -6,10 +6,12 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
 #include <tuple>
 #include <unordered_map>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-lm.cc b/sherpa-onnx/csrc/offline-lm.cc
index a452915c..fab209d0 100644
--- a/sherpa-onnx/csrc/offline-lm.cc
+++ b/sherpa-onnx/csrc/offline-lm.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-lm.h"
 
 #include <algorithm>
+#include <memory>
 #include <utility>
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
index 3e2d77f6..dba09023 100644
--- a/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
@@ -6,6 +6,7 @@
 
 #include <algorithm>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-moonshine-model-config.cc b/sherpa-onnx/csrc/offline-moonshine-model-config.cc
index c687507e..93743fb0 100644
--- a/sherpa-onnx/csrc/offline-moonshine-model-config.cc
+++ b/sherpa-onnx/csrc/offline-moonshine-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-moonshine-model.cc b/sherpa-onnx/csrc/offline-moonshine-model.cc
index 7c66b351..59ef8483 100644
--- a/sherpa-onnx/csrc/offline-moonshine-model.cc
+++ b/sherpa-onnx/csrc/offline-moonshine-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-moonshine-model.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
index 9ea26f81..fd5ef33f 100644
--- a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
index 7759c101..6eaa8bf2 100644
--- a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
@@ -4,6 +4,11 @@
 
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
 
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
index b4d0ee9a..e71b77bb 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-paraformer-model-config.h"
 
+#include <memory>
 #include <string>
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/offline-paraformer-model.cc b/sherpa-onnx/csrc/offline-paraformer-model.cc
index 5b8586ef..1ff5c640 100644
--- a/sherpa-onnx/csrc/offline-paraformer-model.cc
+++ b/sherpa-onnx/csrc/offline-paraformer-model.cc
@@ -5,8 +5,10 @@
 #include "sherpa-onnx/csrc/offline-paraformer-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-punctuation-impl.cc b/sherpa-onnx/csrc/offline-punctuation-impl.cc
index ed943fcc..002127ef 100644
--- a/sherpa-onnx/csrc/offline-punctuation-impl.cc
+++ b/sherpa-onnx/csrc/offline-punctuation-impl.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-punctuation-impl.h"
 
+#include <memory>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/offline-punctuation-model-config.cc b/sherpa-onnx/csrc/offline-punctuation-model-config.cc
index e98fe00b..d5ed624c 100644
--- a/sherpa-onnx/csrc/offline-punctuation-model-config.cc
+++ b/sherpa-onnx/csrc/offline-punctuation-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-punctuation-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-punctuation.cc b/sherpa-onnx/csrc/offline-punctuation.cc
index 140e7203..ccaa8119 100644
--- a/sherpa-onnx/csrc/offline-punctuation.cc
+++ b/sherpa-onnx/csrc/offline-punctuation.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-punctuation.h"
 
+#include <string>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index a5482bef..c283c4ae 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
 
+#include <memory>
 #include <string>
 #include <strstream>
 #include <utility>
diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
index 1d2271f2..8a02fc16 100644
--- a/sherpa-onnx/csrc/offline-recognizer.cc
+++ b/sherpa-onnx/csrc/offline-recognizer.cc
@@ -5,6 +5,8 @@
 #include "sherpa-onnx/csrc/offline-recognizer.h"
 
 #include <memory>
+#include <string>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-rnn-lm.cc b/sherpa-onnx/csrc/offline-rnn-lm.cc
index bdc2f903..5a92c796 100644
--- a/sherpa-onnx/csrc/offline-rnn-lm.cc
+++ b/sherpa-onnx/csrc/offline-rnn-lm.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-rnn-lm.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
index fc9884dd..cc18a11a 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-sense-voice-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.cc b/sherpa-onnx/csrc/offline-sense-voice-model.cc
index 95664e22..588cdd37 100644
--- a/sherpa-onnx/csrc/offline-sense-voice-model.cc
+++ b/sherpa-onnx/csrc/offline-sense-voice-model.cc
@@ -5,8 +5,10 @@
 #include "sherpa-onnx/csrc/offline-sense-voice-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-source-separation-impl.cc b/sherpa-onnx/csrc/offline-source-separation-impl.cc
index 3a68ab2a..ec2da1fe 100644
--- a/sherpa-onnx/csrc/offline-source-separation-impl.cc
+++ b/sherpa-onnx/csrc/offline-source-separation-impl.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <memory>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-source-separation-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-model-config.cc
index 00dcbb8e..50a0f8c7 100644
--- a/sherpa-onnx/csrc/offline-source-separation-model-config.cc
+++ b/sherpa-onnx/csrc/offline-source-separation-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-source-separation-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
index 0dc3ee6e..873d9977 100644
--- a/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
+++ b/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc b/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
index e3c16511..0dee12be 100644
--- a/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
+++ b/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
@@ -171,7 +171,7 @@ class OfflineSourceSeparationSpleeterModel::Impl {
 };
 
 OfflineSourceSeparationSpleeterModel::~OfflineSourceSeparationSpleeterModel() =
-    default;
+    default;  // NOLINT
 
 OfflineSourceSeparationSpleeterModel::OfflineSourceSeparationSpleeterModel(
     const OfflineSourceSeparationModelConfig &config)
diff --git a/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
index f95ea307..eaae2833 100644
--- a/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
+++ b/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-source-separation-uvr-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-source-separation.cc b/sherpa-onnx/csrc/offline-source-separation.cc
index d352d9ab..acee51fc 100644
--- a/sherpa-onnx/csrc/offline-source-separation.cc
+++ b/sherpa-onnx/csrc/offline-source-separation.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-source-separation.h"
 
 #include <memory>
+#include <string>
 
 #include "sherpa-onnx/csrc/offline-source-separation-impl.h"
 
diff --git a/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h b/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
index 8573c749..e0a890d7 100644
--- a/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
+++ b/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
@@ -41,11 +41,11 @@ struct PairHash {
 };
 }  // namespace
 
-using Matrix2D =
-    Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+using Matrix2D = Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
+                               Eigen::RowMajor>;  // NOLINT
 
-using Matrix2DInt32 =
-    Eigen::Matrix<int32_t, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+using Matrix2DInt32 = Eigen::Matrix<int32_t, Eigen::Dynamic, Eigen::Dynamic,
+                                    Eigen::RowMajor>;  // NOLINT
 
 using FloatRowVector = Eigen::Matrix<float, 1, Eigen::Dynamic>;
 using Int32RowVector = Eigen::Matrix<int32_t, 1, Eigen::Dynamic>;
diff --git a/sherpa-onnx/csrc/offline-speaker-diarization-result.cc b/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
index 59695728..6a600f4d 100644
--- a/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
+++ b/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
@@ -6,10 +6,12 @@
 
 #include <algorithm>
 #include <array>
+#include <cstdio>
 #include <sstream>
 #include <string>
 #include <unordered_set>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc b/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
index 53671ec4..6fb39849 100644
--- a/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
+++ b/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
@@ -112,18 +113,18 @@ class OfflineSpeakerSegmentationPyannoteModel::Impl {
 };
 
 OfflineSpeakerSegmentationPyannoteModel::
-    OfflineSpeakerSegmentationPyannoteModel(
+    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
         const OfflineSpeakerSegmentationModelConfig &config)
-    : impl_(std::make_unique<Impl>(config)) {}
+    : impl_(std::make_unique<Impl>(config)) {}  // NOLINT
 
 template <typename Manager>
 OfflineSpeakerSegmentationPyannoteModel::
-    OfflineSpeakerSegmentationPyannoteModel(
+    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
         Manager *mgr, const OfflineSpeakerSegmentationModelConfig &config)
-    : impl_(std::make_unique<Impl>(mgr, config)) {}
+    : impl_(std::make_unique<Impl>(mgr, config)) {}  // NOLINT
 
 OfflineSpeakerSegmentationPyannoteModel::
-    ~OfflineSpeakerSegmentationPyannoteModel() = default;
+    ~OfflineSpeakerSegmentationPyannoteModel() = default;  // NOLINT
 
 const OfflineSpeakerSegmentationPyannoteModelMetaData &
 OfflineSpeakerSegmentationPyannoteModel::GetModelMetaData() const {
@@ -137,14 +138,14 @@ Ort::Value OfflineSpeakerSegmentationPyannoteModel::Forward(
 
 #if __ANDROID_API__ >= 9
 template OfflineSpeakerSegmentationPyannoteModel::
-    OfflineSpeakerSegmentationPyannoteModel(
+    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
         AAssetManager *mgr,
         const OfflineSpeakerSegmentationModelConfig &config);
 #endif
 
 #if __OHOS__
 template OfflineSpeakerSegmentationPyannoteModel::
-    OfflineSpeakerSegmentationPyannoteModel(
+    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
         NativeResourceManager *mgr,
         const OfflineSpeakerSegmentationModelConfig &config);
 #endif
diff --git a/sherpa-onnx/csrc/offline-speech-denoiser.cc b/sherpa-onnx/csrc/offline-speech-denoiser.cc
index afdd4d9f..1ef56500 100644
--- a/sherpa-onnx/csrc/offline-speech-denoiser.cc
+++ b/sherpa-onnx/csrc/offline-speech-denoiser.cc
@@ -4,7 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-speech-denoiser.h"
 
-#include "sherpa-onnx/csrc/offline-speech-denoiser-impl.h"
+#include <string>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
@@ -15,6 +15,8 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
+#include "sherpa-onnx/csrc/offline-speech-denoiser-impl.h"
+
 namespace sherpa_onnx {
 
 void OfflineSpeechDenoiserConfig::Register(ParseOptions *po) {
diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
index 0e7fe3b9..9c5d1088 100644
--- a/sherpa-onnx/csrc/offline-stream.cc
+++ b/sherpa-onnx/csrc/offline-stream.cc
@@ -9,7 +9,10 @@
 #include <cmath>
 #include <iomanip>
 #include <limits>
+#include <memory>
+#include <string>
 #include <utility>
+#include <vector>
 
 #include "kaldi-native-fbank/csrc/online-feature.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc b/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
index ca23210f..bc2a5d4b 100644
--- a/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
@@ -4,7 +4,10 @@
 
 #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
 
+#include <memory>
+#include <string>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-tdnn-model-config.cc b/sherpa-onnx/csrc/offline-tdnn-model-config.cc
index be1b11cd..b8d3dced 100644
--- a/sherpa-onnx/csrc/offline-tdnn-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tdnn-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-tdnn-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc b/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
index f6b2574b..c1c38a21 100644
--- a/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
@@ -4,6 +4,11 @@
 
 #include "sherpa-onnx/csrc/offline-telespeech-ctc-model.h"
 
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
index 6fd3bf40..98fe78da 100644
--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <iterator>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/packed-sequence.h"
diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
index e39e3154..4b338d26 100644
--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
+++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <iterator>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-transducer-model-config.cc b/sherpa-onnx/csrc/offline-transducer-model-config.cc
index 72fcfefb..0d99776c 100644
--- a/sherpa-onnx/csrc/offline-transducer-model-config.cc
+++ b/sherpa-onnx/csrc/offline-transducer-model-config.cc
@@ -4,6 +4,7 @@
 #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
 
 #include <sstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/offline-transducer-model.cc b/sherpa-onnx/csrc/offline-transducer-model.cc
index a08854fe..0ee94da6 100644
--- a/sherpa-onnx/csrc/offline-transducer-model.cc
+++ b/sherpa-onnx/csrc/offline-transducer-model.cc
@@ -5,7 +5,9 @@
 #include "sherpa-onnx/csrc/offline-transducer-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #if __ANDROID_API__ >= 9
diff --git a/sherpa-onnx/csrc/offline-transducer-nemo-model.cc b/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
index 82a973cb..01e73926 100644
--- a/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
+++ b/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-transducer-nemo-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-tts-character-frontend.cc b/sherpa-onnx/csrc/offline-tts-character-frontend.cc
index 968e287c..6a1df6e8 100644
--- a/sherpa-onnx/csrc/offline-tts-character-frontend.cc
+++ b/sherpa-onnx/csrc/offline-tts-character-frontend.cc
@@ -7,9 +7,13 @@
 #include <codecvt>
 #include <fstream>
 #include <locale>
+#include <memory>
 #include <sstream>
+#include <string>
 #include <strstream>
+#include <unordered_map>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc b/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
index 690c72e6..2534b1be 100644
--- a/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-tts-kitten-model-config.h"
 
+#include <string>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-tts-kitten-model.cc b/sherpa-onnx/csrc/offline-tts-kitten-model.cc
index f8497e1a..d1d857f9 100644
--- a/sherpa-onnx/csrc/offline-tts-kitten-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-kitten-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-tts-kitten-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc b/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
index b6481bb2..06ab74d2 100644
--- a/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-tts-kokoro-model-config.h"
 
+#include <string>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-tts-kokoro-model.cc b/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
index 599aa64c..d973d7ce 100644
--- a/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-tts-kokoro-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
index 9b379f44..90cf6289 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-tts-matcha-model-config.h"
 
+#include <string>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model.cc b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
index 316a8a5f..b0e42deb 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-tts-matcha-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-tts-model-config.cc b/sherpa-onnx/csrc/offline-tts-model-config.cc
index 8d804dc9..df7bf06f 100644
--- a/sherpa-onnx/csrc/offline-tts-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-tts-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/csrc/offline-tts-vits-model-config.cc b/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
index cd638f64..66face5c 100644
--- a/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-tts-vits-model-config.h"
 
+#include <string>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-tts-vits-model.cc b/sherpa-onnx/csrc/offline-tts-vits-model.cc
index d8bf7325..af2e090d 100644
--- a/sherpa-onnx/csrc/offline-tts-vits-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-vits-model.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/offline-tts-vits-model.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
index be81068e..0cf582c7 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
@@ -4,10 +4,13 @@
 
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
 
+#include <string>
+#include <vector>
+
 #include "espeak-ng/speak_lib.h"
 #include "gtest/gtest.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
index 12714a44..075b8d0c 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
@@ -7,10 +7,14 @@
 #include <codecvt>
 #include <fstream>
 #include <locale>
-#include <regex>  // NOLINT
+#include <memory>
+#include <regex>
 #include <sstream>
+#include <string>
 #include <strstream>
+#include <unordered_map>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
@@ -23,8 +27,8 @@
 
 #include "cppinyin/csrc/cppinyin.h"
 #include "espeak-ng/speak_lib.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
index a5291ffa..453bd6f6 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
 
+#include <string>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
index 4ff7d833..05f324c9 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
@@ -7,6 +7,7 @@
 #include <algorithm>
 #include <cstring>
 #include <iostream>
+#include <memory>
 #include <random>
 #include <string>
 #include <utility>
diff --git a/sherpa-onnx/csrc/offline-websocket-server-impl.cc b/sherpa-onnx/csrc/offline-websocket-server-impl.cc
index b34ebcaa..80f43938 100644
--- a/sherpa-onnx/csrc/offline-websocket-server-impl.cc
+++ b/sherpa-onnx/csrc/offline-websocket-server-impl.cc
@@ -5,6 +5,11 @@
 #include "sherpa-onnx/csrc/offline-websocket-server-impl.h"
 
 #include <algorithm>
+#include <iostream>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 
@@ -44,7 +49,7 @@ void OfflineWebsocketDecoderConfig::Validate() const {
 OfflineWebsocketDecoder::OfflineWebsocketDecoder(OfflineWebsocketServer *server)
     : config_(server->GetConfig().decoder_config),
       server_(server),
-      recognizer_(config_.recognizer_config) {}
+      recognizer_(config_.recognizer_config) {}  // NOLINT
 
 void OfflineWebsocketDecoder::Push(connection_hdl hdl, ConnectionDataPtr d) {
   std::lock_guard<std::mutex> lock(mutex_);
diff --git a/sherpa-onnx/csrc/offline-websocket-server.cc b/sherpa-onnx/csrc/offline-websocket-server.cc
index eb55413d..4be72b05 100644
--- a/sherpa-onnx/csrc/offline-websocket-server.cc
+++ b/sherpa-onnx/csrc/offline-websocket-server.cc
@@ -2,7 +2,9 @@
 //
 // Copyright (c)  2022-2023  Xiaomi Corporation
 
-#include "asio.hpp"
+#include <vector>
+
+#include "asio.hpp"  // NOLINT
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/offline-websocket-server-impl.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc b/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
index 2493971a..b65d1d24 100644
--- a/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-wenet-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-wenet-ctc-model.cc b/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
index 31ffe5b6..e7dd0684 100644
--- a/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
@@ -4,6 +4,11 @@
 
 #include "sherpa-onnx/csrc/offline-wenet-ctc-model.h"
 
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
index ad76a31e..5dbb7697 100644
--- a/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
+++ b/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
@@ -6,6 +6,7 @@
 
 #include <algorithm>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-whisper-model-config.cc b/sherpa-onnx/csrc/offline-whisper-model-config.cc
index 708531f7..6afa1f51 100644
--- a/sherpa-onnx/csrc/offline-whisper-model-config.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-whisper-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
index 5443d37a..3921c3b0 100644
--- a/sherpa-onnx/csrc/offline-whisper-model.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model.cc
@@ -6,10 +6,12 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
 #include <tuple>
 #include <unordered_map>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
@@ -405,13 +407,13 @@ const std::vector<int32_t> &OfflineWhisperModel::GetAllLanguageIDs() const {
   return impl_->GetAllLanguageIDs();
 }
 
-const std::unordered_map<std::string, int32_t>
-    &OfflineWhisperModel::GetLang2ID() const {
+const std::unordered_map<std::string, int32_t> &
+OfflineWhisperModel::GetLang2ID() const {
   return impl_->GetLang2ID();
 }
 
-const std::unordered_map<int32_t, std::string>
-    &OfflineWhisperModel::GetID2Lang() const {
+const std::unordered_map<int32_t, std::string> &
+OfflineWhisperModel::GetID2Lang() const {
   return impl_->GetID2Lang();
 }
 
diff --git a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
index 633bfac8..7d5f8a06 100644
--- a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
@@ -4,6 +4,10 @@
 
 #include "sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.h"
 
+#include <memory>
+#include <string>
+#include <utility>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
index f464074e..3a19d4cf 100644
--- a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
@@ -4,7 +4,9 @@
 
 #include "sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.h"
 
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
index e03e841f..fd5e0321 100644
--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
index 356537e9..4a51e179 100644
--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
+++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
@@ -4,7 +4,10 @@
 
 #include "sherpa-onnx/csrc/offline-zipformer-ctc-model.h"
 
+#include <memory>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/online-cnn-bilstm-model.cc b/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
index 2ca270ca..b2489831 100644
--- a/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
+++ b/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
@@ -4,7 +4,9 @@
 
 #include "sherpa-onnx/csrc/online-cnn-bilstm-model.h"
 
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/online-lm.cc b/sherpa-onnx/csrc/online-lm.cc
index dfec00cc..1aaec374 100644
--- a/sherpa-onnx/csrc/online-lm.cc
+++ b/sherpa-onnx/csrc/online-lm.cc
@@ -6,6 +6,7 @@
 #include "sherpa-onnx/csrc/online-lm.h"
 
 #include <algorithm>
+#include <memory>
 #include <utility>
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc b/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
index c3c22b97..4c8ae94c 100644
--- a/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-nemo-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-nemo-ctc-model.cc b/sherpa-onnx/csrc/online-nemo-ctc-model.cc
index bbe4f1c7..dfce793d 100644
--- a/sherpa-onnx/csrc/online-nemo-ctc-model.cc
+++ b/sherpa-onnx/csrc/online-nemo-ctc-model.cc
@@ -6,7 +6,10 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/online-paraformer-model-config.cc b/sherpa-onnx/csrc/online-paraformer-model-config.cc
index 25a99262..a2ee8029 100644
--- a/sherpa-onnx/csrc/online-paraformer-model-config.cc
+++ b/sherpa-onnx/csrc/online-paraformer-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-paraformer-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-paraformer-model.cc b/sherpa-onnx/csrc/online-paraformer-model.cc
index e75b70a4..5e71c019 100644
--- a/sherpa-onnx/csrc/online-paraformer-model.cc
+++ b/sherpa-onnx/csrc/online-paraformer-model.cc
@@ -6,7 +6,10 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/online-punctuation-impl.cc b/sherpa-onnx/csrc/online-punctuation-impl.cc
index ebdbc848..84031354 100644
--- a/sherpa-onnx/csrc/online-punctuation-impl.cc
+++ b/sherpa-onnx/csrc/online-punctuation-impl.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-punctuation-impl.h"
 
+#include <memory>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/online-punctuation-model-config.cc b/sherpa-onnx/csrc/online-punctuation-model-config.cc
index 5dab600f..c7bc0610 100644
--- a/sherpa-onnx/csrc/online-punctuation-model-config.cc
+++ b/sherpa-onnx/csrc/online-punctuation-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-punctuation-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-punctuation.cc b/sherpa-onnx/csrc/online-punctuation.cc
index 6435b1c4..a7f83c9b 100644
--- a/sherpa-onnx/csrc/online-punctuation.cc
+++ b/sherpa-onnx/csrc/online-punctuation.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-punctuation.h"
 
+#include <string>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/online-recognizer-impl.cc b/sherpa-onnx/csrc/online-recognizer-impl.cc
index 7b96c5b8..1397a8ed 100644
--- a/sherpa-onnx/csrc/online-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/online-recognizer-impl.cc
@@ -4,8 +4,11 @@
 
 #include "sherpa-onnx/csrc/online-recognizer-impl.h"
 
+#include <memory>
+#include <string>
 #include <strstream>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
@@ -244,8 +247,8 @@ OnlineRecognizerImpl::OnlineRecognizerImpl(Manager *mgr,
         itn_list_.push_back(
             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
       }  // for (; !reader->Done(); reader->Next())
-    }    // for (const auto &f : files)
-  }      // if (!config.rule_fars.empty())
+    }  // for (const auto &f : files)
+  }  // if (!config.rule_fars.empty())
   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
     auto hr_config = config.hr;
     hr_config.debug = config.model_config.debug;
diff --git a/sherpa-onnx/csrc/online-recognizer.cc b/sherpa-onnx/csrc/online-recognizer.cc
index 338a92f3..29177a10 100644
--- a/sherpa-onnx/csrc/online-recognizer.cc
+++ b/sherpa-onnx/csrc/online-recognizer.cc
@@ -10,6 +10,7 @@
 #include <iomanip>
 #include <memory>
 #include <sstream>
+#include <string>
 #include <utility>
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/online-rnn-lm.cc b/sherpa-onnx/csrc/online-rnn-lm.cc
index 8c5b1b45..1b333bc6 100644
--- a/sherpa-onnx/csrc/online-rnn-lm.cc
+++ b/sherpa-onnx/csrc/online-rnn-lm.cc
@@ -6,6 +6,7 @@
 #include "sherpa-onnx/csrc/online-rnn-lm.h"
 
 #include <algorithm>
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
@@ -49,7 +50,7 @@ class OnlineRnnLM::Impl {
     // if LODR enabled, we need to update the LODR state
     if (lodr_fst_ != nullptr) {
       auto next_lodr_state = std::make_unique<LodrStateCost>(
-                            hyp->lodr_state->ForwardOneStep(hyp->ys.back()));
+          hyp->lodr_state->ForwardOneStep(hyp->ys.back()));
       // calculate the score of the latest token
       auto score = next_lodr_state->Score() - hyp->lodr_state->Score();
       hyp->lodr_state = std::move(next_lodr_state);
@@ -108,7 +109,8 @@ class OnlineRnnLM::Impl {
           // apply LODR to hyp score
           if (lodr_fst_ != nullptr) {
             // We scale LODR scale with LM scale to replicate Icefall code
-            lodr_fst_->ComputeScore(config_.lodr_scale*scale, &h, context_size);
+            lodr_fst_->ComputeScore(config_.lodr_scale * scale, &h,
+                                    context_size);
           }
 
           // update NN LM states in hyp
@@ -178,8 +180,8 @@ class OnlineRnnLM::Impl {
     ComputeInitStates();
 
     if (!config_.lodr_fst.empty()) {
-      lodr_fst_ = std::make_unique<LodrFst>(LodrFst(config_.lodr_fst,
-                                                    config_.lodr_backoff_id));
+      lodr_fst_ = std::make_unique<LodrFst>(
+          LodrFst(config_.lodr_fst, config_.lodr_backoff_id));
     }
   }
 
diff --git a/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc b/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
index b37e8802..e7b48ec2 100644
--- a/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-t-one-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-t-one-ctc-model.cc b/sherpa-onnx/csrc/online-t-one-ctc-model.cc
index 2bb4ccf5..5c498443 100644
--- a/sherpa-onnx/csrc/online-t-one-ctc-model.cc
+++ b/sherpa-onnx/csrc/online-t-one-ctc-model.cc
@@ -6,7 +6,10 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc b/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
index a76db5b7..ab445f02 100644
--- a/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
+++ b/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
@@ -8,6 +8,7 @@
 #include <algorithm>
 #include <iterator>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/online-stream.h"
diff --git a/sherpa-onnx/csrc/online-transducer-model-config.cc b/sherpa-onnx/csrc/online-transducer-model-config.cc
index dd757271..10c2846d 100644
--- a/sherpa-onnx/csrc/online-transducer-model-config.cc
+++ b/sherpa-onnx/csrc/online-transducer-model-config.cc
@@ -4,6 +4,7 @@
 #include "sherpa-onnx/csrc/online-transducer-model-config.h"
 
 #include <sstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/online-transducer-model.cc b/sherpa-onnx/csrc/online-transducer-model.cc
index 286fd9cd..157350f2 100644
--- a/sherpa-onnx/csrc/online-transducer-model.cc
+++ b/sherpa-onnx/csrc/online-transducer-model.cc
@@ -17,6 +17,7 @@
 #include <memory>
 #include <sstream>
 #include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/online-websocket-client.cc b/sherpa-onnx/csrc/online-websocket-client.cc
index 2e5ac059..71998e55 100644
--- a/sherpa-onnx/csrc/online-websocket-client.cc
+++ b/sherpa-onnx/csrc/online-websocket-client.cc
@@ -4,6 +4,7 @@
 #include <chrono>  // NOLINT
 #include <fstream>
 #include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/online-websocket-server-impl.cc b/sherpa-onnx/csrc/online-websocket-server-impl.cc
index 8925ebb4..e5ac2b4b 100644
--- a/sherpa-onnx/csrc/online-websocket-server-impl.cc
+++ b/sherpa-onnx/csrc/online-websocket-server-impl.cc
@@ -4,6 +4,10 @@
 
 #include "sherpa-onnx/csrc/online-websocket-server-impl.h"
 
+#include <iostream>
+#include <memory>
+#include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
diff --git a/sherpa-onnx/csrc/online-websocket-server-impl.h b/sherpa-onnx/csrc/online-websocket-server-impl.h
index 4e0582db..b7bd189b 100644
--- a/sherpa-onnx/csrc/online-websocket-server-impl.h
+++ b/sherpa-onnx/csrc/online-websocket-server-impl.h
@@ -9,14 +9,14 @@
 #include <fstream>
 #include <map>
 #include <memory>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <set>
 #include <string>
 #include <unordered_set>
 #include <utility>
 #include <vector>
 
-#include "asio.hpp"
+#include "asio.hpp"  // NOLINT
 #include "sherpa-onnx/csrc/online-recognizer.h"
 #include "sherpa-onnx/csrc/online-stream.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/online-websocket-server.cc b/sherpa-onnx/csrc/online-websocket-server.cc
index 6ba7a198..2e976207 100644
--- a/sherpa-onnx/csrc/online-websocket-server.cc
+++ b/sherpa-onnx/csrc/online-websocket-server.cc
@@ -2,7 +2,9 @@
 //
 // Copyright (c)  2022-2023  Xiaomi Corporation
 
-#include "asio.hpp"
+#include <vector>
+
+#include "asio.hpp"  // NOLINT
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/online-websocket-server-impl.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc b/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
index a47b3e16..2d881e76 100644
--- a/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-wenet-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-wenet-ctc-model.cc b/sherpa-onnx/csrc/online-wenet-ctc-model.cc
index 9024481d..9d78b151 100644
--- a/sherpa-onnx/csrc/online-wenet-ctc-model.cc
+++ b/sherpa-onnx/csrc/online-wenet-ctc-model.cc
@@ -6,7 +6,10 @@
 
 #include <algorithm>
 #include <cmath>
+#include <memory>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc b/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
index ed9e7b8a..925e33e6 100644
--- a/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
+++ b/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-zipformer2-ctc-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc b/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
index f7cccc43..aa083418 100644
--- a/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
+++ b/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
@@ -7,8 +7,11 @@
 #include <algorithm>
 #include <cassert>
 #include <cmath>
+#include <memory>
 #include <numeric>
 #include <string>
+#include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
index fd10c02c..a33252de 100644
--- a/sherpa-onnx/csrc/onnx-utils.cc
+++ b/sherpa-onnx/csrc/onnx-utils.cc
@@ -11,6 +11,7 @@
 #include <numeric>
 #include <sstream>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "onnxruntime_cxx_api.h"  // NOLINT
@@ -258,7 +259,7 @@ void PrintShape(const Ort::Value *v) {
     os << i << ", ";
   }
   os << "\n";
-  fprintf(stderr, "%s", os.str().c_str());
+  SHERPA_ONNX_LOGE("%s", os.str().c_str());
 }
 
 template <typename T /*= float*/>
@@ -270,7 +271,7 @@ void Print1D(const Ort::Value *v) {
     os << d[i] << " ";
   }
   os << "\n";
-  fprintf(stderr, "%s\n", os.str().c_str());
+  SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
 }
 
 template void Print1D<int64_t>(const Ort::Value *v);
@@ -289,7 +290,7 @@ void Print2D(const Ort::Value *v) {
     }
     os << "\n";
   }
-  fprintf(stderr, "%s\n", os.str().c_str());
+  SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
 }
 
 template void Print2D<int64_t>(const Ort::Value *v);
@@ -300,15 +301,15 @@ void Print3D(const Ort::Value *v) {
   const float *d = v->GetTensorData<float>();
 
   for (int32_t p = 0; p != static_cast<int32_t>(shape[0]); ++p) {
-    fprintf(stderr, "---plane %d---\n", p);
+    SHERPA_ONNX_LOGE("---plane %d---\n", p);
     for (int32_t r = 0; r != static_cast<int32_t>(shape[1]); ++r) {
       for (int32_t c = 0; c != static_cast<int32_t>(shape[2]); ++c, ++d) {
-        fprintf(stderr, "%.3f ", *d);
+        SHERPA_ONNX_LOGE("%.3f ", *d);
       }
-      fprintf(stderr, "\n");
+      SHERPA_ONNX_LOGE("\n");
     }
   }
-  fprintf(stderr, "\n");
+  SHERPA_ONNX_LOGE("\n");
 }
 
 void Print4D(const Ort::Value *v) {
@@ -316,19 +317,19 @@ void Print4D(const Ort::Value *v) {
   const float *d = v->GetTensorData<float>();
 
   for (int32_t p = 0; p != static_cast<int32_t>(shape[0]); ++p) {
-    fprintf(stderr, "---plane %d---\n", p);
+    SHERPA_ONNX_LOGE("---plane %d---\n", p);
     for (int32_t q = 0; q != static_cast<int32_t>(shape[1]); ++q) {
-      fprintf(stderr, "---subplane %d---\n", q);
+      SHERPA_ONNX_LOGE("---subplane %d---\n", q);
       for (int32_t r = 0; r != static_cast<int32_t>(shape[2]); ++r) {
         for (int32_t c = 0; c != static_cast<int32_t>(shape[3]); ++c, ++d) {
-          fprintf(stderr, "%.3f ", *d);
+          SHERPA_ONNX_LOGE("%.3f ", *d);
         }
-        fprintf(stderr, "\n");
+        SHERPA_ONNX_LOGE("\n");
       }
-      fprintf(stderr, "\n");
+      SHERPA_ONNX_LOGE("\n");
     }
   }
-  fprintf(stderr, "\n");
+  SHERPA_ONNX_LOGE("\n");
 }
 
 Ort::Value Repeat(OrtAllocator *allocator, Ort::Value *cur_encoder_out,
diff --git a/sherpa-onnx/csrc/packed-sequence-test.cc b/sherpa-onnx/csrc/packed-sequence-test.cc
index eda38914..8ed182f0 100644
--- a/sherpa-onnx/csrc/packed-sequence-test.cc
+++ b/sherpa-onnx/csrc/packed-sequence-test.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/packed-sequence.h"
 
+#include <cstdio>
 #include <numeric>
 
 #include "gtest/gtest.h"
diff --git a/sherpa-onnx/csrc/packed-sequence.cc b/sherpa-onnx/csrc/packed-sequence.cc
index 1c3fe91c..240345a7 100644
--- a/sherpa-onnx/csrc/packed-sequence.cc
+++ b/sherpa-onnx/csrc/packed-sequence.cc
@@ -8,6 +8,7 @@
 #include <cassert>
 #include <numeric>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/slice.h"
 #include "sherpa-onnx/csrc/transpose.h"
diff --git a/sherpa-onnx/csrc/parse-options.cc b/sherpa-onnx/csrc/parse-options.cc
index c0dd9595..68ba1467 100644
--- a/sherpa-onnx/csrc/parse-options.cc
+++ b/sherpa-onnx/csrc/parse-options.cc
@@ -17,6 +17,7 @@
 #include <cstring>
 #include <fstream>
 #include <iomanip>
+#include <string>
 
 #include "sherpa-onnx/csrc/log.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/phrase-matcher.cc b/sherpa-onnx/csrc/phrase-matcher.cc
index 35e09555..bf2069e6 100644
--- a/sherpa-onnx/csrc/phrase-matcher.cc
+++ b/sherpa-onnx/csrc/phrase-matcher.cc
@@ -5,7 +5,10 @@
 
 #include <algorithm>
 #include <sstream>
+#include <string>
+#include <unordered_set>
 #include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/text-utils.h"
diff --git a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
index 21200dd3..8b9e4474 100644
--- a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+++ b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
@@ -8,10 +8,11 @@
 #include <fstream>
 #include <locale>
 #include <map>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <sstream>
 #include <string>
 #include <strstream>
+#include <unordered_map>
 #include <utility>
 #include <vector>
 
@@ -25,8 +26,8 @@
 #endif
 
 #include "espeak-ng/speak_lib.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/piper-phonemize-test.cc b/sherpa-onnx/csrc/piper-phonemize-test.cc
index 47e2c57b..65abfabd 100644
--- a/sherpa-onnx/csrc/piper-phonemize-test.cc
+++ b/sherpa-onnx/csrc/piper-phonemize-test.cc
@@ -2,10 +2,15 @@
 //
 // Copyright (c)  2023  Xiaomi Corporation
 
+#include <iostream>
+#include <map>
+#include <string>
+#include <vector>
+
 #include "espeak-ng/speak_lib.h"
 #include "gtest/gtest.h"
-#include "phoneme_ids.hpp"
-#include "phonemize.hpp"
+#include "phoneme_ids.hpp"  // NOLINT
+#include "phonemize.hpp"    // NOLINT
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/provider-config.cc b/sherpa-onnx/csrc/provider-config.cc
index 165e2d9a..dc93723c 100644
--- a/sherpa-onnx/csrc/provider-config.cc
+++ b/sherpa-onnx/csrc/provider-config.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/provider-config.h"
 
 #include <sstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/provider.cc b/sherpa-onnx/csrc/provider.cc
index 3baed32c..af3759c7 100644
--- a/sherpa-onnx/csrc/provider.cc
+++ b/sherpa-onnx/csrc/provider.cc
@@ -6,6 +6,7 @@
 
 #include <algorithm>
 #include <cctype>
+#include <string>
 
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
index 82155a6b..2276cd79 100644
--- a/sherpa-onnx/csrc/qnn-config.cc
+++ b/sherpa-onnx/csrc/qnn-config.cc
@@ -5,6 +5,7 @@
 #include "sherpa-onnx/csrc/qnn-config.h"
 
 #include <sstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
index 8ed9c281..df4299ac 100644
--- a/sherpa-onnx/csrc/qnn/qnn-backend.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
@@ -8,6 +8,7 @@
 #include <stdio.h>
 
 #include <cstdint>
+#include <memory>
 #include <sstream>
 #include <string>
 #include <vector>
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
index d3c53788..735995ea 100644
--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
+++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
@@ -617,7 +617,7 @@ QnnModel::QnnModel(const std::string &binary_context_file,
                    const std::string &system_lib, const QnnBackend *backend,
                    BinaryContextTag tag)
     : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
-                                   tag)) {}
+                                   tag)) {}  // NOLINT
 
 bool QnnModel::SaveBinaryContext(const std::string &filename) const {
   return impl_->SaveBinaryContext(filename);
diff --git a/sherpa-onnx/csrc/regex-lang-test.cc b/sherpa-onnx/csrc/regex-lang-test.cc
index 11df85cc..7bd63440 100644
--- a/sherpa-onnx/csrc/regex-lang-test.cc
+++ b/sherpa-onnx/csrc/regex-lang-test.cc
@@ -2,7 +2,10 @@
 //
 // Copyright (c)  2025  Xiaomi Corporation
 
+#include <iostream>
 #include <regex>  // NOLINT
+#include <string>
+#include <vector>
 
 #include "gtest/gtest.h"
 #include "sherpa-onnx/csrc/text-utils.cc"
diff --git a/sherpa-onnx/csrc/resample.cc b/sherpa-onnx/csrc/resample.cc
index c2a768ee..f393deec 100644
--- a/sherpa-onnx/csrc/resample.cc
+++ b/sherpa-onnx/csrc/resample.cc
@@ -29,6 +29,7 @@
 #include <cstdio>
 #include <cstdlib>
 #include <type_traits>
+#include <vector>
 
 #ifndef M_2PI
 #define M_2PI 6.283185307179586476925286766559005
diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
index 1a44eafb..2a13deb8 100644
--- a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+++ b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
@@ -442,7 +442,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
 };
 
 OnlineZipformerTransducerModelRknn::~OnlineZipformerTransducerModelRknn() =
-    default;
+    default;  // NOLINT
 
 OnlineZipformerTransducerModelRknn::OnlineZipformerTransducerModelRknn(
     const OnlineModelConfig &config)
diff --git a/sherpa-onnx/csrc/rknn/utils.cc b/sherpa-onnx/csrc/rknn/utils.cc
index 5f092c61..d0522d6e 100644
--- a/sherpa-onnx/csrc/rknn/utils.cc
+++ b/sherpa-onnx/csrc/rknn/utils.cc
@@ -7,6 +7,7 @@
 #include <string.h>
 
 #include <sstream>
+#include <string>
 #include <unordered_map>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/sherpa-display.h b/sherpa-onnx/csrc/sherpa-display.h
index 0fb2dc4d..2da5d31d 100644
--- a/sherpa-onnx/csrc/sherpa-display.h
+++ b/sherpa-onnx/csrc/sherpa-display.h
@@ -5,6 +5,7 @@
 
 #include <stdlib.h>
 
+#include <cstdio>
 #include <ctime>
 #include <iomanip>
 #include <sstream>
diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
index 6a5b701e..da1b8c87 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
@@ -7,8 +7,11 @@
 #include <stdlib.h>
 
 #include <algorithm>
-#include <mutex>   // NOLINT
+#include <mutex>  // NOLINT
+#include <string>
 #include <thread>  // NOLINT
+#include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/audio-tagging.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
index a14ff5c7..1404afb3 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
@@ -10,7 +10,11 @@
 #include <fstream>
 #include <mutex>  // NOLINT
 #include <sstream>
+#include <string>
 #include <thread>  // NOLINT
+#include <unordered_map>
+#include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
index b69ec6cd..8b144f76 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
@@ -8,9 +8,12 @@
 
 #include <algorithm>
 #include <cctype>  // std::tolower
-#include <chrono>  // NOLINT
-#include <mutex>   // NOLINT
-#include <thread>  // NOLINT
+#include <chrono>
+#include <mutex>
+#include <string>
+#include <thread>
+#include <utility>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
index 2aae076d..c070a23e 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
@@ -8,6 +8,8 @@
 #include <algorithm>
 #include <cctype>  // std::tolower
 #include <cstdint>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/display.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
index cfa46dc9..0213e0a0 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
@@ -7,6 +7,8 @@
 
 #include <algorithm>
 #include <cstdint>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/display.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
index e52ef8ac..050b4f50 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
@@ -4,10 +4,12 @@
 
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <iomanip>
 #include <iostream>
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/keyword-spotter.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
index b94ba09a..03b4e7d3 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
@@ -8,8 +8,10 @@
 
 #include <algorithm>
 #include <cctype>  // std::tolower
-#include <mutex>   // NOLINT
-#include <thread>  // NOLINT
+#include <mutex>
+#include <thread>
+#include <utility>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/audio-tagging.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
index 3d22f0d0..da6bd6f5 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
@@ -8,9 +8,13 @@
 
 #include <algorithm>
 #include <fstream>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <sstream>
-#include <thread>  // NOLINT
+#include <string>
+#include <thread>
+#include <unordered_map>
+#include <utility>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
index 5a012a8b..3dab1fee 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
@@ -8,8 +8,10 @@
 
 #include <algorithm>
 #include <cctype>  // std::tolower
-#include <mutex>   // NOLINT
-#include <thread>  // NOLINT
+#include <mutex>
+#include <thread>
+#include <utility>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
index b70797a3..f18cdc3e 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
@@ -9,6 +9,8 @@
 #include <algorithm>
 #include <clocale>
 #include <cwctype>
+#include <string>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/display.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
index 9367b017..0e5de242 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
@@ -3,6 +3,9 @@
 // Copyright (c)  2024  Xiaomi Corporation
 #include <stdio.h>
 
+#include <string>
+#include <vector>
+
 #include "sherpa-onnx/csrc/audio-tagging.h"
 #include "sherpa-onnx/csrc/parse-options.h"
 #include "sherpa-onnx/csrc/wave-reader.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
index 0ec31487..fcd2de38 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
@@ -3,7 +3,9 @@
 // Copyright (c)  2025  Xiaomi Corporation
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/offline-speech-denoiser.h"
 #include "sherpa-onnx/csrc/wave-reader.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
index 1b563943..a3148a21 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
@@ -5,11 +5,12 @@
 #include <stdio.h>
 
 #include <atomic>
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <fstream>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <string>
-#include <thread>  // NOLINT
+#include <thread>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/offline-recognizer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
index 7f220734..b7891d85 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
@@ -3,7 +3,8 @@
 // Copyright (c)  2022-2024  Xiaomi Corporation
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
+#include <string>
 
 #include "sherpa-onnx/csrc/offline-punctuation.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
index 31cda85f..a27f1d3a 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
@@ -2,6 +2,11 @@
 //
 // Copyright (c)  2024  Xiaomi Corporation
 
+#include <cstdio>
+#include <iostream>
+#include <string>
+#include <vector>
+
 #include "sherpa-onnx/csrc/offline-speaker-diarization.h"
 #include "sherpa-onnx/csrc/parse-options.h"
 #include "sherpa-onnx/csrc/wave-reader.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
index c5915cbc..401c0d84 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
@@ -11,9 +11,11 @@
 #include <algorithm>
 #include <chrono>              // NOLINT
 #include <condition_variable>  // NOLINT
+#include <cstdio>
 #include <fstream>
 #include <mutex>  // NOLINT
 #include <queue>
+#include <string>
 #include <thread>  // NOLINT
 #include <vector>
 
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
index a8ec6b5e..ae496566 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
@@ -5,12 +5,15 @@
 #include <signal.h>
 
 #include <algorithm>
-#include <chrono>              // NOLINT
-#include <condition_variable>  // NOLINT
+#include <chrono>
+#include <condition_variable>
+#include <cstdio>
 #include <fstream>
-#include <mutex>  // NOLINT
+#include <mutex>
 #include <queue>
-#include <thread>  // NOLINT
+#include <string>
+#include <thread>
+#include <utility>
 #include <vector>
 
 #include "portaudio.h"  // NOLINT
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
index 0426367b..7c12af13 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
@@ -3,7 +3,9 @@
 // Copyright (c)  2023  Xiaomi Corporation
 
 #include <chrono>  // NOLINT
+#include <cstdio>
 #include <fstream>
+#include <string>
 
 #include "sherpa-onnx/csrc/offline-tts.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
index b424fba3..ad2e3352 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
@@ -2,8 +2,11 @@
 //
 // Copyright (c)  2025  Xiaomi Corporation
 
-#include <chrono>  // NOLINT
+#include <chrono>
+#include <cstdio>
 #include <fstream>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/offline-tts.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
index 5509a861..41fb3a4f 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
@@ -4,8 +4,9 @@
 
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/offline-recognizer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc b/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
index faca83b9..35783a11 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
@@ -4,8 +4,9 @@
 
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <iostream>
+#include <string>
 
 #include "sherpa-onnx/csrc/online-punctuation.h"
 #include "sherpa-onnx/csrc/parse-options.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
index 225dcc20..4de8dbb2 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
@@ -7,7 +7,10 @@
 #include <stdlib.h>
 
 #include <algorithm>
+#include <memory>
 #include <mutex>  // NOLINT
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/circular-buffer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
index dcf7c85a..d96d3884 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
@@ -8,6 +8,9 @@
 
 #include <algorithm>
 #include <iomanip>
+#include <memory>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/alsa.h"
 #include "sherpa-onnx/csrc/voice-activity-detector.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
index 5940f1d6..b8823983 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
@@ -7,7 +7,10 @@
 #include <stdlib.h>
 
 #include <algorithm>
-#include <mutex>  // NOLINT
+#include <memory>
+#include <mutex>
+#include <utility>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/circular-buffer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
index fa7afe4c..27ea6587 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
@@ -7,9 +7,10 @@
 #include <stdlib.h>
 
 #include <algorithm>
-#include <chrono>              // NOLINT
-#include <condition_variable>  // NOLINT
-#include <mutex>               // NOLINT
+#include <chrono>
+#include <condition_variable>
+#include <memory>
+#include <mutex>
 #include <queue>
 #include <string>
 #include <vector>
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
index 49802e86..e774dacb 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
@@ -7,7 +7,10 @@
 #include <stdlib.h>
 
 #include <algorithm>
-#include <mutex>  // NOLINT
+#include <memory>
+#include <mutex>
+#include <utility>
+#include <vector>
 
 #include "portaudio.h"  // NOLINT
 #include "sherpa-onnx/csrc/circular-buffer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
index 94d7ff6b..32097856 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
@@ -4,8 +4,11 @@
 
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <algorithm>
+#include <chrono>
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/offline-recognizer.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
index 4ccc8245..9fc57220 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
@@ -9,8 +9,10 @@
 #include <stdio.h>
 
 #include <algorithm>
-#include <chrono>  // NOLINT
+#include <chrono>
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/online-recognizer.h"
@@ -138,13 +140,13 @@ for a list of pre-trained models to download.
     samples = std::move(out_samples);
     fprintf(stderr, "Resampling done\n");
   }
-  const float tail_padding_len =  1.28;  // related to model chunk-size
-  std::vector<float> tail_paddings(
-      static_cast<int>(tail_padding_len * 16000));
+  const float tail_padding_len = 1.28;  // related to model chunk-size
+  std::vector<float> tail_paddings(static_cast<int>(tail_padding_len * 16000));
 
   fprintf(stderr, "Started!\n");
   int32_t window_size = vad_config.ten_vad.model.empty()
-    ? vad_config.silero_vad.window_size : vad_config.ten_vad.window_size;
+                            ? vad_config.silero_vad.window_size
+                            : vad_config.ten_vad.window_size;
   int32_t offset = 0;
   int32_t segment_id = 0;
   bool speech_started = false;
@@ -178,8 +180,8 @@ for a list of pre-trained models to download.
       }
       auto text = recognizer.GetResult(s.get()).text;
       if (!text.empty()) {
-        fprintf(stderr, "vad segment(%d:%.3f-%.3f) results: %s\n",
-            segment_id, start_time, end_time, text.c_str());
+        fprintf(stderr, "vad segment(%d:%.3f-%.3f) results: %s\n", segment_id,
+                start_time, end_time, text.c_str());
       }
       vad->Pop();
     }
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad.cc b/sherpa-onnx/csrc/sherpa-onnx-vad.cc
index 7cf82c3b..b452f729 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad.cc
@@ -7,6 +7,9 @@
 
 #include <algorithm>
 #include <iomanip>
+#include <memory>
+#include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/voice-activity-detector.h"
 #include "sherpa-onnx/csrc/wave-reader.h"
diff --git a/sherpa-onnx/csrc/sherpa-onnx.cc b/sherpa-onnx/csrc/sherpa-onnx.cc
index 5470925b..14783fd8 100644
--- a/sherpa-onnx/csrc/sherpa-onnx.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx.cc
@@ -4,10 +4,12 @@
 
 #include <stdio.h>
 
-#include <chrono>  // NOLINT
+#include <chrono>
 #include <iomanip>
 #include <iostream>
+#include <memory>
 #include <string>
+#include <utility>
 #include <vector>
 
 #include "sherpa-onnx/csrc/online-recognizer.h"
diff --git a/sherpa-onnx/csrc/silero-vad-model-config.cc b/sherpa-onnx/csrc/silero-vad-model-config.cc
index a21ac63b..2623a179 100644
--- a/sherpa-onnx/csrc/silero-vad-model-config.cc
+++ b/sherpa-onnx/csrc/silero-vad-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/silero-vad-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/silero-vad-model.cc b/sherpa-onnx/csrc/silero-vad-model.cc
index 8a2db3db..00a0f399 100644
--- a/sherpa-onnx/csrc/silero-vad-model.cc
+++ b/sherpa-onnx/csrc/silero-vad-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/silero-vad-model.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
index 3abb21a2..6b94bc96 100644
--- a/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
@@ -3,6 +3,8 @@
 // Copyright (c)  2024  Xiaomi Corporation
 #include "sherpa-onnx/csrc/speaker-embedding-extractor-impl.h"
 
+#include <memory>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
index b2192431..2c870761 100644
--- a/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/speaker-embedding-extractor-model.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
index 66f79aac..fe652b1e 100644
--- a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor.cc b/sherpa-onnx/csrc/speaker-embedding-extractor.cc
index 5d52fb2f..237678b1 100644
--- a/sherpa-onnx/csrc/speaker-embedding-extractor.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-extractor.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/speaker-embedding-extractor.h"
 
+#include <memory>
+#include <string>
 #include <vector>
 
 #if __ANDROID_API__ >= 9
diff --git a/sherpa-onnx/csrc/speaker-embedding-manager-test.cc b/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
index 6f115ca5..12a624c5 100644
--- a/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
@@ -4,6 +4,9 @@
 
 #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
 
+#include <string>
+#include <vector>
+
 #include "gtest/gtest.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/csrc/speaker-embedding-manager.cc b/sherpa-onnx/csrc/speaker-embedding-manager.cc
index 44f41370..c61597c5 100644
--- a/sherpa-onnx/csrc/speaker-embedding-manager.cc
+++ b/sherpa-onnx/csrc/speaker-embedding-manager.cc
@@ -5,16 +5,18 @@
 #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
 
 #include <algorithm>
+#include <string>
 #include <unordered_map>
 #include <utility>
+#include <vector>
 
 #include "Eigen/Dense"
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
 
-using FloatMatrix =
-    Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+using FloatMatrix = Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
+                                  Eigen::RowMajor>;  // NOLINT
 
 class SpeakerEmbeddingManager::Impl {
  public:
diff --git a/sherpa-onnx/csrc/spoken-language-identification.cc b/sherpa-onnx/csrc/spoken-language-identification.cc
index 3797586a..4880c5b8 100644
--- a/sherpa-onnx/csrc/spoken-language-identification.cc
+++ b/sherpa-onnx/csrc/spoken-language-identification.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/spoken-language-identification.h"
 
+#include <memory>
 #include <string>
 
 #if __ANDROID_API__ >= 9
diff --git a/sherpa-onnx/csrc/stack.cc b/sherpa-onnx/csrc/stack.cc
index 1d0fac51..76689d72 100644
--- a/sherpa-onnx/csrc/stack.cc
+++ b/sherpa-onnx/csrc/stack.cc
@@ -8,7 +8,9 @@
 #include <functional>
 #include <numeric>
 #include <utility>
+#include <vector>
 
+#include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 
 namespace sherpa_onnx {
@@ -26,9 +28,9 @@ static bool Compare(const std::vector<int64_t> &a,
 
 static void PrintShape(const std::vector<int64_t> &a) {
   for (auto i : a) {
-    fprintf(stderr, "%d ", static_cast<int32_t>(i));
+    SHERPA_ONNX_LOGE("%d ", static_cast<int32_t>(i));
   }
-  fprintf(stderr, "\n");
+  SHERPA_ONNX_LOGE("\n");
 }
 
 template <typename T /*=float*/>
@@ -41,12 +43,12 @@ Ort::Value Stack(OrtAllocator *allocator,
     auto s = values[i]->GetTensorTypeAndShapeInfo().GetShape();
     bool ret = Compare(v0_shape, s);
     if (!ret) {
-      fprintf(stderr, "Incorrect shape in Stack !\n");
+      SHERPA_ONNX_LOGE("Incorrect shape in Stack !\n");
 
-      fprintf(stderr, "Shape for tensor 0: ");
+      SHERPA_ONNX_LOGE("Shape for tensor 0: ");
       PrintShape(v0_shape);
 
-      fprintf(stderr, "Shape for tensor %d: ", i);
+      SHERPA_ONNX_LOGE("Shape for tensor %d: ", i);
       PrintShape(s);
 
       exit(-1);
diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
index 2bc2c7f4..eafd4958 100644
--- a/sherpa-onnx/csrc/symbol-table.cc
+++ b/sherpa-onnx/csrc/symbol-table.cc
@@ -11,6 +11,7 @@
 #include <sstream>
 #include <string>
 #include <strstream>
+#include <unordered_map>
 #include <utility>
 
 #if __ANDROID_API__ >= 9
diff --git a/sherpa-onnx/csrc/ten-vad-model-config.cc b/sherpa-onnx/csrc/ten-vad-model-config.cc
index 0e7bcba0..77061ec1 100644
--- a/sherpa-onnx/csrc/ten-vad-model-config.cc
+++ b/sherpa-onnx/csrc/ten-vad-model-config.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/ten-vad-model-config.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
diff --git a/sherpa-onnx/csrc/text-utils-test.cc b/sherpa-onnx/csrc/text-utils-test.cc
index 3240fe37..1fef71e4 100644
--- a/sherpa-onnx/csrc/text-utils-test.cc
+++ b/sherpa-onnx/csrc/text-utils-test.cc
@@ -4,8 +4,13 @@
 
 #include "sherpa-onnx/csrc/text-utils.h"
 
-#include <regex>  // NOLINT
+#include <cstdio>
+#include <cstring>
+#include <iostream>
+#include <regex>
 #include <sstream>
+#include <string>
+#include <vector>
 
 #include "gtest/gtest.h"
 
diff --git a/sherpa-onnx/csrc/text2token-test.cc b/sherpa-onnx/csrc/text2token-test.cc
index 0ad912df..be04a15e 100644
--- a/sherpa-onnx/csrc/text2token-test.cc
+++ b/sherpa-onnx/csrc/text2token-test.cc
@@ -3,8 +3,10 @@
 // Copyright (c)  2024  Xiaomi Corporation
 
 #include <fstream>
+#include <memory>
 #include <sstream>
 #include <string>
+#include <vector>
 
 #include "gtest/gtest.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/csrc/unbind-test.cc b/sherpa-onnx/csrc/unbind-test.cc
index 8159685b..b2b02cb6 100644
--- a/sherpa-onnx/csrc/unbind-test.cc
+++ b/sherpa-onnx/csrc/unbind-test.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/unbind.h"
 
+#include <vector>
+
 #include "gtest/gtest.h"
 #include "sherpa-onnx/csrc/cat.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/utfcpp-test.cc b/sherpa-onnx/csrc/utfcpp-test.cc
index fcc3ae74..ef06c1cb 100644
--- a/sherpa-onnx/csrc/utfcpp-test.cc
+++ b/sherpa-onnx/csrc/utfcpp-test.cc
@@ -3,7 +3,9 @@
 // Copyright (c)  2023  Xiaomi Corporation
 
 #include <cctype>
+#include <iostream>
 #include <string>
+#include <vector>
 
 #include "gtest/gtest.h"
 #include "sherpa-onnx/csrc/text-utils.h"
diff --git a/sherpa-onnx/csrc/vad-model.cc b/sherpa-onnx/csrc/vad-model.cc
index 781b7fe0..90658383 100644
--- a/sherpa-onnx/csrc/vad-model.cc
+++ b/sherpa-onnx/csrc/vad-model.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/vad-model.h"
 
+#include <memory>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/vocoder.cc b/sherpa-onnx/csrc/vocoder.cc
index d9821cec..21963b3d 100644
--- a/sherpa-onnx/csrc/vocoder.cc
+++ b/sherpa-onnx/csrc/vocoder.cc
@@ -4,6 +4,9 @@
 
 #include "sherpa-onnx/csrc/vocoder.h"
 
+#include <memory>
+#include <vector>
+
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
 #include "android/asset_manager_jni.h"
diff --git a/sherpa-onnx/csrc/vocos-vocoder.cc b/sherpa-onnx/csrc/vocos-vocoder.cc
index a869bebd..26ac3b4d 100644
--- a/sherpa-onnx/csrc/vocos-vocoder.cc
+++ b/sherpa-onnx/csrc/vocos-vocoder.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/vocos-vocoder.h"
 
+#include <memory>
 #include <string>
 #include <utility>
 #include <vector>
diff --git a/sherpa-onnx/csrc/voice-activity-detector.cc b/sherpa-onnx/csrc/voice-activity-detector.cc
index be8b8811..c82a77e3 100644
--- a/sherpa-onnx/csrc/voice-activity-detector.cc
+++ b/sherpa-onnx/csrc/voice-activity-detector.cc
@@ -5,8 +5,10 @@
 #include "sherpa-onnx/csrc/voice-activity-detector.h"
 
 #include <algorithm>
+#include <memory>
 #include <queue>
 #include <utility>
+#include <vector>
 
 #if __ANDROID_API__ >= 9
 #include "android/asset_manager.h"
diff --git a/sherpa-onnx/csrc/wave-reader.cc b/sherpa-onnx/csrc/wave-reader.cc
index 90db0d51..56fa2718 100644
--- a/sherpa-onnx/csrc/wave-reader.cc
+++ b/sherpa-onnx/csrc/wave-reader.cc
@@ -7,6 +7,7 @@
 #include <cassert>
 #include <cstdint>
 #include <fstream>
+#include <string>
 #include <utility>
 #include <vector>
 
diff --git a/sherpa-onnx/jni/audio-tagging.cc b/sherpa-onnx/jni/audio-tagging.cc
index 767761d1..8f560006 100644
--- a/sherpa-onnx/jni/audio-tagging.cc
+++ b/sherpa-onnx/jni/audio-tagging.cc
@@ -4,6 +4,9 @@
 
 #include "sherpa-onnx/csrc/audio-tagging.h"
 
+#include <memory>
+#include <vector>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/jni.cc b/sherpa-onnx/jni/jni.cc
index 42fd0d72..19df23a7 100644
--- a/sherpa-onnx/jni/jni.cc
+++ b/sherpa-onnx/jni/jni.cc
@@ -5,6 +5,7 @@
 //                2023       Zhaoming
 
 #include <fstream>
+#include <vector>
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/jni/keyword-spotter.cc b/sherpa-onnx/jni/keyword-spotter.cc
index 4002a6ab..df5aa6d4 100644
--- a/sherpa-onnx/jni/keyword-spotter.cc
+++ b/sherpa-onnx/jni/keyword-spotter.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/keyword-spotter.h"
 
+#include <memory>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/offline-punctuation.cc b/sherpa-onnx/jni/offline-punctuation.cc
index b94d5c26..37ddf669 100644
--- a/sherpa-onnx/jni/offline-punctuation.cc
+++ b/sherpa-onnx/jni/offline-punctuation.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-punctuation.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index 772fecc3..baa58813 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-recognizer.h"
 
+#include <memory>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/text-utils.h"
 #include "sherpa-onnx/jni/common.h"
diff --git a/sherpa-onnx/jni/offline-speaker-diarization.cc b/sherpa-onnx/jni/offline-speaker-diarization.cc
index a6de825c..8a888b2b 100644
--- a/sherpa-onnx/jni/offline-speaker-diarization.cc
+++ b/sherpa-onnx/jni/offline-speaker-diarization.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/offline-speaker-diarization.h"
 
+#include <vector>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/online-punctuation.cc b/sherpa-onnx/jni/online-punctuation.cc
index 8b87352d..e562f192 100644
--- a/sherpa-onnx/jni/online-punctuation.cc
+++ b/sherpa-onnx/jni/online-punctuation.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-punctuation.h"
 
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/online-recognizer.cc b/sherpa-onnx/jni/online-recognizer.cc
index b518ffe8..c2486866 100644
--- a/sherpa-onnx/jni/online-recognizer.cc
+++ b/sherpa-onnx/jni/online-recognizer.cc
@@ -4,6 +4,8 @@
 
 #include "sherpa-onnx/csrc/online-recognizer.h"
 
+#include <memory>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/text-utils.h"
 #include "sherpa-onnx/jni/common.h"
diff --git a/sherpa-onnx/jni/speaker-embedding-extractor.cc b/sherpa-onnx/jni/speaker-embedding-extractor.cc
index 16cf02ae..b6821d44 100644
--- a/sherpa-onnx/jni/speaker-embedding-extractor.cc
+++ b/sherpa-onnx/jni/speaker-embedding-extractor.cc
@@ -3,6 +3,9 @@
 // Copyright (c)  2024  Xiaomi Corporation
 #include "sherpa-onnx/csrc/speaker-embedding-extractor.h"
 
+#include <memory>
+#include <vector>
+
 #include "sherpa-onnx/jni/common.h"
 
 namespace sherpa_onnx {
diff --git a/sherpa-onnx/jni/speaker-embedding-manager.cc b/sherpa-onnx/jni/speaker-embedding-manager.cc
index d73a8788..3c169fdb 100644
--- a/sherpa-onnx/jni/speaker-embedding-manager.cc
+++ b/sherpa-onnx/jni/speaker-embedding-manager.cc
@@ -3,6 +3,9 @@
 // Copyright (c)  2024  Xiaomi Corporation
 #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
 
+#include <string>
+#include <vector>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/spoken-language-identification.cc b/sherpa-onnx/jni/spoken-language-identification.cc
index 4003a061..80b2ce55 100644
--- a/sherpa-onnx/jni/spoken-language-identification.cc
+++ b/sherpa-onnx/jni/spoken-language-identification.cc
@@ -4,6 +4,9 @@
 
 #include "sherpa-onnx/csrc/spoken-language-identification.h"
 
+#include <memory>
+#include <string>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/jni/common.h"
 
diff --git a/sherpa-onnx/jni/wave-reader.cc b/sherpa-onnx/jni/wave-reader.cc
index ebed51b0..a6c31a8a 100644
--- a/sherpa-onnx/jni/wave-reader.cc
+++ b/sherpa-onnx/jni/wave-reader.cc
@@ -4,6 +4,7 @@
 #include "sherpa-onnx/csrc/wave-reader.h"
 
 #include <fstream>
+#include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
diff --git a/sherpa-onnx/python/csrc/faked-alsa.cc b/sherpa-onnx/python/csrc/faked-alsa.cc
index 26ce28ff..765c81b5 100644
--- a/sherpa-onnx/python/csrc/faked-alsa.cc
+++ b/sherpa-onnx/python/csrc/faked-alsa.cc
@@ -2,6 +2,8 @@
 //
 // Copyright (c)  2024  Xiaomi Corporation
 
+#include <vector>
+
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/python/csrc/alsa.h"
 

commit bf2924489f83553fa7880b8d8f16f2374cf4b4c4
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Nov 12 13:58:38 2025 +0800

    Add QnnConfig. (#2768)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 577048ec..10e90fd8 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -160,6 +160,10 @@ list(APPEND sources
   offline-zipformer-audio-tagging-model.cc
 )
 
+list(APPEND sources
+  qnn-config.cc
+)
+
 # punctuation
 list(APPEND sources
   offline-ct-transformer-model.cc
diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
index 44cc3848..1d2271f2 100644
--- a/sherpa-onnx/csrc/offline-recognizer.cc
+++ b/sherpa-onnx/csrc/offline-recognizer.cc
@@ -136,6 +136,11 @@ std::string OfflineRecognizerConfig::ToString() const {
   os << "model_config=" << model_config.ToString() << ", ";
   os << "lm_config=" << lm_config.ToString() << ", ";
   os << "ctc_fst_decoder_config=" << ctc_fst_decoder_config.ToString() << ", ";
+
+  if (!qnn_config.backend_lib.empty()) {
+    os << "qnn_config=" << qnn_config.ToString() << ", ";
+  }
+
   os << "decoding_method=\"" << decoding_method << "\", ";
   os << "max_active_paths=" << max_active_paths << ", ";
   os << "hotwords_file=\"" << hotwords_file << "\", ";
diff --git a/sherpa-onnx/csrc/offline-recognizer.h b/sherpa-onnx/csrc/offline-recognizer.h
index 1fcc1016..ae2c23a4 100644
--- a/sherpa-onnx/csrc/offline-recognizer.h
+++ b/sherpa-onnx/csrc/offline-recognizer.h
@@ -17,6 +17,7 @@
 #include "sherpa-onnx/csrc/offline-stream.h"
 #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
 #include "sherpa-onnx/csrc/parse-options.h"
+#include "sherpa-onnx/csrc/qnn-config.h"
 
 namespace sherpa_onnx {
 
@@ -27,6 +28,7 @@ struct OfflineRecognizerConfig {
   OfflineModelConfig model_config;
   OfflineLMConfig lm_config;
   OfflineCtcFstDecoderConfig ctc_fst_decoder_config;
+  QnnConfig qnn_config;
 
   std::string decoding_method = "greedy_search";
   int32_t max_active_paths = 4;
diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
new file mode 100644
index 00000000..82155a6b
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn-config.cc
@@ -0,0 +1,74 @@
+// sherpa-onnx/csrc/qnn-config.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn-config.h"
+
+#include <sstream>
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+void QnnConfig::Register(ParseOptions *po) {
+  po->Register("qnn-backend-lib", &backend_lib,
+               "Path to libQnnHtp.so "
+               "Used only when provider is qnn."
+               "Leave it empty if you don't use qnn");
+
+  po->Register(
+      "qnn-context-binary", &context_binary,
+      "Path to model.bin. Used only when provider is qnn."
+      "If it exists, libmodel.so is ignored."
+      "If it does not exist, Context binary is saved to this path so that "
+      "it is loaded the next time you run it. You can leave it empty if you "
+      "don't use qnn");
+
+  po->Register("qnn-system-lib", &system_lib,
+               "Required and used only when --qnn-context-binary is not empty "
+               "and exists. You can leave it empty if you don't use qnn.");
+}
+
+bool QnnConfig::Validate() const {
+  if (backend_lib.empty()) {
+    SHERPA_ONNX_LOGE("Please provide path to libQnnHtp.so if you use qnn");
+    return false;
+  }
+
+  if (!FileExists(backend_lib)) {
+    SHERPA_ONNX_LOGE("--qnn-backend-lib: '%s' does not exist",
+                     backend_lib.c_str());
+    return false;
+  }
+
+  if (!context_binary.empty() && FileExists(context_binary)) {
+    if (system_lib.empty()) {
+      SHERPA_ONNX_LOGE(
+          "Please provide --qnn-system-lib when you provide "
+          "--qnn-context-binary");
+      return false;
+    }
+
+    if (!FileExists(system_lib)) {
+      SHERPA_ONNX_LOGE("--qnn-system-lib: '%s' does not exist",
+                       system_lib.c_str());
+      return false;
+    }
+  }
+
+  return true;
+}
+
+std::string QnnConfig::ToString() const {
+  std::ostringstream os;
+
+  os << "QnnConfig(";
+  os << "backend_lib=\"" << backend_lib << "\", ";
+  os << "context_binary=\"" << context_binary << "\", ";
+  os << "system_lib=\"" << system_lib << "\")";
+
+  return os.str();
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn-config.h b/sherpa-onnx/csrc/qnn-config.h
new file mode 100644
index 00000000..01db111e
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn-config.h
@@ -0,0 +1,38 @@
+// sherpa-onnx/csrc/qnn-config.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_QNN_CONFIG_H_
+#define SHERPA_ONNX_CSRC_QNN_CONFIG_H_
+
+#include <string>
+
+#include "sherpa-onnx/csrc/parse-options.h"
+
+namespace sherpa_onnx {
+
+struct QnnConfig {
+  // Path to the backend library, e.g.,
+  // /some/path/to/libQnnHtp.so
+  std::string backend_lib;
+
+  // If it exists, you need to also provide system_lib.
+  // In this case, the model lib, i.e., libmodel.so, is ignored
+  //
+  // If it does not exist and if the user want to save the context binary,
+  // it will save it to this path.
+  std::string context_binary;
+
+  // Required and used only when context_binary exists
+  // Example value: /some/path/to/libQnnSystem.so
+  std::string system_lib;
+
+  std::string ToString() const;
+
+  void Register(ParseOptions *po);
+
+  bool Validate() const;
+};
+
+}  // namespace sherpa_onnx
+#endif  // SHERPA_ONNX_CSRC_QNN_CONFIG_H_

commit 8d09756cc6bc67aced7cecbdf6a851d2ad4c0cf1
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 11 19:11:28 2025 +0800

    Begin to add qnn C API (#2766)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 6c1564be..92a3ff01 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -61,6 +61,7 @@ option(SHERPA_ONNX_ENABLE_SANITIZER "Whether to enable ubsan and asan" OFF)
 option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUGGEST_BUILD_BINARIES})
 option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
 option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
+option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
 
 set(SHERPA_ONNX_LINUX_ARM64_GPU_ONNXRUNTIME_VERSION "1.11.0" CACHE STRING "Used only for Linux ARM64 GPU. Set to 1.11.0 if you use CUDA 10.2 and cudnn8. Set it to 1.16.0 if you use CUDA 11.4 and cudnn8. Set it to 1.18.0 if you use CUDA 12.2 and cudnn8. Set it to 1.18.1 if you use CUDA 12.6 and cudnn9")
 
@@ -179,6 +180,7 @@ message(STATUS "SHERPA_ONNX_ENABLE_SANITIZER: ${SHERPA_ONNX_ENABLE_SANITIZER}")
 message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAMPLES}")
 message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
 message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
+message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
 message(STATUS "SHERPA_ONNX_LINK_D3D: ${SHERPA_ONNX_LINK_D3D}")
 
 if(BUILD_SHARED_LIBS OR SHERPA_ONNX_ENABLE_JNI)
@@ -337,6 +339,44 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   message(STATUS "Build with Ascend NPU")
 endif()
 
+if(SHERPA_ONNX_ENABLE_QNN)
+  if(NOT DEFINED ENV{QNN_SDK_ROOT})
+      message(FATAL_ERROR "\
+      Please specify the installation directory of the QNN SDK toolkit.
+      For instance, if it is installed in
+
+        /mnt/sdb/open-source/qairt/2.33.0.250327
+
+      You can run
+
+        source /mnt/sdb/open-source/qairt/2.33.0.250327/bin/envsetup.sh
+
+      which will give you the following output
+
+      [INFO] AISW SDK environment set
+      [INFO] QNN_SDK_ROOT: /mnt/sdb/open-source/qairt/2.33.0.250327
+      [INFO] SNPE_ROOT: /mnt/sdb/open-source/qairt/2.33.0.250327
+
+      Then run
+
+        echo $QNN_SDK_ROOT
+
+      It should print:
+
+        /mnt/sdb/open-source/qairt/2.33.0.250327
+
+      You can choose a version of QNN SDK by yourself. You don't need
+      to use 2.33.0.250327
+      ")
+  endif()
+
+  set(QNN_SDK_ROOT $ENV{QNN_SDK_ROOT})
+
+  if(NOT EXISTS ${QNN_SDK_ROOT}/include/QNN/QnnInterface.h)
+    message(FATAL_ERROR "${QNN_SDK_ROOT}/include/QNN/QnnInterface.h does not exist")
+  endif()
+endif()
+
 if(UNIX AND NOT APPLE AND NOT SHERPA_ONNX_ENABLE_WASM AND NOT CMAKE_SYSTEM_NAME STREQUAL Android AND NOT CMAKE_SYSTEM_NAME STREQUAL OHOS)
   check_include_file_cxx(alsa/asoundlib.h SHERPA_ONNX_HAS_ALSA)
   if(SHERPA_ONNX_HAS_ALSA)
diff --git a/build-android-arm64-v8a.sh b/build-android-arm64-v8a.sh
index 7de2e681..1491c8ba 100755
--- a/build-android-arm64-v8a.sh
+++ b/build-android-arm64-v8a.sh
@@ -101,6 +101,10 @@ if [ -z $SHERPA_ONNX_ENABLE_RKNN ]; then
   SHERPA_ONNX_ENABLE_RKNN=OFF
 fi
 
+if [ -z $SHERPA_ONNX_ENABLE_QNN ]; then
+  SHERPA_ONNX_ENABLE_QNN=OFF
+fi
+
 if [ $SHERPA_ONNX_ENABLE_RKNN == ON ]; then
   rknn_version=2.2.0
   if [ ! -d ./librknnrt-android ]; then
@@ -130,6 +134,10 @@ if [ -z $SHERPA_ONNX_ENABLE_C_API ]; then
   SHERPA_ONNX_ENABLE_C_API=OFF
 fi
 
+if [ -z $SHERPA_ONNX_ANDROID_PLATFORM ]; then
+  SHERPA_ONNX_ANDROID_PLATFORM=android-21
+fi
+
 if [ -z $SHERPA_ONNX_ENABLE_JNI ]; then
   SHERPA_ONNX_ENABLE_JNI=ON
 fi
@@ -153,8 +161,9 @@ cmake -DCMAKE_TOOLCHAIN_FILE="$ANDROID_NDK/build/cmake/android.toolchain.cmake"
     -DSHERPA_ONNX_ENABLE_C_API=$SHERPA_ONNX_ENABLE_C_API \
     -DCMAKE_INSTALL_PREFIX=./install \
     -DSHERPA_ONNX_ENABLE_RKNN=$SHERPA_ONNX_ENABLE_RKNN \
+    -DSHERPA_ONNX_ENABLE_QNN=$SHERPA_ONNX_ENABLE_QNN \
     -DANDROID_ABI="arm64-v8a" \
-    -DANDROID_PLATFORM=android-21 ..
+    -DANDROID_PLATFORM=$SHERPA_ONNX_ANDROID_PLATFORM ..
 
     # By default, it links to libc++_static.a
     # -DANDROID_STL=c++_shared \
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index d44e8e8a..577048ec 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -200,6 +200,14 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   )
 endif()
 
+if(SHERPA_ONNX_ENABLE_QNN)
+  list(APPEND sources
+    ./qnn/qnn-backend.cc
+    ./qnn/qnn-model.cc
+    ./qnn/utils.cc
+  )
+endif()
+
 if(SHERPA_ONNX_ENABLE_TTS)
   list(APPEND sources
     character-lexicon.cc
@@ -314,6 +322,10 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
     )
 endif()
 
+if(SHERPA_ONNX_ENABLE_QNN)
+  target_include_directories(sherpa-onnx-core PRIVATE ${QNN_SDK_ROOT}/include/QNN)
+endif()
+
 if(TARGET onnxruntime)
   target_link_libraries(sherpa-onnx-core onnxruntime)
 else()
diff --git a/sherpa-onnx/csrc/qnn/macros.h b/sherpa-onnx/csrc/qnn/macros.h
new file mode 100644
index 00000000..58c65cf3
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/macros.h
@@ -0,0 +1,19 @@
+// sherpa-onnx/csrc/qnn/macros.h
+//
+// Copyright      2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_QNN_MACROS_H_
+#define SHERPA_ONNX_CSRC_QNN_MACROS_H_
+
+#include "sherpa-onnx/csrc/macros.h"
+
+#define SHERPA_ONNX_QNN_CHECK(ret, msg, ...)                             \
+  do {                                                                   \
+    if (ret != QNN_SUCCESS) {                                            \
+      SHERPA_ONNX_LOGE("Return code is: %d", static_cast<int32_t>(ret)); \
+      SHERPA_ONNX_LOGE(msg, ##__VA_ARGS__);                              \
+      SHERPA_ONNX_EXIT(-1);                                              \
+    }                                                                    \
+  } while (0)
+
+#endif  // SHERPA_ONNX_CSRC_QNN_MACROS_H_
diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
new file mode 100644
index 00000000..8ed9c281
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
@@ -0,0 +1,251 @@
+// sherpa-onnx/csrc/qnn/qnn-backend.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
+
+#include <dlfcn.h>
+#include <stdio.h>
+
+#include <cstdint>
+#include <sstream>
+#include <string>
+#include <vector>
+
+#include "QnnInterface.h"
+#include "System/QnnSystemInterface.h"
+#include "sherpa-onnx/csrc/qnn/macros.h"
+#include "sherpa-onnx/csrc/qnn/utils.h"
+
+namespace sherpa_onnx {
+
+class QnnBackend::Impl {
+ public:
+  explicit Impl(const std::string &backend_lib) {
+    bool ok = InitQnnInterface(backend_lib);
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to init qnn interface from '%s'",
+                       backend_lib.c_str());
+      return;
+    }
+
+    InitLog();
+    InitBackend();
+    InitDevice();
+
+    is_initialized_ = true;
+  }
+
+  ~Impl() {
+    if (context_handle_) {
+      auto ret = qnn_interface_.contextFree(context_handle_, nullptr);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextFree");
+    }
+
+    if (device_handle_) {
+      auto ret = qnn_interface_.deviceFree(device_handle_);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call deviceFree");
+    }
+
+    if (backend_handle_) {
+      auto ret = qnn_interface_.backendFree(backend_handle_);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendFree");
+    }
+
+    if (log_handle_) {
+      auto ret = qnn_interface_.logFree(log_handle_);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call logFree");
+    }
+  }
+
+  void InitContext() {
+    if (context_handle_) {
+      SHERPA_ONNX_LOGE("context handle is already initialized");
+      return;
+    }
+
+    auto ret = qnn_interface_.contextCreate(backend_handle_, device_handle_,
+                                            context_config_, &context_handle_);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextCreate");
+  }
+
+  void InitContext(Qnn_ContextHandle_t t) { context_handle_ = t; }
+
+  Qnn_LogHandle_t LogHandle() const { return log_handle_; }
+
+  Qnn_BackendHandle_t BackendHandle() const { return backend_handle_; }
+
+  Qnn_DeviceHandle_t DeviceHandle() const { return device_handle_; }
+
+  Qnn_ContextHandle_t ContextHandle() const { return context_handle_; }
+
+  QNN_INTERFACE_VER_TYPE QnnInterface() const { return qnn_interface_; }
+
+  QnnLog_Level_t LogLevel() const { return log_level_; }
+
+  bool IsInitialized() const { return is_initialized_; }
+
+ private:
+  bool InitQnnInterface(const std::string &backend_lib) {
+    backend_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
+        dlopen(backend_lib.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
+    if (!backend_lib_handle_) {
+      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
+                       backend_lib.c_str(), dlerror());
+      return false;
+    }
+    SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
+
+    const char *symbol = "QnnInterface_getProviders";
+    auto get_interface_providers =
+        reinterpret_cast<QnnInterfaceGetProvidersFnType>(
+            dlsym(backend_lib_handle_.get(), symbol));
+    if (!get_interface_providers) {
+      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
+                       dlerror());
+      return false;
+    }
+    SHERPA_ONNX_LOGE("Got %s", symbol);
+
+    const QnnInterface_t **interface_providers = nullptr;
+    uint32_t num_providers = 0;
+
+    auto ret = get_interface_providers(&interface_providers, &num_providers);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call get_interface_providers");
+
+    if (!interface_providers) {
+      SHERPA_ONNX_LOGE("interface_providers is nullptr");
+      return false;
+    }
+
+    if (num_providers == 0) {
+      SHERPA_ONNX_LOGE("Number of providers is 0");
+      return false;
+    }
+
+    bool found_valid_interface = false;
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("QNN_API_VERSION_MAJOR: %d", QNN_API_VERSION_MAJOR);
+      SHERPA_ONNX_LOGE("QNN_API_VERSION_MINOR: %d", QNN_API_VERSION_MINOR);
+      SHERPA_ONNX_LOGE("QNN_API_VERSION_PATCH: %d", QNN_API_VERSION_PATCH);
+    }
+
+    for (size_t idx = 0; idx < num_providers; ++idx) {
+      auto p = interface_providers[idx];
+
+      if (debug_) {
+        std::ostringstream os;
+        os << "---" << idx << "----\n";
+        os << "backendId: " << p->backendId << "\n";
+        os << "coreApiVersion.major: " << p->apiVersion.coreApiVersion.major
+           << "\n";
+        os << "coreApiVersion.minor: " << p->apiVersion.coreApiVersion.minor
+           << "\n";
+        os << "coreApiVersion.patch: " << p->apiVersion.coreApiVersion.patch
+           << "\n";
+
+        os << "backendApiVersion.major: "
+           << p->apiVersion.backendApiVersion.major << "\n";
+        os << "backendApiVersion.minor: "
+           << p->apiVersion.backendApiVersion.minor << "\n";
+        os << "backendApiVersion.patch: "
+           << p->apiVersion.backendApiVersion.patch << "\n";
+        SHERPA_ONNX_LOGE("%s", os.str().c_str());
+      }
+
+      qnn_interface_ = p->QNN_INTERFACE_VER_NAME;
+      found_valid_interface = true;
+      break;
+    }
+
+    if (!found_valid_interface) {
+      SHERPA_ONNX_LOGE("Failed to find valid interface");
+      return false;
+    }
+
+    if (debug_) {
+      const char *build_id = nullptr;
+      ret = qnn_interface_.backendGetBuildId(&build_id);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendGetBuildId()");
+
+      SHERPA_ONNX_LOGE("backend build ID: %s", build_id);
+    }
+
+    return true;
+  }
+
+  void InitLog() {
+    auto ret = qnn_interface_.logCreate(LogCallback, log_level_, &log_handle_);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call logCreate");
+  }
+
+  void InitBackend() {
+    auto ret = qnn_interface_.backendCreate(log_handle_, backend_config_,
+                                            &backend_handle_);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendCreate");
+  }
+
+  void InitDevice() {
+    auto ret =
+        qnn_interface_.deviceCreate(log_handle_, nullptr, &device_handle_);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call deviceCreate");
+  }
+
+ private:
+  bool debug_ = true;
+  std::unique_ptr<void, decltype(&dlclose)> backend_lib_handle_{nullptr,
+                                                                &dlclose};
+
+  QNN_INTERFACE_VER_TYPE qnn_interface_;
+
+  QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_WARN;
+  // QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_INFO;
+  // QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_VERBOSE;
+
+  Qnn_LogHandle_t log_handle_ = nullptr;
+
+  const QnnBackend_Config_t **backend_config_ = nullptr;
+  Qnn_BackendHandle_t backend_handle_ = nullptr;
+
+  Qnn_DeviceHandle_t device_handle_ = nullptr;
+
+  Qnn_ContextHandle_t context_handle_ = nullptr;
+  const QnnContext_Config_t **context_config_ = nullptr;
+  bool is_initialized_ = false;
+};
+
+QnnBackend::~QnnBackend() = default;
+
+QnnBackend::QnnBackend(const std::string &backend_lib)
+    : impl_(std::make_unique<Impl>(backend_lib)) {}
+
+void QnnBackend::InitContext() const { impl_->InitContext(); }
+
+void QnnBackend::InitContext(Qnn_ContextHandle_t context_handle) const {
+  impl_->InitContext(context_handle);
+}
+
+Qnn_LogHandle_t QnnBackend::LogHandle() const { return impl_->LogHandle(); }
+
+Qnn_BackendHandle_t QnnBackend::BackendHandle() const {
+  return impl_->BackendHandle();
+}
+
+Qnn_DeviceHandle_t QnnBackend::DeviceHandle() const {
+  return impl_->DeviceHandle();
+}
+
+Qnn_ContextHandle_t QnnBackend::ContextHandle() const {
+  return impl_->ContextHandle();
+}
+
+QNN_INTERFACE_VER_TYPE QnnBackend::QnnInterface() const {
+  return impl_->QnnInterface();
+}
+
+QnnLog_Level_t QnnBackend::LogLevel() const { return impl_->LogLevel(); }
+
+bool QnnBackend::IsInitialized() const { return impl_->IsInitialized(); }
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.h b/sherpa-onnx/csrc/qnn/qnn-backend.h
new file mode 100644
index 00000000..62049a69
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/qnn-backend.h
@@ -0,0 +1,36 @@
+// sherpa-onnx/csrc/qnn/qnn-backend.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
+#define SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
+
+#include <memory>
+#include <string>
+
+#include "QnnInterface.h"
+
+namespace sherpa_onnx {
+
+class QnnBackend {
+ public:
+  explicit QnnBackend(const std::string &backend_lib);
+  ~QnnBackend();
+
+  void InitContext() const;
+  void InitContext(Qnn_ContextHandle_t context_handle) const;
+  Qnn_LogHandle_t LogHandle() const;
+  Qnn_BackendHandle_t BackendHandle() const;
+  Qnn_DeviceHandle_t DeviceHandle() const;
+  Qnn_ContextHandle_t ContextHandle() const;
+  QNN_INTERFACE_VER_TYPE QnnInterface() const;
+  QnnLog_Level_t LogLevel() const;
+  bool IsInitialized() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
new file mode 100644
index 00000000..d3c53788
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
@@ -0,0 +1,665 @@
+// sherpa-onnx/csrc/qnn/qnn-model.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/qnn/qnn-model.h"
+
+#include <dlfcn.h>
+
+#include <fstream>
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/qnn/macros.h"
+#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
+#include "sherpa-onnx/csrc/qnn/utils.h"
+
+namespace sherpa_onnx {
+
+class QnnModel::Impl {
+ public:
+  Impl(const std::string &model_so, const QnnBackend *backend)
+      : backend_(backend) {
+    bool ok = InitModel(model_so);
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to load '%s'", model_so.c_str());
+      return;
+    }
+
+    ok = InitSymbols();
+    if (!ok) {
+      SHERPA_ONNX_LOGE("Failed to get model symbols from '%s'",
+                       model_so.c_str());
+      return;
+    }
+
+    InitGraph();
+
+    PostInit();
+  }
+
+  Impl(const std::string &binary_context_file, const std::string &system_lib,
+       const QnnBackend *backend, BinaryContextTag)
+      : backend_(backend) {
+    bool ok = LoadSystemLib(binary_context_file, system_lib);
+    if (!ok) {
+      return;
+    }
+
+    PostInit();
+  }
+
+  bool LoadSystemLib(const std::string &binary_context_file,
+                     const std::string &system_lib) {
+    system_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
+        dlopen(system_lib.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
+    if (!system_lib_handle_) {
+      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
+                       system_lib.c_str(), dlerror());
+      return false;
+    }
+    SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
+
+    auto get_system_interface_providers =
+        reinterpret_cast<QnnSystemInterfaceGetProvidersFnType>(
+            dlsym(system_lib_handle_.get(), "QnnSystemInterface_getProviders"));
+
+    if (!get_system_interface_providers) {
+      SHERPA_ONNX_LOGE("Failed to get QnnSystemInterface_getProviders");
+      return false;
+    }
+
+    const QnnSystemInterface_t **system_interface_providers = nullptr;
+    uint32_t num_providers = 0;
+    if (get_system_interface_providers(&system_interface_providers,
+                                       &num_providers) != QNN_SUCCESS) {
+      SHERPA_ONNX_LOGE("Failed to get system interface providers.");
+      return false;
+    }
+
+    if (!system_interface_providers) {
+      SHERPA_ONNX_LOGE(
+          "Failed to get system interface providers: null "
+          "interface providers received.");
+      return false;
+    }
+
+    if (!num_providers) {
+      SHERPA_ONNX_LOGE(
+          "Failed to get interface providers: 0 interface providers.");
+      return false;
+    }
+
+    for (uint32_t i = 0; i < num_providers; ++i) {
+      if (debug_) {
+        SHERPA_ONNX_LOGE("QNN_SYSTEM_API_VERSION_MAJOR: %d",
+                         static_cast<int32_t>(QNN_SYSTEM_API_VERSION_MAJOR));
+        SHERPA_ONNX_LOGE("QNN_SYSTEM_API_VERSION_MINOR: %d",
+                         static_cast<int32_t>(QNN_SYSTEM_API_VERSION_MINOR));
+        SHERPA_ONNX_LOGE(
+            "systemApiVersion.major: %d",
+            static_cast<int32_t>(
+                system_interface_providers[i]->systemApiVersion.major));
+        SHERPA_ONNX_LOGE(
+            "systemApiVersion.minor: %d",
+            static_cast<int32_t>(
+                system_interface_providers[i]->systemApiVersion.minor));
+      }
+
+      qnn_system_interface_ =
+          system_interface_providers[i]->QNN_SYSTEM_INTERFACE_VER_NAME;
+    }
+
+    // read file into a buffer
+    std::vector<uint8_t> buffer = ReadFile<uint8_t>(binary_context_file);
+
+    QnnSystemContext_Handle_t sys_ctx_handle = nullptr;
+    if (qnn_system_interface_.systemContextCreate(&sys_ctx_handle) !=
+        QNN_SUCCESS) {
+      SHERPA_ONNX_LOGE("Could not create system handle.");
+      return false;
+    }
+
+    const QnnSystemContext_BinaryInfo_t *binary_info = nullptr;
+    Qnn_ContextBinarySize_t binary_info_size = 0;
+
+    auto ret = qnn_system_interface_.systemContextGetBinaryInfo(
+        sys_ctx_handle, static_cast<void *>(buffer.data()), buffer.size(),
+        &binary_info, &binary_info_size);
+    if (ret != QNN_SUCCESS) {
+      SHERPA_ONNX_LOGE("Failed to get context binary info from '%s'",
+                       binary_context_file.c_str());
+
+      qnn_system_interface_.systemContextFree(sys_ctx_handle);
+      return false;
+    }
+
+    const GraphConfigInfo **graph_configs_info = nullptr;
+
+    uint32_t graph_configs_info_count = 0;
+    GraphInfo **graphs_info = nullptr;
+    uint32_t graphs_count = 0;
+
+    if (!CopyMetadataToGraphsInfo(binary_info, graphs_info, graphs_count)) {
+      SHERPA_ONNX_LOGE("Failed to call CopyMetadataToGraphsInfo");
+
+      qnn_system_interface_.systemContextFree(sys_ctx_handle);
+      return false;
+    }
+
+    qnn_system_interface_.systemContextFree(sys_ctx_handle);
+
+    auto free_graphs_info = [&graphs_info, &graphs_count] {
+      for (uint32_t i = 0; i < graphs_count; ++i) {
+        for (uint32_t k = 0; k < graphs_info[i]->num_input_tensors; ++k) {
+          FreeTensor(&graphs_info[i]->input_tensors[k]);
+        }
+
+        for (uint32_t k = 0; k < graphs_info[i]->num_output_tensors; ++k) {
+          FreeTensor(&graphs_info[i]->output_tensors[k]);
+        }
+
+        free(graphs_info[i]->input_tensors);
+        free(graphs_info[i]->output_tensors);
+
+        free(graphs_info[i]->graph_name);
+      }
+
+      free(graphs_info[0]);
+      free(graphs_info);
+    };
+
+    if (graphs_count > 1) {
+      SHERPA_ONNX_LOGE("Only the first graph is used");
+    }
+
+    Qnn_ContextHandle_t context_handle = nullptr;
+
+    if (backend_->QnnInterface().contextCreateFromBinary(
+            backend_->BackendHandle(), backend_->DeviceHandle(),
+            context_config_, static_cast<void *>(buffer.data()), buffer.size(),
+            &context_handle, nullptr) != QNN_SUCCESS) {
+      free_graphs_info();
+      SHERPA_ONNX_LOGE("Could not create context from binary.");
+      return false;
+    }
+
+    backend_->InitContext(context_handle);
+
+    if (backend_->QnnInterface().graphRetrieve(
+            context_handle, (*graphs_info)[0].graph_name,
+            &((*graphs_info)[0].graph)) != QNN_SUCCESS) {
+      free_graphs_info();
+      SHERPA_ONNX_LOGE("Unable to retrieve graph handle for graph %d", 0);
+      return false;
+    }
+
+    graph_handle_ = (*graphs_info)[0].graph;
+
+    InitInputTensors((*graphs_info)[0]);
+    InitOutputTensors((*graphs_info)[0]);
+
+    free_graphs_info();
+
+    return true;
+  }
+
+  ~Impl() = default;
+
+  bool SaveBinaryContext(const std::string &filename) {
+    auto qnn_interface = backend_->QnnInterface();
+
+    if (!qnn_interface.contextGetBinarySize ||
+        !qnn_interface.contextGetBinary) {
+      SHERPA_ONNX_LOGE(
+          "contextGetBinarySizeFnHandle or "
+          "contextGetBinaryFnHandle is nullptr.");
+      return false;
+    }
+
+    uint64_t required_buffer_size{0};
+    auto ret = qnn_interface.contextGetBinarySize(backend_->ContextHandle(),
+                                                  &required_buffer_size);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextGetBinarySize");
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("context binary size: %.3f MB",
+                       static_cast<float>(required_buffer_size) / 1024 / 1024);
+    }
+    std::vector<uint8_t> saveBuffer(required_buffer_size);
+    uint64_t writtenBufferSize{0};
+
+    ret = qnn_interface.contextGetBinary(
+        backend_->ContextHandle(), reinterpret_cast<void *>(saveBuffer.data()),
+        required_buffer_size, &writtenBufferSize);
+
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextGetBinary");
+
+    if (required_buffer_size < writtenBufferSize) {
+      SHERPA_ONNX_LOGE(
+          "Illegal written buffer size %d bytes. Cannot exceed "
+          "allocated memory of %d bytes",
+          static_cast<int32_t>(writtenBufferSize),
+          static_cast<int32_t>(required_buffer_size));
+      return false;
+    }
+    std::ofstream ofs(filename, std::ios::binary | std::ios::trunc);
+    ofs.write(reinterpret_cast<const char *>(saveBuffer.data()),
+              saveBuffer.size());
+
+    if (!ofs) {
+      SHERPA_ONNX_LOGE("Failed to write '%s'", filename.c_str());
+      return false;
+    }
+
+    return true;
+  }
+
+  const std::vector<std::string> &InputTensorNames() const {
+    return input_tensor_names_;
+  }
+
+  const std::vector<std::string> &OutputTensorNames() const {
+    return output_tensor_names_;
+  }
+
+  std::vector<int32_t> TensorShape(const std::string &name) const {
+    std::vector<int32_t> shape;
+
+    if (!HasTensor(name)) {
+      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
+      return shape;
+    }
+
+    auto t = name2tensor_.at(name);
+
+    shape = {t->v1.dimensions, t->v1.dimensions + t->v1.rank};
+
+    return shape;
+  }
+
+  int32_t TensorSizeInBytes(const std::string &name) const {
+    if (!HasTensor(name)) {
+      return 0;
+    }
+
+    return name2tensor_.at(name)->v1.clientBuf.dataSize;
+  }
+
+  bool HasTensor(const std::string &name) const {
+    return name2tensor_.count(name);
+  }
+
+  bool SetInputTensorData(const std::string &name, const float *p, int32_t n) {
+    if (!HasTensor(name)) {
+      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
+      return false;
+    }
+
+    auto t = name2tensor_.at(name);
+    if (t->v1.dataType != QNN_DATATYPE_UFIXED_POINT_16) {
+      SHERPA_ONNX_LOGE(
+          "tensor '%s' should be of type "
+          "QNN_DATATYPE_UFIXED_POINT_16, but it is %s",
+          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
+      return false;
+    }
+
+    if (t->v1.quantizeParams.quantizationEncoding !=
+        QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
+      SHERPA_ONNX_LOGE(
+          "tensor '%s' should be quantized with "
+          "QNN_QUANTIZATION_ENCODING_SCALE_OFFSET, but it is %s",
+          name.c_str(),
+          QuantizationEncodingToString(
+              t->v1.quantizeParams.quantizationEncoding)
+              .c_str());
+      return false;
+    }
+
+    if (n * sizeof(uint16_t) != t->v1.clientBuf.dataSize) {
+      SHERPA_ONNX_LOGE("tensor '%s' expects %d bytes, but you provide %d bytes",
+                       name.c_str(),
+                       static_cast<int32_t>(t->v1.clientBuf.dataSize),
+                       static_cast<int32_t>(n * sizeof(uint16_t)));
+      return false;
+    }
+
+    FillData(t, p, n);
+    SHERPA_ONNX_LOGE("set %s", name.c_str());
+
+    return true;
+  }
+
+  bool SetInputTensorData(const std::string &name, const int32_t *p,
+                          int32_t n) {
+    if (!HasTensor(name)) {
+      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
+      return false;
+    }
+
+    auto t = name2tensor_.at(name);
+    if (t->v1.dataType != QNN_DATATYPE_INT_32) {
+      SHERPA_ONNX_LOGE(
+          "tensor '%s' should be of type "
+          "QNN_DATATYPE_INT_32, but it is %s",
+          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
+      return false;
+    }
+
+    if (n * sizeof(int32_t) != t->v1.clientBuf.dataSize) {
+      SHERPA_ONNX_LOGE("tensor '%s' expects %d bytes, but you provide %d bytes",
+                       name.c_str(),
+                       static_cast<int32_t>(t->v1.clientBuf.dataSize),
+                       static_cast<int32_t>(n * sizeof(int32_t)));
+      return false;
+    }
+
+    FillData(t, p, n);
+    SHERPA_ONNX_LOGE("set %s", name.c_str());
+
+    return true;
+  }
+
+  std::vector<float> GetOutputTensorData(const std::string &name) {
+    if (!HasTensor(name)) {
+      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
+      return {};
+    }
+
+    auto t = name2tensor_.at(name);
+    if (t->v1.dataType != QNN_DATATYPE_UFIXED_POINT_16) {
+      SHERPA_ONNX_LOGE(
+          "tensor '%s' should be of type "
+          "QNN_DATATYPE_UFIXED_POINT_16, but it is %s",
+          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
+      return {};
+    }
+
+    if (t->v1.quantizeParams.quantizationEncoding !=
+        QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
+      SHERPA_ONNX_LOGE(
+          "tensor '%s' should be quantized with "
+          "QNN_QUANTIZATION_ENCODING_SCALE_OFFSET, but it is %s",
+          name.c_str(),
+          QuantizationEncodingToString(
+              t->v1.quantizeParams.quantizationEncoding)
+              .c_str());
+      return {};
+    }
+
+    int32_t n = t->v1.clientBuf.dataSize / sizeof(uint16_t);
+    std::vector<float> ans(n);
+
+    GetData(t, ans.data(), n);
+
+    return ans;
+  }
+
+  bool Run() {
+    std::vector<Qnn_Tensor_t> input_tensors_raw;
+    std::vector<Qnn_Tensor_t> output_tensors_raw;
+
+    input_tensors_raw.reserve(input_tensors_.size());
+    output_tensors_raw.reserve(output_tensors_.size());
+
+    for (const auto &p : input_tensors_) {
+      input_tensors_raw.push_back(*p);
+    }
+
+    for (const auto &p : output_tensors_) {
+      output_tensors_raw.push_back(*p);
+    }
+
+    auto ret = backend_->QnnInterface().graphExecute(
+        graph_handle_, input_tensors_raw.data(), input_tensors_raw.size(),
+        output_tensors_raw.data(), output_tensors_raw.size(), nullptr, nullptr);
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to run graphExecute");
+
+    return true;
+  }
+
+  bool IsInitialized() const { return is_initialized_; }
+
+ private:
+  void PostInit() {
+    AllocateBuffer();
+    SetupPointers();
+
+    is_initialized_ = true;
+  }
+
+  bool InitModel(const std::string &model_so) {
+    model_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
+        dlopen(model_so.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
+    if (!model_lib_handle_) {
+      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
+                       model_so.c_str(), dlerror());
+      return false;
+    }
+    SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
+
+    return true;
+  }
+
+  bool InitSymbols() {
+    const char *symbol = "QnnModel_composeGraphs";
+
+    compose_graphs_fn_handle_ = reinterpret_cast<ComposeGraphsFnHandleType>(
+        dlsym(model_lib_handle_.get(), symbol));
+    if (!compose_graphs_fn_handle_) {
+      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
+                       dlerror());
+      return false;
+    }
+
+    symbol = "QnnModel_freeGraphsInfo";
+    free_graph_info_fn_handle_ = reinterpret_cast<FreeGraphInfoFnHandleType>(
+        dlsym(model_lib_handle_.get(), symbol));
+    if (!free_graph_info_fn_handle_) {
+      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
+                       dlerror());
+      return false;
+    }
+    return true;
+  }
+
+  void InitGraph() {
+    const GraphConfigInfo **graph_configs_info = nullptr;
+
+    uint32_t graph_configs_info_count = 0;
+    GraphInfo **graphs_info = nullptr;
+    uint32_t graphs_count = 0;
+
+    auto ret = compose_graphs_fn_handle_(
+        backend_->BackendHandle(), backend_->QnnInterface(),
+        backend_->ContextHandle(), graph_configs_info, graph_configs_info_count,
+        &graphs_info, &graphs_count, debug_, LogCallback, backend_->LogLevel());
+    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call compose_graphs_fn_handle_");
+
+    SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
+
+    for (uint32_t i = 0; i < graphs_count; ++i) {
+      if (debug_) {
+        SHERPA_ONNX_LOGE(
+            "Finalizing graph %d/%d: '%s'", static_cast<int32_t>(i),
+            static_cast<int32_t>(graphs_count), (*graphs_info)[i].graph_name);
+      }
+      ret = backend_->QnnInterface().graphFinalize((*graphs_info)[i].graph,
+                                                   nullptr, nullptr);
+      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call graph_finalize");
+    }
+
+    if (graphs_count > 1) {
+      SHERPA_ONNX_LOGE("We only use the first graph: %s",
+                       (*graphs_info)[0].graph_name);
+    }
+
+    InitInputTensors((*graphs_info)[0]);
+    InitOutputTensors((*graphs_info)[0]);
+
+    graph_handle_ = (*graphs_info)[0].graph;
+  }
+
+  void InitInputTensors(GraphInfo graph) {
+    input_tensors_.reserve(graph.num_input_tensors);
+    input_tensor_names_.reserve(graph.num_input_tensors);
+
+    for (uint32_t i = 0; i < graph.num_input_tensors; ++i) {
+      SHERPA_ONNX_LOGE("input %d", (int)i);
+      auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
+
+      CopyTensorInfo(graph.input_tensors[i], *p);
+      PrintTensor(p->v2);
+
+      std::string name = p->v1.name;
+      name2tensor_[name] = p.get();
+      input_tensor_names_.push_back(std::move(name));
+
+      input_tensors_.push_back(std::move(p));
+    }
+  }
+
+  void InitOutputTensors(GraphInfo graph) {
+    output_tensors_.reserve(graph.num_output_tensors);
+    output_tensor_names_.reserve(graph.num_output_tensors);
+    for (uint32_t i = 0; i < graph.num_output_tensors; ++i) {
+      auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
+
+      CopyTensorInfo(graph.output_tensors[i], *p);
+
+      SHERPA_ONNX_LOGE("output %d", (int)i);
+      PrintTensor(p->v2);
+
+      std::string name = p->v1.name;
+      name2tensor_[name] = p.get();
+      output_tensor_names_.push_back(std::move(name));
+
+      output_tensors_.push_back(std::move(p));
+    }
+  }
+
+  void AllocateBuffer() {
+    uint32_t n = 0;
+    for (const auto &p : name2tensor_) {
+      n += p.second->v1.clientBuf.dataSize;
+    }
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("Allocate %d bytes, or %.3f MB", static_cast<int32_t>(n),
+                       static_cast<float>(n) / 1024 / 1024);
+    }
+
+    buffer_.resize(n);
+  }
+
+  void SetupPointers() {
+    uint8_t *p = buffer_.data();
+    uint32_t n = 0;
+    for (auto &t : input_tensors_) {
+      t->v1.clientBuf.data = p;
+      p += t->v1.clientBuf.dataSize;
+    }
+
+    for (auto &t : output_tensors_) {
+      t->v1.clientBuf.data = p;
+      p += t->v1.clientBuf.dataSize;
+    }
+
+    if (debug_) {
+      if (p == buffer_.data() + buffer_.size()) {
+        SHERPA_ONNX_LOGE("Setup pointers successfully.");
+      } else {
+        SHERPA_ONNX_LOGE("Bad things happened in setting up pointers.");
+      }
+    }
+  }
+
+ private:
+  bool debug_ = true;
+  std::unique_ptr<void, decltype(&dlclose)> model_lib_handle_{nullptr,
+                                                              &dlclose};
+
+  std::unique_ptr<void, decltype(&dlclose)> system_lib_handle_{nullptr,
+                                                               &dlclose};
+
+  QNN_SYSTEM_INTERFACE_VER_TYPE qnn_system_interface_;
+
+  ComposeGraphsFnHandleType compose_graphs_fn_handle_ = nullptr;
+  FreeGraphInfoFnHandleType free_graph_info_fn_handle_ = nullptr;
+
+  std::vector<TensorPtr> input_tensors_;
+  std::vector<TensorPtr> output_tensors_;
+
+  std::vector<std::string> input_tensor_names_;
+  std::vector<std::string> output_tensor_names_;
+
+  std::unordered_map<std::string, Qnn_Tensor_t *> name2tensor_;
+
+  std::vector<uint8_t> buffer_;
+  const QnnBackend *backend_ = nullptr;
+
+  Qnn_GraphHandle_t graph_handle_ = nullptr;
+
+  const QnnContext_Config_t **context_config_ = nullptr;
+  bool is_initialized_ = false;
+};
+
+QnnModel::~QnnModel() = default;
+
+QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend)
+    : impl_(std::make_unique<Impl>(model_so, backend)) {}
+
+QnnModel::QnnModel(const std::string &binary_context_file,
+                   const std::string &system_lib, const QnnBackend *backend,
+                   BinaryContextTag tag)
+    : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
+                                   tag)) {}
+
+bool QnnModel::SaveBinaryContext(const std::string &filename) const {
+  return impl_->SaveBinaryContext(filename);
+}
+
+const std::vector<std::string> &QnnModel::InputTensorNames() const {
+  return impl_->InputTensorNames();
+}
+
+const std::vector<std::string> &QnnModel::OutputTensorNames() const {
+  return impl_->OutputTensorNames();
+}
+
+std::vector<int32_t> QnnModel::TensorShape(const std::string &name) const {
+  return impl_->TensorShape(name);
+}
+
+int32_t QnnModel::TensorSizeInBytes(const std::string &name) const {
+  return impl_->TensorSizeInBytes(name);
+}
+
+bool QnnModel::HasTensor(const std::string &name) const {
+  return impl_->HasTensor(name);
+}
+
+bool QnnModel::SetInputTensorData(const std::string &name, const float *p,
+                                  int32_t n) const {
+  return impl_->SetInputTensorData(name, p, n);
+}
+
+bool QnnModel::SetInputTensorData(const std::string &name, const int32_t *p,
+                                  int32_t n) const {
+  return impl_->SetInputTensorData(name, p, n);
+}
+
+std::vector<float> QnnModel::GetOutputTensorData(
+    const std::string &name) const {
+  return impl_->GetOutputTensorData(name);
+}
+
+bool QnnModel::Run() const { return impl_->Run(); }
+
+bool QnnModel::IsInitialized() const { return impl_->IsInitialized(); }
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/qnn/qnn-model.h b/sherpa-onnx/csrc/qnn/qnn-model.h
new file mode 100644
index 00000000..d4cbfa3d
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/qnn-model.h
@@ -0,0 +1,55 @@
+// sherpa-onnx/csrc/qnn/qnn-model.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
+#define SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
+
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "QnnInterface.h"
+
+namespace sherpa_onnx {
+
+class QnnBackend;
+
+struct BinaryContextTag {};
+
+class QnnModel {
+ public:
+  QnnModel(const std::string &model_so, const QnnBackend *backend);
+  QnnModel(const std::string &binary_context_file,
+           const std::string &system_lib, const QnnBackend *backend,
+           BinaryContextTag tag);
+  ~QnnModel();
+
+  bool SaveBinaryContext(const std::string &filename) const;
+
+  const std::vector<std::string> &InputTensorNames() const;
+  const std::vector<std::string> &OutputTensorNames() const;
+
+  std::vector<int32_t> TensorShape(const std::string &name) const;
+  int32_t TensorSizeInBytes(const std::string &name) const;
+
+  bool HasTensor(const std::string &name) const;
+
+  bool SetInputTensorData(const std::string &name, const float *p,
+                          int32_t n) const;
+
+  bool SetInputTensorData(const std::string &name, const int32_t *p,
+                          int32_t n) const;
+
+  std::vector<float> GetOutputTensorData(const std::string &name) const;
+
+  bool Run() const;
+  bool IsInitialized() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
diff --git a/sherpa-onnx/csrc/qnn/utils.cc b/sherpa-onnx/csrc/qnn/utils.cc
new file mode 100644
index 00000000..20c498f7
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/utils.cc
@@ -0,0 +1,546 @@
+// sherpa-onnx/csrc/qnn/utils.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#include "sherpa-onnx/csrc/qnn/utils.h"
+
+#include <math.h>
+#include <stdio.h>
+
+#include <algorithm>
+#include <functional>
+#include <numeric>
+#include <sstream>
+#include <string>
+
+#include "sherpa-onnx/csrc/qnn/macros.h"
+
+#define SHERPA_ONNX_TO_STRING(s) \
+  case s:                        \
+    return #s
+
+std::string TensorTypeToString(Qnn_TensorType_t t) {
+  switch (t) {
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_WRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_READ);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_READWRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_NATIVE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_STATIC);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_NULL);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_STATIC);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_NATIVE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_WRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_READ);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_READWRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_WRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_READ);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_READWRITE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UNDEFINED);
+  }
+  return "Unknown";
+}
+
+std::string QuantizationEncodingToString(Qnn_QuantizationEncoding_t q) {
+  switch (q) {
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_SCALE_OFFSET);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_AXIS_SCALE_OFFSET);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BW_SCALE_OFFSET);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BW_AXIS_SCALE_OFFSET);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BLOCK);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BLOCKWISE_EXPANSION);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_VECTOR);
+    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_UNDEFINED);
+  }
+  return "Unknown";
+}
+
+std::string TensorDataTypeToString(Qnn_DataType_t t) {
+  switch (t) {
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_8);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_16);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_32);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_64);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_8);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_16);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_32);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_64);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_16);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_32);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_64);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_4);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_8);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_16);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_32);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_4);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_8);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_16);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_32);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_BOOL_8);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_STRING);
+    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UNDEFINED);
+  }
+  return "unknown";
+}
+
+std::string TensorMemTypeToString(Qnn_TensorMemType_t t) {
+  switch (t) {
+    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_RAW);
+    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_MEMHANDLE);
+    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_RETRIEVE_RAW);
+    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_UNDEFINED);
+  }
+  return "Unknown";
+}
+
+#undef SHERPA_ONNX_TO_STRING
+
+// quantized = float / scale - offset;
+void FillData(Qnn_Tensor_t *t, const float *data, int32_t n) {
+  float scale = t->v1.quantizeParams.scaleOffsetEncoding.scale;
+  int32_t offset = t->v1.quantizeParams.scaleOffsetEncoding.offset;
+
+  size_t bit_width = 16;
+  double true_bit_width_max = pow(2, bit_width) - 1;
+  double encoding_min = offset * scale;
+  double encoding_max = (true_bit_width_max + offset) * scale;
+  double encoding_range = encoding_max - encoding_min;
+
+  uint16_t *out = reinterpret_cast<uint16_t *>(t->v1.clientBuf.data);
+
+  for (size_t i = 0; i < n; ++i) {
+    int32_t quantized_value =
+        round(true_bit_width_max * (data[i] - encoding_min) / encoding_range);
+
+    if (quantized_value < 0) {
+      quantized_value = 0;
+    } else if (quantized_value > static_cast<int32_t>(true_bit_width_max)) {
+      quantized_value = static_cast<int32_t>(true_bit_width_max);
+    }
+    out[i] = static_cast<uint16_t>(quantized_value);
+  }
+}
+
+void FillData(Qnn_Tensor_t *t, const int32_t *data, int32_t n) {
+  int32_t *out = reinterpret_cast<int32_t *>(t->v1.clientBuf.data);
+  std::copy(data, data + n, out);
+}
+
+void GetData(const Qnn_Tensor_t *t, float *data, int32_t n) {
+  double scale = t->v1.quantizeParams.scaleOffsetEncoding.scale;
+  double offset = t->v1.quantizeParams.scaleOffsetEncoding.offset;
+
+  const uint16_t *p = reinterpret_cast<const uint16_t *>(t->v1.clientBuf.data);
+  for (int32_t i = 0; i < n; ++i) {
+    double quantizedValue = static_cast<double>(p[i]);
+    data[i] = (quantizedValue + offset) * scale;
+  }
+}
+
+static void FreeTensorV1(Qnn_Tensor_t *t) {
+  free(const_cast<char *>(t->v1.name));
+
+  delete[] t->v1.dimensions;
+}
+
+static void FreeTensorV2(Qnn_Tensor_t *t) {
+  free(const_cast<char *>(t->v2.name));
+
+  delete[] t->v2.dimensions;
+  delete[] t->v2.isDynamicDimensions;
+}
+
+void FreeTensor(Qnn_Tensor_t *t) {
+  if (t->version == QNN_TENSOR_VERSION_1) {
+    FreeTensorV1(t);
+  } else if (t->version == QNN_TENSOR_VERSION_2) {
+    FreeTensorV2(t);
+  } else {
+    SHERPA_ONNX_LOGE("Unknown tensor version: %d", t->version);
+  }
+}
+
+uint32_t GetSizeInBytes(const uint32_t *dimensions, uint32_t n,
+                        Qnn_DataType_t type) {
+  if (n == 0) {
+    return 0;
+  }
+
+  auto count = std::accumulate(dimensions, dimensions + n, 1,
+                               std::multiplies<uint32_t>());
+
+  uint32_t b = 1;
+  switch (type) {
+    case QNN_DATATYPE_INT_8:
+      b = 1;
+      break;
+    case QNN_DATATYPE_INT_16:
+      b = 2;
+      break;
+    case QNN_DATATYPE_INT_32:
+      b = 4;
+      break;
+    case QNN_DATATYPE_INT_64:
+      b = 8;
+      break;
+    case QNN_DATATYPE_UINT_8:
+      b = 1;
+      break;
+    case QNN_DATATYPE_UINT_16:
+      b = 2;
+      break;
+    case QNN_DATATYPE_UINT_32:
+      b = 4;
+      break;
+    case QNN_DATATYPE_UINT_64:
+      b = 8;
+      break;
+    case QNN_DATATYPE_FLOAT_16:
+      b = 2;
+      break;
+    case QNN_DATATYPE_FLOAT_32:
+      b = 4;
+      break;
+    case QNN_DATATYPE_FLOAT_64:
+      b = 8;
+      break;
+    case QNN_DATATYPE_SFIXED_POINT_8:
+      b = 1;
+      break;
+    case QNN_DATATYPE_SFIXED_POINT_16:
+      b = 2;
+      break;
+    case QNN_DATATYPE_SFIXED_POINT_32:
+      b = 4;
+      break;
+    case QNN_DATATYPE_UFIXED_POINT_8:
+      b = 1;
+      break;
+    case QNN_DATATYPE_UFIXED_POINT_16:
+      b = 2;
+      break;
+    case QNN_DATATYPE_UFIXED_POINT_32:
+      b = 4;
+      break;
+    case QNN_DATATYPE_BOOL_8:
+      b = 1;
+      break;
+    default:
+      SHERPA_ONNX_LOGE("Unsupported data type: %s",
+                       TensorDataTypeToString(type).c_str());
+      break;
+  }
+
+  return count * b;
+}
+
+template <typename T>
+void CopyDimensions(const T *src, uint32_t n, T **dst) {
+  if (!src || n == 0) {
+    *dst = nullptr;
+    return;
+  }
+
+  *dst = new T[n];
+  std::copy(src, src + n, *dst);
+}
+
+static void CopyQuantizeParams(const Qnn_QuantizeParams_t &src,
+                               Qnn_QuantizeParams_t &dst) {  // NOLINT
+  dst.encodingDefinition = src.encodingDefinition;
+  dst.quantizationEncoding = src.quantizationEncoding;
+
+  switch (src.quantizationEncoding) {
+    case QNN_QUANTIZATION_ENCODING_SCALE_OFFSET:
+      dst.scaleOffsetEncoding = src.scaleOffsetEncoding;
+      break;
+    case QNN_QUANTIZATION_ENCODING_UNDEFINED:
+      // do nothing in this case
+      break;
+    default:
+      SHERPA_ONNX_LOGE(
+          "Unsupported quantizationEncoding: %s",
+          QuantizationEncodingToString(src.quantizationEncoding).c_str());
+  }
+}
+
+static void CopyTensorInfoV1(const Qnn_Tensor_t &src,
+                             Qnn_Tensor_t &dst) {  // NOLINT
+  dst.version = src.version;
+  dst.v1.id = src.v1.id;
+  if (src.v1.name) {
+    dst.v1.name = strdup(src.v1.name);
+  } else {
+    dst.v1.name = strdup("");
+  }
+
+  dst.v1.type = src.v1.type;
+  dst.v1.dataFormat = src.v1.dataFormat;
+  dst.v1.dataType = src.v1.dataType;
+
+  CopyQuantizeParams(src.v1.quantizeParams, dst.v1.quantizeParams);
+
+  dst.v1.rank = src.v1.rank;
+
+  CopyDimensions(src.v1.dimensions, src.v1.rank, &dst.v1.dimensions);
+
+  dst.v1.memType = src.v1.memType;
+  if (dst.v1.memType != QNN_TENSORMEMTYPE_RAW) {
+    SHERPA_ONNX_LOGE("Unsupported mem type: %s",
+                     TensorMemTypeToString(dst.v1.memType).c_str());
+  } else {
+    dst.v1.clientBuf.data = nullptr;
+    dst.v1.clientBuf.dataSize =
+        GetSizeInBytes(dst.v1.dimensions, dst.v1.rank, dst.v1.dataType);
+  }
+}
+
+static void CopyTensorInfoV2(const Qnn_Tensor_t &src,
+                             Qnn_Tensor_t &dst) {  // NOLINT
+  dst.version = src.version;
+  dst.v2.id = src.v2.id;
+  if (src.v2.name) {
+    dst.v2.name = strdup(src.v2.name);
+  } else {
+    dst.v2.name = strdup("");
+  }
+
+  dst.v2.type = src.v2.type;
+  dst.v2.dataFormat = src.v2.dataFormat;
+  dst.v2.dataType = src.v2.dataType;
+
+  CopyQuantizeParams(src.v2.quantizeParams, dst.v2.quantizeParams);
+
+  dst.v2.rank = src.v2.rank;
+
+  CopyDimensions(src.v2.dimensions, src.v2.rank, &dst.v2.dimensions);
+
+  dst.v2.memType = src.v2.memType;
+  if (dst.v2.memType != QNN_TENSORMEMTYPE_RAW) {
+    SHERPA_ONNX_LOGE("Unsupported mem type: %s",
+                     TensorMemTypeToString(dst.v2.memType).c_str());
+  } else {
+    dst.v2.clientBuf.data = nullptr;
+    dst.v2.clientBuf.dataSize =
+        GetSizeInBytes(dst.v2.dimensions, dst.v2.rank, dst.v2.dataType);
+  }
+
+  CopyDimensions(src.v2.isDynamicDimensions, src.v2.rank,
+                 &dst.v2.isDynamicDimensions);
+
+  dst.v2.sparseParams.type = src.v2.sparseParams.type;
+  dst.v2.sparseParams.hybridCoo.numSpecifiedElements =
+      src.v2.sparseParams.hybridCoo.numSpecifiedElements;
+  dst.v2.sparseParams.hybridCoo.numSparseDimensions =
+      src.v2.sparseParams.hybridCoo.numSparseDimensions;
+  dst.v2.isProduced = src.v2.isProduced;
+}
+
+void CopyTensorInfo(const Qnn_Tensor_t &src, Qnn_Tensor_t &dst) {  // NOLINT
+  if (src.version == QNN_TENSOR_VERSION_1) {
+    CopyTensorInfoV1(src, dst);
+  } else if (src.version == QNN_TENSOR_VERSION_2) {
+    CopyTensorInfoV2(src, dst);
+  } else {
+    SHERPA_ONNX_LOGE("Unknown tensor version: %d", dst.version);
+  }
+}
+
+void LogCallback(const char *fmt, QnnLog_Level_t level, uint64_t timestamp,
+                 va_list args) {
+  std::string s;
+  switch (level) {
+    case QNN_LOG_LEVEL_ERROR:
+      s = "ERROR";
+      break;
+    case QNN_LOG_LEVEL_WARN:
+      s = "WARN";
+      break;
+    case QNN_LOG_LEVEL_INFO:
+      s = "INFO";
+      break;
+    case QNN_LOG_LEVEL_DEBUG:
+      s = "DEBUG";
+      break;
+    case QNN_LOG_LEVEL_VERBOSE:
+      s = "VERBOSE";
+      break;
+    case QNN_LOG_LEVEL_MAX:
+      s = "UNKNOWN";
+      break;
+  }
+
+  double ms = timestamp / 1000000.0;
+  fprintf(stdout, "%8.1fms [%-7s] ", ms, s.c_str());
+  vfprintf(stdout, fmt, args);
+}
+
+void PrintTensor(Qnn_TensorV2_t t) {
+  std::ostringstream os;
+  os << "  id: " << t.id << "\n";
+  os << "  name: " << t.name << "\n";
+  os << "  type: " << TensorTypeToString(t.type) << "\n";
+  os << "  data format: " << t.dataFormat << "\n";
+  os << "  data type: " << TensorDataTypeToString(t.dataType) << "\n";
+  os << "  quantize info: \n";
+  auto qp = t.quantizeParams;
+  os << "    encodingDefinition: " << std::hex << "0x" << qp.encodingDefinition
+     << std::dec << "\n";
+  os << "    quantizationEncoding: "
+     << QuantizationEncodingToString(qp.quantizationEncoding) << "\n";
+  if (qp.quantizationEncoding == QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
+    Qnn_ScaleOffset_t s = qp.scaleOffsetEncoding;
+    os << "     scale: " << s.scale << "\n";
+    os << "     offset: " << s.offset << "\n";
+  }
+  os << "  rank: " << t.rank << "\n";
+  os << "  dimensions: ";
+  for (int32_t i = 0; i < t.rank; ++i) {
+    os << t.dimensions[i] << ", ";
+    if (i + 1 == t.rank) {
+      os << "\n";
+    }
+  }
+  os << "  memType: " << TensorMemTypeToString(t.memType) << "\n";
+  if (t.memType == QNN_TENSORMEMTYPE_RAW) {
+    os << " memType raw data size: " << t.clientBuf.dataSize << "\n";
+  }
+  os << "  isDynamicDimensions: "
+     << ((t.isDynamicDimensions != nullptr) ? "True" : "False") << "\n";
+  os << "  isProduced: " << static_cast<int32_t>(t.isProduced) << "\n";
+
+  SHERPA_ONNX_LOGE("%s", os.str().c_str());
+}
+
+static bool CopyGraphsInfoV3(const QnnSystemContext_GraphInfoV3_t *src,
+                             GraphInfo *dst) {
+  if (src->graphName) {
+    dst->graph_name = strdup(src->graphName);
+  } else {
+    dst->graph_name = strdup("");
+  }
+
+  dst->input_tensors = nullptr;
+  dst->num_input_tensors = 0;
+
+  if (src->graphInputs) {
+    dst->input_tensors = reinterpret_cast<Qnn_Tensor_t *>(
+        calloc(src->numGraphInputs, sizeof(Qnn_Tensor_t)));
+
+    for (uint32_t i = 0; i < src->numGraphInputs; ++i) {
+      dst->input_tensors[i] = QNN_TENSOR_INIT;
+
+      CopyTensorInfo(src->graphInputs[i], dst->input_tensors[i]);
+    }
+
+    dst->num_input_tensors = src->numGraphInputs;
+  }
+
+  dst->output_tensors = nullptr;
+  dst->num_output_tensors = 0;
+
+  if (src->graphOutputs) {
+    dst->output_tensors = reinterpret_cast<Qnn_Tensor_t *>(
+        calloc(src->numGraphOutputs, sizeof(Qnn_Tensor_t)));
+
+    for (uint32_t i = 0; i < src->numGraphOutputs; ++i) {
+      dst->output_tensors[i] = QNN_TENSOR_INIT;
+
+      CopyTensorInfo(src->graphOutputs[i], dst->output_tensors[i]);
+    }
+
+    dst->num_output_tensors = src->numGraphOutputs;
+  }
+
+  return true;
+}
+
+static bool CopyGraphsInfo(const QnnSystemContext_GraphInfo_t *graphs_input,
+                           uint32_t num_graphs,
+                           GraphInfo **&graphs_info) {  // NOLINT
+  if (num_graphs == 0) {
+    SHERPA_ONNX_LOGE("empty graphs");
+    graphs_info = nullptr;
+    return false;
+  }
+
+  SHERPA_ONNX_LOGE("version: %d", (int)graphs_input[0].version);
+
+  // remember to free graphs_info
+  graphs_info =
+      reinterpret_cast<GraphInfo **>(calloc(num_graphs, sizeof(GraphInfo *)));
+
+  GraphInfo *graph_info_arr =
+      reinterpret_cast<GraphInfo *>(calloc(num_graphs, sizeof(GraphInfo)));
+
+  if (!graphs_info || !graph_info_arr) {
+    SHERPA_ONNX_LOGE("Failure to allocate memory for *graphInfo");
+    return false;
+  }
+
+  for (uint32_t i = 0; i < num_graphs; ++i) {
+    switch (graphs_input[i].version) {
+      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_1:
+        SHERPA_ONNX_LOGE("Unsupported version: %d",
+                         static_cast<int32_t>(graphs_input[i].version));
+        return false;
+
+      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_2:
+        SHERPA_ONNX_LOGE("Unsupported version: %d",
+                         static_cast<int32_t>(graphs_input[i].version));
+        return false;
+
+      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_3: {
+        bool ok =
+            CopyGraphsInfoV3(&graphs_input[i].graphInfoV3, &graph_info_arr[i]);
+        if (!ok) {
+          SHERPA_ONNX_LOGE("Failed to copy graphs info v3");
+        }
+        graphs_info[i] = graph_info_arr + i;
+
+        break;
+      }
+
+      default:
+        SHERPA_ONNX_LOGE("Unsupported version: %d",
+                         static_cast<int32_t>(graphs_input[i].version));
+        return false;
+    }
+  }
+
+  return true;
+}
+
+bool CopyMetadataToGraphsInfo(const QnnSystemContext_BinaryInfo_t *binary_info,
+                              GraphInfo **&graphs_info,  // NOLINT
+                              uint32_t &graphs_count) {  // NOLINT
+  graphs_count = 0;
+
+  switch (binary_info->version) {
+    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1: {
+      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
+                       binary_info->version);
+      return false;
+    }
+    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2: {
+      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
+                       binary_info->version);
+      return false;
+    }
+    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3: {
+      bool ok = CopyGraphsInfo(binary_info->contextBinaryInfoV3.graphs,
+                               binary_info->contextBinaryInfoV3.numGraphs,
+                               graphs_info);
+
+      if (!ok) {
+        SHERPA_ONNX_LOGE("Failed while copying graphs Info v3.");
+        return false;
+      }
+      graphs_count = binary_info->contextBinaryInfoV3.numGraphs;
+      return true;
+    }
+    default: {
+      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
+                       binary_info->version);
+      return false;
+    }
+  }
+}
diff --git a/sherpa-onnx/csrc/qnn/utils.h b/sherpa-onnx/csrc/qnn/utils.h
new file mode 100644
index 00000000..53663ecd
--- /dev/null
+++ b/sherpa-onnx/csrc/qnn/utils.h
@@ -0,0 +1,94 @@
+// sherpa-onnx/csrc/qnn/utils.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_QNN_UTILS_H_
+#define SHERPA_ONNX_CSRC_QNN_UTILS_H_
+#include <stdio.h>
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "QnnInterface.h"
+#include "System/QnnSystemInterface.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+template <typename T>
+std::vector<T> ReadFile(const std::string &filename) {
+  FILE *fp = fopen(filename.c_str(), "rb");
+  if (!fp) {
+    SHERPA_ONNX_LOGE("Failed to open '%s'", filename.c_str());
+    return {};
+  }
+
+  fseek(fp, 0, SEEK_END);
+  int32_t n = ftell(fp);
+  fseek(fp, 0, SEEK_SET);
+
+  std::vector<T> ans(n / sizeof(T));
+  fread(ans.data(), sizeof(T), ans.size(), fp);
+  fclose(fp);
+
+  return ans;
+}
+
+void PrintTensor(Qnn_TensorV2_t t);
+
+// float -> uint16_t
+void FillData(Qnn_Tensor_t *t, const float *data, int32_t n);
+
+// int32_t -> int32_t
+void FillData(Qnn_Tensor_t *t, const int32_t *data, int32_t n);
+
+// uint16_t -> float
+void GetData(const Qnn_Tensor_t *t, float *data, int32_t n);
+
+void FreeTensor(Qnn_Tensor_t *t);
+
+using TensorPtr = std::unique_ptr<Qnn_Tensor_t, decltype(&FreeTensor)>;
+
+void CopyTensorInfo(const Qnn_Tensor_t &src, Qnn_Tensor_t &dst);  // NOLINT
+
+std::string QuantizationEncodingToString(Qnn_QuantizationEncoding_t q);
+
+std::string TensorDataTypeToString(Qnn_DataType_t t);
+
+using QnnInterfaceGetProvidersFnType = Qnn_ErrorHandle_t (*)(
+    const QnnInterface_t ***provider_list, uint32_t *num_providers);
+
+using QnnSystemInterfaceGetProvidersFnType = Qnn_ErrorHandle_t (*)(
+    const QnnSystemInterface_t ***provider_list, uint32_t *num_providers);
+
+struct GraphInfo {
+  Qnn_GraphHandle_t graph;
+  char *graph_name;
+  Qnn_Tensor_t *input_tensors;
+  uint32_t num_input_tensors;
+  Qnn_Tensor_t *output_tensors;
+  uint32_t num_output_tensors;
+};
+
+struct GraphConfigInfo {
+  char *graph_name;
+  const QnnGraph_Config_t **graph_configs;
+};
+
+using ComposeGraphsFnHandleType = Qnn_ErrorHandle_t (*)(
+    Qnn_BackendHandle_t backend_handle, QNN_INTERFACE_VER_TYPE interface,
+    Qnn_ContextHandle_t context_handle,
+    const GraphConfigInfo **graphs_config_info,
+    const uint32_t num_graphs_config_info, GraphInfo ***graphs_info,
+    uint32_t *num_graphs_info, bool debug, QnnLog_Callback_t logCallback,
+    QnnLog_Level_t max_log_level);
+
+using FreeGraphInfoFnHandleType =
+    Qnn_ErrorHandle_t (*)(GraphInfo ***, uint32_t num_graphs_info);
+
+void LogCallback(const char *fmt, QnnLog_Level_t level, uint64_t timestamp,
+                 va_list args);
+
+bool CopyMetadataToGraphsInfo(const QnnSystemContext_BinaryInfo_t *binary_info,
+                              GraphInfo **&graphs_info,  // NOLINT
+                              uint32_t &graphs_count);   // NOLINT
+#endif  // SHERPA_ONNX_CSRC_QNN_UTILS_H_

commit 2b81e4d54236b04095334c84122dc0925201746a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Nov 11 14:29:02 2025 +0800

    Support passing multiple lexicon files for matcha tts models. (#2765)

diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index e5c9711d..1cf0eef7 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -742,7 +742,7 @@ struct OnlinePunctuationModelConfig {
   std::string cnn_bilstm;
   std::string bpe_vocab;
   int32_t num_threads = 1;
-  int32_t debug = false;
+  bool debug = false;
   std::string provider = "cpu";
 };
 
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
index 11f58f7c..b557fe08 100644
--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -57,10 +57,7 @@ class MatchaTtsLexicon::Impl {
       InitTokens(is);
     }
 
-    {
-      std::ifstream is(lexicon);
-      InitLexicon(is);
-    }
+    InitLexicon(lexicon);
 
     if (data_dir.empty()) {
       SHERPA_ONNX_LOGE("Please provide data dir for this model");
@@ -86,8 +83,11 @@ class MatchaTtsLexicon::Impl {
       InitTokens(is);
     }
 
-    {
-      auto buf = ReadFile(mgr, lexicon);
+    std::vector<std::string> files;
+    SplitStringToVector(lexicon, ",", false, &files);
+    for (const auto &f : files) {
+      auto buf = ReadFile(mgr, f);
+
       std::istrstream is(buf.data(), buf.size());
       InitLexicon(is);
     }
@@ -103,7 +103,7 @@ class MatchaTtsLexicon::Impl {
   std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
     std::string text = _text;
     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
-        {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
+        {"", ","}, {"", ","}, {"", ";"}, {"", ":"},
         {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
     };
     for (const auto &p : replace_str_pairs) {
@@ -205,6 +205,18 @@ class MatchaTtsLexicon::Impl {
       this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
 
       if (IsPunct(w)) {
+        if (debug_) {
+          std::ostringstream os;
+          std::string sep;
+          os << "new sentence: [";
+          for (auto i : this_sentence) {
+            os << sep << i;
+            sep = ", ";
+          }
+          os << "]";
+          SHERPA_ONNX_LOGE("%s", os.str().c_str());
+        }
+
         ans.emplace_back(std::move(this_sentence));
         this_sentence = {};
       }
@@ -220,7 +232,6 @@ class MatchaTtsLexicon::Impl {
  private:
   std::vector<int32_t> ConvertWordToIds(const std::string &w) const {
     std::vector<int32_t> ans;
-
     if (word2ids_.count(w)) {
       ans = word2ids_.at(w);
     } else if (token2id_.count(w)) {
@@ -255,11 +266,15 @@ class MatchaTtsLexicon::Impl {
       }
     }
 
+    if (IsAlphaOrPunct(w.front())) {
+      ans.push_back(token2id_.at(" "));
+    }
+
     if (debug_) {
       std::ostringstream os;
       os << w << ": ";
       for (auto i : ans) {
-        os << id2token_.at(i) << " ";
+        os << "'" << id2token_.at(i) << "'(" << i << ")" << ",";
       }
 #if __OHOS__
       SHERPA_ONNX_LOGE("%{public}s", os.str().c_str());
@@ -274,29 +289,6 @@ class MatchaTtsLexicon::Impl {
   void InitTokens(std::istream &is) {
     token2id_ = ReadTokens(is);
 
-    std::vector<std::pair<std::string, std::string>> puncts = {
-        {",", ""}, {".", ""}, {"!", ""}, {"?", ""}, {":", ""},
-        {"\"", ""}, {"\"", ""}, {"'", ""},  {"'", ""},  {";", ""},
-    };
-
-    for (const auto &p : puncts) {
-      if (token2id_.count(p.first) && !token2id_.count(p.second)) {
-        token2id_[p.second] = token2id_[p.first];
-      }
-
-      if (!token2id_.count(p.first) && token2id_.count(p.second)) {
-        token2id_[p.first] = token2id_[p.second];
-      }
-    }
-
-    if (!token2id_.count("") && token2id_.count("")) {
-      token2id_[""] = token2id_[""];
-    }
-
-    if (!token2id_.count(";") && token2id_.count(",")) {
-      token2id_[";"] = token2id_[","];
-    }
-
     if (debug_) {
       for (const auto &p : token2id_) {
         id2token_[p.second] = p.first;
@@ -327,6 +319,20 @@ class MatchaTtsLexicon::Impl {
     }
   }
 
+  void InitLexicon(const std::string &lexicon) {
+    if (lexicon.empty()) {
+      SHERPA_ONNX_LOGE("Empty lexicon!");
+      return;
+    }
+
+    std::vector<std::string> files;
+    SplitStringToVector(lexicon, ",", false, &files);
+    for (const auto &f : files) {
+      std::ifstream is(f);
+      InitLexicon(is);
+    }
+  }
+
   void InitLexicon(std::istream &is) {
     std::string word;
     std::vector<std::string> token_list;
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
index 1fdbad53..9b379f44 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
@@ -8,6 +8,7 @@
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/text-utils.h"
 
 namespace sherpa_onnx {
 
@@ -15,8 +16,10 @@ void OfflineTtsMatchaModelConfig::Register(ParseOptions *po) {
   po->Register("matcha-acoustic-model", &acoustic_model,
                "Path to matcha acoustic model");
   po->Register("matcha-vocoder", &vocoder, "Path to matcha vocoder");
-  po->Register("matcha-lexicon", &lexicon,
-               "Path to lexicon.txt for Matcha models");
+  po->Register(
+      "matcha-lexicon", &lexicon,
+      "Path to lexicon.txt for Matcha models. You can pass multiple "
+      "files separated by comma , e.g., lexicon.txt,lexicon2.txt,lexicon3.txt");
   po->Register("matcha-tokens", &tokens,
                "Path to tokens.txt for Matcha models");
   po->Register("matcha-data-dir", &data_dir,
@@ -82,9 +85,17 @@ bool OfflineTtsMatchaModelConfig::Validate() const {
     }
   }
 
-  if (!lexicon.empty() && !FileExists(lexicon)) {
-    SHERPA_ONNX_LOGE("--matcha-lexicon: '%s' does not exist", lexicon.c_str());
-    return false;
+  if (!lexicon.empty()) {
+    std::vector<std::string> files;
+    SplitStringToVector(lexicon, ",", false, &files);
+    for (const auto &f : files) {
+      if (!FileExists(f)) {
+        SHERPA_ONNX_LOGE(
+            "lexicon '%s' does not exist. Please re-check --matcha-lexicon",
+            f.c_str());
+        return false;
+      }
+    }
   }
 
   if (!dict_dir.empty()) {
diff --git a/sherpa-onnx/csrc/phrase-matcher.cc b/sherpa-onnx/csrc/phrase-matcher.cc
index bea8bded..35e09555 100644
--- a/sherpa-onnx/csrc/phrase-matcher.cc
+++ b/sherpa-onnx/csrc/phrase-matcher.cc
@@ -56,34 +56,43 @@ class PhraseMatcher::Impl {
     int32_t num_words = static_cast<int32_t>(words.size());
     for (int32_t i = 0; i < num_words;) {
       int32_t start = i;
-      int32_t end = std::min(i + max_search_len_ - 1, num_words - 1);
 
       std::string w;
-      while (end > start) {
-        auto this_word = GetWord(words, start, end);
-        if (debug_) {
+
+      if (!IsAlphaOrPunct(words[i].front())) {
+        int32_t end = std::min(i + max_search_len_ - 1, num_words - 1);
+
+        while (end > start) {
+          auto this_word = GetWord(words, start, end);
+          if (IsAlphaOrPunct(this_word.back())) {
+            --end;
+            continue;
+          }
+
+          if (debug_) {
 #if __OHOS__
-          SHERPA_ONNX_LOGE("%{public}d-%{public}d: %{public}s", start, end,
-                           this_word.c_str());
+            SHERPA_ONNX_LOGE("%{public}d-%{public}d: %{public}s", start, end,
+                             this_word.c_str());
 #else
-          SHERPA_ONNX_LOGE("%d-%d: %s", start, end, this_word.c_str());
+            SHERPA_ONNX_LOGE("%d-%d: %s", start, end, this_word.c_str());
 #endif
-        }
-        if (lexicon_->count(this_word)) {
-          i = end + 1;
-          w = std::move(this_word);
-          if (debug_) {
+          }
+          if (lexicon_->count(this_word)) {
+            i = end + 1;
+            w = std::move(this_word);
+            if (debug_) {
 #if __OHOS__
-            SHERPA_ONNX_LOGE("matched %{public}d-%{public}d: %{public}s", start,
-                             end, w.c_str());
+              SHERPA_ONNX_LOGE("matched %{public}d-%{public}d: %{public}s",
+                               start, end, w.c_str());
 #else
-            SHERPA_ONNX_LOGE("matched %d-%d: %s", start, end, w.c_str());
+              SHERPA_ONNX_LOGE("matched %d-%d: %s", start, end, w.c_str());
 #endif
+            }
+            break;
           }
-          break;
-        }
 
-        end -= 1;
+          end -= 1;
+        }
       }
 
       if (w.empty()) {
diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
index 760ac329..993f2bb0 100644
--- a/sherpa-onnx/csrc/text-utils.cc
+++ b/sherpa-onnx/csrc/text-utils.cc
@@ -808,6 +808,8 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
   return ans;
 }
 
+bool IsAlphaOrPunct(int ch) { return std::isalpha(ch) || std::ispunct(ch); }
+
 bool IsPunct(const std::string &s) {
   static const std::unordered_set<std::string> puncts = {
       ",",  ".",  "!",  "?", ":", "\"", "'", "",
diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
index 6f56dc53..5a3fff09 100644
--- a/sherpa-onnx/csrc/text-utils.h
+++ b/sherpa-onnx/csrc/text-utils.h
@@ -165,6 +165,8 @@ std::string ToUpperAscii(const std::string &str);
 // unchanged)
 std::string ToLowerAscii(const std::string &str);
 
+bool IsAlphaOrPunct(int ch);
+
 // Detect if a codepoint is a CJK character
 bool IsCJK(char32_t cp);
 

commit 5509c1b8cad17db01ca4b40795520593d0c41269
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 10 18:22:26 2025 +0800

    Fix zipvoice. (#2764)
    
    Fixes #2701
    
    The pull request addresses a critical memory management issue in OfflineTtsZipvoiceModel::Impl::Run. The previous implementation had a potential use-after-free bug by directly returning an Ort::Value pointing to the internal buffer of a std::vector (out_data), which would be deallocated when out_data went out of scope. The updated code correctly allocates memory for the Ort::Value using allocator_ and then copies the data, ensuring proper memory ownership and preventing undefined behavior. This is a crucial fix for the stability and correctness of the application.

diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
index 7c04622b..4ff7d833 100644
--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
@@ -176,9 +176,14 @@ class OfflineTtsZipvoiceModel::Impl {
                   keep_frames * feat_dim * sizeof(float));
     }
     std::vector<int64_t> out_shape = {batch_size, keep_frames, feat_dim};
-    return Ort::Value::CreateTensor<float>(memory_info, out_data.data(),
-                                           out_data.size(), out_shape.data(),
-                                           out_shape.size());
+
+    Ort::Value ans = Ort::Value::CreateTensor<float>(
+        allocator_, out_shape.data(), out_shape.size());
+
+    std::copy(out_data.begin(), out_data.end(),
+              ans.GetTensorMutableData<float>());
+
+    return ans;
   }
 
  private:

commit 60736d2f7e46c74b6fe13fe95af40dc68650320f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 10 18:07:23 2025 +0800

    Support MatchaTTS models for Chinese+English. (#2763)

diff --git a/scripts/matcha-tts/zh-en/README.md b/scripts/matcha-tts/zh-en/README.md
index ed2ecbca..0fba2f75 100644
--- a/scripts/matcha-tts/zh-en/README.md
+++ b/scripts/matcha-tts/zh-en/README.md
@@ -9,3 +9,25 @@ vocos-16khz-univ.onnx
 You can download it from 
  https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocos-16khz-univ.onnx
 or
+
+```
+{'am': './model-steps-3.onnx', 'vocoder': './vocos-16khz-univ.onnx', 'tokens': './tokens.txt', 'lexicon': './lexicon.txt', 'text': '. It supports both English ', 'out_wav': 'generated.wav'}
+
+{'use_eos_bos': '1', 'modelscope_url': 'https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010', 'sample_rate': '16000', 'language': 'chinese English', 'model_type': 'matcha-tts', 'n_speakers': '1', 'model_author': 'dengcunqin', 'version': '1', 'pad_id': '0', 'voice': 'zh en-us', 'demo_url': 'https://www.tulingyun.com/tts.html', 'num_ode_steps': '3'}
+
+NodeArg(name='x', type='tensor(int64)', shape=['N', 'L'])
+NodeArg(name='x_length', type='tensor(int64)', shape=['N'])
+NodeArg(name='noise_scale', type='tensor(float)', shape=[1])
+NodeArg(name='length_scale', type='tensor(float)', shape=[1])
+-----
+NodeArg(name='mel', type='tensor(float)', shape=['N', 80, 'L'])
+
+vocos {'modelscope_url': 'https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010', 'use_eos_bos': '1', 'n_speakers': '1', 'sample_rate': '16000', 'pad_id': '0', 'language': 'chinese English', 'model_type': 'matcha-tts vocos', 'voice': 'zh en-us', 'version': '1', 'demo_url': 'https://www.tulingyun.com/tts.html', 'model_author': 'dengcunqin'}
+
+----------vocos----------
+NodeArg(name='mels', type='tensor(float)', shape=['batch_size', 80, 'time'])
+-----
+NodeArg(name='mag', type='tensor(float)', shape=['batch_size', 'Clipmag_dim_1', 'time'])
+NodeArg(name='x', type='tensor(float)', shape=['batch_size', 'Cosx_dim_1', 'time'])
+NodeArg(name='y', type='tensor(float)', shape=['batch_size', 'Cosx_dim_1', 'time'])
+```
diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
index bcc7e8dc..15bc5f44 100755
--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
+++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
@@ -8,6 +8,11 @@ load_phrases_dict(
         "": [["yin2"], ["hang2"], ["hang2"], ["zhang3"]],
     }
 )
+user_defined = {
+    "": ["wei1", "tiao2"],
+    "": ["zhe4", "ge4"],
+    "": ["fang1", "bian2", "de1"],
+}
 
 
 def main():
@@ -32,7 +37,12 @@ def main():
 
             f.write(f"{w} {tokens}\n")
 
+        for key, value in user_defined.items():
+            f.write(f"{key} {' '.join(value)}\n")
+
         for key in phrases:
+            if key in user_defined:
+                continue
             tokens = lazy_pinyin(key, style=Style.TONE3, tone_sandhi=True)
             for i in range(len(tokens)):
                 if tokens[i] == "shei2":
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index 6d6bb53c..d44e8e8a 100644
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -206,6 +206,7 @@ if(SHERPA_ONNX_ENABLE_TTS)
     hifigan-vocoder.cc
     kokoro-multi-lang-lexicon.cc
     lexicon.cc
+    matcha-tts-lexicon.cc
     melo-tts-lexicon.cc
     offline-tts-character-frontend.cc
     offline-tts-frontend.cc
@@ -220,8 +221,8 @@ if(SHERPA_ONNX_ENABLE_TTS)
     offline-tts-vits-model-config.cc
     offline-tts-vits-model.cc
     offline-tts-zipvoice-frontend.cc
-    offline-tts-zipvoice-model.cc
     offline-tts-zipvoice-model-config.cc
+    offline-tts-zipvoice-model.cc
     offline-tts.cc
     piper-phonemize-lexicon.cc
     vocoder.cc
diff --git a/sherpa-onnx/csrc/character-lexicon.cc b/sherpa-onnx/csrc/character-lexicon.cc
index 2dc1ad6e..60d48b55 100644
--- a/sherpa-onnx/csrc/character-lexicon.cc
+++ b/sherpa-onnx/csrc/character-lexicon.cc
@@ -34,14 +34,6 @@
 
 namespace sherpa_onnx {
 
-static bool IsPunct(const std::string &s) {
-  static const std::unordered_set<std::string> puncts = {
-      ",",  ".",  "!",  "?", ":", "\"", "'", "",
-      "", "", "", "", "", "",  "",
-  };
-  return puncts.count(s);
-}
-
 class CharacterLexicon::Impl {
  public:
   Impl(const std::string &lexicon, const std::string &tokens, bool debug)
diff --git a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
index 38ac2fd6..77cb5123 100644
--- a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+++ b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
@@ -4,6 +4,7 @@
 
 #include "sherpa-onnx/csrc/kokoro-multi-lang-lexicon.h"
 
+#include <codecvt>
 #include <fstream>
 #include <regex>  // NOLINT
 #include <sstream>
@@ -23,8 +24,6 @@
 #include "rawfile/raw_file_manager.h"
 #endif
 
-#include <codecvt>
-
 #include "espeak-ng/speak_lib.h"
 #include "phoneme_ids.hpp"
 #include "phonemize.hpp"
@@ -72,9 +71,6 @@ class KokoroMultiLangLexicon::Impl {
     // we cannot convert text to lowercase here since it will affect
     // how piper_phonemize handles punctuations inside the text
     std::string text = _text;
-    if (debug_) {
-      SHERPA_ONNX_LOGE("After converting to lowercase:\n%s", text.c_str());
-    }
 
     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
         {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
new file mode 100644
index 00000000..11f58f7c
--- /dev/null
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
@@ -0,0 +1,432 @@
+// sherpa-onnx/csrc/matcha-tts-lexicon.cc
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+
+#include <algorithm>
+#include <codecvt>
+#include <fstream>
+#include <memory>
+#include <regex>  // NOLINT
+#include <sstream>
+#include <string>
+#include <strstream>
+#include <unordered_map>
+#include <unordered_set>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "espeak-ng/speak_lib.h"
+#include "phoneme_ids.hpp"
+#include "phonemize.hpp"
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/onnx-utils.h"
+#include "sherpa-onnx/csrc/phrase-matcher.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+void CallPhonemizeEspeak(const std::string &text,
+                         piper::eSpeakPhonemeConfig &config,  // NOLINT
+                         std::vector<std::vector<piper::Phoneme>> *phonemes);
+
+class MatchaTtsLexicon::Impl {
+ public:
+  Impl(const std::string &lexicon, const std::string &tokens,
+       const std::string &data_dir, bool debug)
+      : debug_(debug) {
+    if (lexicon.empty()) {
+      SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    {
+      std::ifstream is(tokens);
+      InitTokens(is);
+    }
+
+    {
+      std::ifstream is(lexicon);
+      InitLexicon(is);
+    }
+
+    if (data_dir.empty()) {
+      SHERPA_ONNX_LOGE("Please provide data dir for this model");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitEspeak(data_dir);  // See ./piper-phonemize-lexicon.cc
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const std::string &lexicon, const std::string &tokens,
+       const std::string &data_dir, bool debug)
+      : debug_(debug) {
+    if (lexicon.empty()) {
+      SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    {
+      auto buf = ReadFile(mgr, tokens);
+      std::istrstream is(buf.data(), buf.size());
+
+      InitTokens(is);
+    }
+
+    {
+      auto buf = ReadFile(mgr, lexicon);
+      std::istrstream is(buf.data(), buf.size());
+      InitLexicon(is);
+    }
+
+    if (data_dir.empty()) {
+      SHERPA_ONNX_LOGE("Please provide data dir for this model");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitEspeak(data_dir);  // See ./piper-phonemize-lexicon.cc
+  }
+
+  std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
+    std::string text = _text;
+    std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
+        {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
+        {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
+    };
+    for (const auto &p : replace_str_pairs) {
+      std::regex re(p.first);
+      text = std::regex_replace(text, re, p.second);
+    }
+
+    if (debug_) {
+      SHERPA_ONNX_LOGE("After replacing punctuations and merging spaces:\n%s",
+                       text.c_str());
+    }
+
+    std::vector<std::string> words = SplitUtf8(text);
+
+    if (debug_) {
+#if __OHOS__
+      SHERPA_ONNX_LOGE("input text:\n%{public}s", _text.c_str());
+      SHERPA_ONNX_LOGE("after replacing punctuations:\n%{public}s",
+                       text.c_str());
+#else
+      SHERPA_ONNX_LOGE("input text:\n%s", _text.c_str());
+      SHERPA_ONNX_LOGE("after replacing punctuations:\n%s", text.c_str());
+#endif
+
+      std::ostringstream os;
+      std::string sep = "";
+      for (const auto &w : words) {
+        os << sep << w;
+        sep = "_";
+      }
+
+#if __OHOS__
+      SHERPA_ONNX_LOGE("after splitting into UTF8:\n%{public}s",
+                       os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("after splitting into UTF8:\n%s", os.str().c_str());
+#endif
+    }
+
+    // remove spaces after punctuations
+    std::vector<std::string> words2 = std::move(words);
+    words.reserve(words2.size());
+
+    for (int32_t i = 0; i < words2.size(); ++i) {
+      if (i == 0) {
+        words.push_back(std::move(words2[i]));
+      } else if (words2[i] == " ") {
+        if (words.back() == " " || IsPunct(words.back())) {
+          continue;
+        } else {
+          words.push_back(std::move(words2[i]));
+        }
+      } else if (IsPunct(words2[i])) {
+        if (words.back() == " " || IsPunct(words.back())) {
+          continue;
+        } else {
+          words.push_back(std::move(words2[i]));
+        }
+      } else {
+        words.push_back(std::move(words2[i]));
+      }
+    }
+
+    if (debug_) {
+      std::ostringstream os;
+      std::string sep = "";
+      for (const auto &w : words) {
+        os << sep << w;
+        sep = "_";
+      }
+
+#if __OHOS__
+      SHERPA_ONNX_LOGE("after removing spaces after punctuations:\n%{public}s",
+                       os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("after removing spaces after punctuations:\n%s",
+                       os.str().c_str());
+#endif
+    }
+
+    std::vector<TokenIDs> ans;
+    std::vector<int64_t> this_sentence;
+
+    PhraseMatcher matcher(&all_words_, words, debug_);
+
+    std::vector<int32_t> ids;
+    for (const std::string &w : matcher) {
+      ids = ConvertWordToIds(w);
+
+      if (ids.empty()) {
+#if __OHOS__
+        SHERPA_ONNX_LOGE("Ignore OOV '%{public}s'", w.c_str());
+#else
+        SHERPA_ONNX_LOGE("Ignore OOV '%s'", w.c_str());
+#endif
+        continue;
+      }
+
+      this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
+
+      if (IsPunct(w)) {
+        ans.emplace_back(std::move(this_sentence));
+        this_sentence = {};
+      }
+    }  // for (const std::string &w : matcher)
+
+    if (!this_sentence.empty()) {
+      ans.emplace_back(std::move(this_sentence));
+    }
+
+    return ans;
+  }
+
+ private:
+  std::vector<int32_t> ConvertWordToIds(const std::string &w) const {
+    std::vector<int32_t> ans;
+
+    if (word2ids_.count(w)) {
+      ans = word2ids_.at(w);
+    } else if (token2id_.count(w)) {
+      ans = {token2id_.at(w)};
+    } else {
+      if (ContainsCJK(w)) {
+        std::vector<std::string> words = SplitUtf8(w);
+        for (const auto &word : words) {
+          if (word2ids_.count(word)) {
+            auto ids = ConvertWordToIds(word);
+            ans.insert(ans.end(), ids.begin(), ids.end());
+          }
+        }
+      } else {
+        SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
+        // use espeak
+        piper::eSpeakPhonemeConfig config;
+        config.voice = "en-us";
+        std::vector<std::vector<piper::Phoneme>> phonemes;
+        CallPhonemizeEspeak(w, config, &phonemes);
+        for (const auto &ps : phonemes) {
+          for (const auto &p : ps) {
+            if (phoneme2id_.count(p)) {
+              ans.push_back(phoneme2id_.at(p));
+            } else {
+              SHERPA_ONNX_LOGE(
+                  "Skip unknown phonemes. Unicode codepoint: \\U+%04x. for %s",
+                  static_cast<uint32_t>(p), w.c_str());
+            }
+          }
+        }
+      }
+    }
+
+    if (debug_) {
+      std::ostringstream os;
+      os << w << ": ";
+      for (auto i : ans) {
+        os << id2token_.at(i) << " ";
+      }
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s", os.str().c_str());
+#endif
+    }
+
+    return ans;
+  }
+
+  void InitTokens(std::istream &is) {
+    token2id_ = ReadTokens(is);
+
+    std::vector<std::pair<std::string, std::string>> puncts = {
+        {",", ""}, {".", ""}, {"!", ""}, {"?", ""}, {":", ""},
+        {"\"", ""}, {"\"", ""}, {"'", ""},  {"'", ""},  {";", ""},
+    };
+
+    for (const auto &p : puncts) {
+      if (token2id_.count(p.first) && !token2id_.count(p.second)) {
+        token2id_[p.second] = token2id_[p.first];
+      }
+
+      if (!token2id_.count(p.first) && token2id_.count(p.second)) {
+        token2id_[p.first] = token2id_[p.second];
+      }
+    }
+
+    if (!token2id_.count("") && token2id_.count("")) {
+      token2id_[""] = token2id_[""];
+    }
+
+    if (!token2id_.count(";") && token2id_.count(",")) {
+      token2id_[";"] = token2id_[","];
+    }
+
+    if (debug_) {
+      for (const auto &p : token2id_) {
+        id2token_[p.second] = p.first;
+      }
+    }
+
+    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
+    std::u32string s;
+    for (const auto &p : token2id_) {
+      if ((p.first.front() == '<' && p.first.back() == '>') ||
+          p.first.back() == '1' || p.first.back() == '2' ||
+          p.first.back() == '3' || p.first.back() == '4' ||
+          p.first.back() == '5') {
+        continue;
+      }
+      s = conv.from_bytes(p.first);
+
+      if (s.size() != 1) {
+        SHERPA_ONNX_LOGE("Error for token %s with id %d", p.first.c_str(),
+                         p.second);
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      char32_t c = s[0];
+      if (!phoneme2id_.count(c)) {
+        phoneme2id_.insert({c, p.second});
+      }
+    }
+  }
+
+  void InitLexicon(std::istream &is) {
+    std::string word;
+    std::vector<std::string> token_list;
+    std::string line;
+    std::string phone;
+    int32_t line_num = 0;
+
+    while (std::getline(is, line)) {
+      ++line_num;
+
+      std::istringstream iss(line);
+
+      token_list.clear();
+
+      iss >> word;
+      ToLowerCase(&word);
+
+      if (word2ids_.count(word)) {
+#if __OHOS__
+        SHERPA_ONNX_LOGE(
+            "Duplicated word: %{public}s at line %{public}d:%{public}s. Ignore "
+            "it.",
+            word.c_str(), line_num, line.c_str());
+#else
+        SHERPA_ONNX_LOGE("Duplicated word: %s at line %d:%s. Ignore it.",
+                         word.c_str(), line_num, line.c_str());
+#endif
+        continue;
+      }
+
+      while (iss >> phone) {
+        token_list.push_back(std::move(phone));
+      }
+
+      std::vector<int32_t> ids = ConvertTokensToIds(token2id_, token_list);
+      if (ids.empty()) {
+        if (debug_) {
+#if __OHOS__
+          SHERPA_ONNX_LOGE("Empty token ids for '%{public}s'", line.c_str());
+#else
+          SHERPA_ONNX_LOGE("Empty token ids for '%s'", line.c_str());
+#endif
+        }
+        continue;
+      }
+
+      word2ids_.insert({std::move(word), std::move(ids)});
+    }
+
+    for (const auto &[key, _] : word2ids_) {
+      all_words_.insert(key);
+    }
+  }
+
+ private:
+  // lexicon.txt is saved in word2ids_
+  std::unordered_map<std::string, std::vector<int32_t>> word2ids_;
+  std::unordered_set<std::string> all_words_;
+
+  // tokens.txt is saved in token2id_
+  std::unordered_map<std::string, int32_t> token2id_;
+  std::unordered_map<char32_t, int32_t> phoneme2id_;
+
+  std::unordered_map<int32_t, std::string> id2token_;
+
+  bool debug_ = false;
+};  // namespace sherpa_onnx
+
+MatchaTtsLexicon::~MatchaTtsLexicon() = default;
+
+MatchaTtsLexicon::MatchaTtsLexicon(const std::string &lexicon,
+                                   const std::string &tokens,
+                                   const std::string &data_dir, bool debug)
+    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug)) {}
+
+template <typename Manager>
+MatchaTtsLexicon::MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
+                                   const std::string &tokens,
+                                   const std::string &data_dir, bool debug)
+    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug)) {}
+
+std::vector<TokenIDs> MatchaTtsLexicon::ConvertTextToTokenIds(
+    const std::string &text, const std::string & /*unused_voice = ""*/) const {
+  return impl_->ConvertTextToTokenIds(text);
+}
+
+#if __ANDROID_API__ >= 9
+template MatchaTtsLexicon::MatchaTtsLexicon(AAssetManager *mgr,
+                                            const std::string &lexicon,
+                                            const std::string &tokens,
+                                            const std::string &data_dir,
+                                            bool debug);
+#endif
+
+#if __OHOS__
+template MatchaTtsLexicon::MatchaTtsLexicon(NativeResourceManager *mgr,
+                                            const std::string &lexicon,
+                                            const std::string &tokens,
+                                            const std::string &data_dir,
+                                            bool debug);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.h b/sherpa-onnx/csrc/matcha-tts-lexicon.h
new file mode 100644
index 00000000..f9da31a6
--- /dev/null
+++ b/sherpa-onnx/csrc/matcha-tts-lexicon.h
@@ -0,0 +1,41 @@
+// sherpa-onnx/csrc/matcha-tts-lexicon.h
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
+#define SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
+
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-tts-frontend.h"
+
+namespace sherpa_onnx {
+
+// For Chinese+English matcha tts
+class MatchaTtsLexicon : public OfflineTtsFrontend {
+ public:
+  ~MatchaTtsLexicon() override;
+
+  MatchaTtsLexicon(const std::string &lexicon, const std::string &tokens,
+                   const std::string &data_dir, bool debug);
+
+  template <typename Manager>
+  MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
+                   const std::string &tokens, const std::string &data_dir,
+                   bool debug);
+
+  std::vector<TokenIDs> ConvertTextToTokenIds(
+      const std::string &text,
+      const std::string &unused_voice = "") const override;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
index dc887ab2..3588b2d2 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
@@ -4,6 +4,7 @@
 #ifndef SHERPA_ONNX_CSRC_OFFLINE_TTS_MATCHA_IMPL_H_
 #define SHERPA_ONNX_CSRC_OFFLINE_TTS_MATCHA_IMPL_H_
 
+#include <algorithm>
 #include <memory>
 #include <string>
 #include <strstream>
@@ -16,6 +17,7 @@
 #include "sherpa-onnx/csrc/character-lexicon.h"
 #include "sherpa-onnx/csrc/lexicon.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
 #include "sherpa-onnx/csrc/melo-tts-lexicon.h"
 #include "sherpa-onnx/csrc/offline-tts-character-frontend.h"
 #include "sherpa-onnx/csrc/offline-tts-frontend.h"
@@ -32,8 +34,26 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
  public:
   explicit OfflineTtsMatchaImpl(const OfflineTtsConfig &config)
       : config_(config),
-        model_(std::make_unique<OfflineTtsMatchaModel>(config.model)),
-        vocoder_(Vocoder::Create(config.model)) {
+        model_(std::make_unique<OfflineTtsMatchaModel>(config.model)) {
+    const auto &meta_data = model_->GetMetaData();
+    if (meta_data.need_vocoder) {
+      if (config.model.matcha.vocoder.empty()) {
+        SHERPA_ONNX_LOGE("Please provide vocoder for this model");
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      if (!FileExists(config.model.matcha.vocoder)) {
+        SHERPA_ONNX_LOGE("Please vocoder '%s' does not exist",
+                         config.model.matcha.vocoder.c_str());
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      vocoder_ = Vocoder::Create(config.model);
+    } else if (!config.model.matcha.vocoder.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You don't need to provide vocoder for this model. Ignore it");
+    }
+
     InitFrontend();
 
     if (!config.rule_fsts.empty()) {
@@ -84,13 +104,38 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
         SHERPA_ONNX_LOGE("FST archives loaded!");
       }
     }
+
+    if (meta_data.sample_rate == 16000 && meta_data.is_zh_en == 1) {
+      if (!Contains(config.model.matcha.vocoder, "16") &&
+          Contains(config.model.matcha.vocoder, "2")) {
+        SHERPA_ONNX_LOGE(
+            "This Chinese+English TTS model requires a 16khz Vocoder.");
+        SHERPA_ONNX_LOGE("You should use vocos-16khz-univ.onnx.");
+        SHERPA_ONNX_LOGE(
+            "Please re-download a vocoder from "
+            "https://github.com/k2-fsa/sherpa-onnx/releases/tag/"
+            "vocoder-models.");
+      }
+    }
   }
 
   template <typename Manager>
   OfflineTtsMatchaImpl(Manager *mgr, const OfflineTtsConfig &config)
       : config_(config),
-        model_(std::make_unique<OfflineTtsMatchaModel>(mgr, config.model)),
-        vocoder_(Vocoder::Create(mgr, config.model)) {
+        model_(std::make_unique<OfflineTtsMatchaModel>(mgr, config.model)) {
+    const auto &meta_data = model_->GetMetaData();
+    if (meta_data.need_vocoder) {
+      if (config.model.matcha.vocoder.empty()) {
+        SHERPA_ONNX_LOGE("Please provide vocoder for this model");
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      vocoder_ = Vocoder::Create(mgr, config.model);
+    } else if (!config.model.matcha.vocoder.empty()) {
+      SHERPA_ONNX_LOGE(
+          "You don't need to provide vocoder for this model. Ignore it");
+    }
+
     InitFrontend(mgr);
 
     if (!config.rule_fsts.empty()) {
@@ -142,6 +187,19 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
         }  // for (; !reader->Done(); reader->Next())
       }  // for (const auto &f : files)
     }  // if (!config.rule_fars.empty())
+
+    if (meta_data.sample_rate == 16000 && meta_data.is_zh_en == 1) {
+      if (!Contains(config.model.matcha.vocoder, "16") &&
+          Contains(config.model.matcha.vocoder, "2")) {
+        SHERPA_ONNX_LOGE(
+            "This Chinese+English TTS model requires a 16khz Vocoder.");
+        SHERPA_ONNX_LOGE("You should use vocos-16khz-univ.onnx.");
+        SHERPA_ONNX_LOGE(
+            "Please re-download a vocoder from "
+            "https://github.com/k2-fsa/sherpa-onnx/releases/tag/"
+            "vocoder-models.");
+      }
+    }
   }
 
   int32_t SampleRate() const override {
@@ -325,7 +383,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
     // from assets to disk
     const auto &meta_data = model_->GetMetaData();
 
-    if (meta_data.jieba || meta_data.voice == "zh en-us") {
+    if (meta_data.is_zh_en) {
+      frontend_ = std::make_unique<MatchaTtsLexicon>(
+          mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
+          config_.model.matcha.data_dir, config_.model.debug);
+    } else if (meta_data.jieba) {
       frontend_ = std::make_unique<CharacterLexicon>(
           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
           config_.model.debug);
@@ -334,7 +396,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
           mgr, config_.model.matcha.tokens, config_.model.matcha.data_dir,
           meta_data);
     } else {
-      SHERPA_ONNX_LOGE("jieba + espeaker-ng is not supported yet");
+      SHERPA_ONNX_LOGE("Unsupported matcha tts model. Please ask for help");
       SHERPA_ONNX_EXIT(-1);
     }
   }
@@ -342,7 +404,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
   void InitFrontend() {
     const auto &meta_data = model_->GetMetaData();
 
-    if (meta_data.jieba || meta_data.voice == "zh en-us") {
+    if (meta_data.is_zh_en) {
+      frontend_ = std::make_unique<MatchaTtsLexicon>(
+          config_.model.matcha.lexicon, config_.model.matcha.tokens,
+          config_.model.matcha.data_dir, config_.model.debug);
+    } else if (meta_data.jieba) {
       frontend_ = std::make_unique<CharacterLexicon>(
           config_.model.matcha.lexicon, config_.model.matcha.tokens,
           config_.model.debug);
@@ -351,7 +417,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
           config_.model.matcha.tokens, config_.model.matcha.data_dir,
           meta_data);
     } else {
-      SHERPA_ONNX_LOGE("jieba + espeaker-ng is not supported yet");
+      SHERPA_ONNX_LOGE("Unsupported matcha tts model. Please ask for help");
       SHERPA_ONNX_EXIT(-1);
     }
   }
@@ -376,11 +442,24 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
     Ort::Value x_tensor = Ort::Value::CreateTensor(
         memory_info, x.data(), x.size(), x_shape.data(), x_shape.size());
 
+    GeneratedAudio ans;
+
     Ort::Value mel = model_->Run(std::move(x_tensor), sid, speed);
 
-    GeneratedAudio ans;
+    const auto &meta_data = model_->GetMetaData();
+    if (meta_data.need_vocoder) {
+      ans.samples = vocoder_->Run(std::move(mel));
+    } else {
+      std::vector<int64_t> shape = mel.GetTensorTypeAndShapeInfo().GetShape();
+      int64_t num_samples = 1;
+      for (auto s : shape) {
+        num_samples *= s;
+      }
+      ans.samples.resize(num_samples);
+      auto p = mel.GetTensorData<float>();
+      std::copy(p, p + num_samples, ans.samples.data());
+    }
 
-    ans.samples = vocoder_->Run(std::move(mel));
     ans.sample_rate = model_->GetMetaData().sample_rate;
 
     float silence_scale = config_.silence_scale;
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
index 10c66fc6..1fdbad53 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
@@ -42,16 +42,6 @@ bool OfflineTtsMatchaModelConfig::Validate() const {
     return false;
   }
 
-  if (vocoder.empty()) {
-    SHERPA_ONNX_LOGE("Please provide --matcha-vocoder");
-    return false;
-  }
-
-  if (!FileExists(vocoder)) {
-    SHERPA_ONNX_LOGE("--matcha-vocoder: '%s' does not exist", vocoder.c_str());
-    return false;
-  }
-
   if (tokens.empty()) {
     SHERPA_ONNX_LOGE("Please provide --matcha-tokens");
     return false;
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h b/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
index b4ec6c4d..1e83e16e 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
@@ -22,6 +22,8 @@ struct OfflineTtsMatchaModelMetaData {
   int32_t use_eos_bos = 0;
   int32_t pad_id = 0;
   int32_t add_blank = 1;
+  int32_t is_zh_en = 0;
+  bool need_vocoder = true;
 
   std::string voice;
 };
diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model.cc b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
index 992109b3..316a8a5f 100644
--- a/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+++ b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
@@ -171,6 +171,11 @@ class OfflineTtsMatchaModel::Impl {
       // for models from
       // https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010
       meta_data_.add_blank = 0;
+      meta_data_.is_zh_en = 1;
+    }
+
+    if (output_names_.front() == "audio_output") {
+      meta_data_.need_vocoder = false;
     }
   }
 
diff --git a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
index 2a3c3daa..21200dd3 100644
--- a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+++ b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
@@ -92,7 +92,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
     iss >> std::ws;
     if (!iss.eof()) {
       SHERPA_ONNX_LOGE("Error when reading tokens: %s", line.c_str());
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 
     s = conv.from_bytes(sym);
@@ -105,7 +105,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
 
       SHERPA_ONNX_LOGE("Error when reading tokens at Line %s. size: %d",
                        line.c_str(), static_cast<int32_t>(s.size()));
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 
     char32_t c = s[0];
@@ -113,7 +113,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
     if (token2id.count(c)) {
       SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
                        sym.c_str(), line.c_str(), token2id.at(c));
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 
     token2id.insert({c, id});
@@ -322,7 +322,7 @@ void InitEspeak(const std::string &data_dir) {
           "Failed to initialize espeak-ng with data dir: %s. Return code is: "
           "%d",
           data_dir.c_str(), result);
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
   });
 }
@@ -541,7 +541,7 @@ std::vector<TokenIDs> PiperPhonemizeLexicon::ConvertTextToTokenIdsVits(
 
   } else {
     SHERPA_ONNX_LOGE("Unsupported model");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
 
   return ans;
diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
index 15fdf125..2bc2c7f4 100644
--- a/sherpa-onnx/csrc/symbol-table.cc
+++ b/sherpa-onnx/csrc/symbol-table.cc
@@ -130,14 +130,14 @@ std::unordered_map<std::string, int32_t> ReadTokens(
     iss >> std::ws;
     if (!iss.eof()) {
       SHERPA_ONNX_LOGE("Error: %s", line.c_str());
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 
 #if 0
     if (token2id.count(sym)) {
       SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
                        sym.c_str(), line.c_str(), token2id.at(sym));
-      exit(-1);
+      SHERPA_ONNX_EXIT(-1);
     }
 #endif
     if (id2token) {
diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
index 020c6041..760ac329 100644
--- a/sherpa-onnx/csrc/text-utils.cc
+++ b/sherpa-onnx/csrc/text-utils.cc
@@ -16,6 +16,7 @@
 #include <sstream>
 #include <string>
 #include <unordered_map>
+#include <unordered_set>
 #include <utility>
 #include <vector>
 
@@ -708,6 +709,14 @@ bool EndsWith(const std::string &haystack, const std::string &needle) {
   return std::equal(needle.rbegin(), needle.rend(), haystack.rbegin());
 }
 
+bool Contains(const std::string &haystack, const std::string &needle) {
+  if (needle.size() > haystack.size()) {
+    return false;
+  }
+
+  return haystack.find(needle) != std::string::npos;
+}
+
 std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size) {
   std::vector<std::string> ans;
   if (chunk_size < 1 || chunk_size > s.size()) {
@@ -799,4 +808,12 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
   return ans;
 }
 
+bool IsPunct(const std::string &s) {
+  static const std::unordered_set<std::string> puncts = {
+      ",",  ".",  "!",  "?", ":", "\"", "'", "",
+      "", "", "", "", "", "",  "",
+  };
+  return puncts.count(s);
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
index 369ba2e6..6f56dc53 100644
--- a/sherpa-onnx/csrc/text-utils.h
+++ b/sherpa-onnx/csrc/text-utils.h
@@ -147,6 +147,8 @@ std::string ToString(const std::wstring &s);
 
 bool EndsWith(const std::string &haystack, const std::string &needle);
 
+bool Contains(const std::string &haystack, const std::string &needle);
+
 std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size);
 
 // Converts a UTF-8 std::string to a UTF-32 std::u32string
@@ -176,6 +178,8 @@ bool StringToBool(const std::string &s);
 std::string GetWord(const std::vector<std::string> &words, int32_t start,
                     int32_t end);
 
+bool IsPunct(const std::string &s);
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_TEXT_UTILS_H_
diff --git a/sherpa-onnx/csrc/vocoder.cc b/sherpa-onnx/csrc/vocoder.cc
index 38c1e4f7..d9821cec 100644
--- a/sherpa-onnx/csrc/vocoder.cc
+++ b/sherpa-onnx/csrc/vocoder.cc
@@ -76,15 +76,12 @@ static ModelType GetModelType(char *model_data, size_t model_data_length,
 std::unique_ptr<Vocoder> Vocoder::Create(const OfflineTtsModelConfig &config) {
   std::vector<char> buffer;
   if (!config.matcha.vocoder.empty()) {
-    SHERPA_ONNX_LOGE("Using matcha vocoder: %s", config.matcha.vocoder.c_str());
     buffer = ReadFile(config.matcha.vocoder);
   } else if (!config.zipvoice.vocoder.empty()) {
-    SHERPA_ONNX_LOGE("Using zipvoice vocoder: %s",
-                     config.zipvoice.vocoder.c_str());
     buffer = ReadFile(config.zipvoice.vocoder);
   } else {
     SHERPA_ONNX_LOGE("No vocoder model provided in the config!");
-    exit(-1);
+    SHERPA_ONNX_EXIT(-1);
   }
   auto model_type = GetModelType(buffer.data(), buffer.size(), config.debug);
 

commit 9beac05020a484cbf626d1305d95b8070af8479f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 10 14:15:32 2025 +0800

    Export models to Ascend 910B3 (#2761)

diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
index da9a3d2a..2fbd89f1 100644
--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
+++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-npu-acl-api
+      - ascend-910b3
   workflow_dispatch:
 
 concurrency:
@@ -62,6 +62,27 @@ jobs:
             framework: "WSChuan-ASR"
             cann: "8.2"
 
+          # ===== Ascend 910B3 =====
+          - soc_version: "910B3"
+            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+            framework: "FunASR"
+            cann: "8.0"
+
+          - soc_version: "910B3"
+            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+            framework: "WSChuan-ASR"
+            cann: "8.0"
+
+          - soc_version: "910B3"
+            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+            framework: "FunASR"
+            cann: "8.2"
+
+          - soc_version: "910B3"
+            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+            framework: "WSChuan-ASR"
+            cann: "8.2"
+
           # ===== Ascend 310 =====
           - soc_version: "310P3"
             image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
@@ -339,7 +360,7 @@ jobs:
           overwrite: true
           repo_name: k2-fsa/sherpa-onnx
           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: asr-models
+          tag: asr-models-ascend
 
       - name: Release
         if: github.repository_owner == 'k2-fsa'
@@ -348,4 +369,4 @@ jobs:
           file_glob: true
           file: ./*.tar.bz2
           overwrite: true
-          tag: asr-models
+          tag: asr-models-ascend
diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
index df235fe1..4f10df79 100644
--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-npu-310
+      - ascend-910b3
   workflow_dispatch:
 
 concurrency:
@@ -62,6 +62,27 @@ jobs:
             framework: "WSYue-ASR"
             cann: "8.2"
 
+          # ===== Ascend 910B3 =====
+          - soc_version: "910B3"
+            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+            framework: "FunASR"
+            cann: "8.0"
+
+          - soc_version: "910B3"
+            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+            framework: "WSYue-ASR"
+            cann: "8.0"
+
+          - soc_version: "910B3"
+            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+            framework: "FunASR"
+            cann: "8.2"
+
+          - soc_version: "910B3"
+            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+            framework: "WSYue-ASR"
+            cann: "8.2"
+
 
           # ===== Ascend 310 =====
           - soc_version: "310P3"
@@ -279,7 +300,7 @@ jobs:
           overwrite: true
           repo_name: k2-fsa/sherpa-onnx
           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
-          tag: asr-models
+          tag: asr-models-ascend
 
       - name: Release
         if: github.repository_owner == 'k2-fsa'
@@ -288,4 +309,4 @@ jobs:
           file_glob: true
           file: ./*.tar.bz2
           overwrite: true
-          tag: asr-models
+          tag: asr-models-ascend

commit 049d525cb39f2dd7334f6d407c841caa071c5182
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 10 11:52:20 2025 +0800

    Export sense voice to qnn (#2760)

diff --git a/.github/workflows/export-sense-voice-to-qnn.yaml b/.github/workflows/export-sense-voice-to-qnn.yaml
new file mode 100644
index 00000000..73c8a4ec
--- /dev/null
+++ b/.github/workflows/export-sense-voice-to-qnn.yaml
@@ -0,0 +1,475 @@
+name: export-sense-voice-to-qnn
+
+on:
+  push:
+    branches:
+      - export-sense-voice-qnn-2
+  workflow_dispatch:
+
+concurrency:
+  group: export-sense-voice-to-qnn-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-sense-voice-to-qnn:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-22.04]
+        python-version: ["3.10"]
+        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+        framework: ["FunASR", "WSYue-ASR"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Display NDK HOME
+        shell: bash
+        run: |
+          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
+          ls -lh ${ANDROID_NDK_LATEST_HOME}
+
+      - name: Create Python virtual environment
+        shell: bash
+        run: |
+          python3 -m venv py310
+          which python3
+          source py310/bin/activate
+          which python3
+
+      - name: Show ndk-build help
+        shell: bash
+        run: |
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          ndk-build --help
+
+      - name: Download toolkit
+        shell: bash
+        run: |
+          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
+          ls -lh v2.33.0.250327.zip
+
+      - name: Unzip toolkit
+        shell: bash
+        run: |
+          unzip v2.33.0.250327.zip
+
+      - name: Show
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---ls -lh qairt---"
+
+          ls -lh qairt
+
+          echo "---"
+
+      - name: Install linux dependencies
+        shell: bash
+        run: |
+          ls -lh
+
+          echo "---"
+
+          ls -lh qairt
+
+          cd qairt/2.33.0.250327/bin
+          source envsetup.sh
+
+          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          cd qairt/2.33.0.250327/bin
+          source envsetup.sh
+
+          python3 -m pip install \
+            mock \
+            numpy \
+            opencv-python \
+            optuna \
+            packaging \
+            pandas \
+            paramiko \
+            pathlib2 \
+            pillow \
+            plotly \
+            protobuf \
+            psutil \
+            pydantic \
+            pytest \
+            pyyaml \
+            rich \
+            scikit-optimize \
+            scipy \
+            six \
+            tabulate \
+            typing-extensions \
+            xlsxwriter
+
+          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
+
+          which python3
+
+      - name: Install onnx dependencies
+        shell: bash
+        run: |
+          source py310/bin/activate
+          python3 -m pip install --upgrade \
+            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+            kaldi_native_fbank \
+            pip \
+            "numpy<2" \
+            onnx==1.17.0 \
+            onnxruntime==1.17.1 \
+            soundfile \
+            librosa \
+            onnxsim \
+            sentencepiece \
+            pyyaml
+
+          which python3
+
+      - name: Show qnn-onnx-converter help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-onnx-converter --help
+
+      - name: Show qnn-model-lib-generator help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-model-lib-generator --help
+
+      - name: Show qnn-net-run help
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          qnn-net-run --help
+
+      - name: Run SenseVoice from FunAsr
+        if: matrix.framework == 'FunASR'
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+
+          cd scripts/sense-voice/qnn
+
+          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
+          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/model.pt
+          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ja.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ko.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav
+
+          rm -f README.md || true
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/README.md
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/LICENSE
+
+          echo "export to onnx"
+          t=${{ matrix.input_in_seconds }}
+
+          echo "----$t---"
+          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 17
+
+          ls -lh *.onnx
+
+          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
+
+          echo "test exported onnx models"
+
+          echo "----------$t----------"
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ja.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ko.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav
+
+          echo "export to qnn"
+          echo "----------$t----------"
+          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")
+
+          echo "num_frames: $num_frames"
+
+          ./generate_test_data.py  --num-frames $num_frames --wav ./zh.wav
+          mv input0.raw zh-input0.raw
+          mv input1.raw zh-input1.raw
+          echo "zh-input0.raw zh-input1.raw" > input_list.txt
+
+          for w in ja ko en yue; do
+            ./generate_test_data.py  --num-frames $num_frames --wav ./$w.wav
+            mv input0.raw $w-input0.raw
+            mv input1.raw $w-input1.raw
+            echo "$w-input0.raw $w-input1.raw" >> input_list.txt
+          done
+
+          cat ./input_list.txt
+
+          qnn-onnx-converter \
+            --input_network model-$t-seconds.onnx \
+            --output_path ./model-$t-seconds-quantized \
+            --out_node logits \
+            --input_list ./input_list.txt \
+            --use_native_input_files  \
+            --input_dtype x float32 \
+            --input_dtype prompt int32 \
+            --act_bitwidth 16 \
+            --bias_bitwidth 32 \
+            --input_layout x NTF
+          ls -lh
+          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
+          echo "----"
+          ls -lh
+
+          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+            -c "model-$t-seconds-quantized.cpp" \
+            -b "model-$t-seconds-quantized.bin" \
+            -o model_libs > /dev/null 2>&1
+
+          ls -lh model_libs/*/
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "collect results"
+
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v README.md $d
+            cp -v LICENSE $d
+            cp -v model_libs/$p/lib*.so $d/libmodel.so
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+
+            echo "num_frames=$num_frames" > $d/info.txt
+            echo "target=$p" >> $d/info.txt
+
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../../..
+
+      - name: Run SenseVoice from WSYue-ASR
+        if: matrix.framework == 'WSYue-ASR'
+        shell: bash
+        run: |
+          source py310/bin/activate
+
+          pushd qairt/2.33.0.250327/bin
+          source envsetup.sh
+          popd
+
+          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+          export LDFLAGS="-Wl,-z,max-page-size=16384"
+
+          cd scripts/sense-voice/qnn
+
+          curl -SL -O https://huggingface.co/ASLP-lab/WSYue-ASR/resolve/main/sensevoice_small_yue/model.pt
+
+          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
+          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav
+
+          for i in $(seq 0 17); do
+            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/test_wavs/yue-$i.wav
+          done
+
+          rm -f README.md || true
+
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/README.md
+
+          echo "export to onnx"
+          t=${{ matrix.input_in_seconds }}
+
+          echo "----$t---"
+
+          export model_author="ASLP-lab"
+          export comment="ASLP-lab/WSYue-ASR"
+          export url="https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/sensevoice_small_yue"
+
+          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 17
+
+          ls -lh *.onnx
+
+          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
+
+          echo "test exported onnx models"
+
+          echo "----------$t----------"
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
+          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav
+
+          for i in $(seq 0 17); do
+            echo "yue-$i.wav"
+            python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue-$i.wav
+          done
+
+          echo "export to qnn"
+          echo "----------$t----------"
+          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")
+
+          echo "num_frames: $num_frames"
+
+          ./generate_test_data.py  --num-frames $num_frames --wav ./zh.wav
+          mv input0.raw zh-input0.raw
+          mv input1.raw zh-input1.raw
+          echo "zh-input0.raw zh-input1.raw" > input_list.txt
+
+          for w in en yue; do
+            ./generate_test_data.py  --num-frames $num_frames --wav ./$w.wav
+            mv input0.raw $w-input0.raw
+            mv input1.raw $w-input1.raw
+            echo "$w-input0.raw $w-input1.raw" >> input_list.txt
+          done
+
+          for i in $(seq 0 17); do
+            echo "yue-$i.wav"
+            ./generate_test_data.py  --num-frames $num_frames --wav ./yue-$i.wav
+            mv input0.raw $i-input0.raw
+            mv input1.raw $i-input1.raw
+            echo "$i-input0.raw $i-input1.raw" >> input_list.txt
+          done
+
+          cat ./input_list.txt
+
+          qnn-onnx-converter \
+            --input_network model-$t-seconds.onnx \
+            --output_path ./model-$t-seconds-quantized \
+            --out_node logits \
+            --input_list ./input_list.txt \
+            --use_native_input_files  \
+            --input_dtype x float32 \
+            --input_dtype prompt int32 \
+            --act_bitwidth 16 \
+            --bias_bitwidth 32 \
+            --input_layout x NTF
+          ls -lh
+          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
+          echo "----"
+          ls -lh
+
+          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
+            -c "model-$t-seconds-quantized.cpp" \
+            -b "model-$t-seconds-quantized.bin" \
+            -o model_libs > /dev/null 2>&1
+
+          ls -lh model_libs/*/
+
+          readelf -lW model_libs/*/lib*.so
+
+          echo "collect results"
+          for p in x86_64-linux-clang aarch64-android; do
+            if [[ $p == x86_64-linux-clang ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8-linux-x64
+            elif [[ $p == aarch64-android ]]; then
+              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8-android-aarch64
+            else
+              echo "Unknown $p"
+              exit -1
+            fi
+
+            mkdir -p $d
+            mkdir -p $d/test_wavs
+
+            cp -v README.md $d
+            cp -v model_libs/$p/lib*.so $d/libmodel.so
+            cp -v tokens.txt $d
+            cp -v *.wav $d/test_wavs
+
+            echo "num_frames=$num_frames" > $d/info.txt
+            echo "target=$p" >> $d/info.txt
+
+            ls -lh $d
+            tar cjfv $d.tar.bz2 $d
+            ls -lh *.tar.bz2
+            rm -rf $d
+          done
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../../..
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: ${{ matrix.framework }}-${{ matrix.input_in_seconds }}-seconds
+          path: ./scripts/sense-voice/qnn/*.json
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-qnn
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          tag: asr-models-qnn
diff --git a/scripts/sense-voice/ascend-npu/test_om.py b/scripts/sense-voice/ascend-npu/test_om.py
index ff6a11c1..29c6187b 100755
--- a/scripts/sense-voice/ascend-npu/test_om.py
+++ b/scripts/sense-voice/ascend-npu/test_om.py
@@ -1,13 +1,11 @@
 #!/usr/bin/env python3
 # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
 
-import argparse
 from typing import Tuple
 
 import kaldi_native_fbank as knf
 import numpy as np
 import soundfile as sf
-import torch
 from ais_bench.infer.interface import InferSession
 
 
diff --git a/scripts/sense-voice/qnn/.gitignore b/scripts/sense-voice/qnn/.gitignore
new file mode 100644
index 00000000..e51c42e9
--- /dev/null
+++ b/scripts/sense-voice/qnn/.gitignore
@@ -0,0 +1 @@
+*.raw
diff --git a/scripts/sense-voice/qnn/decode_logits.py b/scripts/sense-voice/qnn/decode_logits.py
new file mode 100755
index 00000000..27047940
--- /dev/null
+++ b/scripts/sense-voice/qnn/decode_logits.py
@@ -0,0 +1,34 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+import numpy as np
+
+
+def load_tokens(filename):
+    ans = dict()
+    i = 0
+    with open(filename, encoding="utf-8") as f:
+        for line in f:
+            ans[i] = line.strip().split()[0]
+            i += 1
+    return ans
+
+
+logits = np.fromfile("./logits.raw", dtype=np.float32).reshape((-1, 25055))
+
+idx = logits.argmax(axis=-1)
+print("idx", idx)
+print(len(idx))
+prev = -1
+ids = []
+for i in idx:
+    if i != prev:
+        ids.append(i)
+    prev = i
+ids = [i for i in ids if i != 0]
+print(ids)
+
+tokens = load_tokens("./tokens.txt")
+text = "".join([tokens[i] for i in ids])
+
+text = text.replace("_", " ")
+print(text)
diff --git a/scripts/sense-voice/qnn/export-onnx.py b/scripts/sense-voice/qnn/export-onnx.py
new file mode 120000
index 00000000..06c05488
--- /dev/null
+++ b/scripts/sense-voice/qnn/export-onnx.py
@@ -0,0 +1 @@
+../rknn/export-onnx.py
\ No newline at end of file
diff --git a/scripts/sense-voice/qnn/generate_test_data.py b/scripts/sense-voice/qnn/generate_test_data.py
new file mode 100755
index 00000000..e816f536
--- /dev/null
+++ b/scripts/sense-voice/qnn/generate_test_data.py
@@ -0,0 +1,119 @@
+#!/usr/bin/env python3
+# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import argparse
+from typing import Tuple
+
+import kaldi_native_fbank as knf
+import numpy as np
+import soundfile as sf
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--num-frames",
+        type=int,
+        required=True,
+    )
+
+    parser.add_argument(
+        "--wav",
+        type=str,
+        required=True,
+    )
+    return parser.parse_args()
+
+
+def load_audio(filename: str) -> Tuple[np.ndarray, int]:
+    data, sample_rate = sf.read(
+        filename,
+        always_2d=True,
+        dtype="float32",
+    )
+    data = data[:, 0]  # use only the first channel
+    samples = np.ascontiguousarray(data)
+    return samples, sample_rate
+
+
+def compute_feat(
+    samples,
+    sample_rate,
+    window_size: int = 7,  # lfr_m
+    window_shift: int = 6,  # lfr_n
+):
+    opts = knf.FbankOptions()
+    opts.frame_opts.dither = 0
+    opts.frame_opts.snip_edges = False
+    opts.frame_opts.window_type = "hamming"
+    opts.frame_opts.samp_freq = sample_rate
+    opts.mel_opts.num_bins = 80
+
+    online_fbank = knf.OnlineFbank(opts)
+    online_fbank.accept_waveform(sample_rate, (samples * 32768).tolist())
+    online_fbank.input_finished()
+
+    features = np.stack(
+        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
+    )
+    assert features.data.contiguous is True
+    assert features.dtype == np.float32, features.dtype
+
+    T = (features.shape[0] - window_size) // window_shift + 1
+    features = np.lib.stride_tricks.as_strided(
+        features,
+        shape=(T, features.shape[1] * window_size),
+        strides=((window_shift * features.shape[1]) * 4, 4),
+    )
+
+    return np.copy(features)
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+
+    samples, sample_rate = load_audio(args.wav)
+    if sample_rate != 16000:
+        import librosa
+
+        samples = librosa.resample(samples, orig_sr=sample_rate, target_sr=16000)
+        sample_rate = 16000
+
+    features = compute_feat(
+        samples=samples,
+        sample_rate=sample_rate,
+    )
+    print("features.shape", features.shape)
+    if features.shape[0] > args.num_frames:
+        features = features[: args.num_frames]
+    elif features.shape[0] < args.num_frames:
+        pad_width = ((0, args.num_frames - features.shape[0]), (0, 0))
+        features = np.pad(features, pad_width, mode="constant", constant_values=0)
+
+    features.tofile("input0.raw")
+
+    language_auto = 0
+    language_zh = 3
+    language_en = 4
+    language_yue = 7
+    language_ya = 11
+    language_ko = 12
+    language_nospeech = 13
+
+    language = language_auto
+
+    with_itn = 14
+    without_itn = 15
+
+    text_norm = with_itn
+
+    prompt = np.array([language, 1, 2, text_norm], dtype=np.int32)
+    prompt.tofile("input1.raw")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/sense-voice/qnn/test_onnx.py b/scripts/sense-voice/qnn/test_onnx.py
new file mode 120000
index 00000000..dfb3c92a
--- /dev/null
+++ b/scripts/sense-voice/qnn/test_onnx.py
@@ -0,0 +1 @@
+../rknn/test_onnx.py
\ No newline at end of file
diff --git a/scripts/sense-voice/rknn/export-onnx.py b/scripts/sense-voice/rknn/export-onnx.py
index 6ee01c60..7a0161f8 100755
--- a/scripts/sense-voice/rknn/export-onnx.py
+++ b/scripts/sense-voice/rknn/export-onnx.py
@@ -25,6 +25,12 @@ def get_args():
         how long the model can process.
         """,
     )
+
+    parser.add_argument(
+        "--opset-version",
+        type=int,
+        default=13,
+    )
     return parser.parse_args()
 
 
@@ -118,7 +124,7 @@ def main():
     text_norm = 15
     prompt = torch.tensor([language, 1, 2, text_norm], dtype=torch.int32)
 
-    opset_version = 13
+    opset_version = args.opset_version
     filename = f"model-{input_len_in_seconds}-seconds.onnx"
     torch.onnx.export(
         model,

commit 42d20cccfd994583f921ab22405f24a2d528ef34
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Nov 10 11:30:41 2025 +0800

    Add cxx API for online punctuation models (#2759)

diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
index 891c8140..c35a92cf 100644
--- a/.github/workflows/cxx-api.yaml
+++ b/.github/workflows/cxx-api.yaml
@@ -78,6 +78,75 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
           fi
 
+
+      - name: Test Online punctuation
+        shell: bash
+        run: |
+          name=online-punctuation-cxx-api
+          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-cxx-api \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ls -lh ./$name
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+          tar xvf sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+          rm sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+
+          echo "---"
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm -rf sherpa-onnx-online-punct-*
+          rm -v ./$name
+
+      - name: Test Offline punctuation
+        shell: bash
+        run: |
+          name=offline-punctuation-cxx-api
+          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-cxx-api \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ls -lh ./$name
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+          tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+          rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+
+          echo "---"
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm -rf sherpa-onnx-punct-*
+          rm -v ./$name
+
       - name: Test CED audio tagging
         shell: bash
         run: |
diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
index 3e4104b5..6c47e0a1 100644
--- a/cxx-api-examples/CMakeLists.txt
+++ b/cxx-api-examples/CMakeLists.txt
@@ -42,8 +42,11 @@ target_link_libraries(wenet-ctc-cxx-api sherpa-onnx-cxx-api)
 add_executable(nemo-canary-cxx-api ./nemo-canary-cxx-api.cc)
 target_link_libraries(nemo-canary-cxx-api sherpa-onnx-cxx-api)
 
-add_executable(punctuation-cxx-api ./punctuation-cxx-api.cc)
-target_link_libraries(punctuation-cxx-api sherpa-onnx-cxx-api)
+add_executable(offline-punctuation-cxx-api ./offline-punctuation-cxx-api.cc)
+target_link_libraries(offline-punctuation-cxx-api sherpa-onnx-cxx-api)
+
+add_executable(online-punctuation-cxx-api ./online-punctuation-cxx-api.cc)
+target_link_libraries(online-punctuation-cxx-api sherpa-onnx-cxx-api)
 
 if(SHERPA_ONNX_ENABLE_PORTAUDIO)
   add_executable(sense-voice-simulate-streaming-microphone-cxx-api
diff --git a/cxx-api-examples/punctuation-cxx-api.cc b/cxx-api-examples/offline-punctuation-cxx-api.cc
similarity index 77%
rename from cxx-api-examples/punctuation-cxx-api.cc
rename to cxx-api-examples/offline-punctuation-cxx-api.cc
index e803ff9a..f20a9fc3 100644
--- a/cxx-api-examples/punctuation-cxx-api.cc
+++ b/cxx-api-examples/offline-punctuation-cxx-api.cc
@@ -1,11 +1,12 @@
-// cxx-api-examples/punctuation-cxx-api.cc
+// cxx-api-examples/offline-punctuation-cxx-api.cc
 // Copyright (c)  2025  Xiaomi Corporation
 
 // To use punctuation model:
-// wget
-// https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
-// tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
-// rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
+// clang-format off
+// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+// tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+// rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
+// clang-format on
 
 #include <iostream>
 #include <string>
@@ -17,8 +18,8 @@ int32_t main() {
 
   OfflinePunctuationConfig punctuation_config;
   punctuation_config.model.ct_transformer =
-      "./sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12/"
-      "model.onnx";
+      "./sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8/"
+      "model.int8.onnx";
   punctuation_config.model.num_threads = 1;
   punctuation_config.model.debug = false;
   punctuation_config.model.provider = "cpu";
diff --git a/cxx-api-examples/online-punctuation-cxx-api.cc b/cxx-api-examples/online-punctuation-cxx-api.cc
new file mode 100644
index 00000000..cbba6d86
--- /dev/null
+++ b/cxx-api-examples/online-punctuation-cxx-api.cc
@@ -0,0 +1,41 @@
+// cxx-api-examples/online-punctuation-cxx-api.cc
+// Copyright (c)  2025  Xiaomi Corporation
+
+// To use punctuation model:
+// clang-format off
+// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+// tar xvf sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+// rm sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
+// clang-format on
+
+#include <iostream>
+#include <string>
+
+#include "sherpa-onnx/c-api/cxx-api.h"
+
+int32_t main() {
+  using namespace sherpa_onnx::cxx;  // NOLINT
+
+  OnlinePunctuationConfig punctuation_config;
+  punctuation_config.model.cnn_bilstm =
+      "sherpa-onnx-online-punct-en-2024-08-06/model.int8.onnx";
+  punctuation_config.model.bpe_vocab =
+      "sherpa-onnx-online-punct-en-2024-08-06/bpe.vocab";
+  punctuation_config.model.num_threads = 1;
+  punctuation_config.model.debug = false;
+  punctuation_config.model.provider = "cpu";
+
+  OnlinePunctuation punct = OnlinePunctuation::Create(punctuation_config);
+  if (!punct.Get()) {
+    std::cerr
+        << "Failed to create punctuation model. Please check your config\n";
+    return -1;
+  }
+
+  std::string text = "how are you i am fine thank you";
+  std::string text_with_punct = punct.AddPunctuation(text);
+  std::cout << "Original text: " << text << std::endl;
+  std::cout << "With punctuation: " << text_with_punct << std::endl;
+
+  return 0;
+}
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index dffc1407..3b8f2e6e 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -836,6 +836,38 @@ std::string OfflinePunctuation::AddPunctuation(const std::string &text) const {
   return ans;
 }
 
+// ============================================================
+// For Online Punctuation
+// ============================================================
+OnlinePunctuation OnlinePunctuation::Create(
+    const OnlinePunctuationConfig &config) {
+  struct SherpaOnnxOnlinePunctuationConfig c;
+  memset(&c, 0, sizeof(c));
+  c.model.cnn_bilstm = config.model.cnn_bilstm.c_str();
+  c.model.bpe_vocab = config.model.bpe_vocab.c_str();
+  c.model.num_threads = config.model.num_threads;
+  c.model.debug = config.model.debug;
+  c.model.provider = config.model.provider.c_str();
+
+  const SherpaOnnxOnlinePunctuation *punct =
+      SherpaOnnxCreateOnlinePunctuation(&c);
+  return OnlinePunctuation(punct);
+}
+
+OnlinePunctuation::OnlinePunctuation(const SherpaOnnxOnlinePunctuation *p)
+    : MoveOnly<OnlinePunctuation, SherpaOnnxOnlinePunctuation>(p) {}
+
+void OnlinePunctuation::Destroy(const SherpaOnnxOnlinePunctuation *p) const {
+  SherpaOnnxDestroyOnlinePunctuation(p);
+}
+
+std::string OnlinePunctuation::AddPunctuation(const std::string &text) const {
+  const char *result = SherpaOnnxOnlinePunctuationAddPunct(p_, text.c_str());
+  std::string ans(result);
+  SherpaOnnxOnlinePunctuationFreeText(result);
+  return ans;
+}
+
 // ============================================================
 // For Audio tagging
 // ============================================================
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index fa718db9..e5c9711d 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -735,6 +735,35 @@ class SHERPA_ONNX_API OfflinePunctuation
   explicit OfflinePunctuation(const SherpaOnnxOfflinePunctuation *p);
 };
 
+// ============================================================================
+// Online Punctuation
+// ============================================================================
+struct OnlinePunctuationModelConfig {
+  std::string cnn_bilstm;
+  std::string bpe_vocab;
+  int32_t num_threads = 1;
+  int32_t debug = false;
+  std::string provider = "cpu";
+};
+
+struct OnlinePunctuationConfig {
+  OnlinePunctuationModelConfig model;
+};
+
+class SHERPA_ONNX_API OnlinePunctuation
+    : public MoveOnly<OnlinePunctuation, SherpaOnnxOnlinePunctuation> {
+ public:
+  static OnlinePunctuation Create(const OnlinePunctuationConfig &config);
+
+  void Destroy(const SherpaOnnxOnlinePunctuation *p) const;
+
+  // Add punctuations to the input text and return it.
+  std::string AddPunctuation(const std::string &text) const;
+
+ private:
+  explicit OnlinePunctuation(const SherpaOnnxOnlinePunctuation *p);
+};
+
 // ============================================================================
 // Audio tagging
 // ============================================================================
