commit d5f2643298ee247c57308332e4087163bf2709d2
Merge: 74f79e34 c49fbb68
Author: wangqi <wang.qi@qikuyx.com>
Date:   Wed Jan 14 10:12:16 2026 -0500

    Merge branch 'master' of https://github.com/wangqi/sherpa-onnx

commit 74f79e34bb6d23aac425d617366f655e1ac450ed
Merge: 32415915 69e79f59
Author: wangqi <wang.qi@qikuyx.com>
Date:   Wed Jan 14 10:11:32 2026 -0500

    Merge tag 'v1.12.22'

commit c49fbb68f012a8afd4f04153e5fce85ac9fc15c9
Merge: 32415915 63190210
Author: Wang Qi <wangqi@users.noreply.github.com>
Date:   Wed Jan 14 10:10:35 2026 -0500

    Merge branch 'k2-fsa:master' into master

commit 631902106f85336bf4f8163766bfe828406687f8
Author: Alfredo Maria Milano <alfmarmil@gmail.com>
Date:   Wed Jan 14 15:39:08 2026 +0100

    Node addon api jsdoc (#3005)

diff --git a/scripts/node-addon-api/lib/addon.js b/scripts/node-addon-api/lib/addon.js
index 3fdd637f..26c4c410 100644
--- a/scripts/node-addon-api/lib/addon.js
+++ b/scripts/node-addon-api/lib/addon.js
@@ -1,3 +1,5 @@
+/** @typedef {import('./types').WaveObject} WaveObject */
+
 const os = require('os');
 const path = require('path');
 
@@ -64,3 +66,27 @@ if (!found) {
 
   throw new Error(msg)
 }
+
+/**
+ * Read a wave file from disk.
+ * @function module.exports.readWave
+ * @param {string} filename
+ * @param {boolean} [enableExternalBuffer=true]
+ * @returns {WaveObject}
+ */
+
+/**
+ * Read a wave from binary buffer.
+ * @function module.exports.readWaveFromBinary
+ * @param {Uint8Array} data - Binary contents of a wave file.
+ * @param {boolean} [enableExternalBuffer=true]
+ * @returns {WaveObject}
+ */
+
+/**
+ * Write a wave file to disk.
+ * @function module.exports.writeWave
+ * @param {string} filename
+ * @param {WaveObject} obj - { samples: Float32Array, sampleRate: number }
+ * @returns {boolean}
+ */
diff --git a/scripts/node-addon-api/lib/audio-tagg.js b/scripts/node-addon-api/lib/audio-tagg.js
index b55daf5b..1cf962f3 100644
--- a/scripts/node-addon-api/lib/audio-tagg.js
+++ b/scripts/node-addon-api/lib/audio-tagg.js
@@ -1,20 +1,39 @@
+/** @typedef {import('./types').AudioTaggingConfig} AudioTaggingConfig */
+/** @typedef {import('./types').AudioEvent} AudioEvent */
+/** @typedef {import('./types').AudioTaggingHandle} AudioTaggingHandle */
+/** @typedef {import('./non-streaming-asr').OfflineStream} OfflineStream */
+
 const addon = require('./addon.js');
 const non_streaming_asr = require('./non-streaming-asr.js');
 
+/**
+ * AudioTagging utility.
+ * @class
+ */
 class AudioTagging {
+  /**
+   * Create an AudioTagging instance.
+   * @param {AudioTaggingConfig} config
+   */
   constructor(config) {
     this.handle = addon.createAudioTagging(config);
     this.config = config;
   }
 
+  /**
+   * Create an offline stream bound to this AudioTagging instance.
+   * @returns {OfflineStream}
+   */
   createStream() {
     return new non_streaming_asr.OfflineStream(
         addon.audioTaggingCreateOfflineStream(this.handle));
   }
 
-  /* Return an array. Each element is
-   * an object {name: "xxx", prob: xxx, index: xxx};
-   *
+  /**
+   * Compute audio tags from an offline stream.
+   * @param {OfflineStream} stream - An offline stream created by `AudioTagging.createStream()`.
+   * @param {number} [topK=-1] - Return top K results; -1 for all.
+   * @returns {AudioEvent[]}
    */
   compute(stream, topK = -1) {
     return addon.audioTaggingCompute(this.handle, stream.handle, topK);
diff --git a/scripts/node-addon-api/lib/keyword-spotter.js b/scripts/node-addon-api/lib/keyword-spotter.js
index d06764e8..78fcb99c 100644
--- a/scripts/node-addon-api/lib/keyword-spotter.js
+++ b/scripts/node-addon-api/lib/keyword-spotter.js
@@ -1,29 +1,60 @@
+/** @typedef {import('./types').KeywordSpotterConfig} KeywordSpotterConfig */
+/** @typedef {import('./types').KeywordResult} KeywordResult */
+/** @typedef {import('./streaming-asr').OnlineStream} OnlineStream */
+
 const addon = require('./addon.js');
 const streaming_asr = require('./streaming-asr.js');
 
+/**
+ * KeywordSpotter handles keyword detection.
+ */
 class KeywordSpotter {
+  /**
+   * @param {KeywordSpotterConfig} config
+   */
   constructor(config) {
     this.handle = addon.createKeywordSpotter(config);
     this.config = config
   }
 
+  /**
+   * Create an OnlineStream for the spotter.
+   * @returns {OnlineStream}
+   */
   createStream() {
     const handle = addon.createKeywordStream(this.handle);
     return new streaming_asr.OnlineStream(handle);
   }
 
+  /**
+   * @param {OnlineStream} stream
+   * @returns {boolean}
+   */
   isReady(stream) {
     return addon.isKeywordStreamReady(this.handle, stream.handle);
   }
 
+  /**
+   * Trigger decode on a stream.
+   * @param {OnlineStream} stream
+   */
   decode(stream) {
     addon.decodeKeywordStream(this.handle, stream.handle);
   }
 
+  /**
+   * Reset a stream.
+   * @param {OnlineStream} stream
+   */
   reset(stream) {
     addon.resetKeywordStream(this.handle, stream.handle);
   }
 
+  /**
+   * Get the keyword result for a stream.
+   * @param {OnlineStream} stream
+   * @returns {KeywordResult}
+   */
   getResult(stream) {
     const jsonStr = addon.getKeywordResultAsJson(this.handle, stream.handle);
 
diff --git a/scripts/node-addon-api/lib/non-streaming-asr.js b/scripts/node-addon-api/lib/non-streaming-asr.js
index 1b9c40f1..c260ec74 100644
--- a/scripts/node-addon-api/lib/non-streaming-asr.js
+++ b/scripts/node-addon-api/lib/non-streaming-asr.js
@@ -1,37 +1,72 @@
+/** @typedef {import('./types').OfflineStreamObject} OfflineStreamObject */
+/** @typedef {import('./types').Waveform} Waveform */
+/** @typedef {import('./types').OfflineRecognizerConfig} OfflineRecognizerConfig */
+/** @typedef {import('./types').OfflineRecognizerResult} OfflineRecognizerResult */
+
 const addon = require('./addon.js');
 
+/**
+ * OfflineStream represents a synchronous offline audio stream.
+ */
 class OfflineStream {
+  /**
+   * @param {OfflineStreamObject|Object} handle - Internal stream object with `handle` property.
+   */
   constructor(handle) {
     this.handle = handle;
   }
 
-  // obj is {samples: samples, sampleRate: sampleRate}
-  // samples is a float32 array containing samples in the range [-1, 1]
-  // sampleRate is a number
+  /**
+   * Accept a chunk of waveform samples.
+   * @param {Waveform} obj - { samples: Float32Array, sampleRate: number }
+   */
   acceptWaveform(obj) {
     addon.acceptWaveformOffline(this.handle, obj)
   }
 }
 
+/**
+ * OfflineRecognizer wraps the native offline recognizer.
+ */
 class OfflineRecognizer {
+  /**
+   * @param {OfflineRecognizerConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOfflineRecognizer(config);
     this.config = config
   }
 
+  /**
+   * Create a new OfflineStream bound to this recognizer.
+   * @returns {OfflineStream}
+   */
   createStream() {
     const handle = addon.createOfflineStream(this.handle);
     return new OfflineStream(handle);
   }
 
+  /**
+   * Replace the recognizer config at runtime.
+   * @param {OfflineRecognizerConfig} config
+   */
   setConfig(config) {
     addon.offlineRecognizerSetConfig(this.handle, config);
   }
 
+  /**
+   * Decode an offline stream (synchronous).
+   * @param {OfflineStream} stream
+   */
   decode(stream) {
     addon.decodeOfflineStream(this.handle, stream.handle);
   }
 
+  /**
+   * Get recognition result for a stream.
+   * @param {OfflineStream} stream
+   * @returns {OfflineRecognizerResult}
+   */
   getResult(stream) {
     const jsonStr = addon.getOfflineStreamResultAsJson(stream.handle);
 
diff --git a/scripts/node-addon-api/lib/non-streaming-speaker-diarization.js b/scripts/node-addon-api/lib/non-streaming-speaker-diarization.js
index 37c4a749..383628b0 100644
--- a/scripts/node-addon-api/lib/non-streaming-speaker-diarization.js
+++ b/scripts/node-addon-api/lib/non-streaming-speaker-diarization.js
@@ -1,6 +1,12 @@
+/** @typedef {import('./types').OfflineSpeakerDiarizationConfig} OfflineSpeakerDiarizationConfig */
+/** @typedef {import('./types').SpeakerDiarizationSegment} SpeakerDiarizationSegment */
+
 const addon = require('./addon.js');
 
 class OfflineSpeakerDiarization {
+  /**
+   * @param {OfflineSpeakerDiarizationConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOfflineSpeakerDiarization(config);
     this.config = config;
@@ -9,23 +15,17 @@ class OfflineSpeakerDiarization {
   }
 
   /**
-   * samples is a 1-d float32 array. Each element of the array should be
-   * in the range [-1, 1].
-   *
-   * We assume its sample rate equals to this.sampleRate.
-   *
-   * Returns an array of object, where an object is
-   *
-   *  {
-   *    "start": start_time_in_seconds,
-   *    "end": end_time_in_seconds,
-   *    "speaker": an_integer,
-   *  }
+   * @param {Float32Array} samples - 1-D float32 array in [-1, 1]
+   * @returns {SpeakerDiarizationSegment[]}
    */
   process(samples) {
     return addon.offlineSpeakerDiarizationProcess(this.handle, samples);
   }
 
+  /**
+   * Set clustering configuration.
+   * @param {{clustering: import('./types').FastClusteringConfig}} config
+   */
   setConfig(config) {
     addon.offlineSpeakerDiarizationSetConfig(this.handle, config);
     this.config.clustering = config.clustering;
@@ -34,4 +34,4 @@ class OfflineSpeakerDiarization {
 
 module.exports = {
   OfflineSpeakerDiarization,
-}
+} 
\ No newline at end of file
diff --git a/scripts/node-addon-api/lib/non-streaming-speech-denoiser.js b/scripts/node-addon-api/lib/non-streaming-speech-denoiser.js
index a2cbfef1..8a6a25a6 100644
--- a/scripts/node-addon-api/lib/non-streaming-speech-denoiser.js
+++ b/scripts/node-addon-api/lib/non-streaming-speech-denoiser.js
@@ -1,6 +1,13 @@
+/** @typedef {import('./types').OfflineSpeechDenoiserConfig} OfflineSpeechDenoiserConfig */
+/** @typedef {import('./types').GeneratedAudio} GeneratedAudio */
+/** @typedef {import('./types').AudioProcessRequest} AudioProcessRequest */
+
 const addon = require('./addon.js');
 
 class OfflineSpeechDenoiser {
+  /**
+   * @param {OfflineSpeechDenoiserConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOfflineSpeechDenoiser(config);
     this.config = config;
@@ -9,14 +16,10 @@ class OfflineSpeechDenoiser {
         addon.offlineSpeechDenoiserGetSampleRateWrapper(this.handle);
   }
 
-  /*
-    obj is
-    {samples: samples, sampleRate: sampleRate, enableExternalBuffer: true}
-
-    samples is a float32 array containing samples in the range [-1, 1]
-    sampleRate is a number
-
-   return an object {samples: Float32Array, sampleRate: <a number>}
+  /**
+   * Run denoiser synchronously.
+   * @param {AudioProcessRequest} obj - { samples: Float32Array, sampleRate: number, enableExternalBuffer?: boolean }
+   * @returns {GeneratedAudio}
    */
   run(obj) {
     return addon.offlineSpeechDenoiserRunWrapper(this.handle, obj);
@@ -25,4 +28,4 @@ class OfflineSpeechDenoiser {
 
 module.exports = {
   OfflineSpeechDenoiser,
-}
+} 
\ No newline at end of file
diff --git a/scripts/node-addon-api/lib/non-streaming-tts.js b/scripts/node-addon-api/lib/non-streaming-tts.js
index 168dc5bf..21899f6a 100644
--- a/scripts/node-addon-api/lib/non-streaming-tts.js
+++ b/scripts/node-addon-api/lib/non-streaming-tts.js
@@ -1,6 +1,13 @@
+/** @typedef {import('./types').OfflineTtsConfig} OfflineTtsConfig */
+/** @typedef {import('./types').TtsRequest} TtsRequest */
+/** @typedef {import('./types').GeneratedAudio} GeneratedAudio */
+
 const addon = require('./addon.js');
 
 class OfflineTts {
+  /**
+   * @param {OfflineTtsConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOfflineTts(config);
     this.config = config;
@@ -9,11 +16,10 @@ class OfflineTts {
     this.sampleRate = addon.getOfflineTtsSampleRate(this.handle);
   }
 
-  /*
-   input obj: {text: "xxxx", sid: 0, speed: 1.0}
-   where text is a string, sid is a int32, speed is a float
-
-   return an object {samples: Float32Array, sampleRate: <a number>}
+  /**
+   * Generate audio synchronously.
+   * @param {TtsRequest} obj
+   * @returns {GeneratedAudio}
    */
   generate(obj) {
     return addon.offlineTtsGenerate(this.handle, obj);
@@ -22,4 +28,4 @@ class OfflineTts {
 
 module.exports = {
   OfflineTts,
-}
+} 
\ No newline at end of file
diff --git a/scripts/node-addon-api/lib/punctuation.js b/scripts/node-addon-api/lib/punctuation.js
index ed4156ff..762e965e 100644
--- a/scripts/node-addon-api/lib/punctuation.js
+++ b/scripts/node-addon-api/lib/punctuation.js
@@ -1,20 +1,36 @@
+/** @typedef {import('./types').OfflinePunctuationHandle} OfflinePunctuationHandle */
+/** @typedef {import('./types').OfflinePunctuationConfig} OfflinePunctuationConfig */
+/** @typedef {import('./types').OnlinePunctuationConfig} OnlinePunctuationConfig */
+
 const addon = require('./addon.js');
 
 class OfflinePunctuation {
+  /**
+   * @param {OfflinePunctuationConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOfflinePunctuation(config);
     this.config = config;
   }
+  /**
+   * Add punctuation to `text` and return the punctuated text.
+   * @param {string} text
+   * @returns {string}
+   */
   addPunct(text) {
     return addon.offlinePunctuationAddPunct(this.handle, text);
   }
 }
 
 class OnlinePunctuation {
+  /**
+   * @param {OnlinePunctuationConfig} config
+   */
   constructor(config) {
     this.handle = addon.createOnlinePunctuation(config);
     this.config = config;
   }
+  /** @param {string} text @returns {string} */
   addPunct(text) {
     return addon.onlinePunctuationAddPunct(this.handle, text);
   }
@@ -23,4 +39,4 @@ class OnlinePunctuation {
 module.exports = {
   OfflinePunctuation,
   OnlinePunctuation,
-}
+} 
diff --git a/scripts/node-addon-api/lib/sherpa-onnx.js b/scripts/node-addon-api/lib/sherpa-onnx.js
index d1dabde3..8c80618a 100644
--- a/scripts/node-addon-api/lib/sherpa-onnx.js
+++ b/scripts/node-addon-api/lib/sherpa-onnx.js
@@ -1,3 +1,7 @@
+/** @typedef {import('./types').WaveObject} WaveObject */
+/** @typedef {import('./types').OnlineRecognizerResult} OnlineRecognizerResult */
+/** @typedef {import('./types').OfflineRecognizerResult} OfflineRecognizerResult */
+
 const addon = require('./addon.js')
 const streaming_asr = require('./streaming-asr.js');
 const non_streaming_asr = require('./non-streaming-asr.js');
diff --git a/scripts/node-addon-api/lib/speaker-identification.js b/scripts/node-addon-api/lib/speaker-identification.js
index 29a493a8..47536f85 100644
--- a/scripts/node-addon-api/lib/speaker-identification.js
+++ b/scripts/node-addon-api/lib/speaker-identification.js
@@ -1,29 +1,58 @@
+/** @typedef {import('./types').SpeakerEmbeddingEntry} SpeakerEmbeddingEntry */
+/** @typedef {import('./types').SpeakerEmbeddingManagerSearchObj} SpeakerEmbeddingManagerSearchObj */
+/** @typedef {import('./types').SpeakerEmbeddingManagerVerifyObj} SpeakerEmbeddingManagerVerifyObj */
+/** @typedef {import('./types').SpeakerEmbeddingExtractorConfig} SpeakerEmbeddingExtractorConfig */
+/** @typedef {import('./streaming-asr').OnlineStream} OnlineStream */
+
 const addon = require('./addon.js');
 const streaming_asr = require('./streaming-asr.js');
 
+/**
+ * SpeakerEmbeddingExtractor wraps native speaker embedding extractor.
+ */
 class SpeakerEmbeddingExtractor {
+  /**
+   * @param {SpeakerEmbeddingExtractorConfig} config
+   */
   constructor(config) {
     this.handle = addon.createSpeakerEmbeddingExtractor(config);
     this.config = config;
     this.dim = addon.speakerEmbeddingExtractorDim(this.handle);
   }
 
+  /**
+   * @returns {OnlineStream}
+   */
   createStream() {
     return new streaming_asr.OnlineStream(
         addon.speakerEmbeddingExtractorCreateStream(this.handle));
   }
 
+  /**
+   * @param {OnlineStream} stream
+   * @returns {boolean}
+   */
   isReady(stream) {
     return addon.speakerEmbeddingExtractorIsReady(this.handle, stream.handle);
   }
 
-  // return a float32 array
+  /**
+   * Compute embedding and return a Float32Array
+   * @param {OnlineStream} stream
+   * @param {boolean} [enableExternalBuffer=true]
+   * @returns {Float32Array}
+   */
   compute(stream, enableExternalBuffer = true) {
     return addon.speakerEmbeddingExtractorComputeEmbedding(
         this.handle, stream.handle, enableExternalBuffer);
   }
 }
 
+/**
+ * Flattens an array of Float32Arrays into a single Float32Array.
+ * @param {Float32Array[]} arrayList
+ * @returns {Float32Array}
+ */
 function flatten(arrayList) {
   let n = 0;
   for (let i = 0; i < arrayList.length; ++i) {
@@ -39,22 +68,29 @@ function flatten(arrayList) {
   return ans;
 }
 
+/**
+ * Manager for speaker embeddings.
+ */
 class SpeakerEmbeddingManager {
+  /**
+   * @param {number} dim - The embedding dimension
+   */
   constructor(dim) {
     this.handle = addon.createSpeakerEmbeddingManager(dim);
     this.dim = dim;
   }
 
-  /*
-   obj = {name: "xxx", v: a-float32-array}
+  /**
+   * @param {SpeakerEmbeddingEntry} obj
+   * @returns {boolean}
    */
   add(obj) {
     return addon.speakerEmbeddingManagerAdd(this.handle, obj);
   }
 
-  /*
-   * obj =
-   * {name: "xxx", v: [float32_array1, float32_array2, ..., float32_arrayn]
+  /**
+   * @param {{name:string, v: Float32Array[]}} obj
+   * @returns {boolean}
    */
   addMulti(obj) {
     const c = {
@@ -65,32 +101,44 @@ class SpeakerEmbeddingManager {
     return addon.speakerEmbeddingManagerAddListFlattened(this.handle, c);
   }
 
+  /**
+   * @param {string} name
+   * @returns {boolean}
+   */
   remove(name) {
     return addon.speakerEmbeddingManagerRemove(this.handle, name);
   }
 
-  /*
-   * obj = {v: a-float32-array, threshold: a-float }
+  /**
+   * @param {SpeakerEmbeddingManagerSearchObj} obj
+   * @returns {string}
    */
   search(obj) {
     return addon.speakerEmbeddingManagerSearch(this.handle, obj);
   }
 
-  /*
-   * obj = {name: 'xxx', v: a-float32-array, threshold: a-float }
+  /**
+   * @param {SpeakerEmbeddingManagerVerifyObj} obj
+   * @returns {boolean}
    */
   verify(obj) {
     return addon.speakerEmbeddingManagerVerify(this.handle, obj);
   }
 
+  /**
+   * @param {string} name
+   * @returns {boolean}
+   */
   contains(name) {
     return addon.speakerEmbeddingManagerContains(this.handle, name);
   }
 
+  /** @returns {number} */
   getNumSpeakers() {
     return addon.speakerEmbeddingManagerNumSpeakers(this.handle);
   }
 
+  /** @returns {string[]} */
   getAllSpeakerNames() {
     return addon.speakerEmbeddingManagerGetAllSpeakers(this.handle);
   }
diff --git a/scripts/node-addon-api/lib/spoken-language-identification.js b/scripts/node-addon-api/lib/spoken-language-identification.js
index 1fd93172..0ca3f58b 100644
--- a/scripts/node-addon-api/lib/spoken-language-identification.js
+++ b/scripts/node-addon-api/lib/spoken-language-identification.js
@@ -1,24 +1,31 @@
+/** @typedef {import('./types').SpokenLanguageIdentificationConfig} SpokenLanguageIdentificationConfig */
+/** @typedef {import('./non-streaming-asr').OfflineStream} OfflineStream */
+
 const addon = require('./addon.js');
 const non_streaming_asr = require('./non-streaming-asr.js');
 
 class SpokenLanguageIdentification {
+  /**
+   * @param {SpokenLanguageIdentificationConfig} config
+   */
   constructor(config) {
     this.handle = addon.createSpokenLanguageIdentification(config);
     this.config = config;
   }
 
+  /**
+   * @returns {OfflineStream}
+   */
   createStream() {
     return new non_streaming_asr.OfflineStream(
         addon.createSpokenLanguageIdentificationOfflineStream(this.handle));
   }
 
-  // return a string containing the language code (2 characters),
-  // e.g., en, de, fr, es, zh
-  // en -> English
-  // de -> German
-  // fr -> French
-  // es -> Spanish
-  // zh -> Chinese
+  /**
+   * Return a 2-letter language code, e.g. 'en', 'de', 'fr', 'es', 'zh'
+   * @param {OfflineStream} stream
+   * @returns {string}
+   */
   compute(stream) {
     return addon.spokenLanguageIdentificationCompute(
         this.handle, stream.handle);
@@ -27,4 +34,4 @@ class SpokenLanguageIdentification {
 
 module.exports = {
   SpokenLanguageIdentification,
-}
+} 
\ No newline at end of file
diff --git a/scripts/node-addon-api/lib/streaming-asr.js b/scripts/node-addon-api/lib/streaming-asr.js
index 01471af9..42965c7d 100644
--- a/scripts/node-addon-api/lib/streaming-asr.js
+++ b/scripts/node-addon-api/lib/streaming-asr.js
@@ -1,59 +1,118 @@
+/** @typedef {import('./types').OnlineStreamObject} OnlineStreamObject */
+/** @typedef {import('./types').OnlineRecognizerHandle} OnlineRecognizerHandle */
+/** @typedef {import('./types').DisplayObject} DisplayObject */
+/** @typedef {import('./types').OnlineRecognizerConfig} OnlineRecognizerConfig */
+/** @typedef {import('./types').Waveform} Waveform */
+/** @typedef {import('./types').OnlineRecognizerResult} OnlineRecognizerResult */
+
 const addon = require('./addon.js');
 
+/**
+ * Display helper for printing recognized words.
+ */
 class Display {
+  /**
+   * @param {number} maxWordPerline
+   */
   constructor(maxWordPerline) {
     this.handle = addon.createDisplay(maxWordPerline);
   }
 
+  /**
+   * Print text to display.
+   * @param {number} idx
+   * @param {string} text
+   */
   print(idx, text) {
     addon.print(this.handle, idx, text)
   }
 }
 
+/**
+ * OnlineStream holds an active online stream handle.
+ */
 class OnlineStream {
+  /**
+   * @param {OnlineStreamObject|Object} handle - object with `handle` property
+   */
   constructor(handle) {
     this.handle = handle;
   }
 
-  // obj is {samples: samples, sampleRate: sampleRate}
-  // samples is a float32 array containing samples in the range [-1, 1]
-  // sampleRate is a number
+  /**
+   * Accept waveform data
+   * @param {Waveform} obj - { samples: Float32Array, sampleRate: number }
+   */
   acceptWaveform(obj) {
     addon.acceptWaveformOnline(this.handle, obj)
   }
 
+  /** Notify the stream input has finished. */
   inputFinished() {
     addon.inputFinished(this.handle)
   }
 }
 
+/**
+ * OnlineRecognizer wraps native online recognizer.
+ */
 class OnlineRecognizer {
+  /**
+   * @param {OnlineRecognizerConfig} config - online recognizer config (see C++ for fields)
+   */
   constructor(config) {
     this.handle = addon.createOnlineRecognizer(config);
     this.config = config
   }
 
+  /**
+   * Create a new OnlineStream.
+   * @returns {OnlineStream}
+   */
   createStream() {
     const handle = addon.createOnlineStream(this.handle);
     return new OnlineStream(handle);
   }
 
+  /**
+   * Check whether a stream is ready.
+   * @param {OnlineStream} stream
+   * @returns {boolean}
+   */
   isReady(stream) {
     return addon.isOnlineStreamReady(this.handle, stream.handle);
   }
 
+  /**
+   * Trigger decoding on a stream.
+   * @param {OnlineStream} stream
+   */
   decode(stream) {
     addon.decodeOnlineStream(this.handle, stream.handle);
   }
 
+  /**
+   * Check endpoint condition for a stream.
+   * @param {OnlineStream} stream
+   * @returns {boolean}
+   */
   isEndpoint(stream) {
     return addon.isEndpoint(this.handle, stream.handle);
   }
 
+  /**
+   * Reset a stream.
+   * @param {OnlineStream} stream
+   */
   reset(stream) {
     addon.reset(this.handle, stream.handle);
   }
 
+  /**
+   * Get recognition result for a stream.
+   * @param {OnlineStream} stream
+   * @returns {OnlineRecognizerResult}
+   */
   getResult(stream) {
     const jsonStr =
         addon.getOnlineStreamResultAsJson(this.handle, stream.handle);
diff --git a/scripts/node-addon-api/lib/types.js b/scripts/node-addon-api/lib/types.js
new file mode 100644
index 00000000..10dfaab2
--- /dev/null
+++ b/scripts/node-addon-api/lib/types.js
@@ -0,0 +1,665 @@
+/**
+ * Centralized JSDoc typedefs for the Node addon API.
+ * These typedefs mirror the shapes produced/consumed by the C++ bindings
+ * in `scripts/node-addon-api/src/*` and by the underlying SherpaOnnx C API.
+ *
+ * Keep these typedefs specialized (no `any`/`unknown`) and concise.
+ */
+
+/**
+ * Opaque handle types returned by native constructors. These are opaque
+ * JavaScript objects backed by native pointers. Do not introspect or
+ * mutate their internals; pass them to the API functions as-is.
+ *
+ * @typedef {Object} OfflineStreamHandle
+ * @see src/non-streaming-asr.cc
+ */
+
+/**
+ * @typedef {Object} OnlineStreamHandle
+ * @see src/streaming-asr.cc
+ */
+
+/**
+ * @typedef {Object} OfflineRecognizerHandle
+ * @see src/non-streaming-asr.cc
+ */
+
+/**
+ * @typedef {Object} OnlineRecognizerHandle
+ * @see src/streaming-asr.cc
+ */
+
+/**
+ * @typedef {Object} DisplayHandle
+ * @see src/streaming-asr.cc
+ */
+
+/**
+ * @typedef {Object} CircularBufferHandle
+ * @see src/vad.cc
+ */
+
+/**
+ * @typedef {Object} VoiceActivityDetectorHandle
+ * @see src/vad.cc
+ */
+
+/**
+ * @typedef {Object} AudioTaggingHandle
+ * @see src/audio-tagging.cc
+ */
+
+/**
+ * @typedef {Object} OfflinePunctuationHandle
+ * @see src/punctuation.cc
+ */
+
+/**
+ * A single audio event returned by AudioTagging.compute().
+ * @typedef {Object} AudioEvent
+ * @property {string} name - The event name.
+ * @property {number} prob - Probability in [0,1].
+ * @property {number} index - Index (integer) of the event.
+ */
+
+/**
+ * AudioTagging specific model config for Zipformer variant
+ * @typedef {Object} AudioTaggingZipformerModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * AudioTagging model config.
+ * @typedef {Object} AudioTaggingModelConfig
+ * @property {AudioTaggingZipformerModelConfig} [zipformer]
+ * @property {string} [ced]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * AudioTagging configuration passed to constructor.
+ * @typedef {Object} AudioTaggingConfig
+ * @property {AudioTaggingModelConfig} [model]
+ * @property {string} [labels]
+ * @property {number} [topK]
+ */
+
+/**
+ * Waveform input object used by acceptWaveform methods.
+ * @typedef {Object} Waveform
+ * @property {Float32Array} samples - Float32Array of samples in [-1, 1].
+ * @property {number} sampleRate - Sample rate as an integer (e.g., 16000).
+ */
+
+/**
+ * Feature config used by recognizers and models.
+ * @typedef {Object} FeatureConfig
+ * @property {number} [sampleRate]
+ * @property {number} [featureDim]
+ */
+
+/**
+ * Silero VAD model config
+ * @typedef {Object} SileroVadModelConfig
+ * @property {string} [model]
+ * @property {number} [threshold]
+ * @property {number} [minSilenceDuration]
+ * @property {number} [minSpeechDuration]
+ * @property {number} [windowSize]
+ * @property {number} [maxSpeechDuration]
+ */
+
+/**
+ * Ten-VAD model config
+ * @typedef {Object} TenVadModelConfig
+ * @property {string} [model]
+ * @property {number} [threshold]
+ * @property {number} [minSilenceDuration]
+ * @property {number} [minSpeechDuration]
+ * @property {number} [windowSize]
+ * @property {number} [maxSpeechDuration]
+ */
+
+/**
+ * Voice activity detector configuration.
+ * @typedef {Object} VadConfig
+ * @property {SileroVadModelConfig} [sileroVad]
+ * @property {TenVadModelConfig} [tenVad]
+ * @property {number} [sampleRate]
+ * @property {number} [numThreads]
+ * @property {string} [provider]
+ * @property {boolean|number} [debug]
+ */
+
+/**
+ * Offline Transducer model config
+ * @typedef {Object} OfflineTransducerModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ * @property {string} [joiner]
+ */
+
+/**
+ * Offline Paraformer model config
+ * @typedef {Object} OfflineParaformerModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Zipformer CTC model config
+ * @typedef {Object} OfflineZipformerCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Wenet CTC model config
+ * @typedef {Object} OfflineWenetCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Omnilingual ASR CTC model config
+ * @typedef {Object} OfflineOmnilingualAsrCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Med ASR CTC model config
+ * @typedef {Object} OfflineMedAsrCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Dolphin model config
+ * @typedef {Object} OfflineDolphinModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline NeMo CTC model config
+ * @typedef {Object} OfflineNeMoCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Canary model config
+ * @typedef {Object} OfflineCanaryModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ * @property {string} [srcLang]
+ * @property {string} [tgtLang]
+ * @property {number} [usePnc]
+ */
+
+/**
+ * Offline Whisper model config
+ * @typedef {Object} OfflineWhisperModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ * @property {string} [language]
+ * @property {string} [task]
+ * @property {number} [tailPaddings]
+ */
+
+/**
+ * Offline FireRed ASR model config
+ * @typedef {Object} OfflineFireRedAsrModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ */
+
+/**
+ * Offline Moonshine model config
+ * @typedef {Object} OfflineMoonshineModelConfig
+ * @property {string} [preprocessor]
+ * @property {string} [encoder]
+ * @property {string} [uncachedDecoder]
+ * @property {string} [cachedDecoder]
+ */
+
+/**
+ * Offline TDNN model config
+ * @typedef {Object} OfflineTdnnModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline SenseVoice model config
+ * @typedef {Object} OfflineSenseVoiceModelConfig
+ * @property {string} [model]
+ * @property {string} [language]
+ * @property {number} [useInverseTextNormalization]
+ */
+
+/**
+ * Offline model config.
+ * @typedef {Object} OfflineModelConfig
+ * @property {OfflineTransducerModelConfig} [transducer]
+ * @property {OfflineParaformerModelConfig} [paraformer]
+ * @property {OfflineZipformerCtcModelConfig} [zipformerCtc]
+ * @property {OfflineWenetCtcModelConfig} [wenetCtc]
+ * @property {OfflineOmnilingualAsrCtcModelConfig} [omnilingual]
+ * @property {OfflineMedAsrCtcModelConfig} [medasr]
+ * @property {OfflineDolphinModelConfig} [dolphin]
+ * @property {OfflineNeMoCtcModelConfig} [nemoCtc]
+ * @property {OfflineCanaryModelConfig} [canary]
+ * @property {OfflineWhisperModelConfig} [whisper]
+ * @property {OfflineFireRedAsrModelConfig} [fireRedAsr]
+ * @property {OfflineMoonshineModelConfig} [moonshine]
+ * @property {OfflineTdnnModelConfig} [tdnn]
+ * @property {OfflineSenseVoiceModelConfig} [senseVoice]
+ * @property {string} [tokens]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Transducer model config
+ * @typedef {Object} TransducerModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ * @property {string} [joiner]
+ */
+
+/**
+ * Paraformer model config
+ * @typedef {Object} ParaformerModelConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ */
+
+/**
+ * Zipformer2 CTC model config
+ * @typedef {Object} Zipformer2CtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * NeMo CTC model config
+ * @typedef {Object} NemoCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Tone CTC model config
+ * @typedef {Object} ToneCtcModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Online model config (subset of C++ `OnlineModelConfig`).
+ * @typedef {Object} OnlineModelConfig
+ * @property {TransducerModelConfig} [transducer]
+ * @property {ParaformerModelConfig} [paraformer]
+ * @property {Zipformer2CtcModelConfig} [zipformer2Ctc]
+ * @property {NemoCtcModelConfig} [nemoCtc]
+ * @property {ToneCtcModelConfig} [toneCtc]
+ * @property {string} [tokens]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ * @property {string} [modelType]
+ * @property {string} [modelingUnit]
+ * @property {string} [bpeVocab]
+ * @property {string} [tokensBuf]
+ * @property {number} [tokensBufSize]
+ */
+
+/**
+ * Homophone replacer configuration used both in online and offline recognizers.
+ * @typedef {Object} HomophoneReplacerConfig
+ * @property {string} [lexicon]
+ * @property {string} [ruleFsts]
+ */
+
+/**
+ * Online recognizer configuration passed to createOnlineRecognizer.
+ * @typedef {Object} OnlineRecognizerConfig
+ * @property {FeatureConfig} [featConfig]
+ * @property {OnlineModelConfig} [modelConfig]
+ * @property {HomophoneReplacerConfig} [hr]
+ * @property {string} [decodingMethod]
+ * @property {number} [maxActivePaths]
+ * @property {boolean|number} [enableEndpoint]
+ * @property {number} [rule1MinTrailingSilence]
+ * @property {number} [rule2MinTrailingSilence]
+ * @property {number} [rule3MinUtteranceLength]
+ * @property {string} [hotwordsFile]
+ * @property {number} [hotwordsScore]
+ * @property {string} [ruleFsts]
+ * @property {string} [ruleFars]
+ * @property {number} [blankPenalty]
+ */
+
+/**
+ * Offline recognizer config passed to createOfflineRecognizer.
+ * @typedef {Object} OfflineRecognizerConfig
+ * @property {FeatureConfig} [featConfig]
+ * @property {OfflineModelConfig} [modelConfig]
+ */
+
+/**
+ * Wave object returned by readWave and used by writeWave.
+ * @typedef {Object} WaveObject
+ * @property {Float32Array} samples - 1-D float32 samples in [-1, 1].
+ * @property {number} sampleRate - Sample rate as an integer (e.g., 16000).
+ * @see src/wave-reader.cc
+ */
+
+/**
+ * Speech segment returned by Vad.front().
+ * @typedef {Object} SpeechSegment
+ * @property {number} start - Start index (int32) of this segment.
+ * @property {Float32Array} samples - Float32Array of samples.
+ * @see src/vad.cc
+ */
+
+/**
+ * Audio returned by TTS and speech denoiser.
+ * @typedef {Object} GeneratedAudio
+ * @property {Float32Array} samples - The generated/denoised audio samples.
+ * @property {number} sampleRate - Sample rate in Hz.
+ * @see src/non-streaming-tts.cc
+ * @see src/non-streaming-speech-denoiser.cc
+ */
+
+/**
+ * TTS request object passed to generate/generateAsync.
+ * @typedef {Object} TtsRequest
+ * @property {string} text - Input text to synthesize.
+ * @property {number} sid - Speaker id (integer).
+ * @property {number} speed - Playback speed (float).
+ * @property {boolean} [enableExternalBuffer=true] - Whether to use an external buffer.
+ */
+
+/**
+ * Spoken Language ID whisper config
+ * @typedef {Object} SpokenLanguageIdentificationWhisperConfig
+ * @property {string} [encoder]
+ * @property {string} [decoder]
+ * @property {number} [tailPaddings]
+ */
+
+/**
+ * SpokenLanguageIdentification config
+ * @typedef {Object} SpokenLanguageIdentificationConfig
+ * @property {SpokenLanguageIdentificationWhisperConfig} [whisper]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Speaker embedding extractor config
+ * @typedef {Object} SpeakerEmbeddingExtractorConfig
+ * @property {string} [model]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Offline punctuation model config
+ * @typedef {Object} OfflinePunctuationModelConfig
+ * @property {string} [ctTransformer]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Offline punctuation config
+ * @typedef {Object} OfflinePunctuationConfig
+ * @property {OfflinePunctuationModelConfig} [model]
+ */
+
+/**
+ * Online punctuation model config
+ * @typedef {Object} OnlinePunctuationModelConfig
+ * @property {string} [cnnBilstm]
+ * @property {string} [bpeVocab]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Online punctuation config
+ * @typedef {Object} OnlinePunctuationConfig
+ * @property {OnlinePunctuationModelConfig} [model]
+ */
+
+/**
+ * Generic audio processing request used by denoisers/tts generators.
+ * @typedef {Object} AudioProcessRequest
+ * @property {Float32Array} samples
+ * @property {number} sampleRate
+ * @property {boolean} [enableExternalBuffer]
+ */
+
+/**
+ * Offline TTS model configs
+ * @typedef {Object} OfflineTtsVitsModelConfig
+ * @property {string} [model]
+ * @property {string} [lexicon]
+ * @property {string} [tokens]
+ * @property {string} [dataDir]
+ * @property {number} [noiseScale]
+ * @property {number} [noiseScaleW]
+ * @property {number} [lengthScale]
+ */
+
+/**
+ * @typedef {Object} OfflineTtsMatchaModelConfig
+ * @property {string} [acousticModel]
+ * @property {string} [vocoder]
+ * @property {string} [lexicon]
+ * @property {string} [tokens]
+ * @property {string} [dataDir]
+ * @property {number} [noiseScale]
+ * @property {number} [lengthScale]
+ */
+
+/**
+ * @typedef {Object} OfflineTtsKokoroModelConfig
+ * @property {string} [model]
+ * @property {string} [voices]
+ * @property {string} [tokens]
+ * @property {string} [dataDir]
+ * @property {number} [lengthScale]
+ * @property {string} [lexicon]
+ * @property {string} [lang]
+ */
+
+/**
+ * @typedef {Object} OfflineTtsKittenModelConfig
+ * @property {string} [model]
+ * @property {string} [voices]
+ * @property {string} [tokens]
+ * @property {string} [dataDir]
+ * @property {number} [lengthScale]
+ */
+
+/**
+ * Offline TTS model config
+ * @typedef {Object} OfflineTtsModelConfig
+ * @property {OfflineTtsVitsModelConfig} [vits]
+ * @property {OfflineTtsMatchaModelConfig} [matcha]
+ * @property {OfflineTtsKokoroModelConfig} [kokoro]
+ * @property {OfflineTtsKittenModelConfig} [kitten]
+ */
+
+/**
+ * Offline TTS configuration (partial, commonly used props).
+ * @typedef {Object} OfflineTtsConfig
+ * @property {OfflineTtsModelConfig} [model]
+ * @property {number} [maxNumSentences]
+ * @property {number} [silenceScale]
+ * @property {number} [numThreads]
+ * @property {string} [provider]
+ */
+
+/**
+ * Offline Speech Denoiser model config
+ * @typedef {Object} OfflineSpeechDenoiserGtcrnModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline Speech Denoiser model config
+ * @typedef {Object} OfflineSpeechDenoiserModelConfig
+ * @property {OfflineSpeechDenoiserGtcrnModelConfig} [gtcrn]
+ */
+
+/**
+ * Offline Speech Denoiser configuration (partial).
+ * @typedef {Object} OfflineSpeechDenoiserConfig
+ * @property {OfflineSpeechDenoiserModelConfig} [model]
+ * @property {number} [numThreads]
+ * @property {string} [provider]
+ */
+
+/**
+ * Offline speaker segmentation (pyannote) model config
+ * @typedef {Object} OfflineSpeakerSegmentationPyannoteModelConfig
+ * @property {string} [model]
+ */
+
+/**
+ * Offline speaker segmentation model config
+ * @typedef {Object} OfflineSpeakerSegmentationModelConfig
+ * @property {OfflineSpeakerSegmentationPyannoteModelConfig} [pyannote]
+ * @property {number} [numThreads]
+ * @property {boolean|number} [debug]
+ * @property {string} [provider]
+ */
+
+/**
+ * Offline Speaker Diarization configuration (partial).
+ * @typedef {Object} OfflineSpeakerDiarizationConfig
+ * @property {OfflineSpeakerSegmentationModelConfig} [segmentation]
+ * @property {SpeakerEmbeddingExtractorConfig} [embedding]
+ * @property {FastClusteringConfig} [clustering]
+ * @property {number} [minDurationOn]
+ * @property {number} [minDurationOff]
+ */
+
+/**
+ * Fast clustering configuration used by diarization.
+ * @typedef {Object} FastClusteringConfig
+ * @property {number} [numClusters]
+ * @property {number} [threshold]
+ */
+
+/**
+ * SpeakerEmbeddingManager add-multi flattened object
+ * @typedef {Object} SpeakerEmbeddingManagerAddListFlattenedObj
+ * @property {string} name
+ * @property {Float32Array} vv
+ * @property {number} n
+ */
+
+/**
+ * SpeakerEmbeddingManager search object
+ * @typedef {Object} SpeakerEmbeddingManagerSearchObj
+ * @property {Float32Array} v
+ * @property {number} threshold
+ */
+
+/**
+ * SpeakerEmbeddingManager verify object
+ * @typedef {Object} SpeakerEmbeddingManagerVerifyObj
+ * @property {string} name
+ * @property {Float32Array} v
+ * @property {number} threshold
+ */
+
+/**
+ * KeywordSpotter config (partial)
+ * @typedef {Object} KeywordSpotterConfig
+ * @property {FeatureConfig} [featConfig]
+ * @property {OfflineModelConfig} [modelConfig]
+ * @property {number} [maxActivePaths]
+ * @property {number} [numTrailingBlanks]
+ * @property {number} [keywordsScore]
+ * @property {number} [keywordsThreshold]
+ * @property {string} [keywordsFile]
+ */
+
+/**
+ * Offline recognition result returned by `getOfflineStreamResultAsJson`.
+ * See `OfflineRecognitionResult::AsJsonString()` in C++ for precise fields.
+ * @typedef {Object} OfflineRecognizerResult
+ * @property {string} lang
+ * @property {string} emotion
+ * @property {string} event
+ * @property {string} text
+ * @property {number[]} timestamps
+ * @property {number[]} durations
+ * @property {string[]} tokens
+ * @property {number[]} ys_log_probs
+ * @property {number[]} words
+ */
+
+/**
+ * Online recognition result returned by `getOnlineStreamResultAsJson`.
+ * See `OnlineRecognizerResult::AsJsonString()` in C++.
+ * @typedef {Object} OnlineRecognizerResult
+ * @property {string} text
+ * @property {string[]} tokens
+ * @property {number[]} timestamps
+ * @property {number[]} ys_probs
+ * @property {number[]} lm_probs
+ * @property {number[]} context_scores
+ * @property {number} segment
+ * @property {number[]} words
+ * @property {number} start_time
+ * @property {boolean} is_final
+ * @property {boolean} is_eof
+ */
+
+/**
+ * Keyword spotter result returned by `getKeywordResultAsJson`.
+ * @typedef {Object} KeywordResult
+ * @property {number} start_time
+ * @property {string} keyword
+ * @property {number[]} timestamps
+ * @property {string[]} tokens
+ */
+
+/**
+ * Speaker diarization segment returned by `offlineSpeakerDiarizationProcess`.
+ * @typedef {Object} SpeakerDiarizationSegment
+ * @property {number} start - start time in seconds
+ * @property {number} end - end time in seconds
+ * @property {number} speaker - speaker id (integer)
+ */
+
+/**
+ * Speaker embedding entry used by SpeakerEmbeddingManager.add
+ * @typedef {Object} SpeakerEmbeddingEntry
+ * @property {string} name - speaker name
+ * @property {Float32Array} v - embedding vector
+ */
+
+/**
+ * @typedef {Object} OfflineStreamObject
+ * @property {OfflineStreamHandle} handle
+ */
+
+/**
+ * @typedef {Object} OnlineStreamObject
+ * @property {OnlineStreamHandle} handle
+ */
+
+/**
+ * @typedef {Object} DisplayObject
+ * @property {DisplayHandle} handle
+ */
+
+// Export typedefs so they can be referenced by require('./types.js')
+module.exports = {};
diff --git a/scripts/node-addon-api/lib/vad.js b/scripts/node-addon-api/lib/vad.js
index a71dfdb6..06041175 100644
--- a/scripts/node-addon-api/lib/vad.js
+++ b/scripts/node-addon-api/lib/vad.js
@@ -1,49 +1,77 @@
+/** @typedef {import('./types').CircularBufferHandle} CircularBufferHandle */
+/** @typedef {import('./types').SpeechSegment} SpeechSegment */
+/** @typedef {import('./types').VadConfig} VadConfig */
+
 const addon = require('./addon.js');
 
+/**
+ * CircularBuffer stores float32 samples internally.
+ */
 class CircularBuffer {
+  /**
+   * @param {number} capacity - capacity in samples (integer)
+   */
   constructor(capacity) {
     this.handle = addon.createCircularBuffer(capacity);
   }
 
-  // samples is a float32 array
+  /**
+   * Push samples into the buffer.
+   * @param {Float32Array} samples
+   */
   push(samples) {
     addon.circularBufferPush(this.handle, samples);
   }
 
-  // return a float32 array
+  /**
+   * Get a slice of samples.
+   * @param {number} startIndex
+   * @param {number} n
+   * @param {boolean} [enableExternalBuffer=true]
+   * @returns {Float32Array}
+   */
   get(startIndex, n, enableExternalBuffer = true) {
     return addon.circularBufferGet(
         this.handle, startIndex, n, enableExternalBuffer);
   }
 
+  /**
+   * Pop n samples from the buffer.
+   * @param {number} n
+   */
   pop(n) {
     return addon.circularBufferPop(this.handle, n);
   }
 
+  /**
+   * Get current size in samples.
+   * @returns {number}
+   */
   size() {
     return addon.circularBufferSize(this.handle);
   }
 
+  /**
+   * Get head index.
+   * @returns {number}
+   */
   head() {
     return addon.circularBufferHead(this.handle);
   }
 
+  /** Reset the buffer. */
   reset() {
     addon.circularBufferReset(this.handle);
   }
 }
 
+/**
+ * Voice Activity Detector (VAD).
+ */
 class Vad {
-  /*
-config = {
-  sileroVad: {
-    model: "./silero_vad.onnx",
-    threshold: 0.5,
-    minSilenceDuration: 0.5,
-    minSpeechDuration: 0.25,
-    maxSpeechDuration: 5,
-  }
-}
+  /**
+   * @param {VadConfig} config
+   * @param {number} bufferSizeInSeconds
    */
   constructor(config, bufferSizeInSeconds) {
     this.handle =
@@ -51,40 +79,49 @@ config = {
     this.config = config;
   }
 
+  /**
+   * Accept raw waveform samples.
+   * @param {Float32Array} samples
+   */
   acceptWaveform(samples) {
     addon.voiceActivityDetectorAcceptWaveform(this.handle, samples);
   }
 
+  /** @returns {boolean} */
   isEmpty() {
     return addon.voiceActivityDetectorIsEmpty(this.handle);
   }
 
+  /** @returns {boolean} */
   isDetected() {
     return addon.voiceActivityDetectorIsDetected(this.handle);
   }
 
+  /** Pop the earliest detected speech segment. */
   pop() {
     addon.voiceActivityDetectorPop(this.handle);
   }
 
+  /** Clear internal state. */
   clear() {
     addon.voiceActivityDetectorClear(this.handle);
   }
 
-  /*
-{
-  samples: a 1-d float32 array,
-  start: an int32
-}
+  /**
+   * Get the front speech segment.
+   * @param {boolean} [enableExternalBuffer=true]
+   * @returns {SpeechSegment}
    */
   front(enableExternalBuffer = true) {
     return addon.voiceActivityDetectorFront(this.handle, enableExternalBuffer);
   }
 
+  /** Reset detector state. */
   reset() {
     addon.voiceActivityDetectorReset(this.handle);
   }
 
+  /** Flush pending internal buffer. */
   flush() {
     addon.voiceActivityDetectorFlush(this.handle);
   }
diff --git a/scripts/node-addon-api/package.json b/scripts/node-addon-api/package.json
index 201cb77d..d2be0b1a 100644
--- a/scripts/node-addon-api/package.json
+++ b/scripts/node-addon-api/package.json
@@ -9,7 +9,9 @@
   },
   "scripts": {
     "install": "cmake-js compile --log-level verbose",
-    "test": "node --napi-modules ./test/test_binding.js"
+    "postinstall": "npm run typecheck",
+    "test": "node --napi-modules ./test/test_binding.js",
+    "typecheck": "tsc"
   },
   "repository": {
     "type": "git",
@@ -60,5 +62,9 @@
   "bugs": {
     "url": "https://github.com/k2-fsa/sherpa-onnx/issues"
   },
-  "homepage": "https://github.com/k2-fsa/sherpa-onnx#readme"
+  "homepage": "https://github.com/k2-fsa/sherpa-onnx#readme",
+  "devDependencies": {
+    "@types/node": "^24.10.4",
+    "typescript": "^5.9.3"
+  }
 }
diff --git a/scripts/node-addon-api/tsconfig.json b/scripts/node-addon-api/tsconfig.json
new file mode 100644
index 00000000..22a4ca67
--- /dev/null
+++ b/scripts/node-addon-api/tsconfig.json
@@ -0,0 +1,14 @@
+{
+  "compilerOptions": {
+    "allowJs": true,
+    "checkJs": true,
+    "noEmit": true,
+    "skipLibCheck": true,
+    "esModuleInterop": true,
+    "module": "commonjs",
+    "target": "ES2019",
+    "lib": ["ES2020"],
+    "types": ["node"]
+  },
+  "include": ["lib/**/*.js"]
+}

commit 69e79f5910d4461619680eea0b07e378927ff3fe
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Jan 14 20:25:52 2026 +0800

    Release v1.12.22 (#3048)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 8709678d..772f4788 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,12 @@
+## 1.12.22
+
+* Update wav files for FunASR Nano (#3038)
+* cmake: fix sha256 for onnxruntime linux x86_64 gpu package (#3042)
+* Fix checking funasr nano tokenizer on Windows (#3043)
+* Support nemotron-speech-streaming-en-0.6b (#3044)
+* Build APK for nemotron-speech-streaming-en-0.6b (#3045)
+* Fix building Linux arm wheels (#3047)
+
 ## 1.12.21
 
 * Fix publishing NPM packages (#2909)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 1ae539dc..02d16653 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.21")
+set(SHERPA_ONNX_VERSION "1.12.22")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index d4667dcc..e90216f1 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index d4667dcc..e90216f1 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index 2d334696..ed80172d 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-v1.12.21-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.21-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-v1.12.22-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.22-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.21.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.22.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index b2c50c1e..73764456 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index ee7c122b..3df4f4c9 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index fdc0ad33..421db79e 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.21'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.22'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index d4667dcc..e90216f1 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index a27c68d4..b59f81c0 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index bab85e3b..8f0629e8 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.21")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.22")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index 737f65f3..d232120a 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index 913f78f3..9fbf768d 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index 5d3a3551..29df1043 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 65475bde..db22dcab 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index ebb98c5c..0b93f6ec 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20260112
-        versionName = "1.12.21"
+        versionCode = 20260114
+        versionName = "1.12.22"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index 6ea6c6c1..4282a89f 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index 6ea6c6c1..4282a89f 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 75de557b..e11cb403 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20260112
-        versionName "1.12.21"
+        versionCode 20260114
+        versionName "1.12.22"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index 8d2ce969..dd700cd2 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.21</string>
+	<string>1.12.22</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index 069d42a9..0a1bbd65 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index 48f55860..ba20de72 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index d1cb864f..c3920d48 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index 7a3056d1..b5bdbf45 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index 533e23e2..d6a09fae 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index 052c6991..644a3700 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index 4dad3914..45afeed7 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index 05e04e44..78726967 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index e394622d..174270d0 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index 75bf0e48..8cb216da 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index 74a938aa..3f33d405 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index 923cf452..d4e47c1d 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index ed3f5821..b2412917 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.21
+version: 1.12.22
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index b959ebe6..796c6bec 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.21
+version: 1.12.22
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index 3d6a4a8f..b68c55c2 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.21
+version: 1.12.22
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.21
+  sherpa_onnx: ^1.12.22
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index 1379d4fc..509b7491 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.21
+version: 1.12.22
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.21
+  sherpa_onnx_android: ^1.12.22
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.21
+  sherpa_onnx_macos: ^1.12.22
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.21
+  sherpa_onnx_linux: ^1.12.22
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.21
+  sherpa_onnx_windows: ^1.12.22
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.21
+  sherpa_onnx_ios: ^1.12.22
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index 368b16c3..b3c4dcb4 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.21'
+  s.version          = '1.12.22'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index 465b9879..a46524b8 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.21'
+  s.version          = '1.12.22'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index 0d1c47fd..9ca8c31a 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.21';
+export const HAR_VERSION = '1.12.22';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index af33641f..0bfafd9d 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.21",
+    "sherpa_onnx": "1.12.22",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index b1539e67..dfb41d5c 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.21",
+  "version": "1.12.22",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index 955e3f9a..08539aa9 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.21"
+    "sherpa_onnx": "1.12.22"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 5259c961..4f1afafd 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.21",
+    "sherpa_onnx": "1.12.22",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 5259c961..4f1afafd 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.21",
+    "sherpa_onnx": "1.12.22",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 5259c961..4f1afafd 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.21",
+    "sherpa_onnx": "1.12.22",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index 7e791052..3ee11a62 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.21.har
+Please download ./sherpa_onnx-v1.12.22.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index 15efe0ed..951d5553 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.21",
+    "sherpa_onnx": "1.12.22",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index e07163da..38560ee7 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-1.12.21.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-1.12.22.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.21.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.21 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.22.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.22 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index 6636a84d..0aa7f360 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-asr-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-asr-x86-v1.12.21.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-streaming-asr-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-streaming-asr-x86-v1.12.21.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-tts-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-tts-x86-v1.12.21.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-non-streaming-asr-x64-v1.12.22.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-non-streaming-asr-x86-v1.12.22.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-streaming-asr-x64-v1.12.22.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-streaming-asr-x86-v1.12.22.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-non-streaming-tts-x64-v1.12.22.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.22/sherpa-onnx-non-streaming-tts-x86-v1.12.22.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 50ffafeb..b13dda0f 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20251217
-new_version_code=20260112
+old_version_code=20260112
+new_version_code=20260114
 
-old_version="1\.12\.20"
-new_version="1\.12\.21"
+old_version="1\.12\.21"
+new_version="1\.12\.22"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index c83144f7..be5a0d76 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.21"
+    "sherpa-onnx-node": "^1.12.22"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index 29c68d16..237a7aa4 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.21",
+    "sherpa-onnx": "^1.12.22",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index 159277c0..bcdd4b65 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.21</version>
+    <version>1.12.22</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index d1965639..2d0a9e69 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.21",
+    version="1.12.22",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.21",
+        "sherpa-onnx-core==1.12.22",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index 21514c55..03c20f4e 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.21",
+    version="1.12.22",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 39a7d6c4..a0819755 100644
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.21"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.22"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index f44e3b9e..195f920f 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Mon Jan 12 19:05:54 2026";
+  static const char *date = "Wed Jan 14 18:28:28 2026";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "82b17d2f";
+  static const char *sha1 = "e9726b44";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.21";
+  static const char *version = "1.12.22";
   return version;
 }
 

commit e333b5cf3e58810f8259ca673b886db32146bc70
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Jan 14 20:23:36 2026 +0800

    Fix building Linux arm wheels (#3047)

diff --git a/.github/workflows/build-wheels-armv7l.yaml b/.github/workflows/build-wheels-armv7l.yaml
index accd3784..a453338b 100644
--- a/.github/workflows/build-wheels-armv7l.yaml
+++ b/.github/workflows/build-wheels-armv7l.yaml
@@ -5,26 +5,33 @@ on:
     branches:
       - wheel
   workflow_dispatch:
+    inputs:
+      publish_sherpa_onnx_bin:
+        description: "Publish sherpa-onnx-bin"
+        required: false
+        default: "true"
+        type: boolean
 
 env:
   SHERPA_ONNX_IS_IN_GITHUB_ACTIONS: 1
 
 concurrency:
-  group: build-wheels-armv7l-${{ github.ref }}
+  group: build-wheels-armv7-${{ github.ref }}
   cancel-in-progress: true
 
 jobs:
-  build_wheels_armv7l:
-    name: ${{ matrix.python-version }}
+  core:
+    name: core
     runs-on: ${{ matrix.os }}
     strategy:
       fail-fast: false
       matrix:
         os: [ubuntu-latest]
-        python-version: ["3.8", "3.9", "3.10", "3.11"]
 
     steps:
       - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
 
       - name: Update version
         shell: bash
@@ -37,56 +44,375 @@ jobs:
         with:
           platforms: arm
 
-      - name: Get version
+      - name: Display PWD
         shell: bash
         run: |
-          SHERPA_ONNX_VERSION=$(cat ./CMakeLists.txt | grep SHERPA_ONNX_VERSION | cut -d " " -f 2 | cut -d '"' -f 2)
-          echo "sherpa-onnx version: $SHERPA_ONNX_VERSION"
-          echo SHERPA_ONNX_VERSION=$SHERPA_ONNX_VERSION >> $GITHUB_ENV
-
-          v=${{ matrix.python-version }}
-          PYTHON_VERSION=${v/./}
-          echo PYTHON_VERSION=$PYTHON_VERSION >> $GITHUB_ENV
+          echo "pwd: $PWD"
+          ls -lh
+          du -h -d1 .
 
-      # https://github.com/mshr-h/onnx-dockerfile-for-raspberry-pi/blob/main/3.10-bullseye-build/Dockerfile.arm32v7
-      # https://hub.docker.com/r/balenalib/raspberrypi3-python
-      - name: Run docker
+      - name: Build sherpa-onnx
         uses: addnab/docker-run-action@v3
         with:
-            image: balenalib/raspberrypi3-python:${{ matrix.python-version }}-bullseye-build
+            image: quay.io/pypa/manylinux_2_35_armv7l
             options: |
               --platform linux/arm/v7
-              --volume ${{ github.workspace }}/:/workspace
+              --volume ${{ github.workspace }}/:/home/runner/work/sherpa-onnx/sherpa-onnx
             shell: bash
             run: |
-              uname -a
-              cd /workspace
-              ls -lh
+              find / -name "*gcc*" 2>/dev/null
 
+              uname -a
+              gcc --version
+              cmake --version
+              cat /etc/*release
               id
-              apt install -qq -y git wget
-              wget -qq https://huggingface.co/csukuangfj/sherpa-onnx-cmake-deps/resolve/main/cmake-3.27-for-armv7.tar.bz2
+              pwd
+
+              cd /home/runner/work/sherpa-onnx/sherpa-onnx
+
+              # find /opt -name "python*"
+
+              echo "--------------------"
+              PY_PATH=$(echo /opt/_internal/cpython-3.10*/bin)
+              export PATH=$PY_PATH:$PATH
+              which python3
+              python3 --version
+
+              python3 -m venv my
+
+              source ./my/bin/activate
+
+              python3 -m pip install setuptools wheel twine
+
+              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+              pushd alsa-lib
+              ./gitcompile
+              popd
+
+              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+
+              mkdir build
+              pushd build
+
+              cmake \
+                -D SHERPA_ONNX_ENABLE_TTS=ON \
+                -D CMAKE_BUILD_TYPE=Release \
+                -D BUILD_SHARED_LIBS=ON \
+                -D SHERPA_ONNX_BUILD_C_API_EXAMPLES=OFF \
+                -D CMAKE_INSTALL_PREFIX=./install \
+                ..
+
+              make -j2
+              make install
+
+              ls -lh lib
+              ls -lh bin
+
+              echo "----"
+              ls -lh install/lib
+
+              file install/lib/*
+
+              rm -fv install/lib/libcargs.so
+
+              echo "----"
+              ls -lh install/bin
+
+              file install/bin/*
+
+              ./install/bin/sherpa-onnx --help
+
+              echo 'sherpa-onnx-core'
+              mkdir -p ../scripts/wheel/sherpa-onnx-core/sherpa_onnx/lib
+              cp -v ./install/lib/lib*.so ../scripts/wheel/sherpa-onnx-core/sherpa_onnx/lib
+
+              mkdir -p ../scripts/wheel/sherpa-onnx-core/sherpa_onnx/include/sherpa-onnx/c-api
+              cp -v ./install/include/sherpa-onnx/c-api/*.h ../scripts/wheel/sherpa-onnx-core/sherpa_onnx/include/sherpa-onnx/c-api
+
+              pushd ../scripts/wheel/sherpa-onnx-core
+              python3 setup.py bdist_wheel --plat-name=manylinux_2_35_armv7l
+
+              ls -lh dist
+
+              popd
+
+              echo 'sherpa-onnx-bin'
+
+              mkdir -p ../scripts/wheel/sherpa-onnx-bin/bin
+              cp -v ./install/bin/sherpa-onnx* ../scripts/wheel/sherpa-onnx-bin/bin
+
+              pushd ../scripts/wheel/sherpa-onnx-bin
+              python3 setup.py bdist_wheel --plat-name=manylinux_2_35_armv7l
+
+              ls -lh dist
+
+              popd
+
+      - name: Collect wheels
+        shell: bash
+        run: |
+          sudo chown -R $USER ./scripts/wheel
+          mkdir wheelhouse
+          cp -v ./scripts/wheel/sherpa-onnx-core/dist/*.whl ./wheelhouse
+          cp -v ./scripts/wheel/sherpa-onnx-bin/dist/*.whl ./wheelhouse
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: wheels-core-linux-armv7l
+          path: ./wheelhouse/*.whl
+
+      - name: Show wheels
+        shell: bash
+        run: |
+          sudo chown -R $USER ./scripts/wheel
+          ls -lh ./scripts/wheel/sherpa-onnx-core/dist
+          ls -lh ./scripts/wheel/sherpa-onnx-bin/dist
+
+          unzip -l ./scripts/wheel/sherpa-onnx-core/dist/*.whl
+          echo "---"
+          unzip -l ./scripts/wheel/sherpa-onnx-bin/dist/*.whl
+
+      - name: Install patchelf
+        shell: bash
+        run: |
+          sudo apt-get update -q
+          sudo apt-get install -q -y patchelf
+          patchelf --help
+
+      - name: Patch wheels
+        shell: bash
+        run: |
+          mkdir ./wheels
+          sudo ./scripts/wheel/patch_wheel.py --in-dir ./wheelhouse --out-dir ./wheels
+
+          ls -lh ./wheels/
+          rm -rf ./wheelhouse
+          mv ./wheels ./wheelhouse
+
+      - uses: actions/upload-artifact@v4
+        with:
+          name: wheels-core-linux-armv7l-patched
+          path: ./wheelhouse/*.whl
+
+  test:
+    name: test
+    needs: [core]
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [ubuntu-latest]
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Retrieve artifact from Linux
+        uses: actions/download-artifact@v4
+        with:
+          name: wheels-core-linux-armv7l-patched
+          path: /tmp/wheels
+
+      - name: Setup Python
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.10"
+
+      - name: Show
+        shell: bash
+        run: |
+          ls -lh /tmp/wheels
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            rm -rf huggingface
+            export GIT_LFS_SKIP_SMUDGE=1
+            export GIT_CLONE_PROTECTION_ACTIVE=false
+
+            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+            echo "SHERPA_ONNX_VERSION $SHERPA_ONNX_VERSION"
+
+            d=cpu/$SHERPA_ONNX_VERSION
+
+            git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-wheels huggingface
+            cd huggingface
+            git fetch
+            git pull
+            git merge -m "merge remote" --ff origin main
+
+            mkdir -p $d
+
+            cp -v /tmp/wheels/*.whl $d/
+
+            git status
+            git add .
+            git commit -m "add more wheels"
+            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-wheels main
+
+      - name: Publish wheels to PyPI ${{ github.event.inputs.publish_sherpa_onnx_bin }}
+        if: ${{ (github.event.inputs.publish_sherpa_onnx_bin || 'true') == 'true' }}
+        env:
+          TWINE_USERNAME: ${{ secrets.PYPI_USERNAME }}
+          TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}
+        shell: bash
+        run: |
+          python3 -m pip install --upgrade pip
+          python3 -m pip install wheel twine==5.0.0 setuptools
+
+          twine upload /tmp/wheels/*.whl
+
+
+  build_wheels_armv7l:
+    name: ${{ matrix.manylinux }} ${{ matrix.python-version }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        # see https://github.com/pypa/cibuildwheel/issues/2257
+        # we don't use qemu from now on
+        os: [ubuntu-latest]
+        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12", "3.13", "3.14"]
+        manylinux: [manylinux_2_35] #, manylinux_2_28]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
 
-              ls -lh
-              tar xf cmake-3.27-for-armv7.tar.bz2
-              ls -lh cmake/data/bin
-              chmod +x cmake/data/bin/cmake
-              export PATH=$PWD/cmake/data/bin:$PATH
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v2
+        with:
+          platforms: arm
+
+      - name: Build sherpa-onnx
+        uses: addnab/docker-run-action@v3
+        with:
+            image: quay.io/pypa/manylinux_2_35_armv7l
+            options: |
+              --platform linux/arm/v7
+              --volume ${{ github.workspace }}/:/home/runner/work/sherpa-onnx/sherpa-onnx
+            shell: bash
+            run: |
+              find / -name "*gcc*" 2>/dev/null
 
+              uname -a
+              gcc --version
               cmake --version
+              cat /etc/*release
+              id
+              pwd
+
+              python_version=${{ matrix.python-version }}
+
+              cd /home/runner/work/sherpa-onnx/sherpa-onnx
+
+              # find /opt -name "python*"
+
+              echo "--------------------"
+              # Construct glob pattern
+              PY_GLOB="/opt/_internal/cpython-${python_version}*/bin"
+
+              # Expand the glob safely
+              shopt -s nullglob  # Avoid literal string if no match
+              matches=($PY_GLOB)
+              shopt -u nullglob
+
+              if [[ ${#matches[@]} -eq 0 ]]; then
+                echo "No Python installation found for version $python_version"
+                exit 1
+              elif [[ ${#matches[@]} -gt 1 ]]; then
+                echo "Multiple Python installations found for version $python_version:"
+                printf '  %s\n' "${matches[@]}"
+                echo "Using the first one: ${matches[0]}"
+              fi
+
+              PY_PATH="${matches[0]}"
 
-              export SHERPA_ONNX_CMAKE_ARGS='-DCMAKE_C_FLAGS="-march=armv7-a -mfloat-abi=hard -mfpu=neon" -DCMAKE_CXX_FLAGS="-march=armv7-a -mfloat-abi=hard -mfpu=neon"'
+              echo "$PY_PATH"
+              export PATH="$PY_PATH:$PATH"
+              echo $PY_PATH
+              export PATH=$PY_PATH:$PATH
+              which python3
+              python3 --version
+
+              python3 -m venv my
+
+              source ./my/bin/activate
+
+              python3 -m pip install setuptools wheel twine
+
+              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
+              pushd alsa-lib
+              ./gitcompile
+              popd
+
+              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+              echo "SHERPA_ONNX_ALSA_LIB_DIR: $SHERPA_ONNX_ALSA_LIB_DIR"
+
+              export LD_LIBRARY_PATH=$PWD/build/bdist.linux-aarch64/wheel/sherpa_onnx/lib:$SHERPA_ONNX_ALSA_LIB_DIR:$LD_LIBRARY_PATH
+              export LIBRARY_PATH=$PWD/build/bdist.linux-aarch64/wheel/sherpa_onnx/lib:$SHERPA_ONNX_ALSA_LIB_DIR:$LIBRARY_PATH
+
+              echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
+              echo "LIBRARY_PATH: $LIBRARY_PATH"
+
+              export C_INCLUDE_PATH=$PWD/alsa-lib/include:$C_INCLUDE_PATH
+              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
+              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
+
+              export SHERPA_ONNX_MAKE_ARGS="VERBOSE=1"
+              export SHERPA_ONNX_ENABLE_ALSA=1
+              export SHERPA_ONNX_CMAKE_ARGS="-DCMAKE_C_FLAGS=\"-march=armv7-a -mfloat-abi=hard -mfpu=neon\" -DCMAKE_CXX_FLAGS=\"-march=armv7-a -mfloat-abi=hard -mfpu=neon\" -DSHERPA_ONNX_ENABLE_BINARY=OFF -DSHERPA_ONNX_BUILD_C_API_EXAMPLES=OFF -DSHERPA_ONNX_ENABLE_C_API=ON -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF -DALSA_INCLUDE_DIR=$p/alsa-lib/include -DALSA_LIBRARY=$p/alsa-lib/src/.libs/libasound.so"
               python3 setup.py bdist_wheel
               ls -lh dist
 
               mkdir wheelhouse
               cp -v dist/* wheelhouse/
 
+      - uses: actions/upload-artifact@v4
+        with:
+          name: wheel-${{ matrix.python-version }}-${{ matrix.manylinux }}-linux-armv7l
+          path: ./wheelhouse/*.whl
+
       - name: Display wheels
         shell: bash
         run: |
           ls -lh ./wheelhouse/
 
+      - name: Show wheels
+        shell: bash
+        run: |
+          ls -lh wheelhouse/*.whl
+
+          unzip -l wheelhouse/*.whl
+
+          echo "---"
+
+          mkdir t
+          cp wheelhouse/*.whl ./t
+          cd ./t
+          unzip ./*.whl
+          ls -lh
+          echo "---"
+
+          readelf -d sherpa_onnx/lib/*.so
+
       - name: Publish to huggingface
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
@@ -123,12 +449,6 @@ jobs:
             git commit -m "add more wheels"
             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-wheels main
 
-      - name: Upload wheel
-        uses: actions/upload-artifact@v4
-        with:
-          name: sherpa_onnx-${{ env.SHERPA_ONNX_VERSION }}-cp${{ env.PYTHON_VERSION }}-cp${{ env.PYTHON_VERSION }}-linux_armv7l.whl.zip
-          path: ./wheelhouse/*.whl
-
       - name: Publish wheels to PyPI
         env:
           TWINE_USERNAME: ${{ secrets.PYPI_USERNAME }}
diff --git a/scripts/go/release.sh b/scripts/go/release.sh
index 40872f00..5463a4c4 100755
--- a/scripts/go/release.sh
+++ b/scripts/go/release.sh
@@ -54,8 +54,8 @@ function linux() {
   dst=$(realpath sherpa-onnx-go-linux/lib/arm-unknown-linux-gnueabihf)
   mkdir t
   cd t
-  wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
-  unzip ./sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
+  wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx_core-$SHERPA_ONNX_VERSION-py3-none-manylinux_2_35_armv7l.whl
+  unzip ./sherpa_onnx_core-$SHERPA_ONNX_VERSION-py3-none-manylinux_2_35_armv7l.whl
 
   rm -fv $dst/_sherpa*.so
   cp -v sherpa_onnx/lib/lib*.so* $dst

commit e9726b44720ee3cee0a94b321373998f8f98791f
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Jan 14 18:28:28 2026 +0800

    Build APK for nemotron-speech-streaming-en-0.6b (#3045)

diff --git a/scripts/apk/generate-asr-apk-script.py b/scripts/apk/generate-asr-apk-script.py
index e3b1128a..590892b8 100755
--- a/scripts/apk/generate-asr-apk-script.py
+++ b/scripts/apk/generate-asr-apk-script.py
@@ -481,6 +481,21 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14",
+            idx=28,
+            lang="en",
+            short_name="nemotron-speech-streaming-en-0.6b-int8-2026-01-14",
+            cmd="""
+            pushd $model_name
+
+            rm -rf test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/sherpa-onnx/kotlin-api/OnlineRecognizer.kt b/sherpa-onnx/kotlin-api/OnlineRecognizer.kt
index b31f5513..8462c310 100644
--- a/sherpa-onnx/kotlin-api/OnlineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OnlineRecognizer.kt
@@ -535,6 +535,18 @@ fun getModelConfig(type: Int): OnlineModelConfig? {
             )
         }
 
+        28 -> {
+            val modelDir = "sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14"
+            return OnlineModelConfig(
+                transducer = OnlineTransducerModelConfig(
+                    encoder = "$modelDir/encoder.int8.onnx",
+                    decoder = "$modelDir/decoder.int8.onnx",
+                    joiner = "$modelDir/joiner.int8.onnx",
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
         1000 -> {
             val modelDir = "sherpa-onnx-rk3588-streaming-zipformer-bilingual-zh-en-2023-02-20"
             return OnlineModelConfig(

commit 4fa889824497c22a874fb3cbf5fbd58425da89bb
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Jan 14 17:05:33 2026 +0800

    Support nemotron-speech-streaming-en-0.6b (#3044)
    
    This pull request introduces comprehensive support for integrating the nemotron-speech-streaming-en-0.6b model into the sherpa-onnx framework. It provides the necessary tools to export the NeMo model to an optimized ONNX format, complete with quantization and embedded metadata. Concurrently, the sherpa-onnx C++ core has been enhanced to seamlessly load these exported ONNX models from disk and correctly configure their audio feature extraction parameters, ensuring accurate and efficient speech recognition.

diff --git a/.github/workflows/export-nemotron-speech-streaming-en-0.6b.yaml b/.github/workflows/export-nemotron-speech-streaming-en-0.6b.yaml
new file mode 100644
index 00000000..68f21a30
--- /dev/null
+++ b/.github/workflows/export-nemotron-speech-streaming-en-0.6b.yaml
@@ -0,0 +1,235 @@
+name: export-nemotron-speech-streaming-en-06b
+
+on:
+  push:
+    branches:
+      - export-nemotron-streaming-2
+  workflow_dispatch:
+
+concurrency:
+  group: export-nemotron-streaming-to-onnx-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  export-nemotron-speech-streaming-en-0-6b-to-onnx:
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: nemotron-speech-streaming-en-0-6b-to-onnx
+    runs-on: ${{ matrix.os }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [macos-latest]
+        python-version: ["3.10"]
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python ${{ matrix.python-version }}
+        uses: actions/setup-python@v5
+        with:
+          python-version: ${{ matrix.python-version }}
+
+      - name: Install NeMo
+        shell: bash
+        run: |
+          BRANCH='main'
+          pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]
+          pip install onnxruntime ipython
+          pip install kaldi-native-fbank
+          pip install soundfile librosa
+
+      - name: Run
+        shell: bash
+        run: |
+          cd scripts/nemo/nemotron-speech-streaming-en-0.6b
+
+          python3 ./export_onnx.py
+
+          ls -lh *.onnx
+          echo "---"
+          ls -lh encoder.*
+
+      - name: Collect results
+        shell: bash
+        run: |
+          src=scripts/nemo/nemotron-speech-streaming-en-0.6b
+          d=sherpa-onnx-nemotron-speech-streaming-en-0.6b-2026-01-14
+          mkdir -p $d
+
+          cp -av $src/encoder.onnx $d/
+          cp -av $src/encoder.data $d/
+          cp -av $src/decoder.onnx $d/
+          cp -av $src/joiner.onnx $d/
+          cp -av $src/tokens.txt $d/
+          cat >$d/README.md <<EOF
+          # Introduction
+          This model is from https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b
+          EOF
+
+          ls -lh $d
+
+          d=sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14
+          mkdir -p $d
+
+          cp -av $src/encoder.int8.onnx $d/
+          cp -av $src/decoder.int8.onnx $d/
+          cp -av $src/joiner.int8.onnx $d/
+          cp -av $src/tokens.txt $d/
+          cat >$d/README.md <<EOF
+          # Introduction
+          This model is from https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b
+          EOF
+
+          ls -lh $d
+
+
+      - name: Download test waves
+        if: true
+        shell: bash
+        run: |
+          mkdir test_wavs
+          pushd test_wavs
+          curl -SL -O https://hf-mirror.com/csukuangfj/sherpa-onnx-nemo-ctc-en-conformer-small/resolve/main/test_wavs/0.wav
+          curl -SL -O https://hf-mirror.com/csukuangfj/sherpa-onnx-nemo-ctc-en-conformer-small/resolve/main/test_wavs/1.wav
+          curl -SL -O https://hf-mirror.com/csukuangfj/sherpa-onnx-nemo-ctc-en-conformer-small/resolve/main/test_wavs/8k.wav
+          curl -SL -O https://hf-mirror.com/csukuangfj/sherpa-onnx-nemo-ctc-en-conformer-small/resolve/main/test_wavs/trans.txt
+          popd
+
+          models=(
+            sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14
+            sherpa-onnx-nemotron-speech-streaming-en-0.6b-2026-01-14
+          )
+          for m in ${models[@]}; do
+            cp -av test_wavs $m
+            tar cjvf $m.tar.bz2 $m
+          done
+
+          ls -lh *.tar.bz2
+
+      - name: Release
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          # fp32 models is 2.2GB > 2GB
+          file: sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+
+            models=(
+              sherpa-onnx-nemotron-speech-streaming-en-0.6b-int8-2026-01-14
+              sherpa-onnx-nemotron-speech-streaming-en-0.6b-2026-01-14
+            )
+
+            for m in ${models[@]}; do
+              if [ ! -d $m ]; then
+                echo "skip $m"
+                continue
+              fi
+
+              rm -rf huggingface
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$m huggingface
+              cp -av $m/* huggingface
+              cd huggingface
+              git lfs track "*.onnx"
+              git lfs track "*.data"
+              git lfs track "*.wav"
+              git status
+              git add .
+              git status
+              git commit -m "first commit"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$m main
+              cd ..
+            done
+
+            rm -rf huggingface
+
+      - name: Publish to modelscope
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              mkdir ms/nemo
+              cp -av $m ms/nemo
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh
+              git add .
+
+              git commit -m "add models"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+            rm -rf ms
+
+      - name: Publish to huggingface
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models huggingface
+
+              d=asr-models/nemo
+              mkdir -p huggingface/$d
+
+              cp -v $m huggingface/$d/
+
+              pushd huggingface
+              git lfs track "*.tar.bz2"
+              ls -lh $d/$m
+
+              ls -lh $d
+
+              pushd $d
+              git lfs track "*.tar.bz2"
+              popd
+
+              git status
+              git add .
+
+              git commit -m "add $m"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models main
+              popd
+            done
+            rm -rf huggingface
diff --git a/scripts/nemo/nemotron-speech-streaming-en-0.6b/export_onnx.py b/scripts/nemo/nemotron-speech-streaming-en-0.6b/export_onnx.py
new file mode 100755
index 00000000..cbb1bab4
--- /dev/null
+++ b/scripts/nemo/nemotron-speech-streaming-en-0.6b/export_onnx.py
@@ -0,0 +1,134 @@
+#!/usr/bin/env python3
+# Copyright      2026  Xiaomi Corp.        (authors: Fangjun Kuang)
+from typing import Dict
+
+import nemo.collections.asr as nemo_asr
+import onnx
+import torch
+from onnxruntime.quantization import QuantType, quantize_dynamic
+
+"""
+'_target_': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor',
+'sample_rate': 16000, 'normalize': 'NA', 'window_size': 0.025, 'window_stride': 0.01,
+'window': 'hann', 'features': 128, 'n_fft': 512, 'log': True,
+'frame_splicing': 1, 'dither': 1e-05, 'pad_to': 0, 'pad_value': 0.0}
+
+"""
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, str]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    external_filename = filename.split(".onnx")[0]
+    onnx.save(
+        model,
+        filename,
+        save_as_external_data=True,
+        all_tensors_to_one_file=True,
+        location=external_filename + ".data",
+    )
+
+
+@torch.no_grad()
+def main():
+    model_name = "nvidia/nemotron-speech-streaming-en-0.6b"
+
+    asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)
+
+    with open("./tokens.txt", "w", encoding="utf-8") as f:
+        for i, s in enumerate(asr_model.joint.vocabulary):
+            f.write(f"{s} {i}\n")
+        f.write(f"<blk> {i+1}\n")
+        print("Saved to tokens.txt")
+
+    asr_model.eval()
+
+    assert asr_model.encoder.streaming_cfg is not None
+    if isinstance(asr_model.encoder.streaming_cfg.chunk_size, list):
+        chunk_size = asr_model.encoder.streaming_cfg.chunk_size[1]
+    else:
+        chunk_size = asr_model.encoder.streaming_cfg.chunk_size
+
+    if isinstance(asr_model.encoder.streaming_cfg.pre_encode_cache_size, list):
+        pre_encode_cache_size = asr_model.encoder.streaming_cfg.pre_encode_cache_size[1]
+    else:
+        pre_encode_cache_size = asr_model.encoder.streaming_cfg.pre_encode_cache_size
+    window_size = chunk_size + pre_encode_cache_size
+
+    print("chunk_size", chunk_size)
+    print("pre_encode_cache_size", pre_encode_cache_size)
+    print("window_size", window_size)
+
+    chunk_shift = chunk_size
+
+    # cache_last_channel: (batch_size, dim1, dim2, dim3)
+    cache_last_channel_dim1 = len(asr_model.encoder.layers)
+    cache_last_channel_dim2 = asr_model.encoder.streaming_cfg.last_channel_cache_size
+    cache_last_channel_dim3 = asr_model.encoder.d_model
+
+    # cache_last_time: (batch_size, dim1, dim2, dim3)
+    cache_last_time_dim1 = len(asr_model.encoder.layers)
+    cache_last_time_dim2 = asr_model.encoder.d_model
+    cache_last_time_dim3 = asr_model.encoder.conv_context_size[0]
+
+    asr_model.set_export_config({"cache_support": True})
+
+    asr_model.encoder.export("encoder.onnx")
+    asr_model.decoder.export("decoder.onnx")
+    asr_model.joint.export("joiner.onnx")
+
+    normalize_type = asr_model.cfg.preprocessor.normalize
+    if normalize_type == "NA":
+        normalize_type = ""
+
+    meta_data = {
+        "vocab_size": asr_model.decoder.vocab_size,
+        "window_size": window_size,
+        "chunk_shift": chunk_shift,
+        "normalize_type": normalize_type,
+        "cache_last_channel_dim1": cache_last_channel_dim1,
+        "cache_last_channel_dim2": cache_last_channel_dim2,
+        "cache_last_channel_dim3": cache_last_channel_dim3,
+        "cache_last_time_dim1": cache_last_time_dim1,
+        "cache_last_time_dim2": cache_last_time_dim2,
+        "cache_last_time_dim3": cache_last_time_dim3,
+        "pred_rnn_layers": asr_model.decoder.pred_rnn_layers,
+        "pred_hidden": asr_model.decoder.pred_hidden,
+        "subsampling_factor": 8,
+        "feat_dim": 128,
+        "model_type": "EncDecHybridRNNTCTCBPEModel",
+        "version": "1",
+        "model_author": "NeMo",
+        "url": "https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b",
+        "comment": "Only the transducer branch is exported",
+    }
+    add_meta_data("encoder.onnx", meta_data)
+
+    for m in ["encoder", "decoder", "joiner"]:
+        quantize_dynamic(
+            model_input=f"{m}.onnx",
+            model_output=f"{m}.int8.onnx",
+            weight_type=QuantType.QUInt8,
+        )
+
+    print(meta_data)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
index 60124c2f..3d0c08e4 100644
--- a/sherpa-onnx/csrc/offline-whisper-model.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model.cc
@@ -46,8 +46,8 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{},
-        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
-                                                 OrtMemTypeDefault)),
+        cpu_mem_info_(
+            Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault)),
         is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     encoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
@@ -65,8 +65,8 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{},
-        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
-                                                 OrtMemTypeDefault)),
+        cpu_mem_info_(
+            Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault)),
         is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     encoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
@@ -85,8 +85,8 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{},
-        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
-                                                 OrtMemTypeDefault)),
+        cpu_mem_info_(
+            Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault)),
         is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(mgr, config.whisper.encoder);
@@ -107,8 +107,8 @@ class OfflineWhisperModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{},
-        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
-                                                 OrtMemTypeDefault)),
+        cpu_mem_info_(
+            Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault)),
         is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(mgr, config.whisper.encoder);
@@ -128,7 +128,8 @@ class OfflineWhisperModel::Impl {
 
     if (use_cuda_iobinding_) {
       // Encoder outputs are n_layer_cross_k and n_layer_cross_v, which are used
-      // multiple times in decoder steps. Keep them on GPU to avoid device<->host copies.
+      // multiple times in decoder steps. Keep them on GPU to avoid
+      // device<->host copies.
       Ort::IoBinding binding(*encoder_sess_);
       binding.BindInput(encoder_input_names_ptr_[0], features);
 
@@ -302,8 +303,7 @@ class OfflineWhisperModel::Impl {
     } else if (!encoder_sess_) {
       SHERPA_ONNX_LOGE(
           "Please pass buffer data or initialize encoder session outside of "
-          "this "
-          "function");
+          "this function");
       SHERPA_ONNX_EXIT(-1);
     }
 
diff --git a/sherpa-onnx/csrc/online-recognizer-transducer-nemo-impl.h b/sherpa-onnx/csrc/online-recognizer-transducer-nemo-impl.h
index 6087c2b5..ba1f43d8 100644
--- a/sherpa-onnx/csrc/online-recognizer-transducer-nemo-impl.h
+++ b/sherpa-onnx/csrc/online-recognizer-transducer-nemo-impl.h
@@ -207,14 +207,13 @@ class OnlineRecognizerTransducerNeMoImpl : public OnlineRecognizerImpl {
 
  private:
   void PostInit() {
-    config_.feat_config.nemo_normalize_type =
-        model_->FeatureNormalizationMethod();
+    config_.feat_config.feature_dim = model_->FeatureDim();
 
     config_.feat_config.low_freq = 0;
-    // config_.feat_config.high_freq = 8000;
+    config_.feat_config.high_freq = 8000;
     config_.feat_config.is_librosa = true;
     config_.feat_config.remove_dc_offset = false;
-    // config_.feat_config.window_type = "hann";
+    config_.feat_config.window_type = "hann";
     config_.feat_config.dither = 0;
     config_.feat_config.nemo_normalize_type =
         model_->FeatureNormalizationMethod();
diff --git a/sherpa-onnx/csrc/online-transducer-nemo-model.cc b/sherpa-onnx/csrc/online-transducer-nemo-model.cc
index 53411f8e..2d487a0f 100644
--- a/sherpa-onnx/csrc/online-transducer-nemo-model.cc
+++ b/sherpa-onnx/csrc/online-transducer-nemo-model.cc
@@ -43,20 +43,17 @@ class OnlineTransducerNeMoModel::Impl {
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
         allocator_{} {
-    {
-      auto buf = ReadFile(config.transducer.encoder);
-      InitEncoder(buf.data(), buf.size());
-    }
+    encoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.transducer.encoder), sess_opts_);
+    InitEncoder(nullptr, 0);
 
-    {
-      auto buf = ReadFile(config.transducer.decoder);
-      InitDecoder(buf.data(), buf.size());
-    }
+    decoder_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.transducer.decoder), sess_opts_);
+    InitDecoder(nullptr, 0);
 
-    {
-      auto buf = ReadFile(config.transducer.joiner);
-      InitJoiner(buf.data(), buf.size());
-    }
+    joiner_sess_ = std::make_unique<Ort::Session>(
+        env_, SHERPA_ONNX_TO_ORT_PATH(config.transducer.joiner), sess_opts_);
+    InitJoiner(nullptr, 0);
   }
 
   template <typename Manager>
@@ -199,6 +196,8 @@ class OnlineTransducerNeMoModel::Impl {
 
   int32_t SubsamplingFactor() const { return subsampling_factor_; }
 
+  int32_t FeatureDim() const { return feat_dim_; }
+
   int32_t VocabSize() const { return vocab_size_; }
 
   OrtAllocator *Allocator() { return allocator_; }
@@ -291,8 +290,15 @@ class OnlineTransducerNeMoModel::Impl {
 
  private:
   void InitEncoder(void *model_data, size_t model_data_length) {
-    encoder_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_);
+    if (model_data) {
+      encoder_sess_ = std::make_unique<Ort::Session>(
+          env_, model_data, model_data_length, sess_opts_);
+    } else if (!encoder_sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize encoder session outside of "
+          "this function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(encoder_sess_.get(), &encoder_input_names_,
                   &encoder_input_names_ptr_);
@@ -300,12 +306,17 @@ class OnlineTransducerNeMoModel::Impl {
     GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
                    &encoder_output_names_ptr_);
 
+    feat_dim_ = encoder_sess_->GetInputTypeInfo(0)
+                    .GetTensorTypeAndShapeInfo()
+                    .GetShape()[1];
+
     // get meta data
     Ort::ModelMetadata meta_data = encoder_sess_->GetModelMetadata();
     if (config_.debug) {
       std::ostringstream os;
       os << "---encoder---\n";
       PrintModelMetadata(os, meta_data);
+      os << "feat_dim: " << feat_dim_ << "\n";
 #if __OHOS__
       SHERPA_ONNX_LOGE("%{public}s", os.str().c_str());
 #else
@@ -323,6 +334,7 @@ class OnlineTransducerNeMoModel::Impl {
     SHERPA_ONNX_READ_META_DATA(window_size_, "window_size");
     SHERPA_ONNX_READ_META_DATA(chunk_shift_, "chunk_shift");
     SHERPA_ONNX_READ_META_DATA(subsampling_factor_, "subsampling_factor");
+
     SHERPA_ONNX_READ_META_DATA_STR_ALLOW_EMPTY(normalize_type_,
                                                "normalize_type");
     SHERPA_ONNX_READ_META_DATA(pred_rnn_layers_, "pred_rnn_layers");
@@ -372,8 +384,15 @@ class OnlineTransducerNeMoModel::Impl {
   }
 
   void InitDecoder(void *model_data, size_t model_data_length) {
-    decoder_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_);
+    if (model_data) {
+      decoder_sess_ = std::make_unique<Ort::Session>(
+          env_, model_data, model_data_length, sess_opts_);
+    } else if (!decoder_sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize decoder session outside of "
+          "this function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(decoder_sess_.get(), &decoder_input_names_,
                   &decoder_input_names_ptr_);
@@ -401,8 +420,15 @@ class OnlineTransducerNeMoModel::Impl {
   }
 
   void InitJoiner(void *model_data, size_t model_data_length) {
-    joiner_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_);
+    if (model_data) {
+      joiner_sess_ = std::make_unique<Ort::Session>(
+          env_, model_data, model_data_length, sess_opts_);
+    } else if (!joiner_sess_) {
+      SHERPA_ONNX_LOGE(
+          "Please pass buffer data or initialize joiner session outside of "
+          "this function");
+      SHERPA_ONNX_EXIT(-1);
+    }
 
     GetInputNames(joiner_sess_.get(), &joiner_input_names_,
                   &joiner_input_names_ptr_);
@@ -443,6 +469,7 @@ class OnlineTransducerNeMoModel::Impl {
   int32_t chunk_shift_ = 0;
   int32_t vocab_size_ = 0;
   int32_t subsampling_factor_ = 8;
+  int32_t feat_dim_ = 80;
   std::string normalize_type_;
   int32_t pred_rnn_layers_ = -1;
   int32_t pred_hidden_ = -1;
@@ -513,6 +540,10 @@ int32_t OnlineTransducerNeMoModel::VocabSize() const {
   return impl_->VocabSize();
 }
 
+int32_t OnlineTransducerNeMoModel::FeatureDim() const {
+  return impl_->FeatureDim();
+}
+
 OrtAllocator *OnlineTransducerNeMoModel::Allocator() const {
   return impl_->Allocator();
 }
diff --git a/sherpa-onnx/csrc/online-transducer-nemo-model.h b/sherpa-onnx/csrc/online-transducer-nemo-model.h
index 98390bb1..ff384133 100644
--- a/sherpa-onnx/csrc/online-transducer-nemo-model.h
+++ b/sherpa-onnx/csrc/online-transducer-nemo-model.h
@@ -99,6 +99,8 @@ class OnlineTransducerNeMoModel {
 
   int32_t VocabSize() const;
 
+  int32_t FeatureDim() const;
+
   /** Return an allocator for allocating memory
    */
   OrtAllocator *Allocator() const;
diff --git a/sherpa-onnx/csrc/sherpa-onnx.cc b/sherpa-onnx/csrc/sherpa-onnx.cc
index 1712e833..585376dc 100644
--- a/sherpa-onnx/csrc/sherpa-onnx.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx.cc
@@ -124,9 +124,9 @@ for a list of pre-trained models to download.
 
     auto s = recognizer.CreateStream();
 
-    std::vector<float> left_paddings(static_cast<int>(0.3 * sampling_rate));
-    s->AcceptWaveform(sampling_rate, left_paddings.data(),
-                      left_paddings.size());
+    // std::vector<float> left_paddings(static_cast<int>(0.3 * sampling_rate));
+    // s->AcceptWaveform(sampling_rate, left_paddings.data(),
+    //                   left_paddings.size());
 
     s->AcceptWaveform(sampling_rate, samples.data(), samples.size());
 

commit 2413bcf1816ffea578c01ccd150a3480ec3eaa82
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Wed Jan 14 10:43:52 2026 +0800

    Fix checking funasr nano tokenizer on Windows (#3043)

diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
index 84f1fbaa..c3d30b47 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
@@ -4,8 +4,9 @@
 
 #include "sherpa-onnx/csrc/offline-funasr-nano-model-config.h"
 
-#include <string>
 #include <sstream>
+#include <string>
+
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
 
@@ -21,8 +22,9 @@ void OfflineFunASRNanoModelConfig::Register(ParseOptions *po) {
   po->Register("funasr-nano-embedding", &embedding,
                "Path to embedding.onnx for FunASR-nano");
 
-  po->Register("funasr-nano-tokenizer", &tokenizer,
-               "Path to tokenizer directory (e.g., Qwen3-0.6B) for FunASR-nano");
+  po->Register(
+      "funasr-nano-tokenizer", &tokenizer,
+      "Path to tokenizer directory (e.g., Qwen3-0.6B) for FunASR-nano");
 
   po->Register("funasr-nano-system-prompt", &system_prompt,
                "System prompt for FunASR-nano");
@@ -39,8 +41,7 @@ void OfflineFunASRNanoModelConfig::Register(ParseOptions *po) {
   po->Register("funasr-nano-top-p", &top_p,
                "Top-p (nucleus) sampling threshold for FunASR-nano");
 
-  po->Register("funasr-nano-seed", &seed,
-               "Random seed for FunASR-nano");
+  po->Register("funasr-nano-seed", &seed, "Random seed for FunASR-nano");
 }
 
 bool OfflineFunASRNanoModelConfig::Validate() const {
@@ -70,9 +71,25 @@ bool OfflineFunASRNanoModelConfig::Validate() const {
     return false;
   }
 
-  if (!FileExists(tokenizer)) {
-    SHERPA_ONNX_LOGE("--funasr-nano-tokenizer: '%s' does not exist",
-                     tokenizer.c_str());
+  if (!FileExists(tokenizer + "/vocab.json")) {
+    SHERPA_ONNX_LOGE(
+        "'%s/vocab.json' does not exist. Please check --funasr-nano-tokenizer",
+        tokenizer.c_str());
+    return false;
+  }
+
+  if (!FileExists(tokenizer + "/merges.txt")) {
+    SHERPA_ONNX_LOGE(
+        "'%s/merges.txt' does not exist. Please check --funasr-nano-tokenizer",
+        tokenizer.c_str());
+    return false;
+  }
+
+  if (!FileExists(tokenizer + "/tokenizer.json")) {
+    SHERPA_ONNX_LOGE(
+        "'%s/tokenizer.json' does not exist. Please check "
+        "--funasr-nano-tokenizer",
+        tokenizer.c_str());
     return false;
   }
 
@@ -127,4 +144,3 @@ std::string OfflineFunASRNanoModelConfig::ToString() const {
 }
 
 }  // namespace sherpa_onnx
-

commit a3e11d110274557b098fa2c3aec9f2c62eff0d58
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Wed Jan 14 10:21:17 2026 +0800

    cmake: fix sha256 for onnxruntime linux x86_64 gpu package (#3042)

diff --git a/cmake/onnxruntime-linux-x86_64-gpu.cmake b/cmake/onnxruntime-linux-x86_64-gpu.cmake
index 11e747ca..3f0f151a 100644
--- a/cmake/onnxruntime-linux-x86_64-gpu.cmake
+++ b/cmake/onnxruntime-linux-x86_64-gpu.cmake
@@ -22,7 +22,7 @@ endif()
 # Requres CUDA 12, cudnn 9
 set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-x64-gpu-1.23.2-patched.zip")
 set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-x64-gpu-1.23.2-patched.zip")
-set(onnxruntime_HASH "SHA256=1261de176e8d9d4d2019f8fa8c732c6d11494f3c6e73168ab6d2cc0903f22551")
+set(onnxruntime_HASH "SHA256=e2f622513212304447e34512b99ae4eabb4fd8870dd1baac895f222179dede19")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.

commit 07aa9cf7bdfaf7a4aa7c5124ba32ede476d7a8e4
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Tue Jan 13 16:26:18 2026 +0800

    Update wav files for FunASR Nano (#3038)

diff --git a/.github/workflows/upload-models.yaml b/.github/workflows/upload-models.yaml
index bafbaa80..ea2ed8ae 100644
--- a/.github/workflows/upload-models.yaml
+++ b/.github/workflows/upload-models.yaml
@@ -3,7 +3,7 @@ name: upload-models
 on:
   push:
     branches:
-      - upload-models
+      - upload-models-2
   workflow_dispatch:
 
 concurrency:
@@ -100,9 +100,7 @@ jobs:
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
@@ -121,6 +119,9 @@ jobs:
             && mv "${f}.tmp.wav" "$f"
           done
 
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_1.wav
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_3.wav
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -134,9 +135,9 @@ jobs:
           |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
           |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
           |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
-          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|When I was young I'd listen to the radio. Waiting for my favorite songs. When they played I'd sing along. It made me smile.|
           |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
-          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|An empty street, an empty house, a hole inside my heart. I'm all alone and the rooms are getting smaller. I wonder how, I wonder why, I wonder where they are. The days we had, the songs we sang together.|
           |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
           |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
           |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_3.wav)||
@@ -171,7 +172,7 @@ jobs:
           tar cjvf $d.tar.bz2 $d
 
       - name: Collect funasr-nano with LLM float32
-        if: false
+        if: true
         shell: bash
         run: |
           d=sherpa-onnx-funasr-nano-2025-12-30
@@ -207,9 +208,7 @@ jobs:
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
@@ -228,6 +227,9 @@ jobs:
             && mv "${f}.tmp.wav" "$f"
           done
 
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_1.wav
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_3.wav
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -241,9 +243,9 @@ jobs:
           |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
           |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
           |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
-          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|When I was young I'd listen to the radio. Waiting for my favorite songs. When they played I'd sing along. It made me smile.|
           |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
-          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|An empty street, an empty house, a hole inside my heart. I'm all alone and the rooms are getting smaller. I wonder how, I wonder why, I wonder where they are. The days we had, the songs we sang together.|
           |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
           |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
           |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_3.wav)||
@@ -314,9 +316,7 @@ jobs:
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
-          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
           curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
@@ -335,6 +335,9 @@ jobs:
             && mv "${f}.tmp.wav" "$f"
           done
 
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_1.wav
+          curl -SL -O https://modelscope.cn/models/csukuangfj/sherpa-doc-files/resolve/master/source/_static/fun-asr-nano-2025-12-30/lyrics_en_3.wav
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -348,9 +351,9 @@ jobs:
           |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
           |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
           |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
-          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|When I was young I'd listen to the radio. Waiting for my favorite songs. When they played I'd sing along. It made me smile.|
           |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
-          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|An empty street, an empty house, a hole inside my heart. I'm all alone and the rooms are getting smaller. I wonder how, I wonder why, I wonder where they are. The days we had, the songs we sang together.|
           |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
           |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
           |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_3.wav)||
@@ -801,7 +804,7 @@ jobs:
           mv models/* .
 
       - name: Publish to huggingface
-        if: false
+        if: true
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -898,7 +901,7 @@ jobs:
           path: ./*.tar.bz2
 
       - name: Release
-        if: false
+        if: true
         uses: svenstaro/upload-release-action@v2
         with:
           file_glob: true
diff --git a/cmake/onnxruntime-osx-universal-static.cmake b/cmake/onnxruntime-osx-universal-static.cmake
index 434c6215..509bd9e8 100644
--- a/cmake/onnxruntime-osx-universal-static.cmake
+++ b/cmake/onnxruntime-osx-universal-static.cmake
@@ -15,7 +15,7 @@ endif()
 
 set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-osx-universal2-static_lib-1.23.2.zip")
 set(onnxruntime_URL2  "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-universal2-static_lib-1.23.2.zip")
-set(onnxruntime_HASH "SHA256=00816fda16166859fed41dacb786d3dfc3323bbc1a8fa57a235922f597953986")
+set(onnxruntime_HASH "SHA256=9ea206a621d6e5550ddb9de0b96c4f666b074620f5c685b0479b5fa02c0bba76")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.

commit a9ff1700dc1604fce987386b1b5b5523b04d454e
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 19:13:22 2026 +0800

    Release v1.12.21 (#3034)

diff --git a/CHANGELOG.md b/CHANGELOG.md
index 21d55bf8..8709678d 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,63 @@
+## 1.12.21
+
+* Fix publishing NPM packages (#2909)
+* Refactor ZipVoice C++ code (#2911)
+* Export more zipformer ctc models to qnn (#2921)
+* [KWS] Add phone+ppinyin tokenization with lexicon support (for zh-en model) (#2922)
+* Export Paraformer ASR models to QNN (#2925)
+* Add Transpose for a 2-D matrix. (#2926)
+* Optimize computation with Eigen. (#2928)
+* Add C++ runtime for Paraformer ASR models with Qualcomm NPU using QNN (#2931)
+* Add Android demo for Paraformer ASR with Qualcomm NPU. (#2932)
+* Export Google MedASR to sherpa-onnx (#2934)
+* Add C++ runtime and Python API for Google MedASR models (#2935)
+* Fix creating a view of an Ort::Value tensor. (#2939)
+* Add C and CXX API for Google MedASR model (#2946)
+* [TTS Engine] Fix engine speed (#2895)
+* Add Swift API for Google MedASR model (#2947)
+* Add C# API for Google MedASR model (#2949)
+* Add Pascal API for Google MedASR model (#2950)
+* Add Go API for Google MedAsr model (#2952)
+* Add Dart API for Google MedAsr model (#2953)
+* Add JavaScript API (WebAssembly) for Google MedAsr model (#2954)
+* Add JavaScript API (node-addon) for Google MedAsr model (#2955)
+* Add Kotlin and Java API for Google MedAsr model (#2956)
+* Add funASR-Nano with LLM support (#2936)
+* Fix building for Windows (#2964)
+* Fix building for HarmonyOS (#2972)
+* [feature] add FunASRNano config into golang api (#2974)
+* Update FunAsr-Nano CTC model (#2978)
+* [opt] opt free pointer function in Go API (#2975)
+* [feature] use jinja2 to generate sherpa-onnx-go lib (#2976)
+* Reformat Go API code (#2979)
+* Fix building for onnxruntime >= 1.11.0 (#2981)
+* Export Whisper to RK NPU (#2983)
+* Test Whisper on Ascend NPU using ACL Python API (#2986)
+* FunASR-nano: switch to unified KV-cache LLM (#2995)
+* Remove filesystem header (#2998)
+* Fix(csrc/melotts): Fix V-words pronounciation on MeloTTS_en (#3002)
+* Upload FunASR Nano ASR models with LLM (#3003)
+* Fix download test wav files (#3004)
+* Use onnxruntime 1.23.2 for Windows (#3007)
+* Add CI to export Whisper models to Ascend NPU (#3008)
+* Add C++ runtime for Whisper with Ascend NPU (#3009)
+* Use onnxruntime v1.23.2 for Linux aarch64 (#3016)
+* Use onnxruntime v1.23.2 for Linux arm (#3017)
+* Start to switch from onnxruntime 1.17.1 to v1.23.2 (#2993)
+* Use onnxruntime 1.23.2 for Linux x64 + NVIDIA GPU (#3018)
+* Update CI test for FunASR Nano C/C++ API (#3021)
+* [feature] add FunASRNano Swift api (#2994)
+* swift: add FunASR nano Swift API (#3022)
+* Add Go API test for FunASR Nano (#3025)
+* Add JavaScript API for FunASR Nano (node-addon) (#3026)
+* Add Pascal API for FunASR Nano (#3029)
+* Add C# API for FunASR Nano (#3031)
+* Add Kotlin and Java API for FunASR Nano models (#3030)
+* Fire-Red-ASR: enable ORT I/O binding for encoder/decoder (#3011)
+* whisper: improve ORT IO binding execution (#3023)
+* Add JavaScript API for FunASR Nano (WebAssembly) (#3027)
+* Fix CI test for nodejs (#3033)
+
 ## 1.12.20
 
 * Refactor axcl examples. (#2867)
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 1c34ad66..1ae539dc 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -14,7 +14,7 @@ project(sherpa-onnx)
 # Remember to update
 # ./CHANGELOG.md
 # ./new-release.sh
-set(SHERPA_ONNX_VERSION "1.12.20")
+set(SHERPA_ONNX_VERSION "1.12.21")
 
 # Disable warning about
 #
diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
index df6b6c48..d4667dcc 100644
--- a/android/SherpaOnnx/app/build.gradle
+++ b/android/SherpaOnnx/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
index df6b6c48..d4667dcc 100644
--- a/android/SherpaOnnx2Pass/app/build.gradle
+++ b/android/SherpaOnnx2Pass/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
index e3d6bc56..2d334696 100644
--- a/android/SherpaOnnxAar/README.md
+++ b/android/SherpaOnnxAar/README.md
@@ -4,8 +4,8 @@
 git clone https://github.com/k2-fsa/sherpa-onnx
 cd sherpa-onnx
 
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-v1.12.20-android.tar.bz2
-tar xvf sherpa-onnx-v1.12.20-android.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-v1.12.21-android.tar.bz2
+tar xvf sherpa-onnx-v1.12.21-android.tar.bz2
 
 cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
 cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
 
 ./gradlew :sherpa_onnx:assembleRelease
 ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.20.aar
+cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.21.aar
 ```
diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
index 1af7f56d..b2c50c1e 100644
--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
index e0de0cd4..ee7c122b 100644
--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
         minSdk = 26
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
         vectorDrawables {
             useSupportLibrary = true
         }
diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
index da437a02..fdc0ad33 100644
--- a/android/SherpaOnnxJavaDemo/app/build.gradle
+++ b/android/SherpaOnnxJavaDemo/app/build.gradle
@@ -9,8 +9,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 28
         targetSdk 34
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
@@ -34,5 +34,5 @@ dependencies {
     implementation 'pub.devrel:easypermissions:3.0.0'
     implementation 'androidx.core:core-ktx:1.7.0'
     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.20'
+    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.21'
 }
diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
index df6b6c48..d4667dcc 100644
--- a/android/SherpaOnnxKws/app/build.gradle
+++ b/android/SherpaOnnxKws/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
index bef7ef4a..a27c68d4 100644
--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
index e9d0f91a..bab85e3b 100644
--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
         minSdk = 28
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
         vectorDrawables {
             useSupportLibrary = true
         }
@@ -58,7 +58,7 @@ dependencies {
     implementation(libs.compose.foundation)
     implementation(libs.activity.compose)
     implementation(libs.core.splashscreen)
-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.20")
+    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.21")
     androidTestImplementation(platform(libs.compose.bom))
     androidTestImplementation(libs.ui.test.junit4)
     debugImplementation(libs.ui.tooling)
diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
index a627d6e7..737f65f3 100644
--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
index 5e84ec0e..913f78f3 100644
--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
index 0aad0cf2..5d3a3551 100644
--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.slid"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
index 221b89ea..65475bde 100644
--- a/android/SherpaOnnxTts/app/build.gradle
+++ b/android/SherpaOnnxTts/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
index 9a5e8f26..ebb98c5c 100644
--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
@@ -11,8 +11,8 @@ android {
         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
         minSdk = 21
         targetSdk = 34
-        versionCode = 20251217
-        versionName = "1.12.20"
+        versionCode = 20260112
+        versionName = "1.12.21"
 
         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
         vectorDrawables {
diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
index 7d3afbcb..6ea6c6c1 100644
--- a/android/SherpaOnnxVad/app/build.gradle
+++ b/android/SherpaOnnxVad/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
index 7d3afbcb..6ea6c6c1 100644
--- a/android/SherpaOnnxVadAsr/app/build.gradle
+++ b/android/SherpaOnnxVadAsr/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 33
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
index 1066d5ff..75de557b 100644
--- a/android/SherpaOnnxWebSocket/app/build.gradle
+++ b/android/SherpaOnnxWebSocket/app/build.gradle
@@ -11,8 +11,8 @@ android {
         applicationId "com.k2fsa.sherpa.onnx"
         minSdk 21
         targetSdk 32
-        versionCode 20251217
-        versionName "1.12.20"
+        versionCode 20260112
+        versionName "1.12.21"
 
         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
     }
diff --git a/build-ios-shared.sh b/build-ios-shared.sh
index 064723a8..8d2ce969 100755
--- a/build-ios-shared.sh
+++ b/build-ios-shared.sh
@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
 	<key>CFBundlePackageType</key>
 	<string>FMWK</string>
 	<key>CFBundleShortVersionString</key>
-	<string>1.12.20</string>
+	<string>1.12.21</string>
 	<key>CFBundleSupportedPlatforms</key>
 	<array>
 		<string>iPhoneOS</string>
diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
index a45be760..069d42a9 100644
--- a/dart-api-examples/add-punctuations/pubspec.yaml
+++ b/dart-api-examples/add-punctuations/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
index 8b2f37a7..48f55860 100644
--- a/dart-api-examples/audio-tagging/pubspec.yaml
+++ b/dart-api-examples/audio-tagging/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
index ce9b8725..d1cb864f 100644
--- a/dart-api-examples/keyword-spotter/pubspec.yaml
+++ b/dart-api-examples/keyword-spotter/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
index ff195c35..7a3056d1 100644
--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
index 3090ff9e..533e23e2 100644
--- a/dart-api-examples/speaker-diarization/pubspec.yaml
+++ b/dart-api-examples/speaker-diarization/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
index 95bb5c6d..052c6991 100644
--- a/dart-api-examples/speaker-identification/pubspec.yaml
+++ b/dart-api-examples/speaker-identification/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
index f2d5ab5e..4dad3914 100644
--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
index a46585b6..05e04e44 100644
--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
+++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   path: ^1.9.0
diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
index 1884ef3b..e394622d 100644
--- a/dart-api-examples/streaming-asr/pubspec.yaml
+++ b/dart-api-examples/streaming-asr/pubspec.yaml
@@ -11,7 +11,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
index a9c47b68..75bf0e48 100644
--- a/dart-api-examples/tts/pubspec.yaml
+++ b/dart-api-examples/tts/pubspec.yaml
@@ -8,7 +8,7 @@ environment:
 
 # Add regular dependencies here.
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
index 7b6e0112..74a938aa 100644
--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
@@ -10,7 +10,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
index c73ea2ae..923cf452 100644
--- a/dart-api-examples/vad/pubspec.yaml
+++ b/dart-api-examples/vad/pubspec.yaml
@@ -9,7 +9,7 @@ environment:
   sdk: ">=3.0.0 <4.0.0"
 
 dependencies:
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   path: ^1.9.0
   args: ^2.5.0
 
diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
index ef743d3f..ed3f5821 100644
--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.20
+version: 1.12.21
 
 topics:
   - speech-recognition
@@ -31,7 +31,7 @@ dependencies:
   record: 6.0.0
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
index 7c3cd89b..b959ebe6 100644
--- a/flutter-examples/streaming_asr/pubspec.yaml
+++ b/flutter-examples/streaming_asr/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none'
 
-version: 1.12.20
+version: 1.12.21
 
 topics:
   - speech-recognition
@@ -30,7 +30,7 @@ dependencies:
   record: ^6.1.2
   url_launcher: ^6.2.6
 
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
 
diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
index 2956a40c..3d6a4a8f 100644
--- a/flutter-examples/tts/pubspec.yaml
+++ b/flutter-examples/tts/pubspec.yaml
@@ -5,7 +5,7 @@ description: >
 
 publish_to: 'none' # Remove this line if you wish to publish to pub.dev
 
-version: 1.12.20
+version: 1.12.21
 
 environment:
   sdk: ">=2.17.0 <4.0.0"
@@ -18,7 +18,7 @@ dependencies:
   cupertino_icons: ^1.0.6
   path_provider: ^2.1.3
   path: ^1.9.0
-  sherpa_onnx: ^1.12.20
+  sherpa_onnx: ^1.12.21
   # sherpa_onnx:
   #   path: ../../flutter/sherpa_onnx
   url_launcher: 6.2.6
diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
index ac02b6aa..1379d4fc 100644
--- a/flutter/sherpa_onnx/pubspec.yaml
+++ b/flutter/sherpa_onnx/pubspec.yaml
@@ -17,7 +17,7 @@ topics:
   - voice-activity-detection
 
 # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
-version: 1.12.20
+version: 1.12.21
 
 homepage: https://github.com/k2-fsa/sherpa-onnx
 
@@ -30,23 +30,23 @@ dependencies:
   flutter:
     sdk: flutter
 
-  sherpa_onnx_android: ^1.12.20
+  sherpa_onnx_android: ^1.12.21
   # sherpa_onnx_android:
   #   path: ../sherpa_onnx_android
 
-  sherpa_onnx_macos: ^1.12.20
+  sherpa_onnx_macos: ^1.12.21
   # sherpa_onnx_macos:
   #   path: ../sherpa_onnx_macos
 
-  sherpa_onnx_linux: ^1.12.20
+  sherpa_onnx_linux: ^1.12.21
   # sherpa_onnx_linux:
   #   path: ../sherpa_onnx_linux
 
-  sherpa_onnx_windows: ^1.12.20
+  sherpa_onnx_windows: ^1.12.21
   # sherpa_onnx_windows:
   #   path: ../sherpa_onnx_windows
 
-  sherpa_onnx_ios: ^1.12.20
+  sherpa_onnx_ios: ^1.12.21
   # sherpa_onnx_ios:
   #   path: ../sherpa_onnx_ios
 
diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
index a821c033..368b16c3 100644
--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
@@ -7,7 +7,7 @@
 # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_ios'
-  s.version          = '1.12.20'
+  s.version          = '1.12.21'
   s.summary          = 'A new Flutter FFI plugin project.'
   s.description      = <<-DESC
 A new Flutter FFI plugin project.
diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
index ecefd905..465b9879 100644
--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
@@ -4,7 +4,7 @@
 #
 Pod::Spec.new do |s|
   s.name             = 'sherpa_onnx_macos'
-  s.version          = '1.12.20'
+  s.version          = '1.12.21'
   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
   s.description      = <<-DESC
 sherpa-onnx Flutter FFI plugin project.
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
index 7c18da38..0d1c47fd 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
@@ -1,7 +1,7 @@
 /**
  * Use these variables when you tailor your ArkTS code. They must be of the const type.
  */
-export const HAR_VERSION = '1.12.20';
+export const HAR_VERSION = '1.12.21';
 export const BUILD_MODE_NAME = 'debug';
 export const DEBUG = true;
 export const TARGET_NAME = 'default';
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
index 01087a2d..af33641f 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
 
 ```
   "dependencies": {
-    "sherpa_onnx": "1.12.20",
+    "sherpa_onnx": "1.12.21",
   },
 ```
 
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
index fe2c2ae5..b1539e67 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
@@ -1,6 +1,6 @@
 {
   "name": "sherpa_onnx",
-  "version": "1.12.20",
+  "version": "1.12.21",
   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
   "main": "Index.ets",
   "author": "The next-gen Kaldi team",
diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
index 02da1ac7..955e3f9a 100644
--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.20"
+    "sherpa_onnx": "1.12.21"
   }
 }
 
diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
index 82551df9..5259c961 100644
--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.20",
+    "sherpa_onnx": "1.12.21",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
index 82551df9..5259c961 100644
--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.20",
+    "sherpa_onnx": "1.12.21",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
index 82551df9..5259c961 100644
--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
@@ -6,7 +6,7 @@
   "author": "",
   "license": "",
   "dependencies": {
-    "sherpa_onnx": "1.12.20",
+    "sherpa_onnx": "1.12.21",
   }
 }
 
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
index 672af848..7e791052 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
+++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
@@ -1,6 +1,6 @@
 # Introduction
 
-Please download ./sherpa_onnx-v1.12.20.har
+Please download ./sherpa_onnx-v1.12.21.har
 from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
 
 Hint: For users who have no access to huggingface, please use
diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
index 2f78fe76..15efe0ed 100644
--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
@@ -7,7 +7,7 @@
   "license": "",
   "dependencies": {
     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
-    "sherpa_onnx": "1.12.20",
+    "sherpa_onnx": "1.12.21",
   }
 }
 
diff --git a/jitpack.yml b/jitpack.yml
index 43a41983..e07163da 100644
--- a/jitpack.yml
+++ b/jitpack.yml
@@ -2,8 +2,8 @@ jdk:
   - openjdk17
 
 before_install:
-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-1.12.20.aar
+  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-1.12.21.aar
 
 install:
-  - FILE="-Dfile=sherpa-onnx-1.12.20.aar"
-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.20 -Dpackaging=aar -DgeneratePom=true
+  - FILE="-Dfile=sherpa-onnx-1.12.21.aar"
+  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.21 -Dpackaging=aar -DgeneratePom=true
diff --git a/mfc-examples/README.md b/mfc-examples/README.md
index d9480032..6636a84d 100644
--- a/mfc-examples/README.md
+++ b/mfc-examples/README.md
@@ -5,9 +5,9 @@ for speech recognition.
 
 |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
 |---------|--------------------|-------------------|------------|
-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x86-v1.12.20.exe)| Non-streaming speech recognition|
-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x86-v1.12.20.exe)| Streaming speech recognition|
-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x86-v1.12.20.exe)| Non-streaming text to speech|
+|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-asr-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-asr-x86-v1.12.21.exe)| Non-streaming speech recognition|
+|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-streaming-asr-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-streaming-asr-x86-v1.12.21.exe)| Streaming speech recognition|
+|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-tts-x64-v1.12.21.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.21/sherpa-onnx-non-streaming-tts-x86-v1.12.21.exe)| Non-streaming text to speech|
 
 Caution: You need to use Windows and install Visual Studio 2022 in order to
 compile it.
diff --git a/new-release.sh b/new-release.sh
index 1588a030..50ffafeb 100755
--- a/new-release.sh
+++ b/new-release.sh
@@ -2,11 +2,11 @@
 
 set -ex
 
-old_version_code=20251205
-new_version_code=20251217
+old_version_code=20251217
+new_version_code=20260112
 
-old_version="1\.12\.19"
-new_version="1\.12\.20"
+old_version="1\.12\.20"
+new_version="1\.12\.21"
 
 replace_str="s/$old_version/$new_version/g"
 
diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
index 429c86dd..c83144f7 100644
--- a/nodejs-addon-examples/package.json
+++ b/nodejs-addon-examples/package.json
@@ -1,5 +1,5 @@
 {
   "dependencies": {
-    "sherpa-onnx-node": "^1.12.20"
+    "sherpa-onnx-node": "^1.12.21"
   }
 }
diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
index 710a5447..29c68d16 100644
--- a/nodejs-examples/package.json
+++ b/nodejs-examples/package.json
@@ -2,7 +2,7 @@
   "dependencies": {
     "mic": "^2.1.2",
     "naudiodon2": "^2.4.0",
-    "sherpa-onnx": "^1.12.20",
+    "sherpa-onnx": "^1.12.21",
     "wav": "^1.0.2"
   }
 }
diff --git a/pom.xml b/pom.xml
index dfbb89f2..159277c0 100644
--- a/pom.xml
+++ b/pom.xml
@@ -4,7 +4,7 @@
     <modelVersion>4.0.0</modelVersion>
     <groupId>com.k2fsa.sherpa.onnx</groupId>
     <artifactId>sherpa-onnx-android</artifactId>
-    <version>1.12.20</version>
+    <version>1.12.21</version>
     <url>https://github.com/k2-fsa/sherpa-onnx</url>
     <packaging>pom</packaging>
     <description>First Android Library</description>
diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
index 29b11fb0..d1965639 100644
--- a/scripts/wheel/sherpa-onnx-bin/setup.py
+++ b/scripts/wheel/sherpa-onnx-bin/setup.py
@@ -13,7 +13,7 @@ print("bin_files", bin_files)
 
 setup(
     name="sherpa-onnx-bin",
-    version="1.12.20",
+    version="1.12.21",
     description="Binary executables for sherpa-onnx",
     author="The sherpa-onnx development team",
     url="https://github.com/k2-fsa/sherpa-onnx",
@@ -23,7 +23,7 @@ setup(
     packages=[],
     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
     install_requires=[
-        "sherpa-onnx-core==1.12.20",
+        "sherpa-onnx-core==1.12.21",
     ],
     classifiers=[
         "Programming Language :: Python :: 3",
diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
index 8ac69af9..21514c55 100644
--- a/scripts/wheel/sherpa-onnx-core/setup.py
+++ b/scripts/wheel/sherpa-onnx-core/setup.py
@@ -23,7 +23,7 @@ def get_binaries():
 
 setup(
     name="sherpa-onnx-core",
-    version="1.12.20",
+    version="1.12.21",
     description="Core shared libraries for sherpa-onnx",
     packages=["sherpa_onnx"],
     include_package_data=True,
diff --git a/setup.py b/setup.py
index 3eeb89a7..39a7d6c4 100644
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ setuptools.setup(
         ],
     },
     license="Apache licensed, as found in the LICENSE file",
-    install_requires=["sherpa-onnx-core==1.12.20"] if need_split_package() else None,
+    install_requires=["sherpa-onnx-core==1.12.21"] if need_split_package() else None,
 )
 
 with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
index 2d8a55a9..f44e3b9e 100644
--- a/sherpa-onnx/csrc/version.cc
+++ b/sherpa-onnx/csrc/version.cc
@@ -7,17 +7,17 @@
 namespace sherpa_onnx {
 
 const char *GetGitDate() {
-  static const char *date = "Wed Dec 17 19:57:55 2025";
+  static const char *date = "Mon Jan 12 19:05:54 2026";
   return date;
 }
 
 const char *GetGitSha1() {
-  static const char *sha1 = "3290e1ce";
+  static const char *sha1 = "82b17d2f";
   return sha1;
 }
 
 const char *GetVersionStr() {
-  static const char *version = "1.12.20";
+  static const char *version = "1.12.21";
   return version;
 }
 

commit 82b17d2fa40f7c23834078d9b5095801a18c5c70
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 19:05:54 2026 +0800

    Fix CI test for nodejs (#3033)

diff --git a/.github/scripts/test-nodejs-npm.sh b/.github/scripts/test-nodejs-npm.sh
index cf9897c1..9da82653 100755
--- a/.github/scripts/test-nodejs-npm.sh
+++ b/.github/scripts/test-nodejs-npm.sh
@@ -235,9 +235,9 @@ rm -rf sherpa-onnx-whisper-tiny.en
 
 # offline asr
 #
-curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
 
 node ./test-offline-sense-voice.js
 
@@ -250,7 +250,7 @@ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lex
 
 node ./test-offline-sense-voice-with-hr.js
 
-rm -rf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
+rm -rf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17
 rm -rf dict replace.fst test-hr.wav lexicon.txt
 
 curl -LS -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-paraformer-zh-2023-09-14.tar.bz2

commit 3171d3867dcc6b0bb59b052e5a7b01e1be5442b0
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:56:38 2026 +0800

    Add JavaScript API for FunASR Nano (WebAssembly) (#3027)

diff --git a/.github/scripts/test-nodejs-npm.sh b/.github/scripts/test-nodejs-npm.sh
index 0b929ca9..cf9897c1 100755
--- a/.github/scripts/test-nodejs-npm.sh
+++ b/.github/scripts/test-nodejs-npm.sh
@@ -9,6 +9,14 @@ git status
 ls -lh
 ls -lh node_modules
 
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+node ./test-offline-funasr-nano.js
+
+rm -rf sherpa-onnx-funasr-nano-int8-2025-12-30
+
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
 tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
 rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
diff --git a/nodejs-examples/README.md b/nodejs-examples/README.md
index c6cd4cbb..345cbf25 100644
--- a/nodejs-examples/README.md
+++ b/nodejs-examples/README.md
@@ -203,6 +203,22 @@ rm sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03.tar.bz2
 node ./test-offline-zipformer-ctc.js
 ```
 
+## ./test-offline-funasr-nano.js
+
+[./test-offline-funasr-nano.js](./test-offline-funasr-nano.js) demonstrates
+how to decode a file with a FunASR Nano model. In the code we use
+[sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2).
+
+You can use the following command to run it:
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+node ./test-offline-funasr-nano.js
+```
+
 ## ./test-offline-medasr-ctc.js
 
 [./test-offline-medasr-ctc.js](./test-offline-medasr-ctc.js) demonstrates
diff --git a/nodejs-examples/test-offline-funasr-nano.js b/nodejs-examples/test-offline-funasr-nano.js
new file mode 100644
index 00000000..c1ac7d40
--- /dev/null
+++ b/nodejs-examples/test-offline-funasr-nano.js
@@ -0,0 +1,40 @@
+// Copyright (c)  2026  Xiaomi Corporation (authors: Fangjun Kuang)
+//
+const fs = require('fs');
+const {Readable} = require('stream');
+const wav = require('wav');
+
+const sherpa_onnx = require('sherpa-onnx');
+
+function createOfflineRecognizer() {
+  let config = {
+    modelConfig: {
+      funasrNano: {
+        encoderAdaptor:
+            './sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx',
+        llm: './sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx',
+        embedding:
+            './sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx',
+        tokenizer: './sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B',
+      },
+      tokens: '',
+    }
+  };
+
+  return sherpa_onnx.createOfflineRecognizer(config);
+}
+
+const recognizer = createOfflineRecognizer();
+const stream = recognizer.createStream();
+
+const waveFilename =
+    './sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav';
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform(wave.sampleRate, wave.samples);
+
+recognizer.decode(stream);
+const text = recognizer.getResult(stream).text;
+console.log(text);
+
+stream.free();
+recognizer.free();
diff --git a/wasm/asr/sherpa-onnx-asr.js b/wasm/asr/sherpa-onnx-asr.js
index 876f947b..802b9cc2 100644
--- a/wasm/asr/sherpa-onnx-asr.js
+++ b/wasm/asr/sherpa-onnx-asr.js
@@ -63,6 +63,10 @@ function freeConfig(config, Module) {
     freeConfig(config.medasr, Module)
   }
 
+  if ('funasrNano' in config) {
+    freeConfig(config.funasrNano, Module)
+  }
+
   if ('moonshine' in config) {
     freeConfig(config.moonshine, Module)
   }
@@ -126,8 +130,10 @@ function initSherpaOnnxOnlineTransducerModelConfig(config, Module) {
   Module.setValue(ptr + 8, buffer + offset, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOnlineParaformerModelConfig(config, Module) {
@@ -153,8 +159,10 @@ function initSherpaOnnxOnlineParaformerModelConfig(config, Module) {
   Module.setValue(ptr + 4, buffer + offset, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOnlineZipformer2CtcModelConfig(config, Module) {
@@ -169,8 +177,10 @@ function initSherpaOnnxOnlineZipformer2CtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOnlineNemoCtcModelConfig(config, Module) {
@@ -185,8 +195,10 @@ function initSherpaOnnxOnlineNemoCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOnlineToneCtcModelConfig(config, Module) {
@@ -201,8 +213,10 @@ function initSherpaOnnxOnlineToneCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOnlineModelConfig(config, Module) {
@@ -353,10 +367,15 @@ function initSherpaOnnxOnlineModelConfig(config, Module) {
   offset += toneCtc.len;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, transducer: transducer,
-        paraformer: paraformer, zipformer2Ctc: zipformer2Ctc, nemoCtc: nemoCtc,
-        toneCtc: toneCtc,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    transducer: transducer,
+    paraformer: paraformer,
+    zipformer2Ctc: zipformer2Ctc,
+    nemoCtc: nemoCtc,
+    toneCtc: toneCtc,
+  };
 }
 
 function initSherpaOnnxFeatureConfig(config, Module) {
@@ -540,9 +559,14 @@ function initSherpaOnnxOnlineRecognizerConfig(config, Module) {
   offset += hr.len;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, feat: feat, model: model,
-        ctcFstDecoder: ctcFstDecoder, hr: hr,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    feat: feat,
+    model: model,
+    ctcFstDecoder: ctcFstDecoder,
+    hr: hr,
+  };
 }
 
 function createOnlineRecognizer(Module, myConfig) {
@@ -674,8 +698,10 @@ function initSherpaOnnxOfflineTransducerModelConfig(config, Module) {
   Module.setValue(ptr + 8, buffer + offset, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineParaformerModelConfig(config, Module) {
@@ -691,8 +717,10 @@ function initSherpaOnnxOfflineParaformerModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineNemoEncDecCtcModelConfig(config, Module) {
@@ -708,8 +736,10 @@ function initSherpaOnnxOfflineNemoEncDecCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineDolphinModelConfig(config, Module) {
@@ -725,8 +755,10 @@ function initSherpaOnnxOfflineDolphinModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineZipformerCtcModelConfig(config, Module) {
@@ -742,8 +774,10 @@ function initSherpaOnnxOfflineZipformerCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineWenetCtcModelConfig(config, Module) {
@@ -759,8 +793,10 @@ function initSherpaOnnxOfflineWenetCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(config, Module) {
@@ -776,8 +812,10 @@ function initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineMedAsrCtcModelConfig(config, Module) {
@@ -793,8 +831,85 @@ function initSherpaOnnxOfflineMedAsrCtcModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
+}
+
+function initSherpaOnnxOfflineFunAsrNanoModelConfig(config, Module) {
+  const encoderAdaptorLen =
+      Module.lengthBytesUTF8(config.encoderAdaptor || '') + 1;
+  const llmLen = Module.lengthBytesUTF8(config.llm || '') + 1;
+  const embeddingLen = Module.lengthBytesUTF8(config.embedding || '') + 1;
+  const tokenizerLen = Module.lengthBytesUTF8(config.tokenizer || '') + 1;
+  const systemPromptLen =
+      Module.lengthBytesUTF8(
+          config.systemPrompt || 'You are a helpful assistant.') +
+      1;
+  const userPromptLen =
+      Module.lengthBytesUTF8(config.userPrompt || '') + 1;
+
+  const n = encoderAdaptorLen + llmLen + embeddingLen + tokenizerLen +
+      systemPromptLen + userPromptLen;
+
+  const buffer = Module._malloc(n);
+
+  const len = 10 * 4;  // 6 pointers + 2 int + 2 float
+  const ptr = Module._malloc(len);
+
+  let offset = 0;
+  Module.stringToUTF8(
+      config.encoderAdaptor || '', buffer + offset, encoderAdaptorLen);
+  offset += encoderAdaptorLen;
+
+  Module.stringToUTF8(config.llm || '', buffer + offset, llmLen);
+  offset += llmLen;
+
+  Module.stringToUTF8(config.embedding || '', buffer + offset, embeddingLen);
+  offset += embeddingLen;
+
+  Module.stringToUTF8(config.tokenizer || '', buffer + offset, tokenizerLen);
+  offset += tokenizerLen;
+
+  Module.stringToUTF8(
+      config.systemPrompt || 'You are a helpful assistant.', buffer + offset,
+      systemPromptLen);
+  offset += systemPromptLen;
+
+  Module.stringToUTF8(
+      config.userPrompt || '', buffer + offset, userPromptLen);
+  offset += userPromptLen;
+
+  offset = 0;
+  Module.setValue(ptr + 0 * 4, buffer + offset, 'i8*');
+  offset += encoderAdaptorLen;
+
+  Module.setValue(ptr + 1 * 4, buffer + offset, 'i8*');
+  offset += llmLen;
+
+  Module.setValue(ptr + 2 * 4, buffer + offset, 'i8*');
+  offset += embeddingLen;
+
+  Module.setValue(ptr + 3 * 4, buffer + offset, 'i8*');
+  offset += tokenizerLen;
+
+  Module.setValue(ptr + 4 * 4, buffer + offset, 'i8*');
+  offset += systemPromptLen;
+
+  Module.setValue(ptr + 5 * 4, buffer + offset, 'i8*');
+  offset += userPromptLen;
+
+  Module.setValue(ptr + 6 * 4, config.maxNewTokens || 512, 'i32');
+  Module.setValue(ptr + 7 * 4, config.temperature || 1e-6, 'float');
+  Module.setValue(ptr + 8 * 4, config.topP || 0.8, 'float');
+  Module.setValue(ptr + 9 * 4, config.seed || 42, 'i32');
+
+  return {
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineWhisperModelConfig(config, Module) {
@@ -837,8 +952,10 @@ function initSherpaOnnxOfflineWhisperModelConfig(config, Module) {
   Module.setValue(ptr + 16, config.tailPaddings || 2000, 'i32');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineCanaryModelConfig(config, Module) {
@@ -882,8 +999,10 @@ function initSherpaOnnxOfflineCanaryModelConfig(config, Module) {
   Module.setValue(ptr + 16, config.usePnc ?? 1, 'i32');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineMoonshineModelConfig(config, Module) {
@@ -931,8 +1050,10 @@ function initSherpaOnnxOfflineMoonshineModelConfig(config, Module) {
   offset += cachedDecoderLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineFireRedAsrModelConfig(config, Module) {
@@ -960,8 +1081,10 @@ function initSherpaOnnxOfflineFireRedAsrModelConfig(config, Module) {
   offset += decoderLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTdnnModelConfig(config, Module) {
@@ -976,8 +1099,10 @@ function initSherpaOnnxOfflineTdnnModelConfig(config, Module) {
   Module.setValue(ptr, buffer, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineSenseVoiceModelConfig(config, Module) {
@@ -1008,8 +1133,10 @@ function initSherpaOnnxOfflineSenseVoiceModelConfig(config, Module) {
   Module.setValue(ptr + 8, config.useInverseTextNormalization ?? 0, 'i32');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineLMConfig(config, Module) {
@@ -1024,8 +1151,10 @@ function initSherpaOnnxOfflineLMConfig(config, Module) {
   Module.setValue(ptr + 4, config.scale || 1, 'float');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineModelConfig(config, Module) {
@@ -1079,6 +1208,21 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
     };
   }
 
+  if (!('funasrNano' in config)) {
+    config.funasrNano = {
+      encoderAdaptor: '',
+      llm: '',
+      embedding: '',
+      tokenizer: '',
+      systemPrompt: 'You are a helpful assistant.',
+      userPrompt: '',
+      maxNewTokens: 512,
+      temperature: 1e-6,
+      topP: 0.8,
+      seed: 42,
+    };
+  }
+
   if (!('whisper' in config)) {
     config.whisper = {
       encoder: '',
@@ -1169,10 +1313,13 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   const medasr =
       initSherpaOnnxOfflineMedAsrCtcModelConfig(config.medasr, Module);
 
+  const funasrNano =
+      initSherpaOnnxOfflineFunAsrNanoModelConfig(config.funasrNano, Module);
+
   const len = transducer.len + paraformer.len + nemoCtc.len + whisper.len +
       tdnn.len + 8 * 4 + senseVoice.len + moonshine.len + fireRedAsr.len +
       dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len +
-      omnilingual.len + medasr.len;
+      omnilingual.len + medasr.len + funasrNano.len;
 
   const ptr = Module._malloc(len);
 
@@ -1289,13 +1436,29 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   Module._CopyHeap(medasr.ptr, medasr.len, ptr + offset);
   offset += medasr.len;
 
+  Module._CopyHeap(funasrNano.ptr, funasrNano.len, ptr + offset);
+  offset += funasrNano.len;
+
   return {
-    buffer: buffer, ptr: ptr, len: len, transducer: transducer,
-        paraformer: paraformer, nemoCtc: nemoCtc, whisper: whisper, tdnn: tdnn,
-        senseVoice: senseVoice, moonshine: moonshine, fireRedAsr: fireRedAsr,
-        dolphin: dolphin, zipformerCtc: zipformerCtc, canary: canary,
-        wenetCtc: wenetCtc, omnilingual: omnilingual, medasr: medasr
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    transducer: transducer,
+    paraformer: paraformer,
+    nemoCtc: nemoCtc,
+    whisper: whisper,
+    tdnn: tdnn,
+    senseVoice: senseVoice,
+    moonshine: moonshine,
+    fireRedAsr: fireRedAsr,
+    dolphin: dolphin,
+    zipformerCtc: zipformerCtc,
+    canary: canary,
+    wenetCtc: wenetCtc,
+    omnilingual: omnilingual,
+    medasr: medasr,
+    funasrNano: funasrNano
+  };
 }
 
 function initSherpaOnnxOfflineRecognizerConfig(config, Module) {
@@ -1392,9 +1555,14 @@ function initSherpaOnnxOfflineRecognizerConfig(config, Module) {
   offset += hr.len;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, feat: feat, model: model, lm: lm,
-        hr: hr,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    feat: feat,
+    model: model,
+    lm: lm,
+    hr: hr,
+  };
 }
 
 class OfflineStream {
diff --git a/wasm/kws/sherpa-onnx-kws.js b/wasm/kws/sherpa-onnx-kws.js
index 07ac7881..cfcbbc0e 100644
--- a/wasm/kws/sherpa-onnx-kws.js
+++ b/wasm/kws/sherpa-onnx-kws.js
@@ -56,8 +56,10 @@ function initSherpaOnnxOnlineTransducerModelConfig(config, Module) {
   Module.setValue(ptr + 8, buffer + offset, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 // The user should free the returned pointers
@@ -155,9 +157,7 @@ function initModelConfig(config, Module) {
   offset += 4;
   // skip nemo_ctc and t_one_ctc
 
-  return {
-    buffer: buffer, ptr: ptr, len: len, transducer: transducer
-  }
+  return {buffer: buffer, ptr: ptr, len: len, transducer: transducer};
 }
 
 function initFeatureExtractorConfig(config, Module) {
@@ -165,8 +165,9 @@ function initFeatureExtractorConfig(config, Module) {
   Module.setValue(ptr, config.samplingRate || 16000, 'i32');
   Module.setValue(ptr + 4, config.featureDim || 80, 'i32');
   return {
-    ptr: ptr, len: 8,
-  }
+    ptr: ptr,
+    len: 8,
+  };
 }
 
 function initKwsConfig(config, Module) {
@@ -228,9 +229,12 @@ function initKwsConfig(config, Module) {
   offset += 4;
 
   return {
-    ptr: ptr, len: numBytes, featConfig: featConfig, modelConfig: modelConfig,
-        keywordsBuffer: keywordsBuffer
-  }
+    ptr: ptr,
+    len: numBytes,
+    featConfig: featConfig,
+    modelConfig: modelConfig,
+    keywordsBuffer: keywordsBuffer
+  };
 }
 
 class Stream {
diff --git a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
index 682a1cab..a9273b75 100644
--- a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
+++ b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
@@ -17,6 +17,7 @@ static_assert(sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWenetCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineMedAsrCtcModelConfig) == 4, "");
+static_assert(sizeof(SherpaOnnxOfflineFunASRNanoModelConfig) == 10 * 4, "");
 static_assert(sizeof(SherpaOnnxOfflineDolphinModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineNemoEncDecCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWhisperModelConfig) == 5 * 4, "");
@@ -41,7 +42,8 @@ static_assert(sizeof(SherpaOnnxOfflineModelConfig) ==
                       sizeof(SherpaOnnxOfflineCanaryModelConfig) +
                       sizeof(SherpaOnnxOfflineWenetCtcModelConfig) +
                       sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) +
-                      sizeof(SherpaOnnxOfflineMedAsrCtcModelConfig),
+                      sizeof(SherpaOnnxOfflineMedAsrCtcModelConfig) +
+                      sizeof(SherpaOnnxOfflineFunASRNanoModelConfig),
 
               "");
 static_assert(sizeof(SherpaOnnxFeatureConfig) == 2 * 4, "");
@@ -92,6 +94,7 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   auto wenet_ctc = &model_config->wenet_ctc;
   auto omnilingual = &model_config->omnilingual;
   auto medasr = &model_config->medasr;
+  auto funasr_nano = &model_config->funasr_nano;
 
   fprintf(stdout, "----------offline transducer model config----------\n");
   fprintf(stdout, "encoder: %s\n", transducer->encoder);
@@ -151,6 +154,18 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   fprintf(stdout, "----------offline MedASR model config----------\n");
   fprintf(stdout, "model: %s\n", medasr->model);
 
+  fprintf(stdout, "----------offline FunASR Nano config----------\n");
+  fprintf(stdout, "encoder_adaptor: %s\n", funasr_nano->encoder_adaptor);
+  fprintf(stdout, "llm: %s\n", funasr_nano->llm);
+  fprintf(stdout, "embedding: %s\n", funasr_nano->embedding);
+  fprintf(stdout, "tokenizer: %s\n", funasr_nano->tokenizer);
+  fprintf(stdout, "system_prompt: %s\n", funasr_nano->system_prompt);
+  fprintf(stdout, "user_prompt: %s\n", funasr_nano->user_prompt);
+  fprintf(stdout, "max_new_tokens: %d\n", funasr_nano->max_new_tokens);
+  fprintf(stdout, "temperature: %f\n", funasr_nano->temperature);
+  fprintf(stdout, "top_p: %f\n", funasr_nano->top_p);
+  fprintf(stdout, "seed: %d\n", funasr_nano->seed);
+
   fprintf(stdout, "tokens: %s\n", model_config->tokens);
   fprintf(stdout, "num_threads: %d\n", model_config->num_threads);
   fprintf(stdout, "provider: %s\n", model_config->provider);
diff --git a/wasm/speaker-diarization/sherpa-onnx-speaker-diarization.js b/wasm/speaker-diarization/sherpa-onnx-speaker-diarization.js
index 74101348..52769291 100644
--- a/wasm/speaker-diarization/sherpa-onnx-speaker-diarization.js
+++ b/wasm/speaker-diarization/sherpa-onnx-speaker-diarization.js
@@ -40,8 +40,10 @@ function initSherpaOnnxOfflineSpeakerSegmentationPyannoteModelConfig(
   Module.setValue(ptr, buffer + offset, 'i8*');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineSpeakerSegmentationModelConfig(config, Module) {
@@ -188,9 +190,12 @@ function initSherpaOnnxOfflineSpeakerDiarizationConfig(config, Module) {
   offset += 4;
 
   return {
-    ptr: ptr, len: len, segmentation: segmentation, embedding: embedding,
-        clustering: clustering,
-  }
+    ptr: ptr,
+    len: len,
+    segmentation: segmentation,
+    embedding: embedding,
+    clustering: clustering,
+  };
 }
 
 class OfflineSpeakerDiarization {
diff --git a/wasm/speech-enhancement/sherpa-onnx-speech-enhancement.js b/wasm/speech-enhancement/sherpa-onnx-speech-enhancement.js
index 08651b72..3ee5a771 100644
--- a/wasm/speech-enhancement/sherpa-onnx-speech-enhancement.js
+++ b/wasm/speech-enhancement/sherpa-onnx-speech-enhancement.js
@@ -37,8 +37,10 @@ function initSherpaOnnxOfflineSpeechDenoiserGtcrnModelConfig(config, Module) {
   offset += modelLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineSpeechDenoiserModelConfig(config, Module) {
@@ -91,8 +93,10 @@ function initSherpaOnnxOfflineSpeechDenoiserConfig(config, Module) {
   offset += modelConfig.len;
 
   return {
-    ptr: ptr, len: len, config: modelConfig,
-  }
+    ptr: ptr,
+    len: len,
+    config: modelConfig,
+  };
 }
 
 class OfflineSpeechDenoiser {
diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
index 2234d1b6..f093dc02 100644
--- a/wasm/tts/sherpa-onnx-tts.js
+++ b/wasm/tts/sherpa-onnx-tts.js
@@ -79,8 +79,10 @@ function initSherpaOnnxOfflineTtsVitsModelConfig(config, Module) {
   offset += dictDirLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTtsMatchaModelConfig(config, Module) {
@@ -143,8 +145,10 @@ function initSherpaOnnxOfflineTtsMatchaModelConfig(config, Module) {
   offset += dictDirLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTtsKokoroModelConfig(config, Module) {
@@ -212,8 +216,10 @@ function initSherpaOnnxOfflineTtsKokoroModelConfig(config, Module) {
   offset += langLen;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTtsKittenModelConfig(config, Module) {
@@ -258,8 +264,10 @@ function initSherpaOnnxOfflineTtsKittenModelConfig(config, Module) {
   Module.setValue(ptr + 16, config.lengthScale || 1.0, 'float');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
@@ -322,8 +330,10 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
   Module.setValue(ptr + 36, config.guidanceScale || 1.0, 'float');
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
@@ -439,10 +449,15 @@ function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
   offset += zipVoiceModelConfig.len;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, config: vitsModelConfig,
-        matcha: matchaModelConfig, kokoro: kokoroModelConfig,
-        kitten: kittenModelConfig, zipvoice: zipVoiceModelConfig,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    config: vitsModelConfig,
+    matcha: matchaModelConfig,
+    kokoro: kokoroModelConfig,
+    kitten: kittenModelConfig,
+    zipvoice: zipVoiceModelConfig,
+  };
 }
 
 function initSherpaOnnxOfflineTtsConfig(config, Module) {
@@ -475,8 +490,11 @@ function initSherpaOnnxOfflineTtsConfig(config, Module) {
   offset += 4;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, config: modelConfig,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    config: modelConfig,
+  };
 }
 
 class OfflineTts {
diff --git a/wasm/vad/sherpa-onnx-vad.js b/wasm/vad/sherpa-onnx-vad.js
index 85123625..7d4ee539 100644
--- a/wasm/vad/sherpa-onnx-vad.js
+++ b/wasm/vad/sherpa-onnx-vad.js
@@ -48,8 +48,10 @@ function initSherpaOnnxSileroVadModelConfig(config, Module) {
   offset += 4;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxTenVadModelConfig(config, Module) {
@@ -84,8 +86,10 @@ function initSherpaOnnxTenVadModelConfig(config, Module) {
   offset += 4;
 
   return {
-    buffer: buffer, ptr: ptr, len: len,
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+  };
 }
 
 function initSherpaOnnxVadModelConfig(config, Module) {
@@ -143,8 +147,12 @@ function initSherpaOnnxVadModelConfig(config, Module) {
   offset += tenVad.len;
 
   return {
-    buffer: buffer, ptr: ptr, len: len, sileroVad: sileroVad, tenVad: tenVad
-  }
+    buffer: buffer,
+    ptr: ptr,
+    len: len,
+    sileroVad: sileroVad,
+    tenVad: tenVad
+  };
 }
 
 function createVad(Module, myConfig) {

commit 6d3c4ec8cf8c2fd653b41f51b603b50f1e710d91
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Mon Jan 12 18:32:43 2026 +0800

    whisper: improve ORT IO binding execution (#3023)

diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
index 86a28877..60124c2f 100644
--- a/sherpa-onnx/csrc/offline-whisper-model.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model.cc
@@ -31,13 +31,24 @@
 
 namespace sherpa_onnx {
 
+namespace {
+
+static inline bool IsCudaProvider(const std::string &provider) {
+  return provider == "cuda";
+}
+
+}  // namespace
+
 class OfflineWhisperModel::Impl {
  public:
   explicit Impl(const OfflineModelConfig &config)
       : config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     encoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
     InitEncoder(nullptr, 0);
@@ -45,13 +56,18 @@ class OfflineWhisperModel::Impl {
     decoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
     InitDecoder(nullptr, 0);
+
+    InitCudaIOBinding();
   }
 
   explicit Impl(const SpokenLanguageIdentificationConfig &config)
       : lid_config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     encoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
     InitEncoder(nullptr, 0);
@@ -59,6 +75,8 @@ class OfflineWhisperModel::Impl {
     decoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
     InitDecoder(nullptr, 0);
+
+    InitCudaIOBinding();
   }
 
   template <typename Manager>
@@ -66,7 +84,10 @@ class OfflineWhisperModel::Impl {
       : config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(mgr, config.whisper.encoder);
       InitEncoder(buf.data(), buf.size());
@@ -76,6 +97,8 @@ class OfflineWhisperModel::Impl {
       auto buf = ReadFile(mgr, config.whisper.decoder);
       InitDecoder(buf.data(), buf.size());
     }
+
+    InitCudaIOBinding();
   }
 
   template <typename Manager>
@@ -83,7 +106,10 @@ class OfflineWhisperModel::Impl {
       : lid_config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(mgr, config.whisper.encoder);
       InitEncoder(buf.data(), buf.size());
@@ -93,12 +119,31 @@ class OfflineWhisperModel::Impl {
       auto buf = ReadFile(mgr, config.whisper.decoder);
       InitDecoder(buf.data(), buf.size());
     }
+
+    InitCudaIOBinding();
   }
 
   std::pair<Ort::Value, Ort::Value> ForwardEncoder(Ort::Value features) {
-    auto encoder_out = encoder_sess_->Run(
-        {}, encoder_input_names_ptr_.data(), &features, 1,
-        encoder_output_names_ptr_.data(), encoder_output_names_ptr_.size());
+    std::vector<Ort::Value> encoder_out;
+
+    if (use_cuda_iobinding_) {
+      // Encoder outputs are n_layer_cross_k and n_layer_cross_v, which are used
+      // multiple times in decoder steps. Keep them on GPU to avoid device<->host copies.
+      Ort::IoBinding binding(*encoder_sess_);
+      binding.BindInput(encoder_input_names_ptr_[0], features);
+
+      binding.BindOutput(encoder_output_names_ptr_[0], *cuda_mem_info_);
+      binding.BindOutput(encoder_output_names_ptr_[1], *cuda_mem_info_);
+
+      binding.SynchronizeInputs();
+      encoder_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      encoder_out = binding.GetOutputValues();
+    } else {
+      encoder_out = encoder_sess_->Run(
+          {}, encoder_input_names_ptr_.data(), &features, 1,
+          encoder_output_names_ptr_.data(), encoder_output_names_ptr_.size());
+    }
 
     return {std::move(encoder_out[0]), std::move(encoder_out[1])};
   }
@@ -115,10 +160,30 @@ class OfflineWhisperModel::Impl {
                                                std::move(n_layer_cross_v),
                                                std::move(offset)};
 
-    auto decoder_out = decoder_sess_->Run(
-        {}, decoder_input_names_ptr_.data(), decoder_input.data(),
-        decoder_input.size(), decoder_output_names_ptr_.data(),
-        decoder_output_names_ptr_.size());
+    std::vector<Ort::Value> decoder_out;
+
+    if (use_cuda_iobinding_) {
+      // CPU-side sampling needs logits on CPU, while self KV cache should
+      // remain on GPU to avoid large device<->host copies between decode steps.
+      Ort::IoBinding binding(*decoder_sess_);
+      for (size_t i = 0; i < decoder_input.size(); ++i) {
+        binding.BindInput(decoder_input_names_ptr_[i], decoder_input[i]);
+      }
+
+      binding.BindOutput(decoder_output_names_ptr_[0], cpu_mem_info_);
+      binding.BindOutput(decoder_output_names_ptr_[1], *cuda_mem_info_);
+      binding.BindOutput(decoder_output_names_ptr_[2], *cuda_mem_info_);
+
+      binding.SynchronizeInputs();
+      decoder_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      decoder_out = binding.GetOutputValues();
+    } else {
+      decoder_out = decoder_sess_->Run(
+          {}, decoder_input_names_ptr_.data(), decoder_input.data(),
+          decoder_input.size(), decoder_output_names_ptr_.data(),
+          decoder_output_names_ptr_.size());
+    }
 
     return std::tuple<Ort::Value, Ort::Value, Ort::Value, Ort::Value,
                       Ort::Value, Ort::Value>{
@@ -315,13 +380,34 @@ class OfflineWhisperModel::Impl {
                    &decoder_output_names_ptr_);
   }
 
- private:
+  void InitCudaIOBinding() {
+    use_cuda_iobinding_ = (!is_cpu_provider_ && IsCudaProvider(GetProvider()));
+    if (use_cuda_iobinding_) {
+      // Use device 0 by default. SessionOptions() in sherpa-onnx usually
+      // configures the CUDA EP device; binding here only affects output memory.
+      cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
+          "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
+    }
+  }
+
+  std::string GetProvider() const {
+    if (!config_.provider.empty()) {
+      return config_.provider;
+    }
+    return lid_config_.provider;
+  }
+
   OfflineModelConfig config_;
   SpokenLanguageIdentificationConfig lid_config_;
   Ort::Env env_;
   Ort::SessionOptions sess_opts_;
   Ort::AllocatorWithDefaultOptions allocator_;
 
+  Ort::MemoryInfo cpu_mem_info_;
+  std::unique_ptr<Ort::MemoryInfo> cuda_mem_info_;
+  bool use_cuda_iobinding_ = false;
+  bool is_cpu_provider_ = false;
+
   std::unique_ptr<Ort::Session> encoder_sess_;
   std::unique_ptr<Ort::Session> decoder_sess_;
 

commit 711acccd1cc00c560667e07f6aec2d868d3920a5
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Mon Jan 12 18:31:44 2026 +0800

    Fire-Red-ASR: enable ORT I/O binding for encoder/decoder (#3011)

diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
index b5677f06..a3cb79c4 100644
--- a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
+++ b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
@@ -30,13 +30,24 @@
 
 namespace sherpa_onnx {
 
+namespace {
+
+static inline bool IsCudaProvider(const std::string &provider) {
+  return provider == "cuda";
+}
+
+}  // namespace
+
 class OfflineFireRedAsrModel::Impl {
  public:
   explicit Impl(const OfflineModelConfig &config)
       : config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(config.fire_red_asr.encoder);
       InitEncoder(buf.data(), buf.size());
@@ -46,6 +57,8 @@ class OfflineFireRedAsrModel::Impl {
       auto buf = ReadFile(config.fire_red_asr.decoder);
       InitDecoder(buf.data(), buf.size());
     }
+
+    InitCudaIOBinding();
   }
 
   template <typename Manager>
@@ -53,7 +66,10 @@ class OfflineFireRedAsrModel::Impl {
       : config_(config),
         env_(ORT_LOGGING_LEVEL_ERROR),
         sess_opts_(GetSessionOptions(config)),
-        allocator_{} {
+        allocator_{},
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
     {
       auto buf = ReadFile(mgr, config.fire_red_asr.encoder);
       InitEncoder(buf.data(), buf.size());
@@ -63,6 +79,8 @@ class OfflineFireRedAsrModel::Impl {
       auto buf = ReadFile(mgr, config.fire_red_asr.decoder);
       InitDecoder(buf.data(), buf.size());
     }
+
+    InitCudaIOBinding();
   }
 
   std::pair<Ort::Value, Ort::Value> ForwardEncoder(Ort::Value features,
@@ -70,9 +88,27 @@ class OfflineFireRedAsrModel::Impl {
     std::array<Ort::Value, 2> inputs{std::move(features),
                                      std::move(features_length)};
 
-    auto encoder_out = encoder_sess_->Run(
-        {}, encoder_input_names_ptr_.data(), inputs.data(), inputs.size(),
-        encoder_output_names_ptr_.data(), encoder_output_names_ptr_.size());
+    std::vector<Ort::Value> encoder_out;
+
+    if (use_cuda_iobinding_) {
+      // Encoder outputs (cross_k, cross_v) are used multiple times in decoder
+      // steps, so keep them on GPU to avoid device<->host copies.
+      Ort::IoBinding binding(*encoder_sess_);
+      binding.BindInput(encoder_input_names_ptr_[0], inputs[0]);
+      binding.BindInput(encoder_input_names_ptr_[1], inputs[1]);
+
+      binding.BindOutput(encoder_output_names_ptr_[0], *cuda_mem_info_);
+      binding.BindOutput(encoder_output_names_ptr_[1], *cuda_mem_info_);
+
+      binding.SynchronizeInputs();
+      encoder_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      encoder_out = binding.GetOutputValues();
+    } else {
+      encoder_out = encoder_sess_->Run(
+          {}, encoder_input_names_ptr_.data(), inputs.data(), inputs.size(),
+          encoder_output_names_ptr_.data(), encoder_output_names_ptr_.size());
+    }
 
     return {std::move(encoder_out[0]), std::move(encoder_out[1])};
   }
@@ -89,10 +125,30 @@ class OfflineFireRedAsrModel::Impl {
                                                std::move(n_layer_cross_v),
                                                std::move(offset)};
 
-    auto decoder_out = decoder_sess_->Run(
-        {}, decoder_input_names_ptr_.data(), decoder_input.data(),
-        decoder_input.size(), decoder_output_names_ptr_.data(),
-        decoder_output_names_ptr_.size());
+    std::vector<Ort::Value> decoder_out;
+
+    if (use_cuda_iobinding_) {
+      // CPU-side sampling needs logits on CPU, while self KV cache should
+      // remain on GPU to avoid large device<->host copies between decode steps.
+      Ort::IoBinding binding(*decoder_sess_);
+      for (size_t i = 0; i < decoder_input.size(); ++i) {
+        binding.BindInput(decoder_input_names_ptr_[i], decoder_input[i]);
+      }
+
+      binding.BindOutput(decoder_output_names_ptr_[0], cpu_mem_info_);
+      binding.BindOutput(decoder_output_names_ptr_[1], *cuda_mem_info_);
+      binding.BindOutput(decoder_output_names_ptr_[2], *cuda_mem_info_);
+
+      binding.SynchronizeInputs();
+      decoder_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      decoder_out = binding.GetOutputValues();
+    } else {
+      decoder_out = decoder_sess_->Run(
+          {}, decoder_input_names_ptr_.data(), decoder_input.data(),
+          decoder_input.size(), decoder_output_names_ptr_.data(),
+          decoder_output_names_ptr_.size());
+    }
 
     return std::tuple<Ort::Value, Ort::Value, Ort::Value, Ort::Value,
                       Ort::Value, Ort::Value>{
@@ -179,12 +235,27 @@ class OfflineFireRedAsrModel::Impl {
                    &decoder_output_names_ptr_);
   }
 
+  void InitCudaIOBinding() {
+    use_cuda_iobinding_ = (!is_cpu_provider_ && IsCudaProvider(config_.provider));
+    if (use_cuda_iobinding_) {
+      // Use device 0 by default. SessionOptions() in sherpa-onnx usually
+      // configures the CUDA EP device; binding here only affects output memory.
+      cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
+          "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
+    }
+  }
+
  private:
   OfflineModelConfig config_;
   Ort::Env env_;
   Ort::SessionOptions sess_opts_;
   Ort::AllocatorWithDefaultOptions allocator_;
 
+  Ort::MemoryInfo cpu_mem_info_;
+  std::unique_ptr<Ort::MemoryInfo> cuda_mem_info_;
+  bool use_cuda_iobinding_ = false;
+  bool is_cpu_provider_ = false;
+
   std::unique_ptr<Ort::Session> encoder_sess_;
   std::unique_ptr<Ort::Session> decoder_sess_;
 

commit e4c046ab811c182fde64776f75d3dce05b24f039
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:22:14 2026 +0800

    Add Kotlin and Java API for FunASR Nano models (#3030)

diff --git a/.github/workflows/run-java-test.yaml b/.github/workflows/run-java-test.yaml
index 2b250f6c..6f9a6c05 100644
--- a/.github/workflows/run-java-test.yaml
+++ b/.github/workflows/run-java-test.yaml
@@ -108,6 +108,13 @@ jobs:
           cd ./java-api-examples
           ./run-version-test.sh
 
+      - name:  Run java test (FunASR Nano)
+        shell: bash
+        run: |
+          cd ./java-api-examples
+          ./run-non-streaming-decode-file-funasr-nano.sh
+          rm -rf sherpa-onnx-funasr-*
+
       - name:  Run java test (MedASR CTC)
         shell: bash
         run: |
@@ -115,7 +122,6 @@ jobs:
           ./run-non-streaming-decode-file-medasr-ctc.sh
           rm -rf sherpa-onnx-medasr-*
 
-
       - name:  Run java test (Omnilingual ASR CTC)
         shell: bash
         run: |
diff --git a/java-api-examples/NonStreamingDecodeFileFunAsrNano.java b/java-api-examples/NonStreamingDecodeFileFunAsrNano.java
new file mode 100644
index 00000000..8bdbb218
--- /dev/null
+++ b/java-api-examples/NonStreamingDecodeFileFunAsrNano.java
@@ -0,0 +1,59 @@
+// Copyright 2026 Xiaomi Corporation
+
+// This file shows how to use an offline FunASR Nano model,
+// i.e., non-streaming FunASR Nano model,
+// to decode files.
+import com.k2fsa.sherpa.onnx.*;
+
+public class NonStreamingDecodeFileFunAsrNano {
+  public static void main(String[] args) {
+    // please refer to
+    // https://k2-fsa.github.io/sherpa/onnx/funasr-nano/index.html
+    // to download model files
+    String encoderAdaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx";
+    String llm = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx";
+    String embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx";
+    String tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B";
+
+    String tokens = "";
+
+    String waveFilename = "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav";
+
+    WaveReader reader = new WaveReader(waveFilename);
+
+    OfflineFunAsrNanoModelConfig funasrNano =
+        OfflineFunAsrNanoModelConfig.builder()
+            .setEncoderAdaptor(encoderAdaptor)
+            .setLLM(llm)
+            .setEmbedding(embedding)
+            .setTokenizer(tokenizer)
+            .build();
+
+    OfflineModelConfig modelConfig =
+        OfflineModelConfig.builder()
+            .setFunAsrNano(funasrNano)
+            .setTokens(tokens)
+            .setNumThreads(1)
+            .setDebug(true)
+            .build();
+
+    OfflineRecognizerConfig config =
+        OfflineRecognizerConfig.builder()
+            .setOfflineModelConfig(modelConfig)
+            .setDecodingMethod("greedy_search")
+            .build();
+
+    OfflineRecognizer recognizer = new OfflineRecognizer(config);
+    OfflineStream stream = recognizer.createStream();
+    stream.acceptWaveform(reader.getSamples(), reader.getSampleRate());
+
+    recognizer.decode(stream);
+
+    String text = recognizer.getResult(stream).getText();
+
+    System.out.printf("filename:%s\nresult:%s\n", waveFilename, text);
+
+    stream.release();
+    recognizer.release();
+  }
+}
diff --git a/java-api-examples/run-non-streaming-decode-file-funasr-nano.sh b/java-api-examples/run-non-streaming-decode-file-funasr-nano.sh
new file mode 100755
index 00000000..9a3c5779
--- /dev/null
+++ b/java-api-examples/run-non-streaming-decode-file-funasr-nano.sh
@@ -0,0 +1,37 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [[ ! -f ../build/lib/libsherpa-onnx-jni.dylib  && ! -f ../build/lib/libsherpa-onnx-jni.so ]]; then
+  mkdir -p ../build
+  pushd ../build
+  cmake \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    -DSHERPA_ONNX_ENABLE_JNI=ON \
+    ..
+
+  make -j4
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f ../sherpa-onnx/java-api/build/sherpa-onnx.jar ]; then
+  pushd ../sherpa-onnx/java-api
+  make
+  popd
+fi
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+java \
+  -Djava.library.path=$PWD/../build/lib \
+  -cp ../sherpa-onnx/java-api/build/sherpa-onnx.jar \
+  NonStreamingDecodeFileFunAsrNano.java
diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
index d14ead48..8c29cf60 100755
--- a/kotlin-api-examples/run.sh
+++ b/kotlin-api-examples/run.sh
@@ -537,6 +537,28 @@ function testOfflineMedAsrCtc() {
   java -Djava.library.path=../build/lib -jar $out_filename
 }
 
+function testOfflineFunAsrNano() {
+  if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+    tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+    rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  fi
+
+  out_filename=test_offline_funasr_nano.jar
+  kotlinc-jvm -include-runtime -d $out_filename \
+    test_offline_funasr_nano.kt \
+    FeatureConfig.kt \
+    QnnConfig.kt \
+    HomophoneReplacerConfig.kt \
+    OfflineRecognizer.kt \
+    OfflineStream.kt \
+    WaveReader.kt \
+    faked-asset-manager.kt
+
+  ls -lh $out_filename
+  java -Djava.library.path=../build/lib -jar $out_filename
+}
+
 function testOfflineWenetCtc() {
   if [ ! -f sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10/model.int8.onnx ]; then
     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
@@ -561,6 +583,7 @@ function testOfflineWenetCtc() {
 
 testVersion
 
+testOfflineFunAsrNano
 testOfflineMedAsrCtc
 testOfflineOmnilingualAsrCtc
 testOfflineWenetCtc
diff --git a/kotlin-api-examples/test_offline_funasr_nano.kt b/kotlin-api-examples/test_offline_funasr_nano.kt
new file mode 100644
index 00000000..88a0e460
--- /dev/null
+++ b/kotlin-api-examples/test_offline_funasr_nano.kt
@@ -0,0 +1,31 @@
+package com.k2fsa.sherpa.onnx
+
+fun main() {
+  val recognizer = createOfflineRecognizer()
+  val waveFilename = "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav"
+
+  val objArray = WaveReader.readWaveFromFile(
+      filename = waveFilename,
+  )
+  val samples: FloatArray = objArray[0] as FloatArray
+  val sampleRate: Int = objArray[1] as Int
+
+  var stream = recognizer.createStream()
+  stream.acceptWaveform(samples, sampleRate=sampleRate)
+  recognizer.decode(stream)
+
+  var result = recognizer.getResult(stream)
+  println(result)
+
+  stream.release()
+  recognizer.release()
+}
+
+
+fun createOfflineRecognizer(): OfflineRecognizer {
+  val config = OfflineRecognizerConfig(
+      modelConfig = getOfflineModelConfig(type = 46)!!,
+  )
+
+  return OfflineRecognizer(config = config)
+}
diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
index 020ffa57..eac49339 100755
--- a/scripts/apk/generate-vad-asr-apk-script.py
+++ b/scripts/apk/generate-vad-asr-apk-script.py
@@ -774,6 +774,22 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-funasr-nano-int8-2025-12-30",
+            idx=46,
+            lang="multi",
+            lang2="31_languages",
+            short_name="funasr_nano_int8_2025_12_30",
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
index 8b921ace..26b26ebf 100644
--- a/sherpa-onnx/java-api/Makefile
+++ b/sherpa-onnx/java-api/Makefile
@@ -40,6 +40,7 @@ java_files += OfflineZipformerCtcModelConfig.java
 java_files += OfflineWenetCtcModelConfig.java
 java_files += OfflineOmnilingualAsrCtcModelConfig.java
 java_files += OfflineMedAsrCtcModelConfig.java
+java_files += OfflineFunAsrNanoModelConfig.java
 java_files += OfflineCanaryModelConfig.java
 java_files += OfflineSenseVoiceModelConfig.java
 java_files += OfflineDolphinModelConfig.java
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineFunAsrNanoModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineFunAsrNanoModelConfig.java
new file mode 100644
index 00000000..b8121d8e
--- /dev/null
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineFunAsrNanoModelConfig.java
@@ -0,0 +1,138 @@
+package com.k2fsa.sherpa.onnx;
+
+public class OfflineFunAsrNanoModelConfig {
+    private final String encoderAdaptor;
+    private final String llm;
+    private final String embedding;
+    private final String tokenizer;
+    private final String systemPrompt;
+    private final String userPrompt;
+    private final int maxNewTokens;
+    private final float temperature;
+    private final float topP;
+    private final int seed;
+
+    private OfflineFunAsrNanoModelConfig(Builder builder) {
+        this.encoderAdaptor = builder.encoderAdaptor;
+        this.llm = builder.llm;
+        this.embedding = builder.embedding;
+        this.tokenizer = builder.tokenizer;
+        this.systemPrompt = builder.systemPrompt;
+        this.userPrompt = builder.userPrompt;
+        this.maxNewTokens = builder.maxNewTokens;
+        this.temperature = builder.temperature;
+        this.topP = builder.topP;
+        this.seed = builder.seed;
+    }
+
+    public static Builder builder() {
+        return new Builder();
+    }
+
+    public String getEncoderAdaptor() {
+        return encoderAdaptor;
+    }
+
+    public String getLLM() {
+        return llm;
+    }
+
+    public String getEmbedding() {
+        return embedding;
+    }
+
+    public String getTokenizer() {
+        return tokenizer;
+    }
+
+    public String getSystemPrompt() {
+        return systemPrompt;
+    }
+
+    public String getUserPrompt() {
+        return userPrompt;
+    }
+
+    public int getMaxNewTokens() {
+        return maxNewTokens;
+    }
+
+    public float getTemperature() {
+        return temperature;
+    }
+
+    public float getTopP() {
+        return topP;
+    }
+
+    public int getSeed() {
+        return seed;
+    }
+
+    public static class Builder {
+        private String encoderAdaptor = "";
+        private String llm = "";
+        private String embedding = "";
+        private String tokenizer = "";
+        private String systemPrompt = "You are a helpful assistant.";
+        private String userPrompt = "";
+        private int maxNewTokens = 512;
+        private float temperature = 1e-6f;
+        private float topP = 0.8f;
+        private int seed = 42;
+
+        public OfflineFunAsrNanoModelConfig build() {
+            return new OfflineFunAsrNanoModelConfig(this);
+        }
+
+        public Builder setEncoderAdaptor(String encoderAdaptor) {
+            this.encoderAdaptor = encoderAdaptor;
+            return this;
+        }
+
+        public Builder setLLM(String llm) {
+            this.llm = llm;
+            return this;
+        }
+
+        public Builder setEmbedding(String embedding) {
+            this.embedding = embedding;
+            return this;
+        }
+
+        public Builder setTokenizer(String tokenizer) {
+            this.tokenizer = tokenizer;
+            return this;
+        }
+
+        public Builder setSystemPrompt(String systemPrompt) {
+            this.systemPrompt = systemPrompt;
+            return this;
+        }
+
+        public Builder setUserPrompt(String userPrompt) {
+            this.userPrompt = userPrompt;
+            return this;
+        }
+
+        public Builder setMaxNewTokens(int maxNewTokens) {
+            this.maxNewTokens = maxNewTokens;
+            return this;
+        }
+
+        public Builder setTemperature(float temperature) {
+            this.temperature = temperature;
+            return this;
+        }
+
+        public Builder setTopP(float topP) {
+            this.topP = topP;
+            return this;
+        }
+
+        public Builder setSeed(int seed) {
+            this.seed = seed;
+            return this;
+        }
+    }
+}
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
index ef20fb7b..9b56ebef 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
@@ -15,6 +15,7 @@ public class OfflineModelConfig {
     private final OfflineWenetCtcModelConfig wenetCtc;
     private final OfflineOmnilingualAsrCtcModelConfig omnilingual;
     private final OfflineMedAsrCtcModelConfig medasr;
+    private final OfflineFunAsrNanoModelConfig funasrNano;
     private final OfflineCanaryModelConfig canary;
     private final String teleSpeech;
     private final String tokens;
@@ -38,6 +39,7 @@ public class OfflineModelConfig {
         this.wenetCtc = builder.wenetCtc;
         this.omnilingual = builder.omnilingual;
         this.medasr = builder.medasr;
+        this.funasrNano = builder.funasrNano;
         this.senseVoice = builder.senseVoice;
         this.dolphin = builder.dolphin;
         this.teleSpeech = builder.teleSpeech;
@@ -98,6 +100,10 @@ public class OfflineModelConfig {
         return medasr;
     }
 
+    public OfflineFunAsrNanoModelConfig getFunAsrNano() {
+        return funasrNano;
+    }
+
     public OfflineCanaryModelConfig getCanary() {
         return canary;
     }
@@ -147,6 +153,7 @@ public class OfflineModelConfig {
         private OfflineWenetCtcModelConfig wenetCtc = OfflineWenetCtcModelConfig.builder().build();
         private OfflineOmnilingualAsrCtcModelConfig omnilingual = OfflineOmnilingualAsrCtcModelConfig.builder().build();
         private OfflineMedAsrCtcModelConfig medasr = OfflineMedAsrCtcModelConfig.builder().build();
+        private OfflineFunAsrNanoModelConfig funasrNano = OfflineFunAsrNanoModelConfig.builder().build();
         private OfflineCanaryModelConfig canary = OfflineCanaryModelConfig.builder().build();
         private String teleSpeech = "";
         private String tokens = "";
@@ -201,6 +208,11 @@ public class OfflineModelConfig {
             return this;
         }
 
+        public Builder setFunAsrNano(OfflineFunAsrNanoModelConfig funasrNano) {
+            this.funasrNano = funasrNano;
+            return this;
+        }
+
         public Builder setCanary(OfflineCanaryModelConfig canary) {
             this.canary = canary;
             return this;
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index a7877d96..4a7de118 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -267,6 +267,46 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.medasr.model, model,
                               medasr_ctc_config_cls, medasr_ctc_config);
 
+  // FunASR Nano
+  fid = env->GetFieldID(model_config_cls, "funasrNano",
+                        "Lcom/k2fsa/sherpa/onnx/OfflineFunAsrNanoModelConfig;");
+  jobject funasr_nano_config = env->GetObjectField(model_config, fid);
+  jclass funasr_nano_config_cls = env->GetObjectClass(funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.encoder_adaptor,
+                              encoderAdaptor, funasr_nano_config_cls,
+                              funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.llm, llm,
+                              funasr_nano_config_cls, funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.embedding, embedding,
+                              funasr_nano_config_cls, funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.tokenizer, tokenizer,
+                              funasr_nano_config_cls, funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.system_prompt,
+                              systemPrompt, funasr_nano_config_cls,
+                              funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.funasr_nano.user_prompt,
+                              userPrompt, funasr_nano_config_cls,
+                              funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_INT(ans.model_config.funasr_nano.max_new_tokens,
+                           maxNewTokens, funasr_nano_config_cls,
+                           funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_FLOAT(ans.model_config.funasr_nano.temperature,
+                             temperature, funasr_nano_config_cls,
+                             funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_FLOAT(ans.model_config.funasr_nano.top_p, topP,
+                             funasr_nano_config_cls, funasr_nano_config);
+
+  SHERPA_ONNX_JNI_READ_INT(ans.model_config.funasr_nano.seed, seed,
+                           funasr_nano_config_cls, funasr_nano_config);
   // canary
   fid = env->GetFieldID(model_config_cls, "canary",
                         "Lcom/k2fsa/sherpa/onnx/OfflineCanaryModelConfig;");
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index 64e6531e..4a7a4930 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -50,6 +50,19 @@ data class OfflineMedAsrCtcModelConfig(
     var model: String = "",
 )
 
+data class OfflineFunAsrNanoModelConfig(
+    var encoderAdaptor: String = "",
+    var llm: String = "",
+    var embedding: String = "",
+    var tokenizer: String = "",
+    var systemPrompt: String = "You are a helpful assistant.",
+    var userPrompt: String = "",
+    var maxNewTokens: Int = 512,
+    var temperature: Float = 1e-6f,
+    var topP: Float = 0.8f,
+    var seed: Int = 42,
+)
+
 data class OfflineWhisperModelConfig(
     var encoder: String = "",
     var decoder: String = "",
@@ -98,6 +111,7 @@ data class OfflineModelConfig(
     var wenetCtc: OfflineWenetCtcModelConfig = OfflineWenetCtcModelConfig(),
     var omnilingual: OfflineOmnilingualAsrCtcModelConfig = OfflineOmnilingualAsrCtcModelConfig(),
     var medasr: OfflineMedAsrCtcModelConfig = OfflineMedAsrCtcModelConfig(),
+    var funasrNano: OfflineFunAsrNanoModelConfig = OfflineFunAsrNanoModelConfig(),
     var canary: OfflineCanaryModelConfig = OfflineCanaryModelConfig(),
     var teleSpeech: String = "",
     var numThreads: Int = 1,
@@ -769,6 +783,19 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
             )
         }
 
+        46 -> {
+            val modelDir = "sherpa-onnx-funasr-nano-int8-2025-12-30"
+            return OfflineModelConfig(
+                funasrNano = OfflineFunAsrNanoModelConfig(
+                    encoderAdaptor = "$modelDir/encoder_adaptor.int8.onnx",
+                    llm = "$modelDir/llm.int8.onnx",
+                    embedding = "$modelDir/embedding.int8.onnx",
+                    tokenizer = "$modelDir/Qwen3-0.6B",
+                ),
+                tokens = "",
+            )
+        }
+
         9000 -> {
             val modelDir =
                 "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"

commit c19c471579bd7373fb5d57573c6699f9aab23bcc
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:19:48 2026 +0800

    Add C# API for FunASR Nano (#3031)

diff --git a/.github/scripts/test-dot-net.sh b/.github/scripts/test-dot-net.sh
index ccf61932..ffd5cf1b 100755
--- a/.github/scripts/test-dot-net.sh
+++ b/.github/scripts/test-dot-net.sh
@@ -4,7 +4,19 @@ set -ex
 
 cd dotnet-examples/
 
-cd ./version-test
+cd ./vad-non-streaming-funasr-nano
+./run-ten-vad.sh
+rm -fv *.onnx
+
+./run.sh
+rm -fv *.onnx
+
+cd ../non-streaming-funasr-nano-decode-files
+./run.sh
+ls -lh
+rm -rf sherpa-onnx-funasr-*
+
+cd ../version-test
 ./run.sh
 ls -lh
 
diff --git a/dotnet-examples/non-streaming-funasr-nano-decode-files/Program.cs b/dotnet-examples/non-streaming-funasr-nano-decode-files/Program.cs
new file mode 100644
index 00000000..bea10e87
--- /dev/null
+++ b/dotnet-examples/non-streaming-funasr-nano-decode-files/Program.cs
@@ -0,0 +1,35 @@
+// Copyright (c)  2026  Xiaomi Corporation
+//
+// This file shows how to use a FunASR Nano model for speech recognition.
+//
+// You can find the model doc at
+// https://k2-fsa.github.io/sherpa/onnx/funasr-nano.html
+using SherpaOnnx;
+
+class NonStreamingFunAsrNano
+{
+  static void Main(string[] args)
+  {
+    // please download model files from
+    // https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+    var config = new OfflineRecognizerConfig();
+    config.ModelConfig.FunAsrNano.EncoderAdaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx";
+    config.ModelConfig.FunAsrNano.LLM = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx";
+    config.ModelConfig.FunAsrNano.Embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx";
+    config.ModelConfig.FunAsrNano.Tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B";
+    config.ModelConfig.Tokens = "";
+    config.ModelConfig.Debug = 1;
+    var recognizer = new OfflineRecognizer(config);
+
+    var testWaveFilename = "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav";
+    var reader = new WaveReader(testWaveFilename);
+    var stream = recognizer.CreateStream();
+    stream.AcceptWaveform(reader.SampleRate, reader.Samples);
+    recognizer.Decode(stream);
+    var text = stream.Result.Text;
+    Console.WriteLine("Text: {0}", text);
+  }
+}
+
+
+
diff --git a/dotnet-examples/non-streaming-funasr-nano-decode-files/non-streaming-funasr-nano-decode-files.csproj b/dotnet-examples/non-streaming-funasr-nano-decode-files/non-streaming-funasr-nano-decode-files.csproj
new file mode 100644
index 00000000..917a3ef0
--- /dev/null
+++ b/dotnet-examples/non-streaming-funasr-nano-decode-files/non-streaming-funasr-nano-decode-files.csproj
@@ -0,0 +1,15 @@
+<Project Sdk="Microsoft.NET.Sdk">
+
+  <PropertyGroup>
+    <OutputType>Exe</OutputType>
+    <TargetFramework>net8.0</TargetFramework>
+    <RootNamespace>non_streaming_funasr_nano_decode_files</RootNamespace>
+    <ImplicitUsings>enable</ImplicitUsings>
+    <Nullable>enable</Nullable>
+  </PropertyGroup>
+
+  <ItemGroup>
+    <ProjectReference Include="..\Common\Common.csproj" />
+  </ItemGroup>
+
+</Project>
diff --git a/dotnet-examples/non-streaming-funasr-nano-decode-files/run.sh b/dotnet-examples/non-streaming-funasr-nano-decode-files/run.sh
new file mode 100755
index 00000000..7a1d79ea
--- /dev/null
+++ b/dotnet-examples/non-streaming-funasr-nano-decode-files/run.sh
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2 
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+dotnet run
diff --git a/dotnet-examples/sherpa-onnx.sln b/dotnet-examples/sherpa-onnx.sln
index b47b8766..f58e121c 100644
--- a/dotnet-examples/sherpa-onnx.sln
+++ b/dotnet-examples/sherpa-onnx.sln
@@ -47,6 +47,10 @@ Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "kitten-tts-play", "kitten-t
 EndProject
 Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "offline-audio-tagging", "offline-audio-tagging\offline-audio-tagging.csproj", "{0EBE2CE5-8940-4472-8A38-6A0E976E678F}"
 EndProject
+Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "non-streaming-funasr-nano-decode-files", "non-streaming-funasr-nano-decode-files\non-streaming-funasr-nano-decode-files.csproj", "{32F7534B-117E-4D1D-BAED-A1D1A6C6A62C}"
+EndProject
+Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "vad-non-streaming-funasr-nano", "vad-non-streaming-funasr-nano\vad-non-streaming-funasr-nano.csproj", "{32C8C12B-D7DB-455E-B35C-945A745520CC}"
+EndProject
 Global
 	GlobalSection(SolutionConfigurationPlatforms) = preSolution
 		Debug|Any CPU = Debug|Any CPU
@@ -141,6 +145,14 @@ Global
 		{0EBE2CE5-8940-4472-8A38-6A0E976E678F}.Debug|Any CPU.Build.0 = Debug|Any CPU
 		{0EBE2CE5-8940-4472-8A38-6A0E976E678F}.Release|Any CPU.ActiveCfg = Release|Any CPU
 		{0EBE2CE5-8940-4472-8A38-6A0E976E678F}.Release|Any CPU.Build.0 = Release|Any CPU
+		{32F7534B-117E-4D1D-BAED-A1D1A6C6A62C}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
+		{32F7534B-117E-4D1D-BAED-A1D1A6C6A62C}.Debug|Any CPU.Build.0 = Debug|Any CPU
+		{32F7534B-117E-4D1D-BAED-A1D1A6C6A62C}.Release|Any CPU.ActiveCfg = Release|Any CPU
+		{32F7534B-117E-4D1D-BAED-A1D1A6C6A62C}.Release|Any CPU.Build.0 = Release|Any CPU
+		{32C8C12B-D7DB-455E-B35C-945A745520CC}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
+		{32C8C12B-D7DB-455E-B35C-945A745520CC}.Debug|Any CPU.Build.0 = Debug|Any CPU
+		{32C8C12B-D7DB-455E-B35C-945A745520CC}.Release|Any CPU.ActiveCfg = Release|Any CPU
+		{32C8C12B-D7DB-455E-B35C-945A745520CC}.Release|Any CPU.Build.0 = Release|Any CPU
 	EndGlobalSection
 	GlobalSection(SolutionProperties) = preSolution
 		HideSolutionNode = FALSE
diff --git a/dotnet-examples/vad-non-streaming-funasr-nano/Program.cs b/dotnet-examples/vad-non-streaming-funasr-nano/Program.cs
new file mode 100644
index 00000000..97e18677
--- /dev/null
+++ b/dotnet-examples/vad-non-streaming-funasr-nano/Program.cs
@@ -0,0 +1,122 @@
+// Copyright (c)  2026  Xiaomi Corporation
+//
+// This file shows how to use a silero_vad model or ten-vad model
+// with a non-streaming FunASR Nano for speech recognition.
+using SherpaOnnx;
+using System.IO;
+
+
+class VadNonStreamingFunAsrNano
+{
+  static void Main(string[] args)
+  {
+    // please download model files from
+    // https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+    var config = new OfflineRecognizerConfig();
+    config.ModelConfig.FunAsrNano.EncoderAdaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx";
+    config.ModelConfig.FunAsrNano.LLM = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx";
+    config.ModelConfig.FunAsrNano.Embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx";
+    config.ModelConfig.FunAsrNano.Tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B";
+    config.ModelConfig.Tokens = "";
+    config.ModelConfig.Debug = 0;
+    var recognizer = new OfflineRecognizer(config);
+
+    var vadModelConfig = new VadModelConfig();
+    if (File.Exists("./silero_vad.onnx"))
+    {
+      Console.WriteLine("Use silero-vad");
+      vadModelConfig.SileroVad.Model = "./silero_vad.onnx";
+      vadModelConfig.SileroVad.Threshold = 0.3F;
+      vadModelConfig.SileroVad.MinSilenceDuration = 0.5F;
+      vadModelConfig.SileroVad.MinSpeechDuration = 0.25F;
+      vadModelConfig.SileroVad.MaxSpeechDuration = 5.0F;
+      vadModelConfig.SileroVad.WindowSize = 512;
+    }
+    else if (File.Exists("./ten-vad.onnx"))
+    {
+      Console.WriteLine("Use ten-vad");
+      vadModelConfig.TenVad.Model = "./ten-vad.onnx";
+      vadModelConfig.TenVad.Threshold = 0.3F;
+      vadModelConfig.TenVad.MinSilenceDuration = 0.5F;
+      vadModelConfig.TenVad.MinSpeechDuration = 0.25F;
+      vadModelConfig.TenVad.MaxSpeechDuration = 5.0F;
+      vadModelConfig.TenVad.WindowSize = 256;
+    }
+    else
+    {
+      Console.WriteLine("Please download ./silero_vad.onnx or ./ten-vad.onnx");
+      return;
+    }
+    vadModelConfig.Debug = 0;
+
+    var vad = new VoiceActivityDetector(vadModelConfig, 60);
+
+    var testWaveFilename = "./lei-jun-test.wav";
+    var reader = new WaveReader(testWaveFilename);
+
+    int numSamples = reader.Samples.Length;
+    int windowSize = vadModelConfig.SileroVad.WindowSize;
+
+    if (vadModelConfig.TenVad.Model != "")
+    {
+      windowSize = vadModelConfig.TenVad.WindowSize;
+    }
+
+    int sampleRate = vadModelConfig.SampleRate;
+    int numIter = numSamples / windowSize;
+
+    for (int i = 0; i != numIter; ++i)
+    {
+      int start = i * windowSize;
+      var samples = new float[windowSize];
+      Array.Copy(reader.Samples, start, samples, 0, windowSize);
+      vad.AcceptWaveform(samples);
+      if (vad.IsSpeechDetected())
+      {
+        while (!vad.IsEmpty())
+        {
+          SpeechSegment segment = vad.Front();
+          var startTime = segment.Start / (float)sampleRate;
+          var duration = segment.Samples.Length / (float)sampleRate;
+
+          OfflineStream stream = recognizer.CreateStream();
+          stream.AcceptWaveform(sampleRate, segment.Samples);
+          recognizer.Decode(stream);
+          var text = stream.Result.Text;
+
+          if (!string.IsNullOrEmpty(text))
+          {
+            Console.WriteLine("{0}--{1}: {2}", string.Format("{0:0.00}", startTime),
+                string.Format("{0:0.00}", startTime + duration), text);
+          }
+
+          vad.Pop();
+        }
+      }
+    }
+
+    vad.Flush();
+
+    while (!vad.IsEmpty())
+    {
+      var segment = vad.Front();
+      float startTime = segment.Start / (float)sampleRate;
+      float duration = segment.Samples.Length / (float)sampleRate;
+
+      var stream = recognizer.CreateStream();
+      stream.AcceptWaveform(sampleRate, segment.Samples);
+      recognizer.Decode(stream);
+      var text = stream.Result.Text;
+
+      if (!string.IsNullOrEmpty(text))
+      {
+        Console.WriteLine("{0}--{1}: {2}", string.Format("{0:0.00}", startTime),
+            string.Format("{0:0.00}", startTime + duration), text);
+      }
+
+      vad.Pop();
+    }
+  }
+}
+
+
diff --git a/dotnet-examples/vad-non-streaming-funasr-nano/run-ten-vad.sh b/dotnet-examples/vad-non-streaming-funasr-nano/run-ten-vad.sh
new file mode 100755
index 00000000..0cf11405
--- /dev/null
+++ b/dotnet-examples/vad-non-streaming-funasr-nano/run-ten-vad.sh
@@ -0,0 +1,19 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f ./ten-vad.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/ten-vad.onnx
+fi
+
+if [ ! -f ./lei-jun-test.wav ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/lei-jun-test.wav
+fi
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+dotnet run
diff --git a/dotnet-examples/vad-non-streaming-funasr-nano/run.sh b/dotnet-examples/vad-non-streaming-funasr-nano/run.sh
new file mode 100755
index 00000000..3c1ba3c9
--- /dev/null
+++ b/dotnet-examples/vad-non-streaming-funasr-nano/run.sh
@@ -0,0 +1,19 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f ./silero_vad.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx
+fi
+
+if [ ! -f ./lei-jun-test.wav ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/lei-jun-test.wav
+fi
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+dotnet run
diff --git a/dotnet-examples/vad-non-streaming-funasr-nano/vad-non-streaming-funasr-nano.csproj b/dotnet-examples/vad-non-streaming-funasr-nano/vad-non-streaming-funasr-nano.csproj
new file mode 100644
index 00000000..96195c8e
--- /dev/null
+++ b/dotnet-examples/vad-non-streaming-funasr-nano/vad-non-streaming-funasr-nano.csproj
@@ -0,0 +1,15 @@
+<Project Sdk="Microsoft.NET.Sdk">
+
+  <PropertyGroup>
+    <OutputType>Exe</OutputType>
+    <TargetFramework>net8.0</TargetFramework>
+    <RootNamespace>vad_non_streaming_funasr_nano</RootNamespace>
+    <ImplicitUsings>enable</ImplicitUsings>
+    <Nullable>enable</Nullable>
+  </PropertyGroup>
+
+  <ItemGroup>
+    <ProjectReference Include="..\Common\Common.csproj" />
+  </ItemGroup>
+
+</Project>
diff --git a/scripts/dotnet/OfflineFunAsrNanoModel.cs b/scripts/dotnet/OfflineFunAsrNanoModel.cs
new file mode 100644
index 00000000..3b1127a5
--- /dev/null
+++ b/scripts/dotnet/OfflineFunAsrNanoModel.cs
@@ -0,0 +1,48 @@
+/// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+
+using System.Runtime.InteropServices;
+
+namespace SherpaOnnx
+{
+
+    [StructLayout(LayoutKind.Sequential)]
+    public struct OfflineFunAsrNanoModelConfig
+    {
+        public OfflineFunAsrNanoModelConfig()
+        {
+            EncoderAdaptor = "";
+            LLM = "";
+            Embedding = "";
+            Tokenizer = "";
+            SystemPrompt = "You are a helpful assistant.";
+            UserPrompt = "";
+            MaxNewTokens = 512;
+            Temperature = 1e-6F;
+            TopP = 0.8F;
+            Seed = 42;
+        }
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string EncoderAdaptor;
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string LLM;
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string Embedding;
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string Tokenizer;
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string SystemPrompt;
+
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string UserPrompt;
+
+        public int MaxNewTokens;
+        public float Temperature;
+        public float TopP;
+        public int Seed;
+    }
+}
diff --git a/scripts/dotnet/OfflineModelConfig.cs b/scripts/dotnet/OfflineModelConfig.cs
index a6197fbd..e3323dd2 100644
--- a/scripts/dotnet/OfflineModelConfig.cs
+++ b/scripts/dotnet/OfflineModelConfig.cs
@@ -32,6 +32,7 @@ namespace SherpaOnnx
             WenetCtc = new OfflineWenetCtcModelConfig();
             Omnilingual = new OfflineOmnilingualAsrCtcModelConfig();
             MedAsr = new OfflineMedAsrCtcModelConfig();
+            FunAsrNano = new OfflineFunAsrNanoModelConfig();
         }
         public OfflineTransducerModelConfig Transducer;
         public OfflineParaformerModelConfig Paraformer;
@@ -70,5 +71,6 @@ namespace SherpaOnnx
         public OfflineWenetCtcModelConfig WenetCtc;
         public OfflineOmnilingualAsrCtcModelConfig Omnilingual;
         public OfflineMedAsrCtcModelConfig MedAsr;
+        public OfflineFunAsrNanoModelConfig FunAsrNano;
     }
 }
diff --git a/scripts/dotnet/generate.py b/scripts/dotnet/generate.py
index a423fd24..84db1f96 100755
--- a/scripts/dotnet/generate.py
+++ b/scripts/dotnet/generate.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 # Copyright (c)  2023  Xiaomi Corporation
 
-import argparse
+import glob
 import os
 import re
 from pathlib import Path
@@ -28,7 +28,6 @@ def read_proj_file(filename):
 
 
 def get_dict():
-    version = get_version()
     return {
         "version": get_version(),
     }
@@ -55,17 +54,18 @@ def process_linux(s, rid):
 
 
 def process_macos(s, rid):
-    libs = [
-        "libonnxruntime.1.17.1.dylib",
-        "libsherpa-onnx-c-api.dylib",
-    ]
-    prefix = f"{src_dir}/macos-{rid}/"
-    libs = [prefix + lib for lib in libs]
-    libs = "\n      ;".join(libs)
+    lib_dir = os.path.join(src_dir, f"macos-{rid}")
+    onnx_libs = glob.glob(os.path.join(lib_dir, "libonnxruntime*.dylib"))
+    if not onnx_libs:
+        raise FileNotFoundError(f"No libonnxruntime*.dylib found in {lib_dir}")
+
+    other_libs = [os.path.join(lib_dir, "libsherpa-onnx-c-api.dylib")]
+    libs = onnx_libs + other_libs
+    libs_str = "\n      ;".join(libs)
 
     d = get_dict()
     d["dotnet_rid"] = f"osx-{rid}"
-    d["libs"] = libs
+    d["libs"] = libs_str
 
     environment = jinja2.Environment()
     template = environment.from_string(s)
@@ -80,8 +80,6 @@ def process_windows(s, rid):
         "sherpa-onnx-c-api.dll",
     ]
 
-    version = get_version()
-
     prefix = f"{src_dir}/windows-{rid}/"
     libs = [prefix + lib for lib in libs]
     libs = "\n      ;".join(libs)
diff --git a/scripts/go/release.sh b/scripts/go/release.sh
index e696c058..40872f00 100755
--- a/scripts/go/release.sh
+++ b/scripts/go/release.sh
@@ -94,7 +94,7 @@ function osx() {
   cp -v sherpa_onnx/lib/*.dylib $dst/
 
   pushd $dst
-  cp -v libonnxruntime.1.17.1.dylib libonnxruntime.dylib
+  cp -v libonnxruntime.*.dylib libonnxruntime.dylib
   popd
 
   cd ..
@@ -112,7 +112,7 @@ function osx() {
   cp -v sherpa_onnx/lib/*.dylib $dst/
 
   pushd $dst
-  cp -v libonnxruntime.1.17.1.dylib libonnxruntime.dylib
+  cp -v libonnxruntime.*.dylib libonnxruntime.dylib
   popd
 
   cd ..

commit 95fbb44cd5c693d0ff1bf0657f1b1deeb3db83b0
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:16:48 2026 +0800

    Add Pascal API for FunASR Nano (#3029)

diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
index 44412c02..d4597257 100644
--- a/.github/workflows/pascal.yaml
+++ b/.github/workflows/pascal.yaml
@@ -135,6 +135,10 @@ jobs:
 
           pushd non-streaming-asr
 
+          ./run-funasr-nano.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
           ./run-wenet-ctc.sh
           rm -rf sherpa-onnx-*
           echo "---"
diff --git a/pascal-api-examples/non-streaming-asr/.gitignore b/pascal-api-examples/non-streaming-asr/.gitignore
index 0ff47ac7..406ce141 100644
--- a/pascal-api-examples/non-streaming-asr/.gitignore
+++ b/pascal-api-examples/non-streaming-asr/.gitignore
@@ -14,3 +14,4 @@ wenet_ctc
 nemo_canary
 omnilingual_asr_ctc
 medasr_ctc
+funasr_nano
diff --git a/pascal-api-examples/non-streaming-asr/funasr_nano.pas b/pascal-api-examples/non-streaming-asr/funasr_nano.pas
new file mode 100644
index 00000000..65f65ec8
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/funasr_nano.pas
@@ -0,0 +1,79 @@
+{ Copyright (c)  2026  Xiaomi Corporation }
+
+{
+This file shows how to use a non-streaming FunASR Nano model
+to decode files.
+
+You can download the model files from
+https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+}
+
+program funasr_nano;
+
+{$mode objfpc}
+
+uses
+  sherpa_onnx,
+  DateUtils,
+  SysUtils;
+
+var
+  Wave: TSherpaOnnxWave;
+  WaveFilename: AnsiString;
+
+  Config: TSherpaOnnxOfflineRecognizerConfig;
+  Recognizer: TSherpaOnnxOfflineRecognizer;
+  Stream: TSherpaOnnxOfflineStream;
+  RecognitionResult: TSherpaOnnxOfflineRecognizerResult;
+
+  Start: TDateTime;
+  Stop: TDateTime;
+
+  Elapsed: Single;
+  Duration: Single;
+  RealTimeFactor: Single;
+begin
+  Initialize(Config);
+
+  Config.ModelConfig.FunAsrNano.EncoderAdaptor := './sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx';
+  Config.ModelConfig.FunAsrNano.LLM := './sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx';
+  Config.ModelConfig.FunAsrNano.Embedding := './sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx';
+  Config.ModelConfig.FunAsrNano.Tokenizer := './sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B';
+  Config.ModelConfig.Tokens := '';
+  Config.ModelConfig.Provider := 'cpu';
+  Config.ModelConfig.NumThreads := 2;
+  Config.ModelConfig.Debug := True;
+
+  WaveFilename := './sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav';
+
+  Wave := SherpaOnnxReadWave(WaveFilename);
+
+  Recognizer := TSherpaOnnxOfflineRecognizer.Create(Config);
+  Stream := Recognizer.CreateStream();
+  Start := Now;
+
+  Stream.AcceptWaveform(Wave.Samples, Wave.SampleRate);
+  Recognizer.Decode(Stream);
+
+  RecognitionResult := Recognizer.GetResult(Stream);
+
+  Stop := Now;
+
+  Elapsed := MilliSecondsBetween(Stop, Start) / 1000;
+  Duration := Length(Wave.Samples) / Wave.SampleRate;
+  RealTimeFactor := Elapsed / Duration;
+
+  WriteLn(RecognitionResult.ToString);
+  WriteLn(Format('NumThreads %d', [Config.ModelConfig.NumThreads]));
+  WriteLn(Format('Elapsed %.3f s', [Elapsed]));
+  WriteLn(Format('Wave duration %.3f s', [Duration]));
+  WriteLn(Format('RTF = %.3f/%.3f = %.3f', [Elapsed, Duration, RealTimeFactor]));
+
+  {Free resources to avoid memory leak.
+
+  Note: You don't need to invoke them for this simple script.
+  However, you have to invoke them in your own large/complex project.
+  }
+  FreeAndNil(Stream);
+  FreeAndNil(Recognizer);
+end.
diff --git a/pascal-api-examples/non-streaming-asr/run-funasr-nano.sh b/pascal-api-examples/non-streaming-asr/run-funasr-nano.sh
new file mode 100755
index 00000000..1c33c7ae
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/run-funasr-nano.sh
@@ -0,0 +1,42 @@
+#!/usr/bin/env bash
+
+set -ex
+
+SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
+SHERPA_ONNX_DIR=$(cd $SCRIPT_DIR/../.. && pwd)
+
+echo "SHERPA_ONNX_DIR: $SHERPA_ONNX_DIR"
+
+if [[ ! -f ../../build/install/lib/libsherpa-onnx-c-api.dylib  && ! -f ../../build/install/lib/libsherpa-onnx-c-api.so && ! -f ../../build/install/lib/sherpa-onnx-c-api.dll ]]; then
+  mkdir -p ../../build
+  pushd ../../build
+  cmake \
+    -DCMAKE_INSTALL_PREFIX=./install \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    ..
+
+  cmake --build . --target install --config Release
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+fpc \
+  -dSHERPA_ONNX_USE_SHARED_LIBS \
+  -Fu$SHERPA_ONNX_DIR/sherpa-onnx/pascal-api \
+  -Fl$SHERPA_ONNX_DIR/build/install/lib \
+  ./funasr_nano.pas
+
+export LD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$LD_LIBRARY_PATH
+export DYLD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$DYLD_LIBRARY_PATH
+
+./funasr_nano
diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
index 40f4a485..5218ece4 100644
--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
+++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
@@ -345,6 +345,21 @@ type
     function ToString: AnsiString;
   end;
 
+  TSherpaOnnxOfflineFunAsrNanoModelConfig = record
+    EncoderAdaptor: AnsiString;
+    LLM: AnsiString;
+    Embedding: AnsiString;
+    Tokenizer: AnsiString;
+    SystemPrompt: AnsiString;
+    UserPrompt: AnsiString;
+    MaxNewTokens: Integer;
+    Temperature: Single;
+    TopP: Single;
+    Seed: Integer;
+    function ToString: AnsiString;
+    class operator Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineFunAsrNanoModelConfig);
+  end;
+
   TSherpaOnnxOfflineWhisperModelConfig = record
     Encoder: AnsiString;
     Decoder: AnsiString;
@@ -422,6 +437,7 @@ type
     WenetCtc: TSherpaOnnxOfflineWenetCtcModelConfig;
     Omnilingual: TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
     MedAsr: TSherpaOnnxOfflineMedAsrCtcModelConfig;
+    FunAsrNano: TSherpaOnnxOfflineFunAsrNanoModelConfig;
     class operator Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineModelConfig);
     function ToString: AnsiString;
   end;
@@ -837,6 +853,18 @@ type
   SherpaOnnxOfflineMedAsrCtcModelConfig = record
     Model: PAnsiChar;
   end;
+  SherpaOnnxOfflineFunAsrNanoModelConfig = record
+    EncoderAdaptor: PAnsiChar;
+    LLM: PAnsiChar;
+    Embedding: PAnsiChar;
+    Tokenizer: PAnsiChar;
+    SystemPrompt: PAnsiChar;
+    UserPrompt: PAnsiChar;
+    MaxNewTokens: cint32;
+    Temperature: cfloat;
+    TopP: cfloat;
+    Seed: cint32;
+  end;
   SherpaOnnxOfflineWhisperModelConfig = record
     Encoder: PAnsiChar;
     Decoder: PAnsiChar;
@@ -896,6 +924,7 @@ type
     WenetCtc: SherpaOnnxOfflineWenetCtcModelConfig;
     Omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
     MedAsr: SherpaOnnxOfflineMedAsrCtcModelConfig;
+    FunAsrNano: SherpaOnnxOfflineFunAsrNanoModelConfig;
   end;
 
   SherpaOnnxOfflineRecognizerConfig = record
@@ -1735,6 +1764,25 @@ begin
     [Self.Model]);
 end;
 
+function TSherpaOnnxOfflineFunAsrNanoModelConfig.ToString: AnsiString;
+begin
+  Result := Format('TSherpaOnnxOfflineFunAsrNanoModelConfig(' +
+    'EncoderAdaptor := %s' +
+    ', LLM := %s' +
+    ', Embedding := %s' +
+    ', Tokenizer := %s' +
+    ', SystemPrompt := %s' +
+    ', UserPrompt := %s' +
+    ', MaxNewTokens := %d' +
+    ', Temperature := %.3f' +
+    ', TopP := %.3f' +
+    ', Seed := %d' +
+    ')',
+    [Self.EncoderAdaptor, Self.LLM, Self.Embedding, Self.Tokenizer,
+    Self.SystemPrompt, Self.UserPrompt, Self.MaxNewTokens, Self.Temperature,
+    Self.TopP, Self.Seed]);
+end;
+
 function TSherpaOnnxOfflineWhisperModelConfig.ToString: AnsiString;
 begin
   Result := Format('TSherpaOnnxOfflineWhisperModelConfig(' +
@@ -1826,8 +1874,9 @@ begin
     'ZipformerCtc := %s, ' +
     'Canary := %s, ' +
     'WenetCtc := %s, ' +
-    'Omnilingual := %s, ' +
-    'MedAsr := %s' +
+    'Omnilingual := %s' +
+    ', MedAsr := %s' +
+    ', FunAsrNano := %s' +
     ')',
     [Self.Transducer.ToString, Self.Paraformer.ToString,
      Self.NeMoCtc.ToString, Self.Whisper.ToString, Self.Tdnn.ToString,
@@ -1836,7 +1885,8 @@ begin
      Self.TeleSpeechCtc, Self.SenseVoice.ToString, Self.Moonshine.ToString,
      Self.FireRedAsr.ToString, Self.Dolphin.ToString,
      Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString,
-     Self.Omnilingual.ToString, Self.MedAsr.ToString
+     Self.Omnilingual.ToString, Self.MedAsr.ToString,
+     Self.FunAsrNano.ToString
      ]);
 end;
 
@@ -1919,6 +1969,17 @@ begin
   C.ModelConfig.Omnilingual.Model := PAnsiChar(Config.ModelConfig.Omnilingual.Model);
   C.ModelConfig.MedAsr.Model := PAnsiChar(Config.ModelConfig.MedAsr.Model);
 
+  C.ModelConfig.FunAsrNano.EncoderAdaptor := PAnsiChar(Config.ModelConfig.FunAsrNano.EncoderAdaptor);
+  C.ModelConfig.FunAsrNano.LLM := PAnsiChar(Config.ModelConfig.FunAsrNano.LLM);
+  C.ModelConfig.FunAsrNano.Embedding := PAnsiChar(Config.ModelConfig.FunAsrNano.Embedding);
+  C.ModelConfig.FunAsrNano.Tokenizer := PAnsiChar(Config.ModelConfig.FunAsrNano.Tokenizer);
+  C.ModelConfig.FunAsrNano.SystemPrompt := PAnsiChar(Config.ModelConfig.FunAsrNano.SystemPrompt);
+  C.ModelConfig.FunAsrNano.UserPrompt := PAnsiChar(Config.ModelConfig.FunAsrNano.UserPrompt);
+  C.ModelConfig.FunAsrNano.MaxNewTokens := Config.ModelConfig.FunAsrNano.MaxNewTokens;
+  C.ModelConfig.FunAsrNano.Temperature := Config.ModelConfig.FunAsrNano.Temperature;
+  C.ModelConfig.FunAsrNano.TopP := Config.ModelConfig.FunAsrNano.TopP;
+  C.ModelConfig.FunAsrNano.Seed := Config.ModelConfig.FunAsrNano.Seed;
+
   C.LMConfig.Model := PAnsiChar(Config.LMConfig.Model);
   C.LMConfig.Scale := Config.LMConfig.Scale;
 
@@ -2089,6 +2150,14 @@ begin
     ]);
 end;
 
+class operator TSherpaOnnxOfflineFunAsrNanoModelConfig.Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineFunAsrNanoModelConfig);
+begin
+  Dest.MaxNewTokens := 512;
+  Dest.Temperature := 1e-6;
+  Dest.TopP := 0.8;
+  Dest.Seed := 42;
+end;
+
 class operator TSherpaOnnxSileroVadModelConfig.Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxSileroVadModelConfig);
 begin
   Dest.Threshold := 0.5;

commit b97c8c51cef335a79c46b5a0f7ba82af6d7db379
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:07:23 2026 +0800

    Add JavaScript API for FunASR Nano (node-addon) (#3026)

diff --git a/.github/scripts/test-nodejs-addon-npm.sh b/.github/scripts/test-nodejs-addon-npm.sh
index 938e4475..8317ac3b 100755
--- a/.github/scripts/test-nodejs-addon-npm.sh
+++ b/.github/scripts/test-nodejs-addon-npm.sh
@@ -10,6 +10,15 @@ arch=$(node -p "require('os').arch()")
 platform=$(node -p "require('os').platform()")
 node_version=$(node -p "process.versions.node.split('.')[0]")
 
+echo "----------non-streaming ASR FunASR Nano----------"
+
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+node ./test_asr_non_streaming_funasr_nano.js
+rm -rf sherpa-onnx-funasr-nano-int8-2025-12-30
+
 echo "----------non-streaming ASR Google MedASR CTC----------"
 
 wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
index 6996d079..1513b717 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
@@ -110,6 +110,31 @@ static SherpaOnnxOfflineMedAsrCtcModelConfig GetOfflineMedAsrCtcModelConfig(
   return c;
 }
 
+static SherpaOnnxOfflineFunASRNanoModelConfig GetOfflineFunAsrNanoModelConfig(
+    Napi::Object obj) {
+  SherpaOnnxOfflineFunASRNanoModelConfig c;
+  memset(&c, 0, sizeof(c));
+
+  if (!obj.Has("funasrNano") || !obj.Get("funasrNano").IsObject()) {
+    return c;
+  }
+
+  Napi::Object o = obj.Get("funasrNano").As<Napi::Object>();
+
+  SHERPA_ONNX_ASSIGN_ATTR_STR(encoder_adaptor, encoderAdaptor);
+  SHERPA_ONNX_ASSIGN_ATTR_STR(llm, llm);
+  SHERPA_ONNX_ASSIGN_ATTR_STR(embedding, embedding);
+  SHERPA_ONNX_ASSIGN_ATTR_STR(tokenizer, tokenizer);
+  SHERPA_ONNX_ASSIGN_ATTR_STR(system_prompt, systemPrompt);
+  SHERPA_ONNX_ASSIGN_ATTR_STR(user_prompt, userPrompt);
+  SHERPA_ONNX_ASSIGN_ATTR_INT32(max_new_tokens, maxNewTokens);
+  SHERPA_ONNX_ASSIGN_ATTR_FLOAT(temperature, temperature);
+  SHERPA_ONNX_ASSIGN_ATTR_FLOAT(top_p, topP);
+  SHERPA_ONNX_ASSIGN_ATTR_INT32(seed, seed);
+
+  return c;
+}
+
 static SherpaOnnxOfflineDolphinModelConfig GetOfflineDolphinModelConfig(
     Napi::Object obj) {
   SherpaOnnxOfflineDolphinModelConfig c;
@@ -277,6 +302,7 @@ static SherpaOnnxOfflineModelConfig GetOfflineModelConfig(Napi::Object obj) {
   c.wenet_ctc = GetOfflineWenetCtcModelConfig(o);
   c.omnilingual = GetOfflineOmnilingualAsrCtcModelConfig(o);
   c.medasr = GetOfflineMedAsrCtcModelConfig(o);
+  c.funasr_nano = GetOfflineFunAsrNanoModelConfig(o);
 
   SHERPA_ONNX_ASSIGN_ATTR_STR(tokens, tokens);
   SHERPA_ONNX_ASSIGN_ATTR_INT32(num_threads, numThreads);
@@ -373,6 +399,13 @@ static void FreeConfig(const SherpaOnnxOfflineRecognizerConfig &c) {
   SHERPA_ONNX_DELETE_C_STR(c.model_config.omnilingual.model);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.medasr.model);
 
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.user_prompt);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.system_prompt);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.tokenizer);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.embedding);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.llm);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.funasr_nano.encoder_adaptor);
+
   SHERPA_ONNX_DELETE_C_STR(c.model_config.tokens);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.provider);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.model_type);
diff --git a/nodejs-addon-examples/README.md b/nodejs-addon-examples/README.md
index c6afe1ed..52151e02 100644
--- a/nodejs-addon-examples/README.md
+++ b/nodejs-addon-examples/README.md
@@ -127,6 +127,7 @@ The following tables list the examples in this folder.
 |[./test_asr_non_streaming_wenet_ctc.js](./test_asr_non_streaming_wenet_ctc.js)|Non-streaming speech recognition from a file using a [u2pp_conformer_yue](https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/u2pp_conformer_yue) CTC model with greedy search|
 |[./test_asr_non_streaming_omnilingual_asr_ctc.js](./test_asr_non_streaming_omnilingual_asr_ctc.js)|Non-streaming speech recognition from a file using a [Omnilingual-ASR](https://github.com/facebookresearch/omnilingual-asr) CTC model with greedy search|
 |[./test_asr_non_streaming_medasr_ctc.js](./test_asr_non_streaming_medasr_ctc.js)|Non-streaming speech recognition from a file using a [Google MedASR](https://github.com/google-health/medasr) CTC model with greedy search|
+|[./test_asr_non_streaming_funasr_nano.js](./test_asr_non_streaming_funasr_nano.js)|Non-streaming speech recognition from a file using a [FunASR Nano](https://modelscope.cn/models/FunAudioLLM/Fun-ASR-Nano-2512) model|
 |[./test_asr_non_streaming_nemo_canary.js](./test_asr_non_streaming_nemo_canary.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [Canary](https://k2-fsa.github.io/sherpa/onnx/nemo/canary.html#sherpa-onnx-nemo-canary-180m-flash-en-es-de-fr-int8-english-spanish-german-french) model|
 |[./test_asr_non_streaming_zipformer_ctc.js](./test_asr_non_streaming_zipformer_ctc.js)|Non-streaming speech recognition from a file using a Zipformer CTC model with greedy search|
 |[./test_asr_non_streaming_nemo_parakeet_tdt_v2.js](./test_asr_non_streaming_nemo_parakeet_tdt_v2.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [parakeet-tdt-0.6b-v2](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-parakeet-tdt-0-6b-v2-int8-english) model with greedy search|
@@ -429,6 +430,16 @@ npm install naudiodon2
 node ./test_vad_asr_non_streaming_nemo_ctc_microphone.js
 ```
 
+### Non-streaming speech recognition with FunASR Nano models
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+node ./test_asr_non_streaming_funasr_nano.js
+```
+
 ### Non-streaming speech recognition with Google MedASR CTC models
 
 ```bash
diff --git a/nodejs-addon-examples/test_asr_non_streaming_funasr_nano.js b/nodejs-addon-examples/test_asr_non_streaming_funasr_nano.js
new file mode 100644
index 00000000..069712ad
--- /dev/null
+++ b/nodejs-addon-examples/test_asr_non_streaming_funasr_nano.js
@@ -0,0 +1,51 @@
+// Copyright (c)  2026  Xiaomi Corporation
+const sherpa_onnx = require('sherpa-onnx-node');
+
+// Please download test files from
+// https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+const config = {
+  'featConfig': {
+    'sampleRate': 16000,
+    'featureDim': 80,
+  },
+  'modelConfig': {
+    'funasrNano': {
+      'encoderAdaptor':
+          './sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx',
+      'llm': './sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx',
+      'embedding':
+          './sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx',
+      'tokenizer': './sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B',
+    },
+    'tokens': '',
+    'numThreads': 2,
+    'provider': 'cpu',
+    'debug': 1,
+  }
+};
+
+const waveFilename =
+    './sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav';
+
+const recognizer = new sherpa_onnx.OfflineRecognizer(config);
+console.log('Started')
+let start = Date.now();
+const stream = recognizer.createStream();
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+
+recognizer.decode(stream);
+const result = recognizer.getResult(stream);
+let stop = Date.now();
+console.log('Done')
+
+const elapsed_seconds = (stop - start) / 1000;
+const duration = wave.samples.length / wave.sampleRate;
+const real_time_factor = elapsed_seconds / duration;
+console.log('Wave duration', duration.toFixed(3), 'seconds')
+console.log('Elapsed', elapsed_seconds.toFixed(3), 'seconds')
+console.log(
+    `RTF = ${elapsed_seconds.toFixed(3)}/${duration.toFixed(3)} =`,
+    real_time_factor.toFixed(3))
+console.log(waveFilename)
+console.log('result\n', result)

commit bd5bf41b1c8b276b795fb312dd05e2a9d8ef3a4d
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 18:06:31 2026 +0800

    Add Go API test for FunASR Nano (#3025)

diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
index cdf3089b..58784eb0 100644
--- a/.github/workflows/test-go.yaml
+++ b/.github/workflows/test-go.yaml
@@ -103,6 +103,7 @@ jobs:
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-decode-files/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-medasr-ctc-decode-files
+            cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-funasr-nano-decode-files
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-speaker-diarization/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-tts/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/speaker-identification/
@@ -142,6 +143,19 @@ jobs:
           name: ${{ matrix.os }}-libs
           path: to-upload/
 
+      - name: Test non-streaming decoding files with FunASR Nano
+        shell: bash
+        run: |
+          cd scripts/go/_internal/non-streaming-funasr-nano-decode-files
+          ls -lh
+          go mod tidy
+          cat go.mod
+          go build
+          ls -lh
+
+          ./run.sh
+          rm -rf sherpa-onnx-funasr-*
+
       - name: Test non-streaming decoding files with MedASR
         shell: bash
         run: |
diff --git a/go-api-examples/non-streaming-funasr-nano-decode-files/go.mod b/go-api-examples/non-streaming-funasr-nano-decode-files/go.mod
new file mode 100644
index 00000000..510bd67a
--- /dev/null
+++ b/go-api-examples/non-streaming-funasr-nano-decode-files/go.mod
@@ -0,0 +1,3 @@
+module non-streaming-funasr-nano-decode-files
+
+go 1.17
diff --git a/go-api-examples/non-streaming-funasr-nano-decode-files/main.go b/go-api-examples/non-streaming-funasr-nano-decode-files/main.go
new file mode 100644
index 00000000..b2080e9f
--- /dev/null
+++ b/go-api-examples/non-streaming-funasr-nano-decode-files/main.go
@@ -0,0 +1,101 @@
+package main
+
+import (
+	"bytes"
+	"encoding/binary"
+	"log"
+	"os"
+	"strings"
+
+	sherpa "github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx"
+	"github.com/youpy/go-wav"
+)
+
+func main() {
+	log.SetFlags(log.LstdFlags | log.Lmicroseconds)
+
+	config := sherpa.OfflineRecognizerConfig{}
+
+	config.ModelConfig.FunAsrNano.EncoderAdaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx"
+	config.ModelConfig.FunAsrNano.LLM = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx"
+	config.ModelConfig.FunAsrNano.Embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx"
+	config.ModelConfig.FunAsrNano.Tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B"
+
+	config.ModelConfig.Tokens = ""
+
+	waveFilename := "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav"
+
+	samples, sampleRate := readWave(waveFilename)
+
+	log.Println("Initializing recognizer (may take several seconds)")
+	recognizer := sherpa.NewOfflineRecognizer(&config)
+	log.Println("Recognizer created!")
+	defer sherpa.DeleteOfflineRecognizer(recognizer)
+
+	log.Println("Start decoding!")
+	stream := sherpa.NewOfflineStream(recognizer)
+	defer sherpa.DeleteOfflineStream(stream)
+
+	stream.AcceptWaveform(sampleRate, samples)
+
+	recognizer.Decode(stream)
+	log.Println("Decoding done!")
+	result := stream.GetResult()
+
+	log.Println("Text: " + strings.ToLower(result.Text))
+}
+
+func readWave(filename string) (samples []float32, sampleRate int) {
+	file, _ := os.Open(filename)
+	defer file.Close()
+
+	reader := wav.NewReader(file)
+	format, err := reader.Format()
+	if err != nil {
+		log.Fatalf("Failed to read wave format")
+	}
+
+	if format.AudioFormat != 1 {
+		log.Fatalf("Support only PCM format. Given: %v\n", format.AudioFormat)
+	}
+
+	if format.NumChannels != 1 {
+		log.Fatalf("Support only 1 channel wave file. Given: %v\n", format.NumChannels)
+	}
+
+	if format.BitsPerSample != 16 {
+		log.Fatalf("Support only 16-bit per sample. Given: %v\n", format.BitsPerSample)
+	}
+
+	reader.Duration() // so that it initializes reader.Size
+
+	buf := make([]byte, reader.Size)
+	n, err := reader.Read(buf)
+	if n != int(reader.Size) {
+		log.Fatalf("Failed to read %v bytes. Returned %v bytes\n", reader.Size, n)
+	}
+
+	samples = samplesInt16ToFloat(buf)
+	sampleRate = int(format.SampleRate)
+
+	return
+}
+
+func samplesInt16ToFloat(inSamples []byte) []float32 {
+	numSamples := len(inSamples) / 2
+	outSamples := make([]float32, numSamples)
+
+	for i := 0; i != numSamples; i++ {
+		s := inSamples[i*2 : (i+1)*2]
+
+		var s16 int16
+		buf := bytes.NewReader(s)
+		err := binary.Read(buf, binary.LittleEndian, &s16)
+		if err != nil {
+			log.Fatal("Failed to parse 16-bit sample")
+		}
+		outSamples[i] = float32(s16) / 32768
+	}
+
+	return outSamples
+}
diff --git a/go-api-examples/non-streaming-funasr-nano-decode-files/run.sh b/go-api-examples/non-streaming-funasr-nano-decode-files/run.sh
new file mode 100755
index 00000000..469d7e9b
--- /dev/null
+++ b/go-api-examples/non-streaming-funasr-nano-decode-files/run.sh
@@ -0,0 +1,16 @@
+#!/usr/bin/env bash
+
+set -ex
+
+export CGO_ENABLED=1
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+go mod tidy
+go build
+
+./non-streaming-funasr-nano-decode-files
diff --git a/scripts/go/_internal/non-streaming-funasr-nano-decode-files/go.mod b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/go.mod
new file mode 100644
index 00000000..c0a90924
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/go.mod
@@ -0,0 +1,5 @@
+module non-streaming-funasr-nano-decode-files
+
+go 1.17
+
+replace github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx => ../
diff --git a/scripts/go/_internal/non-streaming-funasr-nano-decode-files/main.go b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/main.go
new file mode 120000
index 00000000..466f44a3
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/main.go
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-funasr-nano-decode-files/main.go
\ No newline at end of file
diff --git a/scripts/go/_internal/non-streaming-funasr-nano-decode-files/run.sh b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/run.sh
new file mode 120000
index 00000000..60fce65e
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-funasr-nano-decode-files/run.sh
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-funasr-nano-decode-files/run.sh
\ No newline at end of file
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 1f17094c..f03f4b40 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -454,7 +454,7 @@ type OfflineFireRedAsrModelConfig struct {
 
 type OfflineFunASRNanoModelConfig struct {
 	EncoderAdaptor string
-	Llm            string
+	LLM            string
 	Embedding      string
 	Tokenizer      string
 	SystemPrompt   string
@@ -497,7 +497,7 @@ type OfflineModelConfig struct {
 	SenseVoice   OfflineSenseVoiceModelConfig
 	Moonshine    OfflineMoonshineModelConfig
 	FireRedAsr   OfflineFireRedAsrModelConfig
-	FunASRNano   OfflineFunASRNanoModelConfig
+	FunAsrNano   OfflineFunASRNanoModelConfig
 	Dolphin      OfflineDolphinModelConfig
 	ZipformerCtc OfflineZipformerCtcModelConfig
 	Canary       OfflineCanaryModelConfig
@@ -596,16 +596,16 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 	c.model_config.fire_red_asr.encoder = C.CString(config.ModelConfig.FireRedAsr.Encoder)
 	c.model_config.fire_red_asr.decoder = C.CString(config.ModelConfig.FireRedAsr.Decoder)
 
-	c.model_config.funasr_nano.encoder_adaptor = C.CString(config.ModelConfig.FunASRNano.EncoderAdaptor)
-	c.model_config.funasr_nano.llm = C.CString(config.ModelConfig.FunASRNano.Llm)
-	c.model_config.funasr_nano.embedding = C.CString(config.ModelConfig.FunASRNano.Embedding)
-	c.model_config.funasr_nano.tokenizer = C.CString(config.ModelConfig.FunASRNano.Tokenizer)
-	c.model_config.funasr_nano.system_prompt = C.CString(config.ModelConfig.FunASRNano.SystemPrompt)
-	c.model_config.funasr_nano.user_prompt = C.CString(config.ModelConfig.FunASRNano.UserPrompt)
-	c.model_config.funasr_nano.max_new_tokens = C.int(config.ModelConfig.FunASRNano.MaxNewTokens)
-	c.model_config.funasr_nano.temperature = C.float(config.ModelConfig.FunASRNano.Temperature)
-	c.model_config.funasr_nano.top_p = C.float(config.ModelConfig.FunASRNano.TopP)
-	c.model_config.funasr_nano.seed = C.int(config.ModelConfig.FunASRNano.Seed)
+	c.model_config.funasr_nano.encoder_adaptor = C.CString(config.ModelConfig.FunAsrNano.EncoderAdaptor)
+	c.model_config.funasr_nano.llm = C.CString(config.ModelConfig.FunAsrNano.LLM)
+	c.model_config.funasr_nano.embedding = C.CString(config.ModelConfig.FunAsrNano.Embedding)
+	c.model_config.funasr_nano.tokenizer = C.CString(config.ModelConfig.FunAsrNano.Tokenizer)
+	c.model_config.funasr_nano.system_prompt = C.CString(config.ModelConfig.FunAsrNano.SystemPrompt)
+	c.model_config.funasr_nano.user_prompt = C.CString(config.ModelConfig.FunAsrNano.UserPrompt)
+	c.model_config.funasr_nano.max_new_tokens = C.int(config.ModelConfig.FunAsrNano.MaxNewTokens)
+	c.model_config.funasr_nano.temperature = C.float(config.ModelConfig.FunAsrNano.Temperature)
+	c.model_config.funasr_nano.top_p = C.float(config.ModelConfig.FunAsrNano.TopP)
+	c.model_config.funasr_nano.seed = C.int(config.ModelConfig.FunAsrNano.Seed)
 
 	c.model_config.dolphin.model = C.CString(config.ModelConfig.Dolphin.Model)
 	c.model_config.zipformer_ctc.model = C.CString(config.ModelConfig.ZipformerCtc.Model)

commit 54374075683c8d16286d4147ef95a622db3ac091
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Mon Jan 12 16:40:37 2026 +0800

    swift: add FunASR nano Swift API (#3022)

diff --git a/.github/scripts/test-swift.sh b/.github/scripts/test-swift.sh
index c150b716..3b3cac00 100755
--- a/.github/scripts/test-swift.sh
+++ b/.github/scripts/test-swift.sh
@@ -12,6 +12,9 @@ ls -lh
 ./run-medasr-ctc-asr.sh
 rm -rf sherpa-onnx-medasr-*
 
+./run-funasr-nano-asr.sh
+rm -rf sherpa-onnx-funasr-nano-*
+
 ./run-omnilingual-asr-ctc-asr.sh
 rm -rf sherpa-onnx-omnilingual-*
 
diff --git a/.gitignore b/.gitignore
index 293813ac..2c6ecd81 100755
--- a/.gitignore
+++ b/.gitignore
@@ -166,6 +166,7 @@ spacemit-toolchain*
 sherpa-onnx-qnn-*
 matcha-icefall-*
 sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+sherpa-onnx-funasr-nano-int8-2025-12-30
 *.raw
 *-input-list.txt
 sherpa-onnx-funasr-nano*2025-12-30
diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index 24b4d34f..aaa2b633 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -488,21 +488,19 @@ func sherpaOnnxOfflineLMConfig(
 
 func sherpaOnnxOfflineFunASRNanoModelConfig(
   encoderAdaptor: String = "",
-  llmPrefill: String = "",
-  llmDecode: String = "",
+  llm: String = "",
   embedding: String = "",
   tokenizer: String = "",
   systemPrompt: String = "You are a helpful assistant.",
   userPrompt: String = "",
   maxNewTokens: Int = 512,
-  temperature: Float = 0.3,
+  temperature: Float = 1e-6,
   topP: Float = 0.8,
   seed: Int = 42
 ) -> SherpaOnnxOfflineFunASRNanoModelConfig {
   return SherpaOnnxOfflineFunASRNanoModelConfig(
     encoder_adaptor: toCPointer(encoderAdaptor),
-    llm_prefill: toCPointer(llmPrefill),
-    llm_decode: toCPointer(llmDecode),
+    llm: toCPointer(llm),
     embedding: toCPointer(embedding),
     tokenizer: toCPointer(tokenizer),
     system_prompt: toCPointer(systemPrompt),
@@ -539,10 +537,10 @@ func sherpaOnnxOfflineModelConfig(
     sherpaOnnxOfflineWenetCtcModelConfig(),
   omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig =
     sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(),
-  funasrNano: SherpaOnnxOfflineFunASRNanoModelConfig =
-    sherpaOnnxOfflineFunASRNanoModelConfig(),
   medasr: SherpaOnnxOfflineMedAsrCtcModelConfig =
-    sherpaOnnxOfflineMedAsrCtcModelConfig()
+    sherpaOnnxOfflineMedAsrCtcModelConfig(),
+  funasrNano: SherpaOnnxOfflineFunASRNanoModelConfig =
+    sherpaOnnxOfflineFunASRNanoModelConfig()
 ) -> SherpaOnnxOfflineModelConfig {
   return SherpaOnnxOfflineModelConfig(
     transducer: transducer,
@@ -566,8 +564,8 @@ func sherpaOnnxOfflineModelConfig(
     canary: canary,
     wenet_ctc: wenetCtc,
     omnilingual: omnilingual,
-    funasr_nano: funasrNano,
-    medasr: medasr
+    medasr: medasr,
+    funasr_nano: funasrNano
   )
 }
 
diff --git a/swift-api-examples/funasr-nano.swift b/swift-api-examples/funasr-nano.swift
new file mode 100644
index 00000000..a53be519
--- /dev/null
+++ b/swift-api-examples/funasr-nano.swift
@@ -0,0 +1,50 @@
+func run() {
+  let encoderAdaptor =
+    "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx"
+  let llm =
+    "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx"
+  let embedding =
+    "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx"
+  let tokenizer =
+    "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B"
+
+  let funasrNano = sherpaOnnxOfflineFunASRNanoModelConfig(
+    encoderAdaptor: encoderAdaptor,
+    llm: llm,
+    embedding: embedding,
+    tokenizer: tokenizer
+  )
+
+  let modelConfig = sherpaOnnxOfflineModelConfig(
+    tokens: "",
+    debug: 1,
+    funasrNano: funasrNano
+  )
+
+  let featConfig = sherpaOnnxFeatureConfig()
+  var config = sherpaOnnxOfflineRecognizerConfig(
+    featConfig: featConfig,
+    modelConfig: modelConfig
+  )
+
+  let recognizer = SherpaOnnxOfflineRecognizer(config: &config)
+
+  let filePath = "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/lyrics.wav"
+  let audio = SherpaOnnxWaveWrapper.readWave(filename: filePath)
+
+  let result = recognizer.decode(samples: audio.samples, sampleRate: audio.sampleRate)
+  print("decode done")
+
+  print("\nresult is:\n\(result.text)")
+  if !result.timestamps.isEmpty {
+    print("\ntimestamps is:\n\(result.timestamps)")
+  }
+}
+
+@main
+struct App {
+  static func main() {
+    run()
+  }
+}
+
diff --git a/swift-api-examples/run-funasr-nano-asr.sh b/swift-api-examples/run-funasr-nano-asr.sh
new file mode 100755
index 00000000..6ad8c21f
--- /dev/null
+++ b/swift-api-examples/run-funasr-nano-asr.sh
@@ -0,0 +1,35 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -d ../build-swift-macos ]; then
+  echo "Please run ../build-swift-macos.sh first!"
+  exit 1
+fi
+
+if [ ! -f ./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+  rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+fi
+
+if [ ! -e ./funasr-nano ]; then
+  # Note: We use -lc++ to link against libc++ instead of libstdc++
+  swiftc \
+    -lc++ \
+    -I ../build-swift-macos/install/include \
+    -import-objc-header ./SherpaOnnx-Bridging-Header.h \
+    ./funasr-nano.swift  ./SherpaOnnx.swift \
+    -L ../build-swift-macos/install/lib/ \
+    -l sherpa-onnx \
+    -l onnxruntime \
+    -o funasr-nano
+
+  strip funasr-nano
+else
+  echo "./funasr-nano exists - skip building"
+fi
+
+export DYLD_LIBRARY_PATH=$PWD/../build-swift-macos/install/lib:$DYLD_LIBRARY_PATH
+./funasr-nano
+

commit b13feaa3b6e63abaa3e5327d8d7f914b3e4d7ffe
Author: Licardo <1014660822@qq.com>
Date:   Mon Jan 12 14:17:51 2026 +0800

    [feature] add FunASRNano Swift api (#2994)

diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index 8c7993e8..24b4d34f 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -486,6 +486,34 @@ func sherpaOnnxOfflineLMConfig(
   )
 }
 
+func sherpaOnnxOfflineFunASRNanoModelConfig(
+  encoderAdaptor: String = "",
+  llmPrefill: String = "",
+  llmDecode: String = "",
+  embedding: String = "",
+  tokenizer: String = "",
+  systemPrompt: String = "You are a helpful assistant.",
+  userPrompt: String = "",
+  maxNewTokens: Int = 512,
+  temperature: Float = 0.3,
+  topP: Float = 0.8,
+  seed: Int = 42
+) -> SherpaOnnxOfflineFunASRNanoModelConfig {
+  return SherpaOnnxOfflineFunASRNanoModelConfig(
+    encoder_adaptor: toCPointer(encoderAdaptor),
+    llm_prefill: toCPointer(llmPrefill),
+    llm_decode: toCPointer(llmDecode),
+    embedding: toCPointer(embedding),
+    tokenizer: toCPointer(tokenizer),
+    system_prompt: toCPointer(systemPrompt),
+    user_prompt: toCPointer(userPrompt),
+    max_new_tokens: Int32(maxNewTokens),
+    temperature: temperature,
+    top_p: topP,
+    seed: Int32(seed)
+  )
+}
+
 func sherpaOnnxOfflineModelConfig(
   tokens: String,
   transducer: SherpaOnnxOfflineTransducerModelConfig = sherpaOnnxOfflineTransducerModelConfig(),
@@ -511,6 +539,8 @@ func sherpaOnnxOfflineModelConfig(
     sherpaOnnxOfflineWenetCtcModelConfig(),
   omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig =
     sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(),
+  funasrNano: SherpaOnnxOfflineFunASRNanoModelConfig =
+    sherpaOnnxOfflineFunASRNanoModelConfig(),
   medasr: SherpaOnnxOfflineMedAsrCtcModelConfig =
     sherpaOnnxOfflineMedAsrCtcModelConfig()
 ) -> SherpaOnnxOfflineModelConfig {
@@ -536,6 +566,7 @@ func sherpaOnnxOfflineModelConfig(
     canary: canary,
     wenet_ctc: wenetCtc,
     omnilingual: omnilingual,
+    funasr_nano: funasrNano,
     medasr: medasr
   )
 }

commit 74c83c986af912add4773b52f529872784aa1a3c
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Jan 12 14:16:58 2026 +0800

    Update CI test for FunASR Nano C/C++ API (#3021)

diff --git a/.github/workflows/c-api.yaml b/.github/workflows/c-api.yaml
index d71e63d7..7df92f60 100644
--- a/.github/workflows/c-api.yaml
+++ b/.github/workflows/c-api.yaml
@@ -75,6 +75,36 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-c-api.dylib
           fi
 
+      - name: Test FunASR Nano
+        shell: bash
+        run: |
+          name=funasr-nano-c-api
+          gcc -o $name ./c-api-examples/$name.c \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+          tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+          rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm $name
+          rm -rf sherpa-onnx-funasr-*
+
       - name: Test MedASR CTC
         shell: bash
         run: |
diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
index 4f741a41..5ae9367d 100644
--- a/.github/workflows/cxx-api.yaml
+++ b/.github/workflows/cxx-api.yaml
@@ -78,6 +78,40 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
           fi
 
+      - name: Test FunASR Nano
+        shell: bash
+        run: |
+          name=funasr-nano-cxx-api
+          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-cxx-api \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ls -lh ./$name
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+          tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+          rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+
+          echo "---"
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm -rf sherpa-onnx-funasr-*
+          rm -v ./$name
+
       - name: Test MedASR CTC
         shell: bash
         run: |
diff --git a/.gitignore b/.gitignore
index 1989ce74..293813ac 100755
--- a/.gitignore
+++ b/.gitignore
@@ -168,3 +168,4 @@ matcha-icefall-*
 sherpa-onnx-medasr-ctc-en-int8-2025-12-25
 *.raw
 *-input-list.txt
+sherpa-onnx-funasr-nano*2025-12-30
diff --git a/c-api-examples/CMakeLists.txt b/c-api-examples/CMakeLists.txt
index f73e7f40..98bd4eb4 100644
--- a/c-api-examples/CMakeLists.txt
+++ b/c-api-examples/CMakeLists.txt
@@ -68,6 +68,9 @@ target_link_libraries(nemo-parakeet-c-api sherpa-onnx-c-api)
 add_executable(sense-voice-c-api sense-voice-c-api.c)
 target_link_libraries(sense-voice-c-api sherpa-onnx-c-api)
 
+add_executable(funasr-nano-c-api funasr-nano-c-api.c)
+target_link_libraries(funasr-nano-c-api sherpa-onnx-c-api)
+
 add_executable(sense-voice-with-hr-c-api sense-voice-with-hr-c-api.c)
 target_link_libraries(sense-voice-with-hr-c-api sherpa-onnx-c-api)
 
diff --git a/c-api-examples/funasr-nano-c-api.c b/c-api-examples/funasr-nano-c-api.c
new file mode 100644
index 00000000..a8e115f1
--- /dev/null
+++ b/c-api-examples/funasr-nano-c-api.c
@@ -0,0 +1,83 @@
+// c-api-examples/funasr-nano-c-api.c
+//
+// Copyright (c)  2026  Xiaomi Corporation
+
+//
+// This file demonstrates how to use FunASR Nano with sherpa-onnx's C API.
+// clang-format off
+//
+// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+// tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+// rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+//
+// clang-format on
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include "sherpa-onnx/c-api/c-api.h"
+
+int32_t main() {
+  // clang-format off
+  const char *wav_filename = "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/dia_yue.wav";
+  const char *encoder_adaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx";
+  const char *embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx";
+  const char *llm = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx";
+  const char *tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B";
+  // clang-format on
+
+  const SherpaOnnxWave *wave = SherpaOnnxReadWave(wav_filename);
+  if (wave == NULL) {
+    fprintf(stderr, "Failed to read %s\n", wav_filename);
+    return -1;
+  }
+
+  SherpaOnnxOfflineFunASRNanoModelConfig funasr_nano;
+  memset(&funasr_nano, 0, sizeof(funasr_nano));
+  funasr_nano.encoder_adaptor = encoder_adaptor;
+  funasr_nano.embedding = embedding;
+  funasr_nano.llm = llm;
+  funasr_nano.tokenizer = tokenizer;
+
+  // Offline model config
+  SherpaOnnxOfflineModelConfig offline_model_config;
+  memset(&offline_model_config, 0, sizeof(offline_model_config));
+  offline_model_config.debug = 1;
+  offline_model_config.num_threads = 2;
+  offline_model_config.provider = "cpu";
+  offline_model_config.funasr_nano = funasr_nano;
+
+  // Recognizer config
+  SherpaOnnxOfflineRecognizerConfig recognizer_config;
+  memset(&recognizer_config, 0, sizeof(recognizer_config));
+  recognizer_config.decoding_method = "greedy_search";
+  recognizer_config.model_config = offline_model_config;
+
+  const SherpaOnnxOfflineRecognizer *recognizer =
+      SherpaOnnxCreateOfflineRecognizer(&recognizer_config);
+
+  if (recognizer == NULL) {
+    fprintf(stderr, "Please check your config!\n");
+    SherpaOnnxFreeWave(wave);
+    return -1;
+  }
+
+  const SherpaOnnxOfflineStream *stream =
+      SherpaOnnxCreateOfflineStream(recognizer);
+
+  SherpaOnnxAcceptWaveformOffline(stream, wave->sample_rate, wave->samples,
+                                  wave->num_samples);
+  SherpaOnnxDecodeOfflineStream(recognizer, stream);
+  const SherpaOnnxOfflineRecognizerResult *result =
+      SherpaOnnxGetOfflineStreamResult(stream);
+
+  fprintf(stderr, "Decoded text: %s\n", result->text);
+
+  SherpaOnnxDestroyOfflineRecognizerResult(result);
+  SherpaOnnxDestroyOfflineStream(stream);
+  SherpaOnnxDestroyOfflineRecognizer(recognizer);
+  SherpaOnnxFreeWave(wave);
+
+  return 0;
+}
diff --git a/cxx-api-examples/funasr-nano-cxx-api.cc b/cxx-api-examples/funasr-nano-cxx-api.cc
index 6b06ec47..8926fd81 100644
--- a/cxx-api-examples/funasr-nano-cxx-api.cc
+++ b/cxx-api-examples/funasr-nano-cxx-api.cc
@@ -4,15 +4,14 @@
 //
 // This file demonstrates how to use FunASR-nano with sherpa-onnx's C++ API.
 //
+//
 // clang-format off
 //
-// Example usage:
-//   ./bin/funasr-nano-cxx-api \
-//     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-//     --funasr-nano-llm=/path/to/llm.onnx \
-//     --funasr-nano-embedding=/path/to/embedding.onnx \
-//     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
-//     /path/to/audio.wav
+// Usage:
+//
+// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+// tar xvf sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+// rm sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
 //
 // clang-format on
 
@@ -26,170 +25,61 @@
 int32_t main(int32_t argc, char *argv[]) {
   using namespace sherpa_onnx::cxx;
 
-  const char *kUsageMessage = R"usage(
-FunASR-nano speech recognition example using sherpa-onnx C++ API.
-
-Usage:
-  ./bin/funasr-nano-cxx-api \
-    --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-    --funasr-nano-llm=/path/to/llm.onnx \
-    --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
-    --funasr-nano-embedding=/path/to/embedding.onnx \
-    [--funasr-nano-user-prompt=""] \
-    [--funasr-nano-max-new-tokens=512] \
-    [--funasr-nano-temperature=1e-6] \
-    [--funasr-nano-top-p=0.8] \
-    /path/to/audio.wav
-
-Required arguments:
-  --funasr-nano-encoder-adaptor: Path to encoder_adaptor.onnx
-  --funasr-nano-llm: Path to llm.onnx (unified KV cache model)
-  --funasr-nano-tokenizer: Path to tokenizer directory (e.g., Qwen3-0.6B)
-  --funasr-nano-embedding: Path to embedding.onnx
-
-Optional arguments:
-  --funasr-nano-user-prompt: User prompt template (default: "")
-  --funasr-nano-max-new-tokens: Maximum tokens to generate (default: 512)
-  --funasr-nano-temperature: Sampling temperature (default: 1e-6)
-  --funasr-nano-top-p: Top-p sampling threshold (default: 0.8)
-  --num-threads: Number of threads (default: 2)
-  --provider: cpu (default) or cuda
-
-Example:
-  ./bin/funasr-nano-cxx-api \
-    --funasr-nano-encoder-adaptor=./models/encoder_adaptor.onnx \
-    --funasr-nano-llm=./models/llm.onnx \
-    --funasr-nano-tokenizer=./models/Qwen3-0.6B \
-    --funasr-nano-embedding=./models/embedding.onnx \
-    ./test.wav
-)usage";
-
-  if (argc < 6) {
-    std::cerr << kUsageMessage << "\n";
-    return -1;
-  }
-
   OfflineRecognizerConfig config;
   config.model_config.num_threads = 2;
   config.model_config.debug = false;
   config.model_config.provider = "cpu";
 
-  // Parse command line arguments
-  const char kEncoderAdaptor[] = "--funasr-nano-encoder-adaptor=";
-  const char kLlm[] = "--funasr-nano-llm=";
-  const char kEmbedding[] = "--funasr-nano-embedding=";
-  const char kTokenizer[] = "--funasr-nano-tokenizer=";
-  const char kUserPrompt[] = "--funasr-nano-user-prompt=";
-  const char kMaxNewTokens[] = "--funasr-nano-max-new-tokens=";
-  const char kTemperature[] = "--funasr-nano-temperature=";
-  const char kTopP[] = "--funasr-nano-top-p=";
-  const char kNumThreads[] = "--num-threads=";
-  const char kProvider[] = "--provider=";
-
-  for (int32_t i = 1; i < argc; ++i) {
-    std::string arg = argv[i];
-    if (arg.find(kEncoderAdaptor) == 0) {
-      config.model_config.funasr_nano.encoder_adaptor =
-          arg.substr(sizeof(kEncoderAdaptor) - 1);
-    } else if (arg.find(kLlm) == 0) {
-      config.model_config.funasr_nano.llm =
-          arg.substr(sizeof(kLlm) - 1);
-    } else if (arg.find(kEmbedding) == 0) {
-      config.model_config.funasr_nano.embedding =
-          arg.substr(sizeof(kEmbedding) - 1);
-    } else if (arg.find(kTokenizer) == 0) {
-      config.model_config.funasr_nano.tokenizer =
-          arg.substr(sizeof(kTokenizer) - 1);
-    } else if (arg.find(kUserPrompt) == 0) {
-      config.model_config.funasr_nano.user_prompt =
-          arg.substr(sizeof(kUserPrompt) - 1);
-    } else if (arg.find(kMaxNewTokens) == 0) {
-      config.model_config.funasr_nano.max_new_tokens =
-          std::stoi(arg.substr(sizeof(kMaxNewTokens) - 1));
-    } else if (arg.find(kTemperature) == 0) {
-      config.model_config.funasr_nano.temperature =
-          std::stof(arg.substr(sizeof(kTemperature) - 1));
-    } else if (arg.find(kTopP) == 0) {
-      config.model_config.funasr_nano.top_p =
-          std::stof(arg.substr(sizeof(kTopP) - 1));
-    } else if (arg.find(kNumThreads) == 0) {
-      config.model_config.num_threads =
-          std::stoi(arg.substr(sizeof(kNumThreads) - 1));
-    } else if (arg.find(kProvider) == 0) {
-      config.model_config.provider = arg.substr(sizeof(kProvider) - 1);
-    } else if (arg[0] != '-') {
-      // This should be the audio file
-      std::string wave_filename = arg;
-
-      std::cout << "Loading model...\n";
-      std::cout << "  encoder_adaptor: "
-                << config.model_config.funasr_nano.encoder_adaptor << "\n";
-      std::cout << "  llm: "
-                << config.model_config.funasr_nano.llm << "\n";
-      std::cout << "  tokenizer: " << config.model_config.funasr_nano.tokenizer
-                << "\n";
-      std::cout << "  embedding: "
-                << config.model_config.funasr_nano.embedding << "\n";
-
-      const auto begin_init = std::chrono::steady_clock::now();
-
-      OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
-      if (!recognizer.Get()) {
-        std::cerr << "Failed to create recognizer. Please check your config.\n";
-        return -1;
-      }
-
-      const auto end_init = std::chrono::steady_clock::now();
-      float elapsed_seconds_init =
-          std::chrono::duration_cast<std::chrono::milliseconds>(end_init -
-                                                                begin_init)
-              .count() /
-          1000.;
-      std::cout << "Model loaded in " << elapsed_seconds_init << " seconds\n";
-
-      Wave wave = ReadWave(wave_filename);
-      if (wave.samples.empty()) {
-        std::cerr << "Failed to read: '" << wave_filename << "'\n";
-        return -1;
-      }
-
-      std::cout << "Audio file: " << wave_filename << "\n";
-      std::cout << "Sample rate: " << wave.sample_rate << " Hz\n";
-      std::cout << "Duration: "
-                << wave.samples.size() / static_cast<float>(wave.sample_rate)
-                << " seconds\n";
-
-      std::cout << "\nStart recognition...\n";
-      const auto begin = std::chrono::steady_clock::now();
-
-      OfflineStream stream = recognizer.CreateStream();
-      stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
-                            wave.samples.size());
-
-      recognizer.Decode(&stream);
-
-      OfflineRecognizerResult result = recognizer.GetResult(&stream);
-
-      const auto end = std::chrono::steady_clock::now();
-      const float elapsed_seconds =
-          std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
-              .count() /
-          1000.;
-      float duration =
-          wave.samples.size() / static_cast<float>(wave.sample_rate);
-      float rtf = elapsed_seconds / duration;
-
-      std::cout << "Text: " << result.text << "\n";
-      std::cout << "Audio duration: " << duration << "s\n";
-      std::cout << "Processing time: " << elapsed_seconds << "s\n";
-      std::cout << "Real-time factor (RTF): " << rtf << "\n";
-
-      return 0;
-    }
+  // clang-format off
+  config.model_config.funasr_nano.encoder_adaptor = "./sherpa-onnx-funasr-nano-int8-2025-12-30/encoder_adaptor.int8.onnx";
+  config.model_config.funasr_nano.llm = "./sherpa-onnx-funasr-nano-int8-2025-12-30/llm.int8.onnx";
+  config.model_config.funasr_nano.embedding = "./sherpa-onnx-funasr-nano-int8-2025-12-30/embedding.int8.onnx";
+  config.model_config.funasr_nano.tokenizer = "./sherpa-onnx-funasr-nano-int8-2025-12-30/Qwen3-0.6B";
+
+  // clang-format on
+
+  std::cout << "Loading model\n";
+  OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
+  if (!recognizer.Get()) {
+    std::cerr << "Please check your config\n";
+    return -1;
   }
+  std::cout << "Loading model done\n";
 
-  std::cerr << "Error: Please provide an audio file.\n";
-  std::cerr << kUsageMessage << "\n";
-  return -1;
-}
+  std::string wave_filename =
+      "./sherpa-onnx-funasr-nano-int8-2025-12-30/test_wavs/dia_yue.wav";
 
+  Wave wave = ReadWave(wave_filename);
+  if (wave.samples.empty()) {
+    std::cerr << "Failed to read: '" << wave_filename << "'\n";
+    return -1;
+  }
+
+  std::cout << "Start recognition\n";
+  const auto begin = std::chrono::steady_clock::now();
+
+  OfflineStream stream = recognizer.CreateStream();
+  stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
+                        wave.samples.size());
+
+  recognizer.Decode(&stream);
+
+  OfflineRecognizerResult result = recognizer.GetResult(&stream);
+
+  const auto end = std::chrono::steady_clock::now();
+  const float elapsed_seconds =
+      std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
+          .count() /
+      1000.;
+  float duration = wave.samples.size() / static_cast<float>(wave.sample_rate);
+  float rtf = elapsed_seconds / duration;
+
+  std::cout << "text: " << result.text << "\n";
+  printf("Number of threads: %d\n", config.model_config.num_threads);
+  printf("Duration: %.3fs\n", duration);
+  printf("Elapsed seconds: %.3fs\n", elapsed_seconds);
+  printf("(Real time factor) RTF = %.3f / %.3f = %.3f\n", elapsed_seconds,
+         duration, rtf);
+
+  return 0;
+}
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index ef12a60c..1958be85 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -511,6 +511,9 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.omnilingual.model =
       SHERPA_ONNX_OR(config->model_config.omnilingual.model, "");
 
+  recognizer_config.model_config.medasr.model =
+      SHERPA_ONNX_OR(config->model_config.medasr.model, "");
+
   recognizer_config.model_config.funasr_nano.encoder_adaptor =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.encoder_adaptor, "");
   recognizer_config.model_config.funasr_nano.llm =
@@ -522,9 +525,8 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.funasr_nano.system_prompt =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.system_prompt,
                      "You are a helpful assistant.");
-  recognizer_config.model_config.funasr_nano.user_prompt =
-      SHERPA_ONNX_OR(config->model_config.funasr_nano.user_prompt,
-                     "");
+  recognizer_config.model_config.funasr_nano.user_prompt = SHERPA_ONNX_OR(
+      config->model_config.funasr_nano.user_prompt, "");
   recognizer_config.model_config.funasr_nano.max_new_tokens =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.max_new_tokens, 512);
   recognizer_config.model_config.funasr_nano.temperature =
@@ -533,8 +535,6 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
       SHERPA_ONNX_OR(config->model_config.funasr_nano.top_p, 0.8f);
   recognizer_config.model_config.funasr_nano.seed =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.seed, 42);
-  recognizer_config.model_config.medasr.model =
-      SHERPA_ONNX_OR(config->model_config.medasr.model, "");
 
   recognizer_config.lm_config.model =
       SHERPA_ONNX_OR(config->lm_config.model, "");
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 337f3855..b84bdac6 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -527,8 +527,8 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineCanaryModelConfig canary;
   SherpaOnnxOfflineWenetCtcModelConfig wenet_ctc;
   SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
-  SherpaOnnxOfflineFunASRNanoModelConfig funasr_nano;
   SherpaOnnxOfflineMedAsrCtcModelConfig medasr;
+  SherpaOnnxOfflineFunASRNanoModelConfig funasr_nano;
 } SherpaOnnxOfflineModelConfig;
 
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerConfig {
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index e1291fc6..e1981f0e 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -319,8 +319,8 @@ struct SHERPA_ONNX_API OfflineModelConfig {
   OfflineCanaryModelConfig canary;
   OfflineWenetCtcModelConfig wenet_ctc;
   OfflineOmnilingualAsrCtcModelConfig omnilingual;
-  OfflineFunASRNanoModelConfig funasr_nano;
   OfflineMedAsrCtcModelConfig medasr;
+  OfflineFunASRNanoModelConfig funasr_nano;
 };
 
 struct SHERPA_ONNX_API OfflineLMConfig {

commit 408b96f08a0ab19e5041fd8ea69a30b928577c73
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Sun Jan 11 14:45:44 2026 +0800

    Use onnxruntime 1.23.2 for Linux x64 + NVIDIA GPU (#3018)

diff --git a/.github/workflows/build-wheels-linux-cuda.yaml b/.github/workflows/build-wheels-linux-cuda.yaml
index 3e360466..a1184b2d 100644
--- a/.github/workflows/build-wheels-linux-cuda.yaml
+++ b/.github/workflows/build-wheels-linux-cuda.yaml
@@ -22,7 +22,7 @@ jobs:
       matrix:
         os: [ubuntu-22.04]
         python-version: ["3.7", "3.8", "3.9", "3.10", "3.11", "3.12", "3.13"]
-        onnxruntime_version: ["1.17.1", "1.22.0"]
+        onnxruntime_version: ["1.17.1", "1.23.2"]
 
     steps:
       - uses: actions/checkout@v4
@@ -74,12 +74,13 @@ jobs:
           export SHERPA_ONNX_CMAKE_ARGS="-DSHERPA_ONNX_ENABLE_GPU=ON"
 
           onnxruntime_version=${{ matrix.onnxruntime_version }}
-          if [[ $onnxruntime_version == "1.22.0" ]]; then
-            curl -SL -O https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.22.0/onnxruntime-linux-x64-gpu-1.22.0-patched.zip
-            unzip  onnxruntime-linux-x64-gpu-1.22.0-patched.zip
+          curl -SL -O https://github.com/csukuangfj/onnxruntime-libs/releases/download/v$onnxruntime_version/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched.zip
+          unzip  onnxruntime-linux-x64-gpu-$onnxruntime_version-patched.zip
 
-            export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-linux-x64-gpu-1.22.0-patched/lib
-            export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-linux-x64-gpu-1.22.0-patched/include
+          export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched/lib
+          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched/include
+
+          if [[ $onnxruntime_version == "1.23.2" ]]; then
             export SHERPA_ONNX_CUDA_VERSION="12.cudnn9"
           fi
 
diff --git a/.github/workflows/build-wheels-win64-cuda.yaml b/.github/workflows/build-wheels-win64-cuda.yaml
index dcbfebb2..cde68f84 100644
--- a/.github/workflows/build-wheels-win64-cuda.yaml
+++ b/.github/workflows/build-wheels-win64-cuda.yaml
@@ -22,7 +22,7 @@ jobs:
       matrix:
         os: [windows-2022]
         python-version: ["3.7", "3.8", "3.9", "3.10", "3.11", "3.12", "3.13"]
-        onnxruntime_version: ["1.17.1", "1.22.0"]
+        onnxruntime_version: ["1.17.1", "1.23.2"]
 
     steps:
       - uses: actions/checkout@v4
@@ -46,13 +46,13 @@ jobs:
           export SHERPA_ONNX_CMAKE_ARGS="-DSHERPA_ONNX_ENABLE_GPU=ON"
 
           onnxruntime_version=${{ matrix.onnxruntime_version }}
-          if [[ $onnxruntime_version == "1.22.0" ]]; then
-            curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v1.22.0/onnxruntime-win-x64-gpu-1.22.0.zip
-            unzip onnxruntime-win-x64-gpu-1.22.0.zip
+          curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v$onnxruntime_version/onnxruntime-win-x64-gpu-$onnxruntime_version.zip
+          unzip onnxruntime-win-x64-gpu-$onnxruntime_version.zip
 
-            export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/lib
-            export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/include
+          export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-win-x64-gpu-$onnxruntime_version/lib
+          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-win-x64-gpu-$onnxruntime_version/include
 
+          if [[ $onnxruntime_version == "1.23.2" ]]; then
             export SHERPA_ONNX_CUDA_VERSION="12.cudnn9"
           fi
 
diff --git a/.github/workflows/jar.yaml b/.github/workflows/jar.yaml
index 23725abc..510897b7 100644
--- a/.github/workflows/jar.yaml
+++ b/.github/workflows/jar.yaml
@@ -101,7 +101,7 @@ jobs:
           dst=sherpa-onnx/java-api/resources/sherpa-onnx/native/osx-aarch64
 
           mkdir -p $dst
-          cp -v $src/lib/libonnxruntime.1.17.1.dylib $dst/
+          cp -v $src/lib/libonnxruntime.1.23.2.dylib $dst/
           cp -v $src/lib/libsherpa-onnx-jni.dylib $dst/
 
           ls -lh $dst
@@ -119,7 +119,7 @@ jobs:
           dst=sherpa-onnx/java-api/resources/sherpa-onnx/native/osx-x64
 
           mkdir -p $dst
-          cp -v $src/lib/libonnxruntime.1.17.1.dylib $dst/
+          cp -v $src/lib/libonnxruntime.1.23.2.dylib $dst/
           cp -v $src/lib/libsherpa-onnx-jni.dylib $dst/
 
           ls -lh $dst
diff --git a/.github/workflows/linux-gpu.yaml b/.github/workflows/linux-gpu.yaml
index 632336c0..6da0b18f 100644
--- a/.github/workflows/linux-gpu.yaml
+++ b/.github/workflows/linux-gpu.yaml
@@ -35,7 +35,7 @@ jobs:
         os: [ubuntu-latest]
         # build_type: [Release, Debug]
         build_type: [Release]
-        onnxruntime_version: ["1.17.1", "1.22.0"]
+        onnxruntime_version: ["1.17.1", "1.23.2"]
 
     steps:
       - uses: actions/checkout@v4
@@ -66,15 +66,13 @@ jobs:
               cd /home/runner/work/sherpa-onnx/sherpa-onnx
 
               onnxruntime_version=${{ matrix.onnxruntime_version }}
-              if [[ $onnxruntime_version == "1.22.0" ]]; then
-                curl -SL -O https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.22.0/onnxruntime-linux-x64-gpu-1.22.0-patched.zip
-                unzip  onnxruntime-linux-x64-gpu-1.22.0-patched.zip
+              curl -SL -O https://github.com/csukuangfj/onnxruntime-libs/releases/download/v$onnxruntime_version/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched.zip
+              unzip  onnxruntime-linux-x64-gpu-$onnxruntime_version-patched.zip
 
-                export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-linux-x64-gpu-1.22.0-patched/lib
-                export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-linux-x64-gpu-1.22.0-patched/include
+              export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched/lib
+              export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched/include
 
-                ls -lh /home/runner/work/sherpa-onnx/sherpa-onnx/onnxruntime-linux-x64-gpu-1.22.0-patched/lib/libonnxruntime.so
-              fi
+              ls -lh /home/runner/work/sherpa-onnx/sherpa-onnx/onnxruntime-linux-x64-gpu-$onnxruntime_version-patched/lib/libonnxruntime.so
 
               git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
               pushd alsa-lib
@@ -144,7 +142,7 @@ jobs:
           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-linux-x64-gpu
 
           onnxruntime_version=${{ matrix.onnxruntime_version }}
-          if [[ $onnxruntime_version == "1.22.0" ]]; then
+          if [[ $onnxruntime_version == "1.23.2" ]]; then
             dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-cuda-12.x-cudnn-9.x-linux-x64-gpu
           fi
 
diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
index 8bb715df..cdf3089b 100644
--- a/.github/workflows/test-go.yaml
+++ b/.github/workflows/test-go.yaml
@@ -124,7 +124,7 @@ jobs:
             rm ./lib/*.a
             rm ./lib/libonnxruntime.dylib
             cd lib
-            ln -s libonnxruntime.1.17.1.dylib libonnxruntime.dylib
+            ln -s libonnxruntime.1.23.2.dylib libonnxruntime.dylib
             cd ..
           fi
 
diff --git a/.github/workflows/windows-x64-cuda.yaml b/.github/workflows/windows-x64-cuda.yaml
index 7c537162..97dd0144 100644
--- a/.github/workflows/windows-x64-cuda.yaml
+++ b/.github/workflows/windows-x64-cuda.yaml
@@ -31,7 +31,7 @@ jobs:
       fail-fast: false
       matrix:
         os: [windows-2022]
-        onnxruntime_version: ["1.17.1", "1.22.0"]
+        onnxruntime_version: ["1.17.1", "1.23.2"]
 
     steps:
       - uses: actions/checkout@v4
@@ -48,13 +48,11 @@ jobs:
         shell: bash
         run: |
           onnxruntime_version=${{ matrix.onnxruntime_version }}
-          if [[ $onnxruntime_version == "1.22.0" ]]; then
-            curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v1.22.0/onnxruntime-win-x64-gpu-1.22.0.zip
-            unzip onnxruntime-win-x64-gpu-1.22.0.zip
+          curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v$onnxruntime_version/onnxruntime-win-x64-gpu-$onnxruntime_version.zip
+          unzip onnxruntime-win-x64-gpu-$onnxruntime_version.zip
 
-            export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/lib
-            export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/include
-          fi
+          export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-win-x64-gpu-$onnxruntime_version/lib
+          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-win-x64-gpu-$onnxruntime_version/include
 
           mkdir build
           cd build
@@ -85,7 +83,7 @@ jobs:
           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-win-x64-cuda
 
           onnxruntime_version=${{ matrix.onnxruntime_version }}
-          if [[ $onnxruntime_version == "1.22.0" ]]; then
+          if [[ $onnxruntime_version == "1.23.2" ]]; then
             dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-cuda-12.x-cudnn-9.x-win-x64-cuda
           fi
 
diff --git a/cmake/onnxruntime-linux-x86_64-gpu.cmake b/cmake/onnxruntime-linux-x86_64-gpu.cmake
index 8807f6f2..11e747ca 100644
--- a/cmake/onnxruntime-linux-x86_64-gpu.cmake
+++ b/cmake/onnxruntime-linux-x86_64-gpu.cmake
@@ -19,19 +19,20 @@ if(NOT SHERPA_ONNX_ENABLE_GPU)
 endif()
 
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-x64-gpu-1.17.1-patched.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-x64-gpu-1.17.1-patched.zip")
+# Requres CUDA 12, cudnn 9
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-x64-gpu-1.23.2-patched.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-x64-gpu-1.23.2-patched.zip")
 set(onnxruntime_HASH "SHA256=1261de176e8d9d4d2019f8fa8c732c6d11494f3c6e73168ab6d2cc0903f22551")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-x64-gpu-1.17.1-patched.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-gpu-1.17.1-patched.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-gpu-1.17.1-patched.zip
-  /tmp/onnxruntime-linux-x64-gpu-1.17.1-patched.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-x64-gpu-1.17.1-patched.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-x64-gpu-1.23.2-patched.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-gpu-1.23.2-patched.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-gpu-1.23.2-patched.zip
+  /tmp/onnxruntime-linux-x64-gpu-1.23.2-patched.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-x64-gpu-1.23.2-patched.zip
 )
 
 foreach(f IN LISTS possible_file_locations)

commit 3241591567dbc4ef0f7296d93c441c917411242e
Author: wangqi <wang.qi@qikuyx.com>
Date:   Sat Jan 10 20:34:29 2026 -0500

    Add Mac Catalyst support but onnxruntimes still does not support it

diff --git a/build-maccatalyst.sh b/build-maccatalyst.sh
new file mode 100755
index 00000000..0fec1a9d
--- /dev/null
+++ b/build-maccatalyst.sh
@@ -0,0 +1,159 @@
+#!/usr/bin/env bash
+#
+# Build sherpa-onnx for Mac Catalyst (arm64 + x86_64)
+#
+set -e
+
+dir=build-maccatalyst
+mkdir -p $dir
+cd $dir
+
+# Verify onnxruntime is available (downloaded by build-ios.sh)
+if [ ! -d "../build-ios/ios-onnxruntime/onnxruntime.xcframework" ]; then
+    echo "Error: onnxruntime.xcframework not found."
+    echo "Please run build-ios.sh first to download onnxruntime."
+    exit 1
+fi
+
+# Use macOS slice of onnxruntime for Mac Catalyst
+# Mac Catalyst links against macOS libraries
+export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/../build-ios/ios-onnxruntime/onnxruntime.xcframework/macos-arm64_x86_64
+export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/../build-ios/ios-onnxruntime/onnxruntime.xcframework/Headers
+
+echo "SHERPA_ONNXRUNTIME_LIB_DIR: $SHERPA_ONNXRUNTIME_LIB_DIR"
+echo "SHERPA_ONNXRUNTIME_INCLUDE_DIR: $SHERPA_ONNXRUNTIME_INCLUDE_DIR"
+
+# Common CMake options
+CMAKE_COMMON_OPTS=(
+  -DBUILD_PIPER_PHONMIZE_EXE=OFF
+  -DBUILD_PIPER_PHONMIZE_TESTS=OFF
+  -DBUILD_ESPEAK_NG_EXE=OFF
+  -DBUILD_ESPEAK_NG_TESTS=OFF
+  -DCMAKE_TOOLCHAIN_FILE=./toolchains/ios.toolchain.cmake
+  -DENABLE_BITCODE=0
+  -DENABLE_ARC=1
+  -DENABLE_VISIBILITY=0
+  -DCMAKE_BUILD_TYPE=Release
+  -DCMAKE_INSTALL_PREFIX=./install
+  -DBUILD_SHARED_LIBS=OFF
+  -DSHERPA_ONNX_ENABLE_PYTHON=OFF
+  -DSHERPA_ONNX_ENABLE_TESTS=OFF
+  -DSHERPA_ONNX_ENABLE_CHECK=OFF
+  -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF
+  -DSHERPA_ONNX_ENABLE_JNI=OFF
+  -DSHERPA_ONNX_ENABLE_C_API=ON
+  -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF
+  -DSHERPA_ONNX_ENABLE_BINARY=OFF
+  -DDEPLOYMENT_TARGET=14.0
+)
+
+echo "Building for Mac Catalyst (arm64)..."
+cmake \
+  "${CMAKE_COMMON_OPTS[@]}" \
+  -S .. \
+  -DPLATFORM=MAC_CATALYST_ARM64 \
+  -B build/catalyst_arm64
+
+cmake --build build/catalyst_arm64 -j 4
+
+echo "Building for Mac Catalyst (x86_64)..."
+cmake \
+  "${CMAKE_COMMON_OPTS[@]}" \
+  -S .. \
+  -DPLATFORM=MAC_CATALYST \
+  -B build/catalyst_x86_64
+
+cmake --build build/catalyst_x86_64 -j 4
+
+# Generate headers from arm64 build
+cmake --build build/catalyst_arm64 --target install
+
+echo "Combining architectures with lipo..."
+mkdir -p "build/catalyst/lib"
+
+# Library list matching build-ios.sh
+LIBS=(
+  libkaldi-native-fbank-core.a
+  libkissfft-float.a
+  libsherpa-onnx-c-api.a
+  libsherpa-onnx-core.a
+  libsherpa-onnx-fstfar.a
+  libssentencepiece_core.a
+  libsherpa-onnx-fst.a
+  libsherpa-onnx-kaldifst-core.a
+  libkaldi-decoder-core.a
+  libucd.a
+  libpiper_phonemize.a
+  libespeak-ng.a
+)
+
+for f in "${LIBS[@]}"; do
+  echo "  Combining $f..."
+  lipo -create \
+    build/catalyst_arm64/lib/${f} \
+    build/catalyst_x86_64/lib/${f} \
+    -output build/catalyst/lib/${f}
+done
+
+echo "Merging into single static library..."
+libtool -static -o build/catalyst/libsherpa-onnx.a \
+  build/catalyst/lib/libkaldi-native-fbank-core.a \
+  build/catalyst/lib/libkissfft-float.a \
+  build/catalyst/lib/libsherpa-onnx-c-api.a \
+  build/catalyst/lib/libsherpa-onnx-core.a \
+  build/catalyst/lib/libsherpa-onnx-fstfar.a \
+  build/catalyst/lib/libsherpa-onnx-fst.a \
+  build/catalyst/lib/libsherpa-onnx-kaldifst-core.a \
+  build/catalyst/lib/libkaldi-decoder-core.a \
+  build/catalyst/lib/libucd.a \
+  build/catalyst/lib/libpiper_phonemize.a \
+  build/catalyst/lib/libespeak-ng.a \
+  build/catalyst/lib/libssentencepiece_core.a
+
+echo "Creating xcframework for Mac Catalyst..."
+rm -rf sherpa-onnx.xcframework
+
+# Create the xcframework directory structure manually
+mkdir -p sherpa-onnx.xcframework/maccatalyst-arm64_x86_64
+
+# Copy the universal library
+cp build/catalyst/libsherpa-onnx.a sherpa-onnx.xcframework/maccatalyst-arm64_x86_64/
+
+# Copy headers
+cp -r install/include sherpa-onnx.xcframework/maccatalyst-arm64_x86_64/Headers
+
+# Create Info.plist
+cat > sherpa-onnx.xcframework/Info.plist << 'EOF'
+<?xml version="1.0" encoding="UTF-8"?>
+<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
+<plist version="1.0">
+<dict>
+    <key>AvailableLibraries</key>
+    <array>
+        <dict>
+            <key>LibraryIdentifier</key>
+            <string>maccatalyst-arm64_x86_64</string>
+            <key>LibraryPath</key>
+            <string>libsherpa-onnx.a</string>
+            <key>HeadersPath</key>
+            <string>Headers</string>
+            <key>SupportedArchitectures</key>
+            <array>
+                <string>arm64</string>
+                <string>x86_64</string>
+            </array>
+            <key>SupportedPlatform</key>
+            <string>maccatalyst</string>
+        </dict>
+    </array>
+    <key>CFBundlePackageType</key>
+    <string>XFWK</string>
+    <key>XCFrameworkFormatVersion</key>
+    <string>1.0</string>
+</dict>
+</plist>
+EOF
+
+echo ""
+echo "Mac Catalyst build complete!"
+echo "Output: build-maccatalyst/sherpa-onnx.xcframework"
diff --git a/build-xcframework.sh b/build-xcframework.sh
index 66d1999c..2fccb818 100755
--- a/build-xcframework.sh
+++ b/build-xcframework.sh
@@ -1,19 +1,20 @@
 #!/usr/bin/env bash
 #
-# Script to build iOS and macOS libraries and merge them into a unified xcframework
-# that supports all Apple platforms (iOS devices, iOS simulators, and macOS)
+# Script to build iOS, macOS, and Mac Catalyst libraries and merge them into a unified xcframework
+# that supports all Apple platforms (iOS devices, iOS simulators, macOS, and Mac Catalyst)
 #
 # Usage: ./build-xcframework.sh [options]
 #
 # Options:
-#   --skip-ios      Skip iOS build (use existing build-ios/sherpa-onnx.xcframework)
-#   --skip-macos    Skip macOS build (use existing build-swift-macos/sherpa-onnx.xcframework)
-#   --skip-build    Skip both builds (only merge existing xcframeworks)
-#   --clean         Clean all build directories before building
-#   -h, --help      Show this help message
+#   --skip-ios          Skip iOS build (use existing build)
+#   --skip-macos        Skip macOS build (use existing build)
+#   --skip-maccatalyst  Skip Mac Catalyst build (use existing build)
+#   --skip-build        Skip all builds (only merge existing xcframeworks)
+#   --clean             Clean all build directories before building
+#   -h, --help          Show this help message
 #
 # Output:
-#   - build-apple/sherpa-onnx.xcframework
+#   - build-ios/sherpa-onnx.xcframework (unified xcframework with all platforms)
 
 set -e
 
@@ -27,6 +28,7 @@ NC='\033[0m' # No Color
 # Default options
 SKIP_IOS=false
 SKIP_MACOS=false
+SKIP_MACCATALYST=false
 CLEAN_BUILD=false
 
 log() {
@@ -57,14 +59,15 @@ show_help() {
   echo "Build sherpa-onnx xcframework for all Apple platforms."
   echo ""
   echo "Options:"
-  echo "  --skip-ios      Skip iOS build (use existing build)"
-  echo "  --skip-macos    Skip macOS build (use existing build)"
-  echo "  --skip-build    Skip both builds (only merge existing xcframeworks)"
-  echo "  --clean         Clean all build directories before building"
-  echo "  -h, --help      Show this help message"
+  echo "  --skip-ios          Skip iOS build (use existing build)"
+  echo "  --skip-macos        Skip macOS build (use existing build)"
+  echo "  --skip-maccatalyst  Skip Mac Catalyst build (use existing build)"
+  echo "  --skip-build        Skip all builds (only merge existing xcframeworks)"
+  echo "  --clean             Clean all build directories before building"
+  echo "  -h, --help          Show this help message"
   echo ""
   echo "Output:"
-  echo "  build-apple/sherpa-onnx.xcframework"
+  echo "  build-ios/sherpa-onnx.xcframework"
   echo ""
   echo "Examples:"
   echo "  ./build-xcframework.sh              # Full build"
@@ -85,9 +88,14 @@ parse_args() {
         SKIP_MACOS=true
         shift
         ;;
+      --skip-maccatalyst)
+        SKIP_MACCATALYST=true
+        shift
+        ;;
       --skip-build)
         SKIP_IOS=true
         SKIP_MACOS=true
+        SKIP_MACCATALYST=true
         shift
         ;;
       --clean)
@@ -119,6 +127,11 @@ clean_builds() {
     rm -rf build-swift-macos
   fi
 
+  if [ -d "build-maccatalyst" ]; then
+    log "Removing build-maccatalyst..."
+    rm -rf build-maccatalyst
+  fi
+
   if [ -d "build-apple" ]; then
     log "Removing build-apple..."
     rm -rf build-apple
@@ -131,8 +144,9 @@ clean_builds() {
 build_ios() {
   if [ "$SKIP_IOS" = true ]; then
     log_step "Skipping iOS build (--skip-ios specified)"
-    if [ ! -d "build-ios/sherpa-onnx.xcframework" ]; then
-      error "iOS xcframework not found at build-ios/sherpa-onnx.xcframework. Cannot skip iOS build."
+    # Check for iOS slices (either in original location or backup)
+    if [ ! -d "build-ios/.backup/ios-arm64" ] && [ ! -d "build-ios/sherpa-onnx.xcframework/ios-arm64" ]; then
+      error "iOS xcframework not found. Neither build-ios/sherpa-onnx.xcframework nor build-ios/.backup exists. Cannot skip iOS build."
     fi
     return 0
   fi
@@ -197,33 +211,85 @@ build_macos() {
   fi
 }
 
+# Build Mac Catalyst xcframework
+build_maccatalyst() {
+  if [ "$SKIP_MACCATALYST" = true ]; then
+    log_step "Skipping Mac Catalyst build (--skip-maccatalyst specified)"
+    if [ ! -d "build-maccatalyst/sherpa-onnx.xcframework" ]; then
+      error "Mac Catalyst xcframework not found at build-maccatalyst/sherpa-onnx.xcframework. Cannot skip Mac Catalyst build."
+    fi
+    return 0
+  fi
+
+  log_step "Building Mac Catalyst xcframework..."
+  echo ""
+
+  # Check if build script exists
+  if [ ! -f "./build-maccatalyst.sh" ]; then
+    error "build-maccatalyst.sh not found in current directory"
+  fi
+
+  # Run the Mac Catalyst build
+  local start_time=$(date +%s)
+
+  if ./build-maccatalyst.sh; then
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    success "Mac Catalyst build completed in ${duration}s"
+  else
+    error "Mac Catalyst build failed. Check the output above for details."
+  fi
+
+  # Verify the output
+  if [ ! -d "build-maccatalyst/sherpa-onnx.xcframework" ]; then
+    error "Mac Catalyst xcframework not found after build. Build may have failed silently."
+  fi
+}
+
 # Check if required source xcframeworks exist
 check_prerequisites() {
   log_step "Checking prerequisites..."
 
-  if [ ! -d "build-ios/sherpa-onnx.xcframework" ]; then
-    error "iOS xcframework not found at build-ios/sherpa-onnx.xcframework"
+  # Check for iOS slices (either in original location or backup)
+  if [ ! -d "build-ios/.backup/ios-arm64" ] && [ ! -d "build-ios/sherpa-onnx.xcframework/ios-arm64" ]; then
+    error "iOS xcframework not found. Neither build-ios/sherpa-onnx.xcframework nor build-ios/.backup exists"
   fi
 
   if [ ! -d "build-swift-macos/sherpa-onnx.xcframework" ]; then
     error "macOS xcframework not found at build-swift-macos/sherpa-onnx.xcframework"
   fi
 
+  if [ ! -d "build-maccatalyst/sherpa-onnx.xcframework" ]; then
+    error "Mac Catalyst xcframework not found at build-maccatalyst/sherpa-onnx.xcframework"
+  fi
+
   success "Prerequisites check passed"
 }
 
 # Clean and prepare output directory
 prepare_output_dir() {
-  log_step "Preparing output directory..."
+  log_step "Preparing output directory for unified xcframework..."
 
-  # Remove existing build-apple directory if it exists
-  if [ -d "build-apple" ]; then
-    log "Removing existing build-apple directory..."
-    rm -rf build-apple
+  # The unified xcframework will replace the iOS-only xcframework at build-ios/
+  # We need to preserve the iOS slices temporarily
+
+  if [ -d "build-ios/sherpa-onnx.xcframework" ]; then
+    log "Backing up iOS xcframework slices..."
+    mkdir -p build-ios/.backup
+
+    # Copy iOS slices to backup
+    if [ -d "build-ios/sherpa-onnx.xcframework/ios-arm64" ]; then
+      cp -r "build-ios/sherpa-onnx.xcframework/ios-arm64" build-ios/.backup/
+    fi
+    if [ -d "build-ios/sherpa-onnx.xcframework/ios-arm64_x86_64-simulator" ]; then
+      cp -r "build-ios/sherpa-onnx.xcframework/ios-arm64_x86_64-simulator" build-ios/.backup/
+    fi
+
+    # Remove the iOS-only xcframework
+    log "Removing iOS-only xcframework to make room for unified xcframework..."
+    rm -rf "build-ios/sherpa-onnx.xcframework"
   fi
 
-  # Create fresh build-apple directory
-  mkdir -p build-apple
   success "Output directory prepared"
 }
 
@@ -238,6 +304,8 @@ get_library_paths() {
     echo "$xcframework_path/ios-arm64_x86_64-simulator"
   elif [ "$platform" = "macos" ]; then
     echo "$xcframework_path/macos-arm64_x86_64"
+  elif [ "$platform" = "maccatalyst" ]; then
+    echo "$xcframework_path/maccatalyst-arm64_x86_64"
   fi
 }
 
@@ -246,9 +314,11 @@ create_unified_xcframework() {
   log_step "Creating unified xcframework..."
 
   # Get library paths from each xcframework
-  local ios_device_path=$(get_library_paths "build-ios/sherpa-onnx.xcframework" "ios")
-  local ios_sim_path=$(get_library_paths "build-ios/sherpa-onnx.xcframework" "ios-simulator")
+  # iOS slices are in the backup directory
+  local ios_device_path="build-ios/.backup/ios-arm64"
+  local ios_sim_path="build-ios/.backup/ios-arm64_x86_64-simulator"
   local macos_path=$(get_library_paths "build-swift-macos/sherpa-onnx.xcframework" "macos")
+  local maccatalyst_path=$(get_library_paths "build-maccatalyst/sherpa-onnx.xcframework" "maccatalyst")
 
   # Verify paths exist
   if [ ! -d "$ios_device_path" ]; then
@@ -260,41 +330,75 @@ create_unified_xcframework() {
   if [ ! -d "$macos_path" ]; then
     error "macOS library not found at $macos_path"
   fi
+  if [ ! -d "$maccatalyst_path" ]; then
+    error "Mac Catalyst library not found at $maccatalyst_path"
+  fi
 
   # Resolve static libraries and headers for each slice
   local ios_device_lib="$ios_device_path/libsherpa-onnx.a"
   local ios_sim_lib="$ios_sim_path/libsherpa-onnx.a"
   local macos_lib="$macos_path/libsherpa-onnx.a"
+  local maccatalyst_lib="$maccatalyst_path/libsherpa-onnx.a"
   local ios_device_headers="$ios_device_path/Headers"
   local ios_sim_headers="$ios_sim_path/Headers"
   local macos_headers="$macos_path/Headers"
+  local maccatalyst_headers="$maccatalyst_path/Headers"
 
-  for lib in "$ios_device_lib" "$ios_sim_lib" "$macos_lib"; do
+  for lib in "$ios_device_lib" "$ios_sim_lib" "$macos_lib" "$maccatalyst_lib"; do
     if [ ! -f "$lib" ]; then
       error "Expected static library not found: $lib"
     fi
   done
-  for hdr in "$ios_device_headers" "$ios_sim_headers" "$macos_headers"; do
+  for hdr in "$ios_device_headers" "$ios_sim_headers" "$macos_headers" "$maccatalyst_headers"; do
     if [ ! -d "$hdr" ]; then
       error "Expected Headers folder not found: $hdr"
     fi
   done
 
-  # Build the xcodebuild command with all libraries
+  # Build the xcodebuild command with iOS and macOS libraries only
+  # Note: Mac Catalyst must be added manually as xcodebuild doesn't support it
   local xcodebuild_cmd="xcodebuild -create-xcframework"
   xcodebuild_cmd="$xcodebuild_cmd -library $ios_device_lib -headers $ios_device_headers"
   xcodebuild_cmd="$xcodebuild_cmd -library $ios_sim_lib -headers $ios_sim_headers"
   xcodebuild_cmd="$xcodebuild_cmd -library $macos_lib -headers $macos_headers"
 
-  # Set output path
-  xcodebuild_cmd="$xcodebuild_cmd -output build-apple/sherpa-onnx.xcframework"
+  # Set output path - create unified xcframework at build-ios
+  xcodebuild_cmd="$xcodebuild_cmd -output build-ios/sherpa-onnx.xcframework"
 
   # Execute the command
   log "Executing: $xcodebuild_cmd"
-  if eval $xcodebuild_cmd; then
-    success "Successfully created unified xcframework"
-  else
-    error "Failed to create unified xcframework"
+  if ! eval $xcodebuild_cmd; then
+    error "Failed to create xcframework with iOS and macOS"
+  fi
+
+  log "Adding Mac Catalyst slice manually..."
+
+  # Copy Mac Catalyst slice to the xcframework
+  cp -r "$maccatalyst_path" build-ios/sherpa-onnx.xcframework/maccatalyst-arm64_x86_64
+
+  # Update Info.plist to include Mac Catalyst
+  # Read existing plist
+  local plist_path="build-ios/sherpa-onnx.xcframework/Info.plist"
+
+  # Add Mac Catalyst entry
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries: dict" "$plist_path" 2>/dev/null || true
+  local last_index=$(/usr/libexec/PlistBuddy -c "Print :AvailableLibraries" "$plist_path" | grep -c "Dict" || echo "0")
+  last_index=$((last_index - 1))
+
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:LibraryIdentifier string maccatalyst-arm64_x86_64" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:LibraryPath string libsherpa-onnx.a" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:HeadersPath string Headers" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:SupportedArchitectures array" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:SupportedArchitectures:0 string arm64" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:SupportedArchitectures:1 string x86_64" "$plist_path"
+  /usr/libexec/PlistBuddy -c "Add :AvailableLibraries:$last_index:SupportedPlatform string maccatalyst" "$plist_path"
+
+  success "Successfully created unified xcframework at build-ios/sherpa-onnx.xcframework"
+
+  # Clean up backup directory
+  if [ -d "build-ios/.backup" ]; then
+    rm -rf build-ios/.backup
+    log "Cleaned up backup directory"
   fi
 }
 
@@ -302,7 +406,7 @@ create_unified_xcframework() {
 verify_xcframework() {
   log_step "Verifying unified xcframework..."
 
-  local xcframework_path="build-apple/sherpa-onnx.xcframework"
+  local xcframework_path="build-ios/sherpa-onnx.xcframework"
 
   if [ ! -d "$xcframework_path" ]; then
     error "Unified xcframework not found at $xcframework_path"
@@ -320,7 +424,7 @@ verify_xcframework() {
   /usr/libexec/PlistBuddy -c "Print :AvailableLibraries" "$xcframework_path/Info.plist" 2>/dev/null | grep -A 2 "SupportedPlatform" | grep -v "^--$" || true
 
   # Check each platform directory
-  local platforms=("ios-arm64" "ios-arm64_x86_64-simulator" "macos-arm64_x86_64")
+  local platforms=("ios-arm64" "ios-arm64_x86_64-simulator" "macos-arm64_x86_64" "maccatalyst-arm64_x86_64")
   for platform in "${platforms[@]}"; do
     if [ -d "$xcframework_path/$platform" ]; then
       echo -e "${GREEN}  [OK] $platform${NC}"
@@ -354,22 +458,23 @@ show_usage_info() {
   echo -e "${GREEN}  Build Complete!${NC}"
   echo -e "${GREEN}========================================${NC}"
   echo ""
-  echo "Output: build-apple/sherpa-onnx.xcframework"
+  echo "Output: build-ios/sherpa-onnx.xcframework"
   echo ""
   echo "Supported Platforms:"
   echo "  - iOS (arm64) - Physical devices"
   echo "  - iOS Simulator (arm64, x86_64)"
   echo "  - macOS (arm64, x86_64)"
+  echo "  - Mac Catalyst (arm64, x86_64)"
   echo ""
   echo "Usage in Xcode:"
-  echo "  1. Drag build-apple/sherpa-onnx.xcframework into your project"
+  echo "  1. Drag build-ios/sherpa-onnx.xcframework into your project"
   echo "  2. Select 'Copy items if needed'"
   echo "  3. Add to 'Frameworks, Libraries, and Embedded Content'"
   echo ""
   echo "Swift Package Manager:"
   echo "  .binaryTarget("
   echo "      name: \"sherpa-onnx\","
-  echo "      path: \"build-apple/sherpa-onnx.xcframework\""
+  echo "      path: \"build-ios/sherpa-onnx.xcframework\""
   echo "  )"
   echo ""
 }
@@ -388,9 +493,10 @@ main() {
 
   # Show build configuration
   log "Build configuration:"
-  echo "  - iOS build:   $([ "$SKIP_IOS" = true ] && echo "SKIP" || echo "BUILD")"
-  echo "  - macOS build: $([ "$SKIP_MACOS" = true ] && echo "SKIP" || echo "BUILD")"
-  echo "  - Clean build: $([ "$CLEAN_BUILD" = true ] && echo "YES" || echo "NO")"
+  echo "  - iOS build:         $([ "$SKIP_IOS" = true ] && echo "SKIP" || echo "BUILD")"
+  echo "  - macOS build:       $([ "$SKIP_MACOS" = true ] && echo "SKIP" || echo "BUILD")"
+  echo "  - Mac Catalyst build: $([ "$SKIP_MACCATALYST" = true ] && echo "SKIP" || echo "BUILD")"
+  echo "  - Clean build:       $([ "$CLEAN_BUILD" = true ] && echo "YES" || echo "NO")"
   echo ""
 
   # Step 0: Clean if requested
@@ -407,19 +513,23 @@ main() {
   build_macos
   echo ""
 
-  # Step 3: Check prerequisites (verify both builds exist)
+  # Step 3: Build Mac Catalyst
+  build_maccatalyst
+  echo ""
+
+  # Step 4: Check prerequisites (verify all builds exist)
   check_prerequisites
   echo ""
 
-  # Step 4: Prepare output directory
+  # Step 5: Prepare output directory
   prepare_output_dir
   echo ""
 
-  # Step 5: Create unified xcframework
+  # Step 6: Create unified xcframework
   create_unified_xcframework
   echo ""
 
-  # Step 6: Verify the result
+  # Step 7: Verify the result
   verify_xcframework
 
   # Calculate total time
@@ -431,7 +541,7 @@ main() {
   echo ""
   log "Total build time: ${minutes}m ${seconds}s"
 
-  # Step 7: Show usage information
+  # Step 8: Show usage information
   show_usage_info
 }
 
diff --git a/flutter-examples/streaming_asr/ios/Runner/AppDelegate.swift b/flutter-examples/streaming_asr/ios/Runner/AppDelegate.swift
index 9074fee9..a9e94294 100644
--- a/flutter-examples/streaming_asr/ios/Runner/AppDelegate.swift
+++ b/flutter-examples/streaming_asr/ios/Runner/AppDelegate.swift
@@ -1,5 +1,7 @@
 import Flutter
+#if canImport(UIKit)
 import UIKit
+#endif
 
 @UIApplicationMain
 @objc class AppDelegate: FlutterAppDelegate {
diff --git a/flutter-examples/streaming_asr/ios/RunnerTests/RunnerTests.swift b/flutter-examples/streaming_asr/ios/RunnerTests/RunnerTests.swift
index 86a7c3b1..7eb88367 100644
--- a/flutter-examples/streaming_asr/ios/RunnerTests/RunnerTests.swift
+++ b/flutter-examples/streaming_asr/ios/RunnerTests/RunnerTests.swift
@@ -1,5 +1,7 @@
 import Flutter
+#if canImport(UIKit)
 import UIKit
+#endif
 import XCTest
 
 class RunnerTests: XCTestCase {
diff --git a/flutter-examples/tts/ios/Runner/AppDelegate.swift b/flutter-examples/tts/ios/Runner/AppDelegate.swift
index 9074fee9..a9e94294 100644
--- a/flutter-examples/tts/ios/Runner/AppDelegate.swift
+++ b/flutter-examples/tts/ios/Runner/AppDelegate.swift
@@ -1,5 +1,7 @@
 import Flutter
+#if canImport(UIKit)
 import UIKit
+#endif
 
 @UIApplicationMain
 @objc class AppDelegate: FlutterAppDelegate {
diff --git a/flutter-examples/tts/ios/RunnerTests/RunnerTests.swift b/flutter-examples/tts/ios/RunnerTests/RunnerTests.swift
index 86a7c3b1..7eb88367 100644
--- a/flutter-examples/tts/ios/RunnerTests/RunnerTests.swift
+++ b/flutter-examples/tts/ios/RunnerTests/RunnerTests.swift
@@ -1,5 +1,7 @@
 import Flutter
+#if canImport(UIKit)
 import UIKit
+#endif
 import XCTest
 
 class RunnerTests: XCTestCase {
diff --git a/ios-swift/SherpaOnnx/SherpaOnnx/AppDelegate.swift b/ios-swift/SherpaOnnx/SherpaOnnx/AppDelegate.swift
index d88ad286..7cd2e6f2 100644
--- a/ios-swift/SherpaOnnx/SherpaOnnx/AppDelegate.swift
+++ b/ios-swift/SherpaOnnx/SherpaOnnx/AppDelegate.swift
@@ -5,7 +5,9 @@
 //  Created by fangjun on 2023/2/25.
 //
 
+#if canImport(UIKit)
 import UIKit
+#endif
 
 @main
 class AppDelegate: UIResponder, UIApplicationDelegate {
diff --git a/ios-swift/SherpaOnnx/SherpaOnnx/SceneDelegate.swift b/ios-swift/SherpaOnnx/SherpaOnnx/SceneDelegate.swift
index 60441898..65dca8ed 100644
--- a/ios-swift/SherpaOnnx/SherpaOnnx/SceneDelegate.swift
+++ b/ios-swift/SherpaOnnx/SherpaOnnx/SceneDelegate.swift
@@ -5,7 +5,9 @@
 //  Created by fangjun on 2023/2/25.
 //
 
+#if canImport(UIKit)
 import UIKit
+#endif
 
 class SceneDelegate: UIResponder, UIWindowSceneDelegate {
 
diff --git a/ios-swift/SherpaOnnx/SherpaOnnx/ViewController.swift b/ios-swift/SherpaOnnx/SherpaOnnx/ViewController.swift
index 969ee63c..3dd18e77 100644
--- a/ios-swift/SherpaOnnx/SherpaOnnx/ViewController.swift
+++ b/ios-swift/SherpaOnnx/SherpaOnnx/ViewController.swift
@@ -6,7 +6,9 @@
 //
 
 import AVFoundation
+#if canImport(UIKit)
 import UIKit
+#endif
 
 extension AudioBuffer {
     func array() -> [Float] {

commit a65753c8c93277c1e3dfcc44574e2f2b9e7b897e
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Sun Jan 11 08:06:00 2026 +0800

    Start to switch from onnxruntime 1.17.1 to v1.23.2 (#2993)

diff --git a/.github/workflows/linux.yaml b/.github/workflows/linux.yaml
index 9cdb3f40..08165940 100644
--- a/.github/workflows/linux.yaml
+++ b/.github/workflows/linux.yaml
@@ -97,6 +97,30 @@ jobs:
             run: |
               uname -a
               gcc --version
+
+              # use gcc 11. the default is gcc 10
+
+              # See https://github.com/nealef/clefos/issues/9
+              echo "multilib_policy=best" >> /etc/yum.conf
+              echo "skip_missing_names_on_install=False" >> /etc/yum.conf
+              sed -i '/^override_install_langs=/d' /etc/yum.conf
+              yum -y update
+              yum -y install yum-utils curl
+              yum-config-manager --enable extras
+              yum -y install centos-release-scl-rh
+              yum -y install devtoolset-11-binutils devtoolset-11-gcc devtoolset-11-gcc-c++ devtoolset-11-gcc-gfortran
+
+              # see https://stackoverflow.com/questions/72904802/can-not-find-required-gcc-version-after-devtoolset-installation
+              ls -lh /opt/rh/devtoolset-11
+
+              source /opt/rh/devtoolset-11/enable
+
+              echo 'which gcc'
+              which gcc
+
+              echo 'gcc --version'
+              gcc --version
+
               cmake --version
               cat /etc/*release
               id
diff --git a/.github/workflows/macos.yaml b/.github/workflows/macos.yaml
index cc3d4e1f..a9e3179b 100644
--- a/.github/workflows/macos.yaml
+++ b/.github/workflows/macos.yaml
@@ -57,7 +57,7 @@ concurrency:
 jobs:
   macos:
     runs-on: ${{ matrix.os }}
-    name: ${{ matrix.build_type }} ${{ matrix.lib_type }} tts-${{ matrix.with_tts }}
+    name: ${{ matrix.build_type }} ${{ matrix.lib_type }} tts-${{ matrix.with_tts }} ${{ matrix.os }} ${{ matrix.arch }}
     strategy:
       fail-fast: false
       matrix:
@@ -65,6 +65,7 @@ jobs:
         build_type: [Release, Debug]
         lib_type: [static, shared]
         with_tts: [ON, OFF]
+        arch: ["arm64;x86_64"]
 
     steps:
       - uses: actions/checkout@v4
@@ -98,11 +99,13 @@ jobs:
             BUILD_SHARED_LIBS=ON
           fi
 
+          arch="${{ matrix.arch }}"
+
           cmake \
             -DSHERPA_ONNX_ENABLE_TTS=${{ matrix.with_tts }} \
             -D BUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
             -D CMAKE_BUILD_TYPE=${{ matrix.build_type }} \
-            -D CMAKE_OSX_ARCHITECTURES='arm64;x86_64' \
+            -D CMAKE_OSX_ARCHITECTURES="$arch" \
             -D CMAKE_INSTALL_PREFIX=./install \
             ..
 
diff --git a/cmake/onnxruntime-linux-x86_64-static.cmake b/cmake/onnxruntime-linux-x86_64-static.cmake
index f72f9ad5..b2a2606c 100644
--- a/cmake/onnxruntime-linux-x86_64-static.cmake
+++ b/cmake/onnxruntime-linux-x86_64-static.cmake
@@ -14,19 +14,19 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip")
-set(onnxruntime_HASH "SHA256=b646beeb983de843a267096d4457d832f93089f5e7264fd54b48cff207cb2068")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip")
+set(onnxruntime_HASH "SHA256=93a52b9d93a0932259a03090291be861ba21ad4b1b58057d3a0f57a4c4108671")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip
-  /tmp/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-x64-static_lib-1.17.1-glibc2_17.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip
+  /tmp/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-x64-static_lib-1.23.2-glibc2_17.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-linux-x86_64.cmake b/cmake/onnxruntime-linux-x86_64.cmake
index 361f4d0d..1d2238f2 100644
--- a/cmake/onnxruntime-linux-x86_64.cmake
+++ b/cmake/onnxruntime-linux-x86_64.cmake
@@ -14,19 +14,19 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip")
-set(onnxruntime_HASH "SHA256=cb90c51a195bdd453aaf1582f3ef63b466dafbb15c4b8a552ca4dce3769e1d1e")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=77ea3532dfdd8d5c66918429f7eacd80c1fea834941a14746adf3109f8e7b830")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip
-  /tmp/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-x64-glibc2_17-Release-1.17.1-patched.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip
+  /tmp/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-x64-glibc2_17-Release-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-osx-arm64-static.cmake b/cmake/onnxruntime-osx-arm64-static.cmake
index 7cd5a63b..30fb5145 100644
--- a/cmake/onnxruntime-osx-arm64-static.cmake
+++ b/cmake/onnxruntime-osx-arm64-static.cmake
@@ -12,18 +12,18 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-osx-arm64-static_lib-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-arm64-static_lib-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=b88a4017251c159fea005aefe836bd0cf4d0bc7454e2810784f84a42143f17eb")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-osx-arm64-static_lib-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-arm64-static_lib-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=febeb7116f075409c554434a317cd51a2efb26abbf364c2ed77191f728a56633")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-arm64-static_lib-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-arm64-static_lib-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-arm64-static_lib-1.17.1.zip
-  /tmp/onnxruntime-osx-arm64-static_lib-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-osx-arm64-static_lib-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-arm64-static_lib-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-arm64-static_lib-1.23.2.zip
+  /tmp/onnxruntime-osx-arm64-static_lib-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
@@ -59,3 +59,6 @@ set(onnxruntime_lib_files ${onnxruntime_lib_files} PARENT_SCOPE)
 
 message(STATUS "onnxruntime lib files: ${onnxruntime_lib_files}")
 install(FILES ${onnxruntime_lib_files} DESTINATION lib)
+
+# disable coreml when using static onnxruntime lib
+add_definitions(-DSHERPA_ONNX_DISABLE_COREML)
diff --git a/cmake/onnxruntime-osx-arm64.cmake b/cmake/onnxruntime-osx-arm64.cmake
index e3c986a4..25b4a47e 100644
--- a/cmake/onnxruntime-osx-arm64.cmake
+++ b/cmake/onnxruntime-osx-arm64.cmake
@@ -12,18 +12,18 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-osx-arm64-1.17.1.tgz")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-arm64-1.17.1.tgz")
-set(onnxruntime_HASH "SHA256=89566f424624a7ad9a7d9d5e413c44b9639a994d7171cf409901d125b16e2bb3")
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-osx-arm64-1.23.2.tgz")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-arm64-1.23.2.tgz")
+set(onnxruntime_HASH "SHA256=b4d513ab2b26f088c66891dbbc1408166708773d7cc4163de7bdca0e9bbb7856")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-arm64-1.17.1.tgz
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-arm64-1.17.1.tgz
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-arm64-1.17.1.tgz
-  /tmp/onnxruntime-osx-arm64-1.17.1.tgz
+  $ENV{HOME}/Downloads/onnxruntime-osx-arm64-1.23.2.tgz
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-arm64-1.23.2.tgz
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-arm64-1.23.2.tgz
+  /tmp/onnxruntime-osx-arm64-1.23.2.tgz
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-osx-universal-static.cmake b/cmake/onnxruntime-osx-universal-static.cmake
index 5bf635b8..434c6215 100644
--- a/cmake/onnxruntime-osx-universal-static.cmake
+++ b/cmake/onnxruntime-osx-universal-static.cmake
@@ -13,18 +13,18 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-osx-universal2-static_lib-1.17.1.zip")
-set(onnxruntime_URL2  "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-universal2-static_lib-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=45599dbd2fb9dd52d6505930c0e82ca165391e222a68f5606b9ea9d4f3922e15")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-osx-universal2-static_lib-1.23.2.zip")
+set(onnxruntime_URL2  "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-universal2-static_lib-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=00816fda16166859fed41dacb786d3dfc3323bbc1a8fa57a235922f597953986")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-universal2-static_lib-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-universal2-static_lib-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-universal2-static_lib-1.17.1.zip
-  /tmp/onnxruntime-osx-universal2-static_lib-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-osx-universal2-static_lib-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-universal2-static_lib-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-universal2-static_lib-1.23.2.zip
+  /tmp/onnxruntime-osx-universal2-static_lib-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
@@ -60,3 +60,6 @@ set(onnxruntime_lib_files ${onnxruntime_lib_files} PARENT_SCOPE)
 
 message(STATUS "onnxruntime lib files: ${onnxruntime_lib_files}")
 install(FILES ${onnxruntime_lib_files} DESTINATION lib)
+
+# disable coreml when using static onnxruntime lib
+add_definitions(-DSHERPA_ONNX_DISABLE_COREML)
diff --git a/cmake/onnxruntime-osx-universal.cmake b/cmake/onnxruntime-osx-universal.cmake
index fe5a53a6..06ff7397 100644
--- a/cmake/onnxruntime-osx-universal.cmake
+++ b/cmake/onnxruntime-osx-universal.cmake
@@ -13,18 +13,18 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-osx-universal2-1.17.1.tgz")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-universal2-1.17.1.tgz")
-set(onnxruntime_HASH "SHA256=9fa57fa6f202a373599377ef75064ae568fda8da838632b26a86024c7378d306")
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-osx-universal2-1.23.2.tgz")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-universal2-1.23.2.tgz")
+set(onnxruntime_HASH "SHA256=49ae8e3a66ccb18d98ad3fe7f5906b6d7887df8a5edd40f49eb2b14e20885809")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-universal2-1.17.1.tgz
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-universal2-1.17.1.tgz
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-universal2-1.17.1.tgz
-  /tmp/onnxruntime-osx-universal2-1.17.1.tgz
+  $ENV{HOME}/Downloads/onnxruntime-osx-universal2-1.23.2.tgz
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-universal2-1.23.2.tgz
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-universal2-1.23.2.tgz
+  /tmp/onnxruntime-osx-universal2-1.23.2.tgz
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-osx-x86_64-static.cmake b/cmake/onnxruntime-osx-x86_64-static.cmake
index a3c98e70..14df0ed3 100644
--- a/cmake/onnxruntime-osx-x86_64-static.cmake
+++ b/cmake/onnxruntime-osx-x86_64-static.cmake
@@ -12,18 +12,18 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-osx-x86_64-static_lib-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-x86_64-static_lib-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=5ff8efb97e50e257943c6c588328d2c57b649278098d3b468036f02755b60903")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-osx-x86_64-static_lib-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-x86_64-static_lib-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=dc632688d5b48e478742ba1ae2d9ebc78ab6cee18fa6eb61e2fb03b8a80d1b66")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-x86_64-static_lib-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-x86_64-static_lib-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-x86_64-static_lib-1.17.1.zip
-  /tmp/onnxruntime-osx-x86_64-static_lib-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-osx-x86_64-static_lib-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-x86_64-static_lib-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-x86_64-static_lib-1.23.2.zip
+  /tmp/onnxruntime-osx-x86_64-static_lib-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
@@ -59,3 +59,6 @@ set(onnxruntime_lib_files ${onnxruntime_lib_files} PARENT_SCOPE)
 
 message(STATUS "onnxruntime lib files: ${onnxruntime_lib_files}")
 install(FILES ${onnxruntime_lib_files} DESTINATION lib)
+
+# disable coreml when using static onnxruntime lib
+add_definitions(-DSHERPA_ONNX_DISABLE_COREML)
diff --git a/cmake/onnxruntime-osx-x86_64.cmake b/cmake/onnxruntime-osx-x86_64.cmake
index 4ca96746..a9566924 100644
--- a/cmake/onnxruntime-osx-x86_64.cmake
+++ b/cmake/onnxruntime-osx-x86_64.cmake
@@ -12,18 +12,18 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-osx-x86_64-1.17.1.tgz")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-osx-x86_64-1.17.1.tgz")
-set(onnxruntime_HASH "SHA256=86c6b6896434084ff5086eebc4e9ea90be1ed4d46743f92864f46ee43e7b5059")
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-osx-x86_64-1.23.2.tgz")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-osx-x86_64-1.23.2.tgz")
+set(onnxruntime_HASH "SHA256=d10359e16347b57d9959f7e80a225a5b4a66ed7d7e007274a15cae86836485a6")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-osx-x86_64-1.17.1.tgz
-  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-x86_64-1.17.1.tgz
-  ${CMAKE_BINARY_DIR}/onnxruntime-osx-x86_64-1.17.1.tgz
-  /tmp/onnxruntime-osx-x86_64-1.17.1.tgz
+  $ENV{HOME}/Downloads/onnxruntime-osx-x86_64-1.23.2.tgz
+  ${CMAKE_SOURCE_DIR}/onnxruntime-osx-x86_64-1.23.2.tgz
+  ${CMAKE_BINARY_DIR}/onnxruntime-osx-x86_64-1.23.2.tgz
+  /tmp/onnxruntime-osx-x86_64-1.23.2.tgz
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
index 0588a20b..5e311651 100755
--- a/sherpa-onnx/csrc/session.cc
+++ b/sherpa-onnx/csrc/session.cc
@@ -12,7 +12,8 @@
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/provider.h"
-#if defined(__APPLE__) && (ORT_API_VERSION >= 15)
+#if defined(__APPLE__) && (ORT_API_VERSION >= 15) && \
+    !defined(SHERPA_ONNX_DISABLE_COREML)
 #include "coreml_provider_factory.h"  // NOLINT
 #endif
 
@@ -204,7 +205,8 @@ Ort::SessionOptions GetSessionOptionsImpl(
       break;
     }
     case Provider::kCoreML: {
-#if defined(__APPLE__) && (ORT_API_VERSION >= 15)
+#if defined(__APPLE__) && (ORT_API_VERSION >= 15) && \
+    !defined(SHERPA_ONNX_DISABLE_COREML)
       uint32_t coreml_flags = 0;
       (void)OrtSessionOptionsAppendExecutionProvider_CoreML(sess_opts,
                                                             coreml_flags);

commit 717753a40b977021972e9d3a8e3fbab5b3eae619
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Sat Jan 10 22:06:20 2026 +0800

    Use onnxruntime v1.23.2 for Linux arm (#3017)

diff --git a/.github/workflows/arm-linux-gnueabihf.yaml b/.github/workflows/arm-linux-gnueabihf.yaml
index 97e9de79..cc3dc4c5 100644
--- a/.github/workflows/arm-linux-gnueabihf.yaml
+++ b/.github/workflows/arm-linux-gnueabihf.yaml
@@ -51,19 +51,16 @@ jobs:
         uses: actions/cache@v4
         with:
           path: toolchain
-          key: gcc-arm-9.2-2019.12-x86_64-arm-none-linux-gnueabihf
+          key: gcc-arm-11.2-2022.02-x86_64-arm-none-linux-gnueabihf
 
       - name: Download toolchain
         if: steps.cache-toolchain.outputs.cache-hit != 'true'
         shell: bash
         run: |
-          git lfs install
-          export GIT_CLONE_PROTECTION_ACTIVE=false
-          git clone https://huggingface.co/csukuangfj/arm-linux-gcc
-          ls -lh arm-linux-gcc
-
+          curl -SL -O https://huggingface.co/csukuangfj/arm-linux-gcc/resolve/main/gcc-arm-11.2-2022.02-x86_64-arm-none-linux-gnueabihf.tar.xz
           mkdir $GITHUB_WORKSPACE/toolchain
-          tar xvf ./arm-linux-gcc/gcc-arm-9.2-2019.12-x86_64-arm-none-linux-gnueabihf.tar.xz --strip-components 1 -C $GITHUB_WORKSPACE/toolchain
+          tar xvf ./gcc-arm-11.2-2022.02-x86_64-arm-none-linux-gnueabihf.tar.xz --strip-components 1 -C $GITHUB_WORKSPACE/toolchain
+          rm -v gcc-arm-11.2-2022.02-x86_64-arm-none-linux-gnueabihf.tar.xz
 
       - name: Display toolchain info
         shell: bash
@@ -96,6 +93,8 @@ jobs:
 
           file build-arm-linux-gnueabihf/bin/sherpa-onnx
 
+          strings build-arm-linux-gnueabihf/bin/sherpa-onnx | grep ^GLIBC
+
       - name: Copy files
         shell: bash
         run: |
diff --git a/cmake/onnxruntime-linux-arm-static.cmake b/cmake/onnxruntime-linux-arm-static.cmake
index fa9170e3..2799c92c 100644
--- a/cmake/onnxruntime-linux-arm-static.cmake
+++ b/cmake/onnxruntime-linux-arm-static.cmake
@@ -14,19 +14,20 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-arm-static_lib-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-arm-static_lib-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=3f2ba38156d2facfb732c0fe53bc1eaaf2791d9a91dd240380e3d53716798b09")
+# requires gcc 11
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-arm-static_lib-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-arm-static_lib-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=334a51dbdc6812f91ee88356cedca14b097ed2907c80aa2b91670680e155ad9f")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-arm-static_lib-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-arm-static_lib-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-arm-static_lib-1.17.1.zip
-  /tmp/onnxruntime-linux-arm-static_lib-1.17.1.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-arm-static_lib-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-arm-static_lib-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-arm-static_lib-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-arm-static_lib-1.23.2.zip
+  /tmp/onnxruntime-linux-arm-static_lib-1.23.2.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-arm-static_lib-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-linux-arm.cmake b/cmake/onnxruntime-linux-arm.cmake
index 28bd4268..b0c042ab 100644
--- a/cmake/onnxruntime-linux-arm.cmake
+++ b/cmake/onnxruntime-linux-arm.cmake
@@ -14,19 +14,20 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-arm-1.17.1-patched.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-arm-1.17.1-patched.zip")
-set(onnxruntime_HASH "SHA256=4ec00f7adc7341c068babea3d0f607349655e598222d4212115ae4f52619efdb")
+# requires gcc 11
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-arm-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-arm-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=c00aae409731930433badaf7d629499b9a1dcfac4dd67ad6b6a4838349bd6ba5")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-arm-1.17.1-patched.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-arm-1.17.1-patched.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-arm-1.17.1-patched.zip
-  /tmp/onnxruntime-linux-arm-1.17.1-patched.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-arm-1.17.1-patched.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-arm-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-arm-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-arm-1.23.2.zip
+  /tmp/onnxruntime-linux-arm-1.23.2.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-arm-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)

commit 90efd842351deaf6f89bd4b3b1ed9843559bb2dd
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Sat Jan 10 10:51:58 2026 +0800

    Use onnxruntime v1.23.2 for Linux aarch64 (#3016)

diff --git a/.github/workflows/aarch64-linux-gnu-shared.yaml b/.github/workflows/aarch64-linux-gnu-shared.yaml
index 1c6fdf3f..8c1b2b61 100644
--- a/.github/workflows/aarch64-linux-gnu-shared.yaml
+++ b/.github/workflows/aarch64-linux-gnu-shared.yaml
@@ -265,6 +265,10 @@ jobs:
           export PATH=$PWD/build/install/bin:$PATH
           export EXE=sherpa-onnx-offline
 
+          ls -lh build/bin/sherpa-onnx-offline
+
           readelf -d build/bin/sherpa-onnx-offline
 
+          strings build/bin/sherpa-onnx-offline | grep ^GLIBC
+
           .github/scripts/test-offline-moonshine.sh
diff --git a/.github/workflows/aarch64-linux-gnu-static.yaml b/.github/workflows/aarch64-linux-gnu-static.yaml
index b83925dc..896556b4 100644
--- a/.github/workflows/aarch64-linux-gnu-static.yaml
+++ b/.github/workflows/aarch64-linux-gnu-static.yaml
@@ -43,7 +43,8 @@ jobs:
       - name: Build sherpa-onnx
         uses: addnab/docker-run-action@v3
         with:
-            image: quay.io/pypa/manylinux2014_aarch64
+            # image: quay.io/pypa/manylinux2014_aarch64 # it does not have gcc 11
+            image: ghcr.io/csukuangfj/manylinux2014-aarch64-gcc11:latest # i have built gcc 11 from source
             options: |
               --volume ${{ github.workspace }}/:/k2-fsa/sherpa-onnx
             shell: bash
@@ -55,6 +56,22 @@ jobs:
               gcc --version
               g++ --version
 
+              ldd --version
+
+              #  FORCE GCC 11
+              export GCC_ROOT=/opt/gcc-11.4.0
+              export CC=$GCC_ROOT/bin/gcc
+              export CXX=$GCC_ROOT/bin/g++
+              export PATH=$GCC_ROOT/bin:$PATH
+
+              gcc --version
+              which gcc
+
+              g++ --version
+              which g++
+
+              ldd --version
+
               echo "pwd"
 
               ls -lh
@@ -179,6 +196,10 @@ jobs:
           export PATH=$PWD/build/bin:$PATH
           export EXE=sherpa-onnx-offline
 
+          ls -lh build/bin/sherpa-onnx-offline
+
           readelf -d build/bin/sherpa-onnx-offline
 
+          strings build/bin/sherpa-onnx-offline | grep ^GLIBC
+
           .github/scripts/test-offline-moonshine.sh
diff --git a/cmake/onnxruntime-linux-aarch64-static.cmake b/cmake/onnxruntime-linux-aarch64-static.cmake
index 4752e010..7df021ac 100644
--- a/cmake/onnxruntime-linux-aarch64-static.cmake
+++ b/cmake/onnxruntime-linux-aarch64-static.cmake
@@ -14,19 +14,19 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-aarch64-static_lib-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-aarch64-static_lib-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=831b9a3869501040b4399de85f34c4f170e2bcbd41881edaeb553f8dc4080985")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip")
+set(onnxruntime_HASH "SHA256=7a603d836aa27d37197eb76f055d3c9e4e81d3a5a343c60000d7b6345bc6c80f")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-aarch64-static_lib-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-aarch64-static_lib-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-aarch64-static_lib-1.17.1.zip
-  /tmp/onnxruntime-linux-aarch64-static_lib-1.17.1.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-aarch64-static_lib-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip
+  /tmp/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-aarch64-static_lib-1.23.2-glibc2_17.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-linux-aarch64.cmake b/cmake/onnxruntime-linux-aarch64.cmake
index a18f59e5..94a826e5 100644
--- a/cmake/onnxruntime-linux-aarch64.cmake
+++ b/cmake/onnxruntime-linux-aarch64.cmake
@@ -14,19 +14,19 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip")
-set(onnxruntime_HASH "SHA256=6e0e68985f8dd1f643e5a4dbe7cd54c9e176a0cc62249c6bee0699b87fc6d4fb")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=2a40a5323827bc59844d00ffdd3697d5e30dccb691233054bace0dc61cfa8341")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip
-  /tmp/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip
-  /star-fj/fangjun/download/github/onnxruntime-linux-aarch64-glibc2_17-Release-1.17.1-patched.zip
+  $ENV{HOME}/Downloads/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip
+  /tmp/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip
+  /star-fj/fangjun/download/github/onnxruntime-linux-aarch64-glibc2_17-Release-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-linux-riscv64-spacemit.cmake b/cmake/onnxruntime-linux-riscv64-spacemit.cmake
old mode 100755
new mode 100644
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
old mode 100755
new mode 100644

commit 151dd9c5fefe2b88023f4abb0f64d0458fbb49ca
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Fri Jan 9 15:55:57 2026 +0800

    Add C++ runtime for Whisper with Ascend NPU (#3009)

diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index a3221341..09d38aec 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -229,6 +229,7 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
   list(APPEND sources
     ./ascend/offline-paraformer-model-ascend.cc
     ./ascend/offline-sense-voice-model-ascend.cc
+    ./ascend/offline-whisper-model-ascend.cc
     ./ascend/offline-zipformer-ctc-model-ascend.cc
     ./ascend/utils.cc
   )
diff --git a/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.cc
new file mode 100644
index 00000000..f1be3152
--- /dev/null
+++ b/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.cc
@@ -0,0 +1,695 @@
+// sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.cc
+//
+// Copyright (c)  2026  Xiaomi Corporation
+#include "sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h"
+
+#include <algorithm>
+#include <array>
+#include <memory>
+#include <mutex>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "sherpa-onnx/csrc/ascend/macros.h"
+#include "sherpa-onnx/csrc/ascend/utils.h"
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/math.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+// masked positions: 1
+// unmasked positions: 0
+static void UpdateCausalMask(int32_t offset, int32_t capacity, int32_t *p) {
+  std::fill(p, p + offset, 0);
+  std::fill(p + offset, p + capacity, 1);
+}
+
+static WhisperModelType ParseWhisperModelFromString(const std::string &s) {
+  auto pos = s.find('-');
+  if (pos == std::string::npos) {
+    SHERPA_ONNX_LOGE("Unexpected model input '%s'", s.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (s.substr(pos + 1) != "mel") {
+    SHERPA_ONNX_LOGE("Unexpected model input '%s'", s.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (pos == 0) {
+    SHERPA_ONNX_LOGE("Empty model name in '%s'", s.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  return ParseWhisperModelType(s.substr(0, pos));
+}
+
+class OfflineWhisperModelAscend::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config) : config_(config) {
+    PreInit();
+
+    InitEncoder(config_.whisper.encoder);
+    InitDecoder(config_.whisper.decoder);
+
+    PostInit();
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+    PreInit();
+
+    {
+      auto buf = ReadFile(mgr, config_.whisper.encoder);
+      InitEncoder(buf.data(), buf.size());
+    }
+
+    {
+      auto buf = ReadFile(mgr, config_.whisper.decoder);
+      InitDecoder(buf.data(), buf.size());
+    }
+
+    PostInit();
+  }
+
+  OfflineWhisperDecoderResult Run(std::vector<float> features) {
+    // TODO(fangjun): Support multi clients
+    std::lock_guard<std::mutex> lock(mutex_);
+
+    OfflineWhisperDecoderResult r;
+
+    if (features.empty()) {
+      return r;
+    }
+
+    int32_t num_frames = features.size() / feat_dim_;
+    if (num_frames > num_frames_) {
+      SHERPA_ONNX_LOGE(
+          "Number of input frames %d is too large. Truncate it to %d frames.",
+          num_frames, num_frames_);
+
+      SHERPA_ONNX_LOGE(
+          "Recognition result may be truncated/incomplete. Please select a "
+          "model accepting longer audios or use VAD to cut your audio into "
+          "small chunks.");
+
+      num_frames = num_frames_;
+    }
+
+    // assume at most 6 tokens per second
+    int32_t num_possible_tokens = num_frames / 100.0 * 6;
+    num_possible_tokens =
+        std::min<int32_t>(num_possible_tokens, n_text_ctx_ / 2);
+
+    features.resize(num_frames_ * feat_dim_, 0);
+
+    // (num_frames_, feat_dim_) -> (feat_dim_, num_frames_)
+    features = Transpose(features.data(), num_frames_, feat_dim_);
+
+    RunEncoder(std::move(features));
+
+    // Note(fangjun): No need to intialize the self kv cache to 0
+
+    std::vector<int32_t> sot_sequence(sot_sequence_);
+
+    if (IsMultilingual(model_type_)) {
+      if (config_.whisper.task == "translate") {
+        sot_sequence[2] = translate_;
+      } else if (config_.whisper.task != "transcribe") {
+        SHERPA_ONNX_LOGE(
+            "Valid task values are: translate, transcribe. Given: '%s'",
+            config_.whisper.task.c_str());
+        SHERPA_ONNX_EXIT(-1);
+      }
+
+      if (!config_.whisper.language.empty()) {
+        int32_t lang_id = GetWhisperLanguageTokenId(config_.whisper.language);
+        if (lang_id < 0) {
+          SHERPA_ONNX_LOGE("Unsupported language: '%s'",
+                           config_.whisper.language.c_str());
+          SHERPA_ONNX_EXIT(-1);
+        }
+        r.lang = config_.whisper.language;
+
+        sot_sequence[1] = lang_id;
+      } else {
+        // detect language
+        if (config_.debug) {
+          SHERPA_ONNX_LOGE("Detecting language.");
+        }
+        token_offset_mask_cpu_[0] = sot_sequence_[0];
+        token_offset_mask_cpu_[1] = 0;
+        UpdateCausalMask(0, n_text_ctx_, token_offset_mask_cpu_.data() + 2);
+
+        int32_t lang_id = DetectLanguage();
+        r.lang = GetWhisperLanguageCode(lang_id);
+
+        if (config_.debug) {
+          SHERPA_ONNX_LOGE("Detected Language: %s", r.lang.c_str());
+        }
+
+        sot_sequence[1] = lang_id;
+      }
+    }
+
+    int32_t &token = token_offset_mask_cpu_[0];
+    int32_t &offset = token_offset_mask_cpu_[1];
+    offset = 0;
+
+    int32_t *p_mask = token_offset_mask_cpu_.data() + 2;
+    UpdateCausalMask(offset, n_text_ctx_, p_mask);
+
+    for (int32_t i = 0; i < sot_sequence.size(); ++i) {
+      token = sot_sequence[i];
+      token = RunDecoder();
+      p_mask[offset] = 0;
+
+      offset += 1;
+    }
+
+    if (token == eot_) {
+      return r;
+    }
+
+    r.tokens.reserve(num_possible_tokens);
+
+    while (offset < num_possible_tokens && token != eot_) {
+      r.tokens.push_back(token);
+      token = RunDecoder();
+
+      p_mask[offset] = 0;
+      offset += 1;
+    }
+
+    return r;
+  }
+
+  int32_t FeatureDim() const { return feat_dim_; }
+
+ private:
+  void RunEncoder(std::vector<float> features) {
+    aclError ret = aclrtMemcpy(features_ptr_, features.size() * sizeof(float),
+                               features.data(), features.size() * sizeof(float),
+                               ACL_MEMCPY_HOST_TO_DEVICE);
+
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    AclMdlDataset input_dataset;
+    input_dataset.AddBuffer(encoder_input_buffer_[0]);
+
+    AclMdlDataset output_dataset;
+
+    for (auto &p : encoder_output_buffer_) {
+      output_dataset.AddBuffer(p);
+    }
+
+    ret = aclmdlExecute(*encoder_model_, input_dataset, output_dataset);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclmdlExecute");
+  }
+
+  int32_t RunDecoder() {
+    RunDecoderImpl();
+
+    UpdateSelfKvCache();
+
+    auto ret = aclrtMemcpy(
+        logits_cpu_.data(), logits_cpu_.size() * sizeof(float), logits_ptr_,
+        logits_cpu_.size() * sizeof(float), ACL_MEMCPY_DEVICE_TO_HOST);
+
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    return MaxElementIndex(logits_cpu_.data(), logits_cpu_.size());
+  }
+
+  int32_t DetectLanguage() {
+    RunDecoderImpl();
+
+    // No need to update the Self KV cache
+
+    auto ret = aclrtMemcpy(
+        logits_cpu_.data(), logits_cpu_.size() * sizeof(float), logits_ptr_,
+        logits_cpu_.size() * sizeof(float), ACL_MEMCPY_DEVICE_TO_HOST);
+
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    const auto &all_lang_ids = GetAllWhisperLanguageTokenIds();
+    int32_t lang_id = all_lang_ids[0];
+    float this_logit = logits_cpu_[lang_id];
+
+    for (int32_t i = 1; i != all_lang_ids.size(); ++i) {
+      int32_t id = all_lang_ids[i];
+      float p = logits_cpu_[id];
+
+      if (p > this_logit) {
+        this_logit = p;
+        lang_id = id;
+      }
+    }
+
+    return lang_id;
+  }
+
+  void RunDecoderImpl() {
+    aclError ret =
+        aclrtMemcpy(token_ptr_, token_offset_mask_cpu_.size() * sizeof(int32_t),
+                    token_offset_mask_cpu_.data(),
+                    token_offset_mask_cpu_.size() * sizeof(int32_t),
+                    ACL_MEMCPY_HOST_TO_DEVICE);
+
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+
+    AclMdlDataset input_dataset;
+
+    for (auto &p : decoder_input_buffer_) {
+      input_dataset.AddBuffer(p);
+    }
+
+    AclMdlDataset output_dataset;
+
+    for (auto &p : decoder_output_buffer_) {
+      output_dataset.AddBuffer(p);
+    }
+
+    ret = aclmdlExecute(*decoder_model_, input_dataset, output_dataset);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclmdlExecute");
+  }
+
+  void UpdateSelfKvCache() {
+    int32_t offset = token_offset_mask_cpu_[1];
+    for (int32_t i = 0; i < n_text_layer_ * 2; ++i) {
+      const float *src = delta_kv_ptr_[i];
+      float *dst = self_kv_ptr_[i] + offset * n_text_state_;
+
+      auto ret = aclrtMemcpy(dst, n_text_state_ * sizeof(float), src,
+                             n_text_state_ * sizeof(float),
+                             ACL_MEMCPY_DEVICE_TO_DEVICE);
+      SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+    }
+  }
+
+  void PreInit() {
+    int32_t device_id = 0;
+    aclError ret = aclrtSetDevice(device_id);
+    SHERPA_ONNX_ASCEND_CHECK(
+        ret, "Failed to call aclrtSetDevice with device id: %d", device_id);
+
+    context_ = std::make_unique<AclContext>(device_id);
+
+    ret = aclrtSetCurrentContext(*context_);
+    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtSetCurrentContext");
+  }
+
+  void PostInit() {
+    PostInitEncoder();
+    PostInitDecoder();
+    Preallocate();
+    InitSotSequence();
+
+    InitEncoderBuffer();
+    InitDecoderBuffer();
+  }
+
+  void InitEncoderBuffer() {
+    AclDataBuffer features_buf(features_ptr_,
+                               feat_dim_ * num_frames_ * sizeof(float));
+    encoder_input_buffer_.clear();
+    encoder_input_buffer_.push_back(std::move(features_buf));
+
+    encoder_output_buffer_.reserve(cross_kv_ptr_.size());
+    for (auto p : cross_kv_ptr_) {
+      AclDataBuffer tmp_buffer(p,
+                               num_out_frames_ * n_text_state_ * sizeof(float));
+      encoder_output_buffer_.push_back(std::move(tmp_buffer));
+    }
+  }
+
+  void InitDecoderBuffer() {
+    decoder_input_buffer_.reserve(1 + 2 * n_text_layer_ + 2 * n_text_layer_ +
+                                  1 + 1);
+    // token, self_kv, cross_kv, offset, mask
+
+    AclDataBuffer token_buf(token_ptr_, sizeof(int32_t));
+    decoder_input_buffer_.push_back(std::move(token_buf));
+
+    for (auto &p : self_kv_ptr_) {
+      AclDataBuffer tmp_buffer(p, n_text_ctx_ * n_text_state_ * sizeof(float));
+      decoder_input_buffer_.push_back(std::move(tmp_buffer));
+    }
+
+    for (auto &p : cross_kv_ptr_) {
+      AclDataBuffer tmp_buffer(p,
+                               num_out_frames_ * n_text_state_ * sizeof(float));
+      decoder_input_buffer_.push_back(std::move(tmp_buffer));
+    }
+
+    AclDataBuffer offset_buf(offset_ptr_, sizeof(int32_t));
+    decoder_input_buffer_.push_back(std::move(offset_buf));
+
+    AclDataBuffer mask_buf(mask_ptr_, n_text_ctx_ * sizeof(int32_t));
+    decoder_input_buffer_.push_back(std::move(mask_buf));
+
+    decoder_output_buffer_.reserve(1 + 2 * n_text_layer_);
+    AclDataBuffer logits_buf(logits_ptr_, vocab_size_ * sizeof(float));
+    decoder_output_buffer_.push_back(std::move(logits_buf));
+
+    for (auto &p : delta_kv_ptr_) {
+      AclDataBuffer tmp_buffer(p, n_text_state_ * sizeof(float));
+      decoder_output_buffer_.push_back(std::move(tmp_buffer));
+    }
+  }
+
+  void InitSotSequence() {
+    switch (model_type_) {
+      case WhisperModelType::TinyEn:
+        // fallthrough
+      case WhisperModelType::BaseEn:
+        // fallthrough
+      case WhisperModelType::SmallEn:
+        // fallthrough
+      case WhisperModelType::MediumEn:
+        // fallthrough
+        // <|startoftranscript|><|notimestamps|>
+        sot_sequence_ = {50257, 50362};
+        eot_ = 50256;
+        break;
+      case WhisperModelType::Tiny:
+      case WhisperModelType::Base:
+        // fallthrough
+      case WhisperModelType::Small:
+        // fallthrough
+      case WhisperModelType::Medium:
+        // fallthrough
+      case WhisperModelType::Large:
+        // <|startoftranscript|><|en|><|transcribe|><|notimestamps|>
+        sot_sequence_ = {50258, 50259, 50359, 50363};
+        eot_ = 50257;
+        translate_ = 50358;
+        break;
+      default:
+        SHERPA_ONNX_LOGE("Unsupported model type: '%s'",
+                         ToString(model_type_).c_str());
+        SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (config_.debug) {
+      std::ostringstream os;
+      os << "sot_sequence: ";
+      for (auto i : sot_sequence_) {
+        os << i << " ";
+      }
+      os << "\n";
+      os << "eot: " << eot_ << "\n";
+      SHERPA_ONNX_LOGE("%s", os.str().c_str());
+    }
+  }
+
+  void Preallocate() {
+    // Allocate a single big block.
+    int32_t total = 0;
+
+    // features: (1, feat_dim_, num_frames_)
+    total += num_frames_ * feat_dim_ * sizeof(float);
+    // token: (1,)
+    total += sizeof(int32_t);
+    // offset: (1,)
+    total += sizeof(int32_t);
+
+    // mask: (1, n_text_ctx_)
+    total += n_text_ctx_ * sizeof(int32_t);
+
+    // logits: (1, 1, vocab_size_)
+    total += vocab_size_ * sizeof(float);
+
+    // cross_kv: n_text_layer_ * 2 * (num_out_frames_, n_text_state_)
+
+    total +=
+        n_text_layer_ * 2 * num_out_frames_ * n_text_state_ * sizeof(float);
+
+    // self_kv: n_text_layer_ * 2 * (n_text_ctx_, n_text_state_)
+    total += n_text_layer_ * 2 * n_text_ctx_ * n_text_state_ * sizeof(float);
+
+    // delta_kv: n_text_layer_ * 2 * (1, 1, n_text_state_)
+    total += n_text_layer_ * 2 * n_text_state_ * sizeof(float);
+
+    ptr_ = std::make_unique<AclDevicePtr>(total);
+    float *start = ptr_->Get<float>();
+    int32_t *start_int32 = ptr_->Get<int32_t>();
+    int32_t offset = 0;
+
+    // (1, feat_dim_, num_frames_)
+    features_ptr_ = start + offset;
+    offset += feat_dim_ * num_frames_;  // in float or in int32_t, not in bytes
+
+    // make sure token,offset,mask are contiguous in device memory
+
+    // (1,)
+    token_ptr_ = start_int32 + offset;
+    offset += 1;
+
+    // (1,)
+    offset_ptr_ = start_int32 + offset;
+    offset += 1;
+
+    // (1, n_text_ctx_)
+    mask_ptr_ = start_int32 + offset;
+    offset += n_text_ctx_;
+
+    // (1, 1, vocab_size_)
+    logits_ptr_ = start + offset;
+    offset += vocab_size_;
+
+    // (1, num_frames_, n_text_state_)
+    cross_kv_ptr_.reserve(n_text_layer_ * 2);
+    for (int32_t i = 0; i < n_text_layer_ * 2; ++i) {
+      auto p = start + offset;
+      offset += num_out_frames_ * n_text_state_;
+      cross_kv_ptr_.push_back(std::move(p));
+    }
+
+    // (1, n_text_ctx_, n_text_state_)
+    self_kv_ptr_.reserve(n_text_layer_ * 2);
+    for (int32_t i = 0; i < n_text_layer_ * 2; ++i) {
+      auto p = start + offset;
+      offset += n_text_ctx_ * n_text_state_;
+      self_kv_ptr_.push_back(std::move(p));
+    }
+
+    // (1, 1, n_text_state_)
+    delta_kv_ptr_.reserve(n_text_layer_ * 2);
+    for (int32_t i = 0; i < n_text_layer_ * 2; ++i) {
+      auto p = start + offset;
+      offset += n_text_state_;
+      delta_kv_ptr_.push_back(std::move(p));
+    }
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("Allocated %d bytes, or %.3f MB", total,
+                       total / 1024. / 1024.);
+    }
+  }
+
+  void PostInitEncoder() {
+    const std::vector<std::string> &names = encoder_model_->GetInputNames();
+    model_type_ = ParseWhisperModelFromString(names[0]);
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("model type: %s", ToString(model_type_).c_str());
+    }
+
+    const std::vector<std::vector<int64_t>> &input_shapes =
+        encoder_model_->GetInputShapes();
+
+    const auto &mel_shape = input_shapes[0];
+    if (mel_shape[0] != 1) {
+      SHERPA_ONNX_LOGE("It supports only batch size == 1. Given: %d",
+                       static_cast<int32_t>(mel_shape[0]));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    feat_dim_ = mel_shape[1];
+    num_frames_ = mel_shape[2];
+
+    const std::vector<std::vector<int64_t>> &output_shapes =
+        encoder_model_->GetOutputShapes();
+
+    n_text_layer_ = output_shapes.size() / 2;
+
+    num_out_frames_ = output_shapes[0][1];
+    n_text_state_ = output_shapes[0].back();
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("feat_dim_: %d", feat_dim_);
+      SHERPA_ONNX_LOGE("num_frames_: %d", num_frames_);
+      SHERPA_ONNX_LOGE("num_out_frames_: %d", num_out_frames_);
+      SHERPA_ONNX_LOGE("n_text_layer_: %d", n_text_layer_);
+      SHERPA_ONNX_LOGE("n_text_state_: %d", n_text_state_);
+    }
+  }
+
+  void PostInitDecoder() {
+    const std::vector<std::vector<int64_t>> &input_shapes =
+        decoder_model_->GetInputShapes();
+    // tokens, self_kv, cross_kv, offset, mask
+    int32_t expected_num_inputs = 1 + 2 * n_text_layer_ + 2 * n_text_layer_ + 2;
+    if (input_shapes.size() != expected_num_inputs) {
+      SHERPA_ONNX_LOGE("Expect %d inputs. Actual: %d", expected_num_inputs,
+                       static_cast<int32_t>(input_shapes.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    const auto &s = input_shapes[1];
+    if (s[0] != 1) {
+      SHERPA_ONNX_LOGE("Support only batch size 1. Given: %d",
+                       static_cast<int32_t>(s[0]));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    n_text_ctx_ = s[1];
+    token_offset_mask_cpu_.resize(1 + 1 + n_text_ctx_);
+
+    if (s[2] != n_text_state_) {
+      SHERPA_ONNX_LOGE("Expect n_text_state_ %d. Given: %d", n_text_state_,
+                       static_cast<int32_t>(s[2]));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("n_text_ctx_: %d", n_text_ctx_);
+    }
+
+    const std::vector<std::vector<int64_t>> &output_shapes =
+        decoder_model_->GetOutputShapes();
+
+    vocab_size_ = output_shapes[0].back();
+    logits_cpu_.resize(vocab_size_);
+
+    if (config_.debug) {
+      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
+    }
+  }
+
+  void InitEncoder(const std::string &filename) {
+    encoder_model_ = std::make_unique<AclModel>(filename);
+    if (config_.debug) {
+      auto s = encoder_model_->GetInfo();
+
+      SHERPA_ONNX_LOGE("----encoder----\n%s\n", s.c_str());
+    }
+  }
+
+  void InitEncoder(void *data, size_t size) {
+    encoder_model_ = std::make_unique<AclModel>(data, size);
+    if (config_.debug) {
+      auto s = encoder_model_->GetInfo();
+      SHERPA_ONNX_LOGE("----encoder----\n%s\n", s.c_str());
+    }
+  }
+
+  void InitDecoder(const std::string &filename) {
+    decoder_model_ = std::make_unique<AclModel>(filename);
+    if (config_.debug) {
+      auto s = decoder_model_->GetInfo();
+
+      SHERPA_ONNX_LOGE("----decoder----\n%s\n", s.c_str());
+    }
+  }
+
+  void InitDecoder(void *data, size_t size) {
+    decoder_model_ = std::make_unique<AclModel>(data, size);
+    if (config_.debug) {
+      auto s = decoder_model_->GetInfo();
+      SHERPA_ONNX_LOGE("----decoder----\n%s\n", s.c_str());
+    }
+  }
+
+ private:
+  std::mutex mutex_;
+  Acl acl_;
+
+  std::unique_ptr<AclContext> context_;
+
+  std::unique_ptr<AclModel> encoder_model_;
+  std::unique_ptr<AclModel> decoder_model_;
+
+  OfflineModelConfig config_;
+
+  // tiny, tiny.en, base.en, base, etc
+  WhisperModelType model_type_;
+  int32_t feat_dim_ = 0;
+  int32_t num_frames_ = 0;
+  int32_t num_out_frames_ = 0;
+  int32_t n_text_layer_ = 0;
+  int32_t n_text_ctx_ = 0;
+  int32_t n_text_state_ = 0;
+  int32_t vocab_size_ = 0;
+
+  std::unique_ptr<AclDevicePtr> ptr_;
+
+  // All of the following raw pointers will point to some already allocated
+  // device memory. No need to free them.
+  float *features_ptr_ = nullptr;
+  int32_t *token_ptr_ = nullptr;
+  int32_t *offset_ptr_ = nullptr;
+  int32_t *mask_ptr_ = nullptr;
+  float *logits_ptr_ = nullptr;
+
+  std::vector<float *> cross_kv_ptr_;
+  std::vector<float *> self_kv_ptr_;
+  std::vector<float *> delta_kv_ptr_;
+
+  std::vector<int32_t> token_offset_mask_cpu_;
+  std::vector<float> logits_cpu_;
+
+  std::vector<int32_t> sot_sequence_;
+  int32_t eot_ = 0;
+  int32_t translate_ = 0;
+
+  std::vector<AclDataBuffer> encoder_input_buffer_;
+  std::vector<AclDataBuffer> encoder_output_buffer_;
+
+  std::vector<AclDataBuffer> decoder_input_buffer_;
+  std::vector<AclDataBuffer> decoder_output_buffer_;
+};
+
+OfflineWhisperModelAscend::OfflineWhisperModelAscend(
+    const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineWhisperModelAscend::OfflineWhisperModelAscend(
+    Manager *mgr, const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineWhisperModelAscend::~OfflineWhisperModelAscend() = default;
+
+OfflineWhisperDecoderResult OfflineWhisperModelAscend::Run(
+    std::vector<float> features) const {
+  return impl_->Run(std::move(features));
+}
+
+int32_t OfflineWhisperModelAscend::FeatureDim() const {
+  return impl_->FeatureDim();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineWhisperModelAscend::OfflineWhisperModelAscend(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineWhisperModelAscend::OfflineWhisperModelAscend(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h b/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h
new file mode 100644
index 00000000..fe74887c
--- /dev/null
+++ b/sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h
@@ -0,0 +1,38 @@
+// sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h
+//
+// Copyright (c)  2026  Xiaomi Corporation
+#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_WHISPER_MODEL_ASCEND_H_
+#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_WHISPER_MODEL_ASCEND_H_
+
+#include <cstdint>
+#include <memory>
+#include <vector>
+
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineWhisperModelAscend {
+ public:
+  ~OfflineWhisperModelAscend();
+
+  explicit OfflineWhisperModelAscend(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineWhisperModelAscend(Manager *mgr, const OfflineModelConfig &config);
+
+  /**
+   * @param features A tensor of shape (1, num_frames, feat_dim)
+   */
+  OfflineWhisperDecoderResult Run(std::vector<float> features) const;
+
+  int32_t FeatureDim() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_WHISPER_MODEL_ASCEND_H_
diff --git a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
index 44547d3b..626b634a 100644
--- a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
+++ b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
@@ -64,7 +64,7 @@ class OfflineZipformerCtcModelAscend::Impl {
             "model accepting longer audios.");
       }
 
-      features.resize(max_num_frames_ * feat_dim_);
+      features.resize(max_num_frames_ * feat_dim_, 0);
 
       num_frames = max_num_frames_;
     }
diff --git a/sherpa-onnx/csrc/ascend/utils.cc b/sherpa-onnx/csrc/ascend/utils.cc
index f424d1f2..b5daace9 100644
--- a/sherpa-onnx/csrc/ascend/utils.cc
+++ b/sherpa-onnx/csrc/ascend/utils.cc
@@ -266,7 +266,7 @@ std::string AclModel::GetInfo() const {
     size_t size_in_bytes = aclmdlGetInputSizeByIndex(desc_->Get(), i);
 
     os << " size in bytes: " << size_in_bytes << "\n";
-    os << " size in MB:    " << size_in_bytes / 1024 / 1024 << "\n";
+    os << " size in MB:    " << size_in_bytes / 1024. / 1024 << "\n";
 
     const char *name = aclmdlGetInputNameByIndex(desc_->Get(), i);
     os << " name: " << name << "\n";
@@ -350,11 +350,14 @@ AclDataBuffer::AclDataBuffer(void *data, size_t size) {
   }
 }
 
-AclDataBuffer::~AclDataBuffer() {
+AclDataBuffer::~AclDataBuffer() { Release(); }
+
+void AclDataBuffer::Release() {
   if (p_) {
     aclError ret = aclDestroyDataBuffer(p_);
     SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclDestroyDataBuffer");
   }
+  p_ = nullptr;
 }
 
 AclTensorDesc::AclTensorDesc(aclDataType data_type, int num_dims,
diff --git a/sherpa-onnx/csrc/ascend/utils.h b/sherpa-onnx/csrc/ascend/utils.h
index 0a3aaf7c..1c87cfcb 100644
--- a/sherpa-onnx/csrc/ascend/utils.h
+++ b/sherpa-onnx/csrc/ascend/utils.h
@@ -61,6 +61,12 @@ class AclDevicePtr {
   AclDevicePtr &operator=(AclDevicePtr &&) = delete;
 
   void *Get() const { return p_; }
+
+  template <typename T>
+  T *Get() const {
+    return reinterpret_cast<T *>(p_);
+  }
+
   operator void *() { return p_; }
 
   size_t Size() const { return size_; }
@@ -171,8 +177,23 @@ class AclDataBuffer {
   AclDataBuffer(const AclDataBuffer &) = delete;
   AclDataBuffer &operator=(const AclDataBuffer &) = delete;
 
-  AclDataBuffer(AclDataBuffer &&) = delete;
-  AclDataBuffer &operator=(AclDataBuffer &&) = delete;
+  AclDataBuffer(AclDataBuffer &&other) {
+    p_ = other.p_;
+    other.p_ = nullptr;
+  }
+  AclDataBuffer &operator=(AclDataBuffer &&other) {
+    if (this == &other) {
+      return *this;
+    }
+
+    Release();
+
+    p_ = other.p_;
+    other.p_ = nullptr;
+    return *this;
+  }
+
+  void Release();
 
   aclDataBuffer *Get() const { return p_; }
   operator aclDataBuffer *() const { return p_; }
diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
index 64b7f1ee..8cd68020 100644
--- a/sherpa-onnx/csrc/math.cc
+++ b/sherpa-onnx/csrc/math.cc
@@ -98,4 +98,33 @@ void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
   inv_stddev->assign(inv_std.data(), inv_std.data() + num_cols);
 }
 
+void NormalizeWhisperFeatures(float *features, int32_t num_frames,
+                              int32_t feat_dim) {
+  // log_spec = torch.clamp(features, min=1e-10).log10()
+  // log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
+  // mel = (log_spec + 4.0) / 4.0
+
+  using Eigen::ArrayXXf;
+  using Eigen::Map;
+
+  Map<ArrayXXf, Eigen::RowMajor> feats(features, num_frames, feat_dim);
+
+  feats = feats.max(1e-10f).log10();
+
+  float max_v = feats.maxCoeff() - 8.0f;
+
+  feats = feats.max(max_v);
+  feats = (feats + 4.0f) / 4.0f;
+}
+
+int32_t MaxElementIndex(const float *v, int32_t n) {
+  // Map raw pointer to an Eigen vector (no copy)
+  Eigen::Map<const Eigen::VectorXf> vec(v, n);
+
+  Eigen::Index maxIndex;
+  vec.maxCoeff(&maxIndex);
+
+  return static_cast<int32_t>(maxIndex);
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
index ac6eb976..60855740 100644
--- a/sherpa-onnx/csrc/math.h
+++ b/sherpa-onnx/csrc/math.h
@@ -161,5 +161,10 @@ void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
                           std::vector<float> *mean,
                           std::vector<float> *inv_stddev);
 
+void NormalizeWhisperFeatures(float *features, int32_t num_frames,
+                              int32_t feat_dim);
+
+int32_t MaxElementIndex(const float *v, int32_t n);
+
 }  // namespace sherpa_onnx
 #endif  // SHERPA_ONNX_CSRC_MATH_H_
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index 4cfcc8c3..cb1c18a4 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -37,6 +37,7 @@
 #include "sherpa-onnx/csrc/offline-recognizer-transducer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-transducer-nemo-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-whisper-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer-whisper-tpl-impl.h"
 #include "sherpa-onnx/csrc/text-utils.h"
 
 #if SHERPA_ONNX_ENABLE_RKNN
@@ -56,6 +57,7 @@
 #include "sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.h"
 #include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
 #include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
+#include "sherpa-onnx/csrc/ascend/offline-whisper-model-ascend.h"
 #endif
 
 #if SHERPA_ONNX_ENABLE_QNN
@@ -154,10 +156,13 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
           config);
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(config);
+    } else if (!config.model_config.whisper.encoder.empty()) {
+      return std::make_unique<
+          OfflineRecognizerWhisperTplImpl<OfflineWhisperModelAscend>>(config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
-          "supported by Ascend NPU for non-streaming ASR.");
+          "Only SenseVoice, Paraformer, Whisper, and Zipformer CTC models are "
+          "currently supported by Ascend NPU for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
@@ -488,10 +493,14 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     } else if (!config.model_config.zipformer_ctc.model.empty()) {
       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(mgr,
                                                                        config);
+    } else if (!config.model_config.whisper.encoder.empty()) {
+      return std::make_unique<
+          OfflineRecognizerWhisperTplImpl<OfflineWhisperModelAscend>>(mgr,
+                                                                      config);
     } else {
       SHERPA_ONNX_LOGE(
-          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
-          "supported by Ascend NPU for non-streaming ASR.");
+          "Only SenseVoice, Paraformer, Whisper, and Zipformer CTC models are "
+          "currently supported by Ascend NPU for non-streaming ASR.");
       SHERPA_ONNX_EXIT(-1);
       return nullptr;
     }
diff --git a/sherpa-onnx/csrc/offline-recognizer-whisper-tpl-impl.h b/sherpa-onnx/csrc/offline-recognizer-whisper-tpl-impl.h
new file mode 100644
index 00000000..fc8700f4
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-recognizer-whisper-tpl-impl.h
@@ -0,0 +1,124 @@
+// sherpa-onnx/csrc/offline-recognizer-whisper-tpl-impl.h
+//
+// Copyright (c)  2026  Xiaomi Corporation
+
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_WHISPER_TPL_IMPL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_WHISPER_TPL_IMPL_H_
+
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/math.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/symbol-table.h"
+
+namespace sherpa_onnx {
+
+template <typename WhisperModel>
+class OfflineRecognizerWhisperTplImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerWhisperTplImpl(
+      const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(config),
+        config_(config),
+        symbol_table_(config_.model_config.tokens),
+        model_(std::make_unique<WhisperModel>(config.model_config)) {
+    Init();
+  }
+
+  template <typename Manager>
+  OfflineRecognizerWhisperTplImpl(Manager *mgr,
+                                  const OfflineRecognizerConfig &config)
+      : OfflineRecognizerImpl(mgr, config),
+        config_(config),
+        symbol_table_(mgr, config_.model_config.tokens),
+        model_(std::make_unique<WhisperModel>(mgr, config.model_config)) {
+    Init();
+  }
+
+  std::unique_ptr<OfflineStream> CreateStream() const override {
+    WhisperTag tag;
+    tag.dim = model_->FeatureDim();
+    return std::make_unique<OfflineStream>(tag);
+  }
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+    // batch decoding is not implemented yet
+    for (int32_t i = 0; i != n; ++i) {
+      DecodeStream(ss[i]);
+    }
+  }
+
+  void SetConfig(const OfflineRecognizerConfig &config) override {
+    config_.model_config.whisper = config.model_config.whisper;
+  }
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void Init() {
+    // tokens.txt from whisper is base64 encoded, so we need to decode it
+    symbol_table_.ApplyBase64Decode();
+
+    if (config_.decoding_method == "greedy_search") {
+      SHERPA_ONNX_LOGE("use greedy_search");
+    } else {
+      SHERPA_ONNX_LOGE(
+          "Only greedy_search is supported at present for whisper. Given '%s'",
+          config_.decoding_method.c_str());
+      SHERPA_ONNX_EXIT(-1);
+    }
+  }
+
+  void DecodeStream(OfflineStream *s) const {
+    int32_t feat_dim = s->FeatureDim();
+    std::vector<float> f = s->GetFrames();
+    int32_t num_frames = f.size() / feat_dim;
+
+    NormalizeWhisperFeatures(f.data(), num_frames, feat_dim);
+
+    auto r = model_->Run(std::move(f));
+    auto res = Convert(r, symbol_table_);
+
+    s->SetResult(res);
+  }
+
+  OfflineRecognitionResult Convert(const OfflineWhisperDecoderResult &src,
+                                   const SymbolTable &sym_table) const {
+    OfflineRecognitionResult r;
+    r.tokens.reserve(src.tokens.size());
+
+    std::string text;
+    for (auto i : src.tokens) {
+      if (!sym_table.Contains(i)) {
+        continue;
+      }
+
+      std::string s = sym_table[i];
+      s = ApplyInverseTextNormalization(s);
+      s = ApplyHomophoneReplacer(std::move(s));
+
+      text += s;
+      r.tokens.push_back(s);
+    }
+
+    r.text = text;
+    r.lang = src.lang;
+
+    return r;
+  }
+
+ private:
+  OfflineRecognizerConfig config_;
+  SymbolTable symbol_table_;
+  std::unique_ptr<WhisperModel> model_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_WHISPER_TPL_IMPL_H_
diff --git a/sherpa-onnx/csrc/offline-whisper-decoder.h b/sherpa-onnx/csrc/offline-whisper-decoder.h
index 6432757f..0aa65574 100644
--- a/sherpa-onnx/csrc/offline-whisper-decoder.h
+++ b/sherpa-onnx/csrc/offline-whisper-decoder.h
@@ -13,12 +13,6 @@
 
 namespace sherpa_onnx {
 
-struct OfflineWhisperDecoderResult {
-  /// The decoded token IDs
-  std::vector<int32_t> tokens;
-  std::string lang;
-};
-
 class OfflineWhisperDecoder {
  public:
   virtual ~OfflineWhisperDecoder() = default;
diff --git a/sherpa-onnx/csrc/offline-whisper-model-config.cc b/sherpa-onnx/csrc/offline-whisper-model-config.cc
index 6afa1f51..3422c8b8 100644
--- a/sherpa-onnx/csrc/offline-whisper-model-config.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model-config.cc
@@ -5,6 +5,8 @@
 #include "sherpa-onnx/csrc/offline-whisper-model-config.h"
 
 #include <string>
+#include <unordered_map>
+#include <vector>
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
@@ -90,4 +92,164 @@ std::string OfflineWhisperModelConfig::ToString() const {
   return os.str();
 }
 
+bool IsMultilingual(WhisperModelType model_type) {
+  switch (model_type) {
+    case WhisperModelType::TinyEn:
+    case WhisperModelType::BaseEn:
+    case WhisperModelType::SmallEn:
+    case WhisperModelType::MediumEn:
+      return false;  // English-only models
+
+    case WhisperModelType::Tiny:
+    case WhisperModelType::Base:
+    case WhisperModelType::Small:
+    case WhisperModelType::Medium:
+    case WhisperModelType::Large:
+      return true;  // Multilingual models
+  }
+
+  SHERPA_ONNX_LOGE("Unsupported model: %s", ToString(model_type).c_str());
+  SHERPA_ONNX_EXIT(-1);
+  // Safety fallback (should never be hit)
+  return false;
+}
+
+std::string ToString(WhisperModelType model) {
+  switch (model) {
+    case WhisperModelType::Tiny:
+      return "tiny";
+    case WhisperModelType::TinyEn:
+      return "tiny.en";
+    case WhisperModelType::Base:
+      return "base";
+    case WhisperModelType::BaseEn:
+      return "base.en";
+    case WhisperModelType::Small:
+      return "small";
+    case WhisperModelType::SmallEn:
+      return "small.en";
+    case WhisperModelType::Medium:
+      return "medium";
+    case WhisperModelType::MediumEn:
+      return "medium.en";
+    case WhisperModelType::Large:
+      return "large";
+  }
+  return "unknown";
+}
+
+WhisperModelType ParseWhisperModelType(const std::string &name) {
+  if (name == "tiny") return WhisperModelType::Tiny;
+  if (name == "tiny.en") return WhisperModelType::TinyEn;
+  if (name == "base") return WhisperModelType::Base;
+  if (name == "base.en") return WhisperModelType::BaseEn;
+  if (name == "small") return WhisperModelType::Small;
+  if (name == "small.en") return WhisperModelType::SmallEn;
+  if (name == "medium") return WhisperModelType::Medium;
+  if (name == "medium.en") return WhisperModelType::MediumEn;
+  if (name == "large") return WhisperModelType::Large;
+
+  SHERPA_ONNX_LOGE("Unknown Whisper model: '%s'", name.c_str());
+  SHERPA_ONNX_EXIT(-1);
+
+  // Unreachable code
+  return WhisperModelType::Tiny;
+}
+
+int32_t GetWhisperLanguageTokenId(const std::string &lang) {
+  static const std::unordered_map<std::string, int32_t> kLangToToken = {
+      {"hi", 50276},  {"cy", 50297}, {"oc", 50328}, {"so", 50326},
+      {"fr", 50265},  {"az", 50304}, {"eu", 50310}, {"ba", 50355},
+      {"no", 50288},  {"as", 50350}, {"nl", 50271}, {"bn", 50302},
+      {"es", 50262},  {"ml", 50296}, {"km", 50323}, {"mk", 50308},
+      {"sq", 50317},  {"mt", 50343}, {"et", 50307}, {"ms", 50282},
+      {"tr", 50268},  {"bg", 50292}, {"ps", 50340}, {"br", 50309},
+      {"ht", 50339},  {"tt", 50351}, {"tk", 50341}, {"la", 50294},
+      {"de", 50261},  {"ur", 50290}, {"ro", 50284}, {"fa", 50300},
+      {"uk", 50280},  {"mg", 50349}, {"lo", 50336}, {"sr", 50303},
+      {"yo", 50325},  {"id", 50275}, {"da", 50285}, {"pt", 50267},
+      {"nn", 50342},  {"sn", 50324}, {"sa", 50344}, {"sd", 50332},
+      {"gl", 50319},  {"ja", 50266}, {"pl", 50269}, {"ru", 50263},
+      {"ko", 50264},  {"ne", 50313}, {"kn", 50306}, {"zh", 50260},
+      {"be", 50330},  {"ca", 50270}, {"el", 50281}, {"it", 50274},
+      {"hu", 50286},  {"lt", 50293}, {"ta", 50287}, {"is", 50311},
+      {"jw", 50356},  {"fi", 50277}, {"bo", 50347}, {"sv", 50273},
+      {"mi", 50295},  {"hr", 50291}, {"bs", 50315}, {"yi", 50335},
+      {"sk", 50298},  {"lv", 50301}, {"af", 50327}, {"vi", 50278},
+      {"ha", 50354},  {"mn", 50314}, {"cs", 50283}, {"sl", 50305},
+      {"pa", 50321},  {"su", 50357}, {"ka", 50329}, {"ln", 50353},
+      {"lb", 50345},  {"sw", 50318}, {"en", 50259}, {"tl", 50348},
+      {"hy", 50312},  {"te", 50299}, {"he", 50279}, {"my", 50346},
+      {"haw", 50352}, {"fo", 50338}, {"kk", 50316}, {"si", 50322},
+      {"tg", 50331},  {"th", 50289}, {"ar", 50272}, {"am", 50334},
+      {"mr", 50320},  {"uz", 50337}, {"gu", 50333}};
+
+  auto it = kLangToToken.find(lang);
+
+  return (it != kLangToToken.end()) ? it->second : -1;
+}
+
+std::string GetWhisperLanguageCode(int32_t token_id) {
+  static const std::unordered_map<int32_t, std::string> kTokenToLang = {
+      {50276, "hi"},  {50297, "cy"}, {50328, "oc"}, {50326, "so"},
+      {50265, "fr"},  {50304, "az"}, {50310, "eu"}, {50355, "ba"},
+      {50288, "no"},  {50350, "as"}, {50271, "nl"}, {50302, "bn"},
+      {50262, "es"},  {50296, "ml"}, {50323, "km"}, {50308, "mk"},
+      {50317, "sq"},  {50343, "mt"}, {50307, "et"}, {50282, "ms"},
+      {50268, "tr"},  {50292, "bg"}, {50340, "ps"}, {50309, "br"},
+      {50339, "ht"},  {50351, "tt"}, {50341, "tk"}, {50294, "la"},
+      {50261, "de"},  {50290, "ur"}, {50284, "ro"}, {50300, "fa"},
+      {50280, "uk"},  {50349, "mg"}, {50336, "lo"}, {50303, "sr"},
+      {50325, "yo"},  {50275, "id"}, {50285, "da"}, {50267, "pt"},
+      {50342, "nn"},  {50324, "sn"}, {50344, "sa"}, {50332, "sd"},
+      {50319, "gl"},  {50266, "ja"}, {50269, "pl"}, {50263, "ru"},
+      {50264, "ko"},  {50313, "ne"}, {50306, "kn"}, {50260, "zh"},
+      {50330, "be"},  {50270, "ca"}, {50281, "el"}, {50274, "it"},
+      {50286, "hu"},  {50293, "lt"}, {50287, "ta"}, {50311, "is"},
+      {50356, "jw"},  {50277, "fi"}, {50347, "bo"}, {50273, "sv"},
+      {50295, "mi"},  {50291, "hr"}, {50315, "bs"}, {50335, "yi"},
+      {50298, "sk"},  {50301, "lv"}, {50327, "af"}, {50278, "vi"},
+      {50354, "ha"},  {50314, "mn"}, {50283, "cs"}, {50305, "sl"},
+      {50321, "pa"},  {50357, "su"}, {50329, "ka"}, {50353, "ln"},
+      {50345, "lb"},  {50318, "sw"}, {50259, "en"}, {50348, "tl"},
+      {50312, "hy"},  {50299, "te"}, {50279, "he"}, {50346, "my"},
+      {50352, "haw"}, {50338, "fo"}, {50316, "kk"}, {50322, "si"},
+      {50331, "tg"},  {50289, "th"}, {50272, "ar"}, {50334, "am"},
+      {50320, "mr"},  {50337, "uz"}, {50333, "gu"}};
+
+  auto it = kTokenToLang.find(token_id);
+  return (it != kTokenToLang.end()) ? it->second : std::string{};
+}
+
+const std::vector<int32_t> &GetAllWhisperLanguageTokenIds() {
+  static const std::vector<int32_t> kLanguageTokenIds = {
+      50276, 50297, 50328, 50326, 50265, 50304, 50310, 50355, 50288, 50350,
+      50271, 50302, 50262, 50296, 50323, 50308, 50317, 50343, 50307, 50282,
+      50268, 50292, 50340, 50309, 50339, 50351, 50341, 50294, 50261, 50290,
+      50284, 50300, 50280, 50349, 50336, 50303, 50325, 50275, 50285, 50267,
+      50342, 50324, 50344, 50332, 50319, 50266, 50269, 50263, 50264, 50313,
+      50306, 50260, 50330, 50270, 50281, 50274, 50286, 50293, 50287, 50311,
+      50356, 50277, 50347, 50273, 50295, 50291, 50315, 50335, 50298, 50301,
+      50327, 50278, 50354, 50314, 50283, 50305, 50321, 50357, 50329, 50353,
+      50345, 50318, 50259, 50348, 50312, 50299, 50279, 50346, 50352, 50338,
+      50316, 50322, 50331, 50289, 50272, 50334, 50320, 50337, 50333};
+
+  return kLanguageTokenIds;
+}
+
+const std::vector<std::string> &GetAllWhisperLanguageCodes() {
+  static const std::vector<std::string> kLanguageCodes = {
+      "hi",  "cy", "oc", "so", "fr", "az", "eu", "ba", "no", "as", "nl",
+      "bn",  "es", "ml", "km", "mk", "sq", "mt", "et", "ms", "tr", "bg",
+      "ps",  "br", "ht", "tt", "tk", "la", "de", "ur", "ro", "fa", "uk",
+      "mg",  "lo", "sr", "yo", "id", "da", "pt", "nn", "sn", "sa", "sd",
+      "gl",  "ja", "pl", "ru", "ko", "ne", "kn", "zh", "be", "ca", "el",
+      "it",  "hu", "lt", "ta", "is", "jw", "fi", "bo", "sv", "mi", "hr",
+      "bs",  "yi", "sk", "lv", "af", "vi", "ha", "mn", "cs", "sl", "pa",
+      "su",  "ka", "ln", "lb", "sw", "en", "tl", "hy", "te", "he", "my",
+      "haw", "fo", "kk", "si", "tg", "th", "ar", "am", "mr", "uz", "gu"};
+
+  return kLanguageCodes;
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-whisper-model-config.h b/sherpa-onnx/csrc/offline-whisper-model-config.h
index a612a46d..cbf4c5bd 100644
--- a/sherpa-onnx/csrc/offline-whisper-model-config.h
+++ b/sherpa-onnx/csrc/offline-whisper-model-config.h
@@ -5,6 +5,7 @@
 #define SHERPA_ONNX_CSRC_OFFLINE_WHISPER_MODEL_CONFIG_H_
 
 #include <string>
+#include <vector>
 
 #include "sherpa-onnx/csrc/parse-options.h"
 
@@ -55,6 +56,48 @@ struct OfflineWhisperModelConfig {
   std::string ToString() const;
 };
 
+struct OfflineWhisperDecoderResult {
+  /// The decoded token IDs
+  std::vector<int32_t> tokens;
+  std::string lang;
+};
+
+// used by ascend/rknn/qnn/axera, etc.
+enum class WhisperModelType {
+  Tiny,
+  TinyEn,
+  Base,
+  BaseEn,
+  Small,
+  SmallEn,
+  Medium,
+  MediumEn,
+  Large
+};
+
+std::string ToString(WhisperModelType model);
+bool IsMultilingual(WhisperModelType model_type);
+
+WhisperModelType ParseWhisperModelType(const std::string &name);
+int32_t GetWhisperLanguageTokenId(const std::string &lang);
+std::string GetWhisperLanguageCode(int32_t token_id);
+const std::vector<int32_t> &GetAllWhisperLanguageTokenIds();
+const std::vector<std::string> &GetAllWhisperLanguageCodes();
+
+struct WhisperModelMultilingualTokens {
+  int32_t sot = 50258;
+  int32_t eot = 50257;
+  int32_t transcribe = 50359;
+  int32_t translate = 50358;
+  int32_t no_timestamps = 50363;
+};
+
+struct WhisperModelEnglishTokens {
+  int32_t sot = 50257;
+  int32_t eot = 50256;
+  int32_t no_timestamps = 50362;
+};
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_OFFLINE_WHISPER_MODEL_CONFIG_H_
diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
index 22c395b2..86a28877 100644
--- a/sherpa-onnx/csrc/offline-whisper-model.cc
+++ b/sherpa-onnx/csrc/offline-whisper-model.cc
@@ -24,6 +24,7 @@
 
 #include "sherpa-onnx/csrc/file-utils.h"
 #include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/math.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/session.h"
 #include "sherpa-onnx/csrc/text-utils.h"
@@ -450,33 +451,7 @@ bool OfflineWhisperModel::IsMultiLingual() const {
 
 void OfflineWhisperModel::NormalizeFeatures(float *features, int32_t num_frames,
                                             int32_t feat_dim) {
-  // log_spec = torch.clamp(features, min=1e-10).log10()
-  // log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
-  // mel = (log_spec + 4.0) / 4.0
-
-  int32_t n = num_frames * feat_dim;
-  float max_v = -1e20;
-  for (int32_t i = 0; i != n; ++i) {
-    float f = features[i];
-
-    f = std::max<float>(f, 1e-10);
-    f = std::log10(f);
-
-    max_v = std::max(f, max_v);
-
-    features[i] = f;
-  }
-
-  max_v -= 8;
-
-  for (int32_t i = 0; i != n; ++i) {
-    float f = features[i];
-    f = std::max(f, max_v);
-
-    f = (f + 4) / 4;
-
-    features[i] = f;
-  }
+  NormalizeWhisperFeatures(features, num_frames, feat_dim);
 }
 
 #if __ANDROID_API__ >= 9

commit e061d42178f3aee94433c6bc21d64b6f23b20924
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Jan 8 16:22:12 2026 +0800

    Add CI to export Whisper models to Ascend NPU (#3008)

diff --git a/.github/scripts/export-ascend/generate_whisper.py b/.github/scripts/export-ascend/generate_whisper.py
new file mode 100755
index 00000000..823d62c0
--- /dev/null
+++ b/.github/scripts/export-ascend/generate_whisper.py
@@ -0,0 +1,56 @@
+#!/usr/bin/env python3
+# Copyright    2026  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import itertools
+import json
+from dataclasses import asdict, dataclass
+
+from generate_zipformer_ctc_20250703 import get_image, get_soc_version, get_cann_version
+
+
+@dataclass
+class Config:
+    # 7.0, 8.0, 8.2
+    cann: str
+
+    # 910B, 910B2, 910B3, 310P3
+    soc_version: str
+
+    model: str
+
+    image: str = ""
+
+    def __post_init__(self):
+        self.image = get_image(self.cann, soc_version=self.soc_version)
+
+
+def main():
+    cann_version = get_cann_version()
+    soc_version = get_soc_version()
+    model_list = [
+        "turbo",
+        "distil-medium.en",
+        "distil-small.en",
+        "tiny.en",
+        "base.en",
+        "small.en",
+        "medium.en",
+        "tiny",
+        "base",
+        "small",
+        "medium",
+        "medium-aishell",
+    ]
+
+    configs = [
+        Config(cann=cann, soc_version=soc, model=model)
+        for cann, soc, model in itertools.product(cann_version, soc_version, model_list)
+    ]
+
+    ans = [asdict(c) for c in configs]
+
+    print(json.dumps({"include": ans}))
+
+
+if __name__ == "__main__":
+    main()
diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
index 9b8d62fc..0f6a0aff 100644
--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
+++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-910b4-2
+      - ci-export-whisper-ascend-npu-2
   workflow_dispatch:
 
 concurrency:
@@ -63,7 +63,7 @@ jobs:
 
       - name: Install curl
         shell: bash
-        run: apt-get update && apt-get install -y curl bzip2
+        run: apt-get update && apt-get install -y curl bzip2 git git-lfs
 
       - name: Verify environment
         shell: bash
@@ -127,6 +127,8 @@ jobs:
           python3 ./export_decoder_onnx.py
           python3 ./export_predictor_onnx.py
 
+          rm -v *.pt
+
           ls -lh *.onnx
 
           source /usr/local/Ascend/ascend-toolkit/set_env.sh
@@ -171,6 +173,8 @@ jobs:
 
           ls -lh *.om
 
+          rm -v *.onnx
+
 
           echo "collect results"
           d=sherpa-onnx-ascend-${soc_version}-cann-$cann-paraformer-zh-2023-03-28
@@ -191,6 +195,8 @@ jobs:
           ls -lh *.tar.bz2
           rm -rf $d
 
+          rm -v *.om
+
           echo "----show---"
           ls -lh *.tar.bz2
 
@@ -267,6 +273,7 @@ jobs:
 
           ls -lh *.om
 
+          rm -v *.onnx
 
           echo "collect results"
           d=sherpa-onnx-ascend-${soc_version}-cann-$cann-paraformer-zh-2025-10-07
@@ -287,6 +294,8 @@ jobs:
           ls -lh *.tar.bz2
           rm -rf $d
 
+          rm -v *.om
+
           echo "----show---"
           ls -lh *.tar.bz2
 
@@ -311,3 +320,81 @@ jobs:
           file: ./*.tar.bz2
           overwrite: true
           tag: asr-models-ascend
+
+      - name: Publish to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models huggingface
+
+              d=asr-models/ascend-npu/paraformer
+              mkdir -p huggingface/$d
+
+              cp -v $m huggingface/$d/
+
+              pushd huggingface
+              git lfs track "*.tar.bz2"
+              ls -lh $d
+              pushd $d
+              git lfs track "*.tar.bz2"
+              popd
+
+              git status
+              git add .
+
+              git commit -m "add $m"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models main
+              popd
+            done
+            rm -rf huggingface
+
+      - name: Publish to modelscope
+        if: true
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              d=ascend-npu/paraformer
+              mkdir -p ms/$d
+
+              cp -av $m ms/$d/
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh $d/$m
+
+              ls -lh $d
+              git add .
+
+              git commit -m "add $m"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+            rm -rf ms
diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
index efc87861..40ee07ac 100644
--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
 on:
   push:
     branches:
-      - ascend-910b4-2
+      - ci-export-whisper-ascend-npu-2
   workflow_dispatch:
 
 concurrency:
@@ -62,7 +62,7 @@ jobs:
 
       - name: Install curl
         shell: bash
-        run: apt-get update && apt-get install -y curl bzip2
+        run: apt-get update && apt-get install -y curl bzip2 git git-lfs
 
       - name: Verify environment
         shell: bash
@@ -116,6 +116,7 @@ jobs:
           echo "export to onnx"
 
           python3 ./export_onnx.py
+          rm -v *.pt
 
           ls -lh *.onnx
 
@@ -137,6 +138,8 @@ jobs:
             --input_shape="x:1,-1,560;prompt:4" \
             --soc_version="Ascend${soc_version}"
 
+          rm -v *.onnx
+
           ls -lh *.om
 
           echo "collect results"
@@ -156,6 +159,8 @@ jobs:
           ls -lh *.tar.bz2
           rm -rf $d
 
+          rm -v *.om
+
           echo "----show---"
           ls -lh *.tar.bz2
 
@@ -186,6 +191,7 @@ jobs:
 
           echo "export to onnx"
           python3 ./export_onnx.py
+          rm -v *.pt
 
           ls -lh *.onnx
 
@@ -207,6 +213,7 @@ jobs:
             --input_shape="x:1,-1,560;prompt:4" \
             --soc_version="Ascend${soc_version}"
 
+          rm -v *.onnx
           ls -lh *.om
 
           echo "collect results"
@@ -225,6 +232,8 @@ jobs:
           ls -lh *.tar.bz2
           rm -rf $d
 
+          rm -v *.om
+
           echo "----show---"
           ls -lh *.tar.bz2
 
@@ -249,3 +258,85 @@ jobs:
           file: ./*.tar.bz2
           overwrite: true
           tag: asr-models-ascend
+
+      - name: Publish to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models huggingface
+
+              d=asr-models/ascend-npu/sense-voice
+              mkdir -p huggingface/$d
+
+              cp -v $m huggingface/$d/
+
+              pushd huggingface
+              git lfs track "*.tar.bz2"
+              ls -lh $d/$m
+
+              ls -lh $d
+
+              pushd $d
+              git lfs track "*.tar.bz2"
+              popd
+
+              git status
+              git add .
+
+              git commit -m "add $m"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models main
+              popd
+            done
+
+            rm -rf huggingface
+
+      - name: Publish to modelscope
+        if: true
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              d=ascend-npu/sense-voice
+              mkdir -p ms/$d
+
+              cp -av $m ms/$d/
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh $d/$m
+
+              ls -lh $d
+              git add .
+
+              git commit -m "add $m"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+            rm -rf ms
diff --git a/.github/workflows/export-whisper-to-ascend-npu.yaml b/.github/workflows/export-whisper-to-ascend-npu.yaml
new file mode 100644
index 00000000..9d9258d2
--- /dev/null
+++ b/.github/workflows/export-whisper-to-ascend-npu.yaml
@@ -0,0 +1,307 @@
+name: export-whisper-to-ascend-npu
+
+on:
+  push:
+    branches:
+      - ci-export-whisper-ascend-npu-2
+  workflow_dispatch:
+
+concurrency:
+  group: export-whisper-to-ascend-npu-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  generate_build_matrix:
+    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
+    # see https://github.com/pytorch/pytorch/pull/50633
+    runs-on: ubuntu-latest
+    outputs:
+      matrix: ${{ steps.set-matrix.outputs.matrix }}
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Generating build matrix
+        id: set-matrix
+        run: |
+          # outputting for debugging purposes
+          python3 .github/scripts/export-ascend/generate_whisper.py
+          MATRIX=$(python3 .github/scripts/export-ascend/generate_whisper.py)
+
+          # deprecated
+          # echo "::set-output name=matrix::${MATRIX}"
+          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
+
+  export-whisper-to-ascend-npu:
+    needs: generate_build_matrix
+    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+    name: ${{ matrix.model }} ${{ matrix.soc_version }} ${{ matrix.cann }}
+    runs-on: ubuntu-latest
+    strategy:
+      fail-fast: false
+      matrix:
+        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+
+    container:
+      image: ${{ matrix.image }}
+
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Setup Python 3.8
+        uses: actions/setup-python@v5
+        with:
+          python-version: "3.8"
+
+      - name: Show Python
+        shell: bash
+        run: |
+          python3 --version
+          which python3
+
+      - name: Install curl
+        shell: bash
+        run: |
+          apt-get update && apt-get install -y curl bzip2 git git-lfs
+
+      - name: Verify environment
+        shell: bash
+        run: |
+          ls -lh /usr/local/Ascend/ascend-toolkit/set_env.sh
+
+          find /usr/local/Ascend -name "libascend*.so" 2>/dev/null
+
+
+          source /usr/local/Ascend/ascend-toolkit/set_env.sh
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
+          echo "CANN environment:"
+          which atc || echo "atc not found"
+          atc --help
+
+      - name: Install Python dependencies
+        shell: bash
+        run: |
+          python3 -m pip install "numpy<2" \
+                  onnx==1.17.0 \
+                  onnxruntime==1.17.1 \
+                  torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
+                  torchaudio==2.0.0+cpu -f https://download.pytorch.org/whl/torchaudio \
+                  openai-whisper \
+                  attrs psutil scipy decorator cloudpickle ml-dtypes tornado \
+                  sentencepiece \
+                  pyyaml
+
+      - name: export ${{ matrix.model }} to ONNX
+        shell: bash
+        run: |
+          cd scripts/whisper/ascend-npu
+          model=${{ matrix.model }}
+          echo "model: $model"
+          if [[ $model == distil-medium.en ]]; then
+            curl -L -s -o distil-medium-en-original-model.bin https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/original-model.bin
+            ls -lh
+          elif [[ $model == distil-large-v2 ]]; then
+            curl -L -s -o distil-large-v2-original-model.bin https://huggingface.co/distil-whisper/distil-large-v2/resolve/main/original-model.bin
+            ls -lh
+          elif [[ $model == distil-large-v3 ]]; then
+            curl -L -s -o distil-large-v3-original-model.bin https://huggingface.co/distil-whisper/distil-large-v3-openai/resolve/main/model.bin
+            ls -lh
+          elif [[ $model == distil-large-v3.5 ]]; then
+            curl -L -s -o distil-large-v3.5-original-model.bin https://huggingface.co/distil-whisper/distil-large-v3.5-openai/resolve/main/model.bin
+            ls -lh
+          elif [[ $model == distil-small.en ]]; then
+            curl -L -s -o distil-small-en-original-model.bin https://huggingface.co/distil-whisper/distil-small.en/resolve/main/original-model.bin
+            ls -lh
+          elif [[ $model == medium-aishell ]]; then
+            curl -L -s -o medium-aishell.pt https://huggingface.co/yuekai/icefall_asr_aishell_whisper/resolve/main/exp_medium/whisper-medium-aishell1-epoch-10-avg-4.pt
+            ls -lh
+          fi
+          python3 ./export_onnx.py --model ${{ matrix.model }}
+
+
+          ls -lh
+
+          ls -lh ~/.cache/whisper || true
+          ls -lh distil*original-model.bin || true
+          rm -rf ~/.cache/whisper
+          rm -f distil*original-model.bin
+          rm -f medium-aishell.pt
+
+      - name: export ${{ matrix.model }} ONNX to Ascend OM
+        shell: bash
+        run: |
+          cd scripts/whisper/ascend-npu
+          ls -lh *.onnx
+
+          source /usr/local/Ascend/ascend-toolkit/set_env.sh
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+
+          # for cann 7.0.0
+          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
+
+          soc_version=${{ matrix.soc_version }}
+          cann=${{ matrix.cann }}
+
+          model=${{ matrix.model }}
+
+          atc --model=./${model}-encoder.onnx \
+            --framework=5 \
+            --host_env_os=linux \
+            --host_env_cpu=aarch64 \
+            --output=${model}-encoder \
+            --input_format=ND \
+            --soc_version="Ascend${soc_version}"
+
+          ls -lh *.om
+
+          atc --model=./${model}-decoder.onnx \
+            --framework=5 \
+            --host_env_os=linux \
+            --host_env_cpu=aarch64 \
+            --output=${model}-decoder \
+            --input_format=ND \
+            --soc_version="Ascend${soc_version}"
+
+          ls -lh *.om
+
+          rm -v *.onnx
+
+          echo "collect results"
+          d=sherpa-onnx-ascend-${soc_version}-cann-${cann}-whisper-$model
+
+          mkdir -p $d
+          mkdir -p $d/test_wavs
+
+          pushd $d/test_wavs
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/0.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/1.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/8k.wav
+          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-whisper-medium.en/resolve/main/test_wavs/trans.txt
+          popd
+
+          cp -v $model-encoder*.om $d/${model}-encoder.om
+          cp -v $model-decoder*.om $d/${model}-decoder.om
+          cp -v $model-tokens.txt $d/
+          cp -v test_om.py $d
+          ls -lh $d
+
+          tar cjfv $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+          rm -rf $d
+
+          rm -v *.om
+
+          echo "----show---"
+          ls -lh *.tar.bz2
+
+          mv *.tar.bz2 ../../..
+
+      - name: Release
+        if: github.repository_owner == 'csukuangfj'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          repo_name: k2-fsa/sherpa-onnx
+          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+          tag: asr-models-ascend
+
+      - name: Release
+        if: github.repository_owner == 'k2-fsa'
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          file: ./*.tar.bz2
+          overwrite: true
+          tag: asr-models-ascend
+
+      - name: Publish to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models huggingface
+
+              d=asr-models/ascend-npu/whisper
+              mkdir -p huggingface/$d
+
+              cp -v $m huggingface/$d/
+
+              pushd huggingface
+              git lfs track "*.tar.bz2"
+              ls -lh $d/$m
+
+              ls -lh $d
+
+              pushd $d
+              git lfs track "*.tar.bz2"
+              popd
+
+              git status
+              git add .
+
+              git commit -m "add $m"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models main
+              popd
+            done
+            rm -rf huggingface
+
+      - name: Publish to modelscope
+        if: true
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            models=(
+              sherpa-onnx-ascend-${{ matrix.soc_version }}-cann-${{ matrix.cann }}-whisper-${{ matrix.model }}.tar.bz2
+            )
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              d=ascend-npu/whisper
+              mkdir -p ms/$d
+
+              cp -av $m ms/$d/
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh $d/$m
+
+              ls -lh $d
+
+              git add .
+
+              git commit -m "add $m"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+            rm -rf ms
diff --git a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
index af517075..87e91858 100644
--- a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+++ b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
@@ -3,7 +3,7 @@ name: export-zipformer-ctc-to-ascend-npu-20250703
 on:
   push:
     branches:
-      - ascend-910b4-2
+      - ci-export-whisper-ascend-npu-2
   workflow_dispatch:
 
 concurrency:
@@ -61,7 +61,7 @@ jobs:
 
       - name: Install curl
         shell: bash
-        run: apt-get update && apt-get install -y curl bzip2
+        run: apt-get update && apt-get install -y curl bzip2 git git-lfs
 
       - name: Verify environment
         shell: bash
@@ -133,6 +133,8 @@ jobs:
             --input_shape="x:1,${num_frames},80" \
             --soc_version="Ascend${soc_version}"
 
+          rm -v *.onnx
+
           ls -lh *.om
 
           echo "collect results"
@@ -151,6 +153,8 @@ jobs:
           ls -lh *.tar.bz2
           rm -rf $d
 
+          rm -v *.om
+
           echo "----show---"
           ls -lh *.tar.bz2
 
@@ -175,3 +179,81 @@ jobs:
           file: ./*.tar.bz2
           overwrite: true
           tag: asr-models-ascend
+
+      - name: Publish to huggingface
+        if: true
+        env:
+          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+              rm -rf huggingface
+              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models huggingface
+
+              d=asr-models/ascend-npu/zipformer-ctc
+              mkdir -p huggingface/$d
+
+              cp -v $m huggingface/$d/
+
+              pushd huggingface
+              git lfs track "*.tar.bz2"
+              ls -lh $d
+              pushd $d
+              git lfs track "*.tar.bz2"
+              popd
+
+              git status
+              git add .
+
+              git commit -m "add $m"
+              git push https://csukuangfj:$HF_TOKEN@huggingface.co/k2-fsa/sherpa-onnx-models main
+              popd
+            done
+            rm -rf huggingface
+
+      - name: Publish to modelscope
+        if: true
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            for m in "*.tar.bz2"; do
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              d=ascend-npu/zipformer-ctc
+              mkdir -p ms/$d
+
+              cp -av $m ms/$d/
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh $d/$m
+
+              ls -lh $d
+              git add .
+
+              git commit -m "add $m"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+            rm -rf ms
diff --git a/scripts/whisper/ascend-npu/test_om.py b/scripts/whisper/ascend-npu/test_om.py
index 306cfc27..934c070a 100755
--- a/scripts/whisper/ascend-npu/test_om.py
+++ b/scripts/whisper/ascend-npu/test_om.py
@@ -1,5 +1,15 @@
 #!/usr/bin/env python3
-# Copyright (c)  2025  Xiaomi Corporation
+# Copyright    2026  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+"""
+Usage example:
+
+./test_om.py \
+  --encoder ./tiny.en-encoder.om \
+  --decoder ./tiny.en-decoder.om \
+  --tokens ./tiny.en-tokens.txt \
+  --wav  ./test_wavs/0.wav
+"""
 
 import argparse
 import base64
@@ -217,6 +227,7 @@ def main():
     for t in model.sot_sequence:
         token = np.array([[t]], dtype=np.int32)  # sot
         mask = causal_mask_1d(offset.item(), model.n_text_ctx)
+        print(t, model.sot_sequence, token, mask.shape, len(cross_kv), len(self_kv))
 
         out = model.run_decoder(
             tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask
diff --git a/scripts/whisper/rknn/export_onnx.py b/scripts/whisper/rknn/export_onnx.py
index 587b50d9..7bbb12fa 100755
--- a/scripts/whisper/rknn/export_onnx.py
+++ b/scripts/whisper/rknn/export_onnx.py
@@ -408,6 +408,7 @@ def convert_tokens(name, model):
     with open(f"{name}-tokens.txt", "w") as f:
         for t, i in tokens.items():
             f.write(f"{t} {i}\n")
+    print(f"Saved to {name}-tokens.txt")
 
 
 @torch.no_grad()

commit 42aa0b8f163dd263e55b99a848598cebf7d8576a
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Thu Jan 8 14:08:31 2026 +0800

    Use onnxruntime 1.23.2 for Windows (#3007)

diff --git a/.github/scripts/test-offline-ctc.sh b/.github/scripts/test-offline-ctc.sh
index 3da2be60..749b3ec1 100755
--- a/.github/scripts/test-offline-ctc.sh
+++ b/.github/scripts/test-offline-ctc.sh
@@ -80,12 +80,12 @@ rm -rf sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24
 log "------------------------------------------------------------"
 log "Run SenseVoice models"
 log "------------------------------------------------------------"
-curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-repo=sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+repo=sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17
 
-for m in model.onnx model.int8.onnx; do
+for m in model.int8.onnx; do
   for w in zh en yue ja ko; do
     for use_itn in 0 1; do
       echo "$m $w $use_itn"
@@ -106,7 +106,7 @@ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/rep
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/test-hr.wav
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lexicon.txt
 
-for m in model.onnx model.int8.onnx; do
+for m in model.int8.onnx; do
   for use_itn in 0 1; do
     echo "$m $w $use_itn"
     time $EXE \
diff --git a/.github/workflows/windows-arm64.yaml b/.github/workflows/windows-arm64.yaml
index 6b4120a5..99a7a889 100644
--- a/.github/workflows/windows-arm64.yaml
+++ b/.github/workflows/windows-arm64.yaml
@@ -48,6 +48,7 @@ jobs:
             -A ARM64 \
             -DSHERPA_ONNX_ENABLE_TTS=${{ matrix.with_tts }} \
             -D CMAKE_BUILD_TYPE=Release \
+            -D SHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
             -D BUILD_SHARED_LIBS=${{ matrix.shared_lib }} \
             -D CMAKE_INSTALL_PREFIX=./install \
             -D BUILD_ESPEAK_NG_EXE=OFF \
diff --git a/.github/workflows/windows-x64-debug.yaml b/.github/workflows/windows-x64-debug.yaml
index 69e9f4a5..daedbf9b 100644
--- a/.github/workflows/windows-x64-debug.yaml
+++ b/.github/workflows/windows-x64-debug.yaml
@@ -52,6 +52,7 @@ jobs:
           cd build
           cmake \
             -A x64 \
+            -D SHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
             -D CMAKE_BUILD_TYPE=${{ matrix.build_type }} \
             -D BUILD_SHARED_LIBS=${{ matrix.shared_lib }} \
             -D CMAKE_INSTALL_PREFIX=./install \
diff --git a/.github/workflows/windows-x64.yaml b/.github/workflows/windows-x64.yaml
index 993139a6..45fad979 100644
--- a/.github/workflows/windows-x64.yaml
+++ b/.github/workflows/windows-x64.yaml
@@ -59,6 +59,7 @@ jobs:
           cmake \
             -A x64 \
             -DSHERPA_ONNX_ENABLE_TTS=${{ matrix.with_tts }} \
+            -D SHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
             -D CMAKE_BUILD_TYPE=Release \
             -D BUILD_SHARED_LIBS=${{ matrix.shared_lib }} \
             -DCMAKE_INSTALL_PREFIX=./install \
diff --git a/.github/workflows/windows-x86-debug.yaml b/.github/workflows/windows-x86-debug.yaml
index 4541b2ae..c6d66baf 100644
--- a/.github/workflows/windows-x86-debug.yaml
+++ b/.github/workflows/windows-x86-debug.yaml
@@ -52,6 +52,7 @@ jobs:
           cd build
           cmake \
             -A Win32 \
+            -D SHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
             -D CMAKE_BUILD_TYPE=${{ matrix.build_type }} \
             -D BUILD_SHARED_LIBS=${{ matrix.shared_lib }} \
             -D CMAKE_INSTALL_PREFIX=./install ..
diff --git a/.github/workflows/windows-x86.yaml b/.github/workflows/windows-x86.yaml
index 09065e76..759394bb 100644
--- a/.github/workflows/windows-x86.yaml
+++ b/.github/workflows/windows-x86.yaml
@@ -60,6 +60,7 @@ jobs:
             -A Win32 \
             -DSHERPA_ONNX_ENABLE_TTS=${{ matrix.with_tts }} \
             -D CMAKE_BUILD_TYPE=Release \
+            -D SHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
             -D BUILD_SHARED_LIBS=${{ matrix.shared_lib }} \
             -D CMAKE_INSTALL_PREFIX=./install \
             ..
diff --git a/cmake/onnxruntime-win-arm64-static.cmake b/cmake/onnxruntime-win-arm64-static.cmake
index 0ebbfc29..a8745e10 100644
--- a/cmake/onnxruntime-win-arm64-static.cmake
+++ b/cmake/onnxruntime-win-arm64-static.cmake
@@ -19,18 +19,18 @@ if(NOT CMAKE_BUILD_TYPE STREQUAL Release)
   message(FATAL_ERROR "This file is for building a release version on Windows arm64")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_HASH "SHA256=534ab5bb8b5495ce45fed866cf3ec9034f89f2057a0152e49120b1088003a17e")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_HASH "SHA256=d3d1a6dd8e886e47f00e17dea521642b029c04936e52673109ef3063b31ad708")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2
-  /tmp/onnxruntime-win-arm64-static_lib-1.17.1.tar.bz2
+  $ENV{HOME}/Downloads/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-arm64-static_lib-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-arm64.cmake b/cmake/onnxruntime-win-arm64.cmake
index 3c080685..11d3d7b0 100644
--- a/cmake/onnxruntime-win-arm64.cmake
+++ b/cmake/onnxruntime-win-arm64.cmake
@@ -15,18 +15,18 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-win-arm64-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-arm64-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=47782cebcab0fd7a1f0a3f0676b088c1bc0f4fbf21666f6fe57570dc362fa5a8")
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-win-arm64-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-arm64-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=1cfe88b6435df3b5fb0e9f6bd7d6f5df1e887b6174de7f6e2a47bab956f3f168")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-arm64-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-arm64-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-arm64-1.17.1.zip
-  /tmp/onnxruntime-win-arm64-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-win-arm64-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-arm64-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-arm64-1.23.2.zip
+  /tmp/onnxruntime-win-arm64-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x64-gpu.cmake b/cmake/onnxruntime-win-x64-gpu.cmake
index 7269c20c..3fc97692 100644
--- a/cmake/onnxruntime-win-x64-gpu.cmake
+++ b/cmake/onnxruntime-win-x64-gpu.cmake
@@ -19,18 +19,19 @@ if(NOT SHERPA_ONNX_ENABLE_GPU)
   message(FATAL_ERROR "This file is for NVIDIA GPU only. Given SHERPA_ONNX_ENABLE_GPU: ${SHERPA_ONNX_ENABLE_GPU}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-win-x64-gpu-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x64-gpu-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=b7a66f50ad146c2ccb43471d2d3b5ad78084c2d4ddbd3ea82d65f86c867408b2")
+# Requires cuda 12.x, cudnn 9.x
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-win-x64-gpu-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x64-gpu-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=e77afdbbc2b8cb6da4e5a50d89841b48c44f3e47dce4fb87b15a2743786d0bb9")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x64-gpu-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-gpu-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-gpu-1.17.1.zip
-  /tmp/onnxruntime-win-x64-gpu-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-win-x64-gpu-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-gpu-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-gpu-1.23.2.zip
+  /tmp/onnxruntime-win-x64-gpu-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x64-static-debug.cmake b/cmake/onnxruntime-win-x64-static-debug.cmake
index 211873cf..e1600e20 100644
--- a/cmake/onnxruntime-win-x64-static-debug.cmake
+++ b/cmake/onnxruntime-win-x64-static-debug.cmake
@@ -15,14 +15,14 @@ if(BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building static libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2")
 if(CMAKE_BUILD_TYPE STREQUAL Debug)
-  set(onnxruntime_HASH "SHA256=ecc68d914541c3b6ebc36148af63fe2a6af0f4f955b35199d612698d23169fa5")
+  set(onnxruntime_HASH "SHA256=5990f20c799a01d5397129b3e792c0b0a8e61deea6b46dffef61aa3d7315b6af")
 elseif(CMAKE_BUILD_TYPE STREQUAL RelWithDebInfo)
-  set(onnxruntime_HASH "SHA256=7cbe58273e55d033568f84fb16d220cea4e25ec29eb7db405c4ac7b6e41f2dfa")
+  set(onnxruntime_HASH "SHA256=1c923107630ffbd045115157b44f2e415cc553d64811a5399009208683ebcf58")
 elseif(CMAKE_BUILD_TYPE STREQUAL MinSizeRel)
-  set(onnxruntime_HASH "SHA256=9eb3adf0f6ac3b0e9f118e0d9e686f50fc651394e0b0cc569275af6e3ffed0e0")
+  set(onnxruntime_HASH "SHA256=0ceab7ac41b220d43bbcb0826e8ecd6294669fd3a13b8643e40dd3c9db2ef896")
 else()
   message(FATAL_ERROR "This file is for building a debug version on Windows x64. Given ${CMAKE_BUILD_TYPE}")
 endif()
@@ -31,10 +31,10 @@ endif()
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  /tmp/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
+  $ENV{HOME}/Downloads/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-x64-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x64-static.cmake b/cmake/onnxruntime-win-x64-static.cmake
index 811d6475..3b672caa 100644
--- a/cmake/onnxruntime-win-x64-static.cmake
+++ b/cmake/onnxruntime-win-x64-static.cmake
@@ -19,18 +19,18 @@ if(NOT CMAKE_BUILD_TYPE STREQUAL Release)
   message(FATAL_ERROR "This file is for building a release version on Windows x64")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_HASH "SHA256=42a0c02fda945d1d72433b2a7cdb2187d51cb4d7f3af462c6ae07b25314d5fb3")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_HASH "SHA256=86f2a87c029554bb685e528ff143090b4d4eb1a0b9ff5d08ba9b676d6c79b76c")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2
-  /tmp/onnxruntime-win-x64-static_lib-1.17.1.tar.bz2
+  $ENV{HOME}/Downloads/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-x64-static_lib-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x64.cmake b/cmake/onnxruntime-win-x64.cmake
index e49eafad..3f1830b6 100644
--- a/cmake/onnxruntime-win-x64.cmake
+++ b/cmake/onnxruntime-win-x64.cmake
@@ -15,18 +15,18 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-win-x64-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x64-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=4802af9598db02153d7da39432a48823ff69b2fb4b59155461937f20782aa91c")
+set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-win-x64-1.23.2.zip")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x64-1.23.2.zip")
+set(onnxruntime_HASH "SHA256=0b38df9af21834e41e73d602d90db5cb06dbd1ca618948b8f1d66d607ac9f3cd")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x64-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-1.17.1.zip
-  /tmp/onnxruntime-win-x64-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-win-x64-1.23.2.zip
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x64-1.23.2.zip
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x64-1.23.2.zip
+  /tmp/onnxruntime-win-x64-1.23.2.zip
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x86-static-debug.cmake b/cmake/onnxruntime-win-x86-static-debug.cmake
index 8f00f2a5..ca93b0f6 100644
--- a/cmake/onnxruntime-win-x86-static-debug.cmake
+++ b/cmake/onnxruntime-win-x86-static-debug.cmake
@@ -16,14 +16,14 @@ if(BUILD_SHARED_LIBS)
 endif()
 
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2")
-set(onnxruntime_URL2  "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2")
+set(onnxruntime_URL2  "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2")
 if(CMAKE_BUILD_TYPE STREQUAL Debug)
-  set(onnxruntime_HASH "SHA256=b08b223fe09a5640472eec487ff42e4df6bf726e8aba9de40f443a1fabea3334")
+  set(onnxruntime_HASH "SHA256=c7740dd4ecf3250c01ff2fd839cb568d8acf9ae017cf8d0c83394adc2e1b8e25")
 elseif(CMAKE_BUILD_TYPE STREQUAL RelWithDebInfo)
-  set(onnxruntime_HASH "SHA256=215c68d4cf07fab47434059544f4b3e1885bb68149fc7ce5b78a9feb08cf6baa")
+  set(onnxruntime_HASH "SHA256=df0e046a7b4a86e84d2fa87612e9d26124aa1361f5d463c805825f4602e7e18c")
 elseif(CMAKE_BUILD_TYPE STREQUAL MinSizeRel)
-  set(onnxruntime_HASH "SHA256=af6ff6f6a7ca6fb9f037bdd1cbd9b973921d069f7fce69833627ce04674bf579")
+  set(onnxruntime_HASH "SHA256=313a601c6bea3e4440889699de3ae3759f405de97add993bce3857f39bc7231c")
 else()
   message(FATAL_ERROR "This file is for building a debug version on Windows x86. Given ${CMAKE_BUILD_TYPE}")
 endif()
@@ -32,10 +32,10 @@ endif()
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
-  /tmp/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.17.1.tar.bz2
+  $ENV{HOME}/Downloads/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-x86-static_lib-${CMAKE_BUILD_TYPE}-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x86-static.cmake b/cmake/onnxruntime-win-x86-static.cmake
index ce424ee8..4320e1df 100644
--- a/cmake/onnxruntime-win-x86-static.cmake
+++ b/cmake/onnxruntime-win-x86-static.cmake
@@ -19,18 +19,18 @@ if(NOT CMAKE_BUILD_TYPE STREQUAL Release)
   message(FATAL_ERROR "This file is for building a release version on Windows x86")
 endif()
 
-set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.17.1/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2")
-set(onnxruntime_HASH "SHA256=52375d3fabc7b437c955a664bfeb9cb7a6391f5219c4b7d3b87ff690416d4b9e")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2")
+set(onnxruntime_HASH "SHA256=21a8a93de825ff1439d85fd235e902c23524af3e27d8dd036d9553b0e818ae41")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2
-  /tmp/onnxruntime-win-x86-static_lib-1.17.1.tar.bz2
+  $ENV{HOME}/Downloads/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-x86-static_lib-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)
diff --git a/cmake/onnxruntime-win-x86.cmake b/cmake/onnxruntime-win-x86.cmake
index 765b98f8..0b7fae70 100644
--- a/cmake/onnxruntime-win-x86.cmake
+++ b/cmake/onnxruntime-win-x86.cmake
@@ -15,18 +15,20 @@ if(NOT BUILD_SHARED_LIBS)
   message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}")
 endif()
 
-set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-win-x86-1.17.1.zip")
-set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/onnxruntime-win-x86-1.17.1.zip")
-set(onnxruntime_HASH "SHA256=9404130825474bd36b2538ed925d6b5f2cf1fb6a443f3e125054ae3470019291")
+# From onnxruntime 1.23.0, microsoft stops providing shared libs for win x86
+# set(onnxruntime_URL  "https://github.com/microsoft/onnxruntime/releases/download/v1.17.1/onnxruntime-win-x86-1.17.1.zip")
+set(onnxruntime_URL  "https://github.com/csukuangfj/onnxruntime-libs/releases/download/v1.23.2/onnxruntime-win-x86-1.23.2.tar.bz2")
+set(onnxruntime_URL2 "https://hf-mirror.com/csukuangfj/onnxruntime-libs/resolve/main/1.23.2/onnxruntime-win-x86-1.23.2.tar.bz2")
+set(onnxruntime_HASH "SHA256=9dc554c65b469d605805b242aba04a8fa486a2347f6263c9152643f677394a41")
 
 # If you don't have access to the Internet,
 # please download onnxruntime to one of the following locations.
 # You can add more if you want.
 set(possible_file_locations
-  $ENV{HOME}/Downloads/onnxruntime-win-x86-1.17.1.zip
-  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-1.17.1.zip
-  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-1.17.1.zip
-  /tmp/onnxruntime-win-x86-1.17.1.zip
+  $ENV{HOME}/Downloads/onnxruntime-win-x86-1.23.2.tar.bz2
+  ${CMAKE_SOURCE_DIR}/onnxruntime-win-x86-1.23.2.tar.bz2
+  ${CMAKE_BINARY_DIR}/onnxruntime-win-x86-1.23.2.tar.bz2
+  /tmp/onnxruntime-win-x86-1.23.2.tar.bz2
 )
 
 foreach(f IN LISTS possible_file_locations)

commit 5f12b088c1aeb079bf434b133bc6cd5636828eaa
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Jan 7 19:39:27 2026 +0800

    Fix download test wav files (#3004)

diff --git a/.github/workflows/upload-models.yaml b/.github/workflows/upload-models.yaml
index 6ade8b99..bafbaa80 100644
--- a/.github/workflows/upload-models.yaml
+++ b/.github/workflows/upload-models.yaml
@@ -24,6 +24,17 @@ jobs:
     steps:
       - uses: actions/checkout@v4
 
+      - name: Install ffmpeg
+        shell: bash
+        run: |
+          sudo apt-get update
+          sudo apt-get install -y ffmpeg
+
+      - name: Verify ffmpeg
+        shell: bash
+        run: |
+          ffmpeg -version
+
       - name: git config
         shell: bash
         run: |
@@ -34,17 +45,24 @@ jobs:
         if: false
         uses: mxschmitt/action-tmate@v3
 
-      - name: Collect funasr-nano with LLM int8
+      - name: Collect funasr-nano with LLM
         if: false
         shell: bash
         run: |
           git lfs install
-          git clone https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30
-          d=sherpa-onnx-funasr-nano-int8-2025-12-30
-          rm -rf $d/.git
-          tar cjfv $d.tar.bz2 $d
-          ls -lh $d.tar.bz2
-          ls -lh $d
+          models=(
+            sherpa-onnx-funasr-nano-int8-2025-12-30
+            sherpa-onnx-funasr-nano-fp16-2025-12-30
+            sherpa-onnx-funasr-nano-2025-12-30
+          )
+          for d in ${models[@]}; do
+            git clone https://huggingface.co/csukuangfj/$d
+            rm -rf $d/.git
+            tar cjfv $d.tar.bz2 $d
+            ls -lh $d.tar.bz2
+            ls -lh $d
+            rm -rf $d
+          done
 
       - name: Collect funasr-nano with LLM int8
         if: false
@@ -96,6 +114,13 @@ jobs:
 
           mv yuenan.wav vietnamese.wav
 
+          for f in *.wav; do
+            ffmpeg -y -loglevel error -i "$f" \
+              -ac 1 -ar 16000 -sample_fmt s16 \
+              "${f}.tmp.wav" \
+            && mv "${f}.tmp.wav" "$f"
+          done
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -196,6 +221,13 @@ jobs:
 
           mv yuenan.wav vietnamese.wav
 
+          for f in *.wav; do
+            ffmpeg -y -loglevel error -i "$f" \
+              -ac 1 -ar 16000 -sample_fmt s16 \
+              "${f}.tmp.wav" \
+            && mv "${f}.tmp.wav" "$f"
+          done
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -296,6 +328,13 @@ jobs:
 
           mv yuenan.wav vietnamese.wav
 
+          for f in *.wav; do
+            ffmpeg -y -loglevel error -i "$f" \
+              -ac 1 -ar 16000 -sample_fmt s16 \
+              "${f}.tmp.wav" \
+            && mv "${f}.tmp.wav" "$f"
+          done
+
           cat >README.md <<EOF
           Audio files in this directory are downloaded from
           https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
@@ -762,7 +801,7 @@ jobs:
           mv models/* .
 
       - name: Publish to huggingface
-        if: true
+        if: false
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3

commit 8d28e728ba73e928025eaea5f4fb8b2004e1d972
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Jan 7 17:50:37 2026 +0800

    Upload FunASR Nano ASR models with LLM (#3003)

diff --git a/.github/workflows/upload-models.yaml b/.github/workflows/upload-models.yaml
index a596c26b..6ade8b99 100644
--- a/.github/workflows/upload-models.yaml
+++ b/.github/workflows/upload-models.yaml
@@ -34,6 +34,318 @@ jobs:
         if: false
         uses: mxschmitt/action-tmate@v3
 
+      - name: Collect funasr-nano with LLM int8
+        if: false
+        shell: bash
+        run: |
+          git lfs install
+          git clone https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30
+          d=sherpa-onnx-funasr-nano-int8-2025-12-30
+          rm -rf $d/.git
+          tar cjfv $d.tar.bz2 $d
+          ls -lh $d.tar.bz2
+          ls -lh $d
+
+      - name: Collect funasr-nano with LLM int8
+        if: false
+        shell: bash
+        run: |
+          d=sherpa-onnx-funasr-nano-int8-2025-12-30
+          mkdir $d
+          pushd $d
+
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/llm_int8/llm.int8.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/embedding.int8.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/encoder_adaptor.int8.onnx
+
+          mkdir Qwen3-0.6B
+          cd Qwen3-0.6B
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/merges.txt
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/tokenizer.json
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/vocab.json
+
+          ls -lh
+          cd ..
+          mkdir test_wavs
+          cd test_wavs
+
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_hunan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_minnan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_sh.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_yue.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_4.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_5.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja_en_codeswitch.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_history.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_math.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_medical.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_physics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/yuenan.wav
+
+          mv yuenan.wav vietnamese.wav
+
+          cat >README.md <<EOF
+          Audio files in this directory are downloaded from
+          https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
+
+          | Filename| Trascript|
+          |---------|----------|
+          |[dia_hunan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/dia_hunan.wav)||
+          |[dia_minnan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/dia_minnan.wav)||
+          |[dia_sh.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/dia_sh.wav)||
+          |[dia_yue.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/dia_yue.wav)||
+          |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
+          |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
+          |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
+          |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
+          |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_3.wav)||
+          |[far_4.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_4.wav)|<music>  <impact_sounds></impact_sounds></music>|
+          |[far_5.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/far_5.wav)|<breathing></breathing>  <impact_sounds> </impact_sounds>|
+          |[ja.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/ja.wav)||
+          |[ja_en_codeswitch.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/ja_en_codeswitch.wav)|wi-fi  google meet |
+          |[vietnamese.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/vietnamese.wav)|i cng vi tip tc ku gi ngi dn  qua li cc  dch ny, khai bo y t v yu cu lin h  c xt nghim.|
+          |[rag_biochemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_biochemistry.wav)||
+          |[rag_chemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_chemistry.wav)||
+          |[rag_history.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_history.wav)||
+          |[rag_math.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_math.wav)||
+          |[rag_medical.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_medical.wav)||
+          |[rag_physics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-int8-2025-12-30/resolve/main/test_wavs/rag_physics.wav)||
+          EOF
+          cd ..
+
+          cat >README.md <<EOF
+
+          # Introduction
+          Models in this directory are downloaded from
+          https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/files
+
+          Export script can be found at
+          https://github.com/Wasser1462/FunASR-nano-onnx
+
+          The author is https://github.com/Wasser1462
+          EOF
+
+          popd
+          ls -lh $d
+          tar cjvf $d.tar.bz2 $d
+
+      - name: Collect funasr-nano with LLM float32
+        if: false
+        shell: bash
+        run: |
+          d=sherpa-onnx-funasr-nano-2025-12-30
+          mkdir $d
+          pushd $d
+
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/llm_fp32/llm.fp32.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/llm_fp32/llm.fp32.data
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/embedding.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/encoder_adaptor.onnx
+
+          mkdir Qwen3-0.6B
+          cd Qwen3-0.6B
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/merges.txt
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/tokenizer.json
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/vocab.json
+
+          ls -lh
+          cd ..
+          mkdir test_wavs
+          cd test_wavs
+
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_hunan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_minnan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_sh.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_yue.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_4.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_5.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja_en_codeswitch.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_history.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_math.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_medical.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_physics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/yuenan.wav
+
+          mv yuenan.wav vietnamese.wav
+
+          cat >README.md <<EOF
+          Audio files in this directory are downloaded from
+          https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
+
+          | Filename| Trascript|
+          |---------|----------|
+          |[dia_hunan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/dia_hunan.wav)||
+          |[dia_minnan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/dia_minnan.wav)||
+          |[dia_sh.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/dia_sh.wav)||
+          |[dia_yue.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/dia_yue.wav)||
+          |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
+          |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
+          |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
+          |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
+          |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_3.wav)||
+          |[far_4.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_4.wav)|<music>  <impact_sounds></impact_sounds></music>|
+          |[far_5.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/far_5.wav)|<breathing></breathing>  <impact_sounds> </impact_sounds>|
+          |[ja.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/ja.wav)||
+          |[ja_en_codeswitch.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/ja_en_codeswitch.wav)|wi-fi  google meet |
+          |[vietnamese.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/vietnamese.wav)|i cng vi tip tc ku gi ngi dn  qua li cc  dch ny, khai bo y t v yu cu lin h  c xt nghim.|
+          |[rag_biochemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_biochemistry.wav)||
+          |[rag_chemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_chemistry.wav)||
+          |[rag_history.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_history.wav)||
+          |[rag_math.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_math.wav)||
+          |[rag_medical.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_medical.wav)||
+          |[rag_physics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-2025-12-30/resolve/main/test_wavs/rag_physics.wav)||
+          EOF
+          cd ..
+
+          cat >README.md <<EOF
+
+          # Introduction
+          Models in this directory are downloaded from
+          https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/files
+
+          Export script can be found at
+          https://github.com/Wasser1462/FunASR-nano-onnx
+
+          The author is https://github.com/Wasser1462
+          EOF
+
+          popd
+          ls -lh $d
+          tar cjvf $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+
+      - name: Collect funasr-nano with LLM fp16
+        if: false
+        shell: bash
+        run: |
+          d=sherpa-onnx-funasr-nano-fp16-2025-12-30
+          mkdir $d
+          pushd $d
+
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/llm_fp16/llm.fp16.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/embedding.int8.onnx
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/encoder_adaptor.int8.onnx
+
+          mkdir Qwen3-0.6B
+          cd Qwen3-0.6B
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/merges.txt
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/tokenizer.json
+          curl -SL -O https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/resolve/master/Qwen3-0.6B/vocab.json
+
+          ls -lh
+          cd ..
+          mkdir test_wavs
+          cd test_wavs
+
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_hunan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_minnan.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_sh.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/dia_yue.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_4.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/far_5.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/ja_en_codeswitch.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_1.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_2.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/lyrics_en_3.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/noise_en.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_biochemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_chemistry.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_history.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_math.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_medical.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/rag_physics.wav
+          curl -SL -O https://github.com/FunAudioLLM/FunAudioLLM.github.io/raw/refs/heads/master/funasr/static/audios/yuenan.wav
+
+          mv yuenan.wav vietnamese.wav
+
+          cat >README.md <<EOF
+          Audio files in this directory are downloaded from
+          https://github.com/FunAudioLLM/FunAudioLLM.github.io/tree/master/funasr/static/audios
+
+          | Filename| Trascript|
+          |---------|----------|
+          |[dia_hunan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/dia_hunan.wav)||
+          |[dia_minnan.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/dia_minnan.wav)||
+          |[dia_sh.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/dia_sh.wav)||
+          |[dia_yue.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/dia_yue.wav)||
+          |[lyrics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics.wav)||
+          |[lyrics_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_2.wav)||
+          |[lyrics_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_3.wav)||
+          |[lyrics_en_1.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_1.wav)|Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon. Hey diddle diddle, the cat and the fiddle, the cow jumped over the moon. The little dog laughed to see such sport, and the dish ran away with the spoon.|
+          |[lyrics_en_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_2.wav)|I see your monsters. I see your pain. Tell me your problems; I'll chase them away. I'll be your lighthouse. I'll make it okay. When I see your monsters, I'll stand there so brave and chase them all away.|
+          |[lyrics_en_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/lyrics_en_3.wav)|It may be a good idea for Joe, but it wouldn't be good for me to sit in a mortgaged bungalow with my little ones on my knee. I'd much rather go and blow my dough on a casual chickadee. I don't want a mark that I'll have to toemy toe can go where it wants to go. It wants to go where the wild girls grow in extravagant quantity to bask in the warm and peaceful glow of connubial constancy. May be awfully good for good old Joe, but it wouldn't be good for me!|
+          |[noise_en.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/noise_en.wav)|So what's interesting here is I feel that you know brands knowing this when people sort of speak to the voice assistance at home and if you want to be the brand.|
+          |[far_2.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_2.wav)|8|
+          |[far_3.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_3.wav)||
+          |[far_4.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_4.wav)|<music>  <impact_sounds></impact_sounds></music>|
+          |[far_5.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/far_5.wav)|<breathing></breathing>  <impact_sounds> </impact_sounds>|
+          |[ja.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/ja.wav)||
+          |[ja_en_codeswitch.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/ja_en_codeswitch.wav)|wi-fi  google meet |
+          |[vietnamese.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/vietnamese.wav)|i cng vi tip tc ku gi ngi dn  qua li cc  dch ny, khai bo y t v yu cu lin h  c xt nghim.|
+          |[rag_biochemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_biochemistry.wav)||
+          |[rag_chemistry.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_chemistry.wav)||
+          |[rag_history.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_history.wav)||
+          |[rag_math.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_math.wav)||
+          |[rag_medical.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_medical.wav)||
+          |[rag_physics.wav](https://huggingface.co/csukuangfj/sherpa-onnx-funasr-nano-fp16-2025-12-30/resolve/main/test_wavs/rag_physics.wav)||
+          EOF
+          cd ..
+
+          cat >README.md <<EOF
+
+          # Introduction
+          Models in this directory are downloaded from
+          https://www.modelscope.cn/models/zengshuishui/FunASR-nano-onnx/files
+
+          Export script can be found at
+          https://github.com/Wasser1462/FunASR-nano-onnx
+
+          The author is https://github.com/Wasser1462
+          EOF
+
+          popd
+          ls -lh $d
+          tar cjvf $d.tar.bz2 $d
+          ls -lh *.tar.bz2
+
       - name: Streaming zipformer from Banafo/Kroko-ASR
         if: false
         shell: bash
@@ -449,8 +761,8 @@ jobs:
 
           mv models/* .
 
-      - name: Publish to huggingface (Russian zipformer)
-        if: false
+      - name: Publish to huggingface
+        if: true
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
         uses: nick-fields/retry@v3
@@ -464,18 +776,31 @@ jobs:
             models=(
               sherpa-onnx-zipformer-ru-2025-04-20
               sherpa-onnx-zipformer-ru-int8-2025-04-20
+              sherpa-onnx-funasr-nano-int8-2025-12-30
+              sherpa-onnx-funasr-nano-fp16-2025-12-30
+              sherpa-onnx-funasr-nano-2025-12-30
             )
             for d in ${models[@]}; do
+              if [ ! -d $d ]; then
+                continue;
+              fi
+
               export GIT_LFS_SKIP_SMUDGE=1
               export GIT_CLONE_PROTECTION_ACTIVE=false
               rm -rf huggingface
               git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
+
+              rm -rf huggingface/*.onnx
+              rm -rf huggingface/*/*.wav
+
               cp -av $d/* huggingface
 
               pushd huggingface
               git lfs track "*.onnx"
+              git lfs track "*.data"
               git lfs track "bpe.model"
               git lfs track "*.wav"
+              git lfs track "*.json"
               git status
               git add .
 
@@ -485,6 +810,48 @@ jobs:
               popd
             done
 
+      - name: Publish to modelscope
+        if: true
+        env:
+          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+        uses: nick-fields/retry@v3
+        with:
+          max_attempts: 20
+          timeout_seconds: 200
+          shell: bash
+          command: |
+            git config --global user.email "csukuangfj@gmail.com"
+            git config --global user.name "Fangjun Kuang"
+            models=(
+              sherpa-onnx-funasr-nano-int8-2025-12-30.tar.bz2
+              sherpa-onnx-funasr-nano-fp16-2025-12-30.tar.bz2
+              sherpa-onnx-funasr-nano-2025-12-30.tar.bz2
+            )
+            for m in ${models[@]}; do
+              if [ ! -f $m ]; then
+                continue;
+              fi
+
+              export GIT_LFS_SKIP_SMUDGE=1
+              export GIT_CLONE_PROTECTION_ACTIVE=false
+
+              rm -rf ms
+              git clone https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git ms
+
+              cp -av $m ms/
+
+              pushd ms
+              git lfs track "*.tar.bz2"
+              git status
+              ls -lh
+              git add .
+
+              git commit -m "add models"
+              git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/csukuangfj/asr-models.git
+
+              popd
+            done
+
       - uses: actions/upload-artifact@v4
         if: false
         with:

commit 3c60c609a2cc9871b118fe98f833c3b77758eb2b
Author: pqsworld <1393608645@qq.com>
Date:   Wed Jan 7 17:27:45 2026 +0800

    Fix(csrc/melotts): Fix V-words pronounciation on MeloTTS_en (#3002)

diff --git a/scripts/melo-tts/test.py b/scripts/melo-tts/test.py
index c5b80804..304c8d46 100755
--- a/scripts/melo-tts/test.py
+++ b/scripts/melo-tts/test.py
@@ -15,6 +15,11 @@ class Lexicon:
             for line in f:
                 s, i = line.split()
                 tokens[s] = int(i)
+        # Map "v" to "V" token ID (same as post_replace_ph in MeloTTS, only for English models)
+        # English models have "V" with token ID 14
+        if tokens.get("V") == 14 and "v" in tokens:
+            tokens["v"] = tokens["V"]
+
 
         lexicon = dict()
         with open(lexion_filename, encoding="utf-8") as f:
diff --git a/sherpa-onnx/csrc/melo-tts-lexicon.cc b/sherpa-onnx/csrc/melo-tts-lexicon.cc
index 033aa607..1210bb05 100644
--- a/sherpa-onnx/csrc/melo-tts-lexicon.cc
+++ b/sherpa-onnx/csrc/melo-tts-lexicon.cc
@@ -228,6 +228,12 @@ class MeloTtsLexicon::Impl {
     if (!token2id_.count("") && token2id_.count("")) {
       token2id_[""] = token2id_[""];
     }
+
+    // Map 'v' to 'V' token (same as post_replace_ph in MeloTTS)
+    // Only for English models
+    if (meta_data_.language == "en" && token2id_.count("V")) {
+      token2id_["v"] = token2id_["V"];
+    }
   }
 
   void InitLexicon(std::istream &is) {

commit 23863f606e4f9a06efaf4adcbd029d9468c809ad
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Wed Jan 7 11:52:40 2026 +0800

    Remove filesystem header (#2998)

diff --git a/sherpa-onnx/csrc/file-utils.cc b/sherpa-onnx/csrc/file-utils.cc
index 6ad16166..21189247 100644
--- a/sherpa-onnx/csrc/file-utils.cc
+++ b/sherpa-onnx/csrc/file-utils.cc
@@ -10,6 +10,13 @@
 #include <string>
 #include <vector>
 
+#ifdef _WIN32
+#include <windows.h>
+#else
+#include <limits.h>
+#include <stdlib.h>
+#endif
+
 #include "sherpa-onnx/csrc/macros.h"
 
 namespace sherpa_onnx {
@@ -107,4 +114,38 @@ std::vector<char> ReadFile(NativeResourceManager *mgr,
 }
 #endif
 
+std::string ResolveAbsolutePath(const std::string &path) {
+  if (path.empty()) {
+    return path;
+  }
+
+#ifdef _WIN32
+  // Check if path is already absolute (drive letter or UNC path)
+  if ((path.size() > 1 && path[1] == ':') ||
+      (path.size() > 1 && path[0] == '\\' && path[1] == '\\')) {
+    return path;
+  }
+
+  char buffer[MAX_PATH];
+  if (GetFullPathNameA(path.c_str(), MAX_PATH, buffer, nullptr)) {
+    return std::string(buffer);
+  }
+
+  return path;  // fallback on failure
+
+#else
+  // POSIX: absolute paths start with '/'
+  if (path[0] == '/') {
+    return path;
+  }
+
+  char buffer[PATH_MAX];
+  if (realpath(path.c_str(), buffer)) {
+    return std::string(buffer);
+  }
+
+  return path;  // fallback on failure
+#endif
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/file-utils.h b/sherpa-onnx/csrc/file-utils.h
index 27167e7f..7f123264 100644
--- a/sherpa-onnx/csrc/file-utils.h
+++ b/sherpa-onnx/csrc/file-utils.h
@@ -44,6 +44,8 @@ std::vector<char> ReadFile(NativeResourceManager *mgr,
                            const std::string &filename);
 #endif
 
+std::string ResolveAbsolutePath(const std::string &path);
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_FILE_UTILS_H_
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.cc b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
index e888f6d6..1ff4bf4d 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
@@ -9,7 +9,6 @@
 #include <cmath>
 #include <cstdint>
 #include <cstring>
-#include <filesystem>
 #include <memory>
 #include <sstream>
 #include <string>
@@ -1016,10 +1015,7 @@ class OfflineFunASRNanoModel::Impl {
     bool has_external_data = FileExists(data_path);
 
     // Resolve absolute path for model file
-    std::string abs_model_path = model_path;
-    if (!model_path.empty() && !std::filesystem::path(model_path).is_absolute()) {
-      abs_model_path = std::filesystem::absolute(model_path).string();
-    }
+    std::string abs_model_path = ResolveAbsolutePath(model_path);
 
     if (has_external_data) {
       // When external data exists, use absolute file path to create session.

commit b82d9c6aff1a4bab2997376c0163543b58056d1d
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Wed Jan 7 10:41:53 2026 +0800

    FunASR-nano: switch to unified KV-cache LLM (#2995)
    
    This PR updates FunASR-nano inference from the prefill+decode dual-model pipeline to a single unified KV-cache model.
    
    ## Summary
    
    Previously, FunASR-nano required two separate ONNX models:
    - `llm_prefill.onnx`
    - `llm_decode.onnx`
    
    This PR switches to a single model:
    - `llm.onnx`
    
    The new pipeline uses a static KV cache + KV-delta incremental update mechanism, and relies on `cache_position` to differentiate prefill vs. decode steps. This significantly simplifies model/session management and reduces deployment complexity.
    
    ## Key changes
    
    - **Single LLM session / single model file**: `llm.onnx` replaces `llm_prefill.onnx` + `llm_decode.onnx`.
    
    - **Unified KV-cache implementation**:
      - static KV cache layout
      - KV-delta update for decode
      - `cache_position` distinguishes prefill vs. decode behavior
    
    - **Config changes (breaking)**:
      - `funasr_nano.llm_prefill` and `funasr_nano.llm_decode` are deprecated/removed
      - use only `funasr_nano.llm`
    
    - **Not backward compatible**:
      - users must re-export models in KV-delta/unified-KV format
    
    - **Trade-off**: slightly slower, but lower VRAM duplication

diff --git a/cxx-api-examples/funasr-nano-cxx-api.cc b/cxx-api-examples/funasr-nano-cxx-api.cc
index c01b8fc0..6b06ec47 100644
--- a/cxx-api-examples/funasr-nano-cxx-api.cc
+++ b/cxx-api-examples/funasr-nano-cxx-api.cc
@@ -9,8 +9,7 @@
 // Example usage:
 //   ./bin/funasr-nano-cxx-api \
 //     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-//     --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
-//     --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+//     --funasr-nano-llm=/path/to/llm.onnx \
 //     --funasr-nano-embedding=/path/to/embedding.onnx \
 //     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
 //     /path/to/audio.wav
@@ -33,27 +32,25 @@ FunASR-nano speech recognition example using sherpa-onnx C++ API.
 Usage:
   ./bin/funasr-nano-cxx-api \
     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
-    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-llm=/path/to/llm.onnx \
     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
     --funasr-nano-embedding=/path/to/embedding.onnx \
     [--funasr-nano-user-prompt=""] \
     [--funasr-nano-max-new-tokens=512] \
-    [--funasr-nano-temperature=0.3] \
+    [--funasr-nano-temperature=1e-6] \
     [--funasr-nano-top-p=0.8] \
     /path/to/audio.wav
 
 Required arguments:
   --funasr-nano-encoder-adaptor: Path to encoder_adaptor.onnx
-  --funasr-nano-llm-prefill: Path to llm_prefill.onnx
-  --funasr-nano-llm-decode: Path to llm_decode.onnx
+  --funasr-nano-llm: Path to llm.onnx (unified KV cache model)
   --funasr-nano-tokenizer: Path to tokenizer directory (e.g., Qwen3-0.6B)
   --funasr-nano-embedding: Path to embedding.onnx
 
 Optional arguments:
   --funasr-nano-user-prompt: User prompt template (default: "")
   --funasr-nano-max-new-tokens: Maximum tokens to generate (default: 512)
-  --funasr-nano-temperature: Sampling temperature (default: 0.3)
+  --funasr-nano-temperature: Sampling temperature (default: 1e-6)
   --funasr-nano-top-p: Top-p sampling threshold (default: 0.8)
   --num-threads: Number of threads (default: 2)
   --provider: cpu (default) or cuda
@@ -61,8 +58,7 @@ Optional arguments:
 Example:
   ./bin/funasr-nano-cxx-api \
     --funasr-nano-encoder-adaptor=./models/encoder_adaptor.onnx \
-    --funasr-nano-llm-prefill=./models/llm_prefill.onnx \
-    --funasr-nano-llm-decode=./models/llm_decode.onnx \
+    --funasr-nano-llm=./models/llm.onnx \
     --funasr-nano-tokenizer=./models/Qwen3-0.6B \
     --funasr-nano-embedding=./models/embedding.onnx \
     ./test.wav
@@ -80,8 +76,7 @@ Example:
 
   // Parse command line arguments
   const char kEncoderAdaptor[] = "--funasr-nano-encoder-adaptor=";
-  const char kLlmPrefill[] = "--funasr-nano-llm-prefill=";
-  const char kLlmDecode[] = "--funasr-nano-llm-decode=";
+  const char kLlm[] = "--funasr-nano-llm=";
   const char kEmbedding[] = "--funasr-nano-embedding=";
   const char kTokenizer[] = "--funasr-nano-tokenizer=";
   const char kUserPrompt[] = "--funasr-nano-user-prompt=";
@@ -96,12 +91,9 @@ Example:
     if (arg.find(kEncoderAdaptor) == 0) {
       config.model_config.funasr_nano.encoder_adaptor =
           arg.substr(sizeof(kEncoderAdaptor) - 1);
-    } else if (arg.find(kLlmPrefill) == 0) {
-      config.model_config.funasr_nano.llm_prefill =
-          arg.substr(sizeof(kLlmPrefill) - 1);
-    } else if (arg.find(kLlmDecode) == 0) {
-      config.model_config.funasr_nano.llm_decode =
-          arg.substr(sizeof(kLlmDecode) - 1);
+    } else if (arg.find(kLlm) == 0) {
+      config.model_config.funasr_nano.llm =
+          arg.substr(sizeof(kLlm) - 1);
     } else if (arg.find(kEmbedding) == 0) {
       config.model_config.funasr_nano.embedding =
           arg.substr(sizeof(kEmbedding) - 1);
@@ -132,10 +124,8 @@ Example:
       std::cout << "Loading model...\n";
       std::cout << "  encoder_adaptor: "
                 << config.model_config.funasr_nano.encoder_adaptor << "\n";
-      std::cout << "  llm_prefill: "
-                << config.model_config.funasr_nano.llm_prefill << "\n";
-      std::cout << "  llm_decode: "
-                << config.model_config.funasr_nano.llm_decode << "\n";
+      std::cout << "  llm: "
+                << config.model_config.funasr_nano.llm << "\n";
       std::cout << "  tokenizer: " << config.model_config.funasr_nano.tokenizer
                 << "\n";
       std::cout << "  embedding: "
diff --git a/python-api-examples/offline-funasr-nano-decode-files.py b/python-api-examples/offline-funasr-nano-decode-files.py
index 31e1c22f..8feee7de 100644
--- a/python-api-examples/offline-funasr-nano-decode-files.py
+++ b/python-api-examples/offline-funasr-nano-decode-files.py
@@ -10,8 +10,7 @@ This script demonstrates how to use FunASR-nano models for offline speech recogn
 Usage:
     python offline-funasr-nano-decode-files.py \
         --encoder-adaptor=/path/to/encoder_adaptor.onnx \
-        --llm-prefill=/path/to/llm_prefill.onnx \
-        --llm-decode=/path/to/llm_decode.onnx \
+        --llm=/path/to/llm.onnx \
         --tokenizer=/path/to/Qwen3-0.6B \
         --embedding=/path/to/embedding.onnx \
         [--num-threads=4] \
@@ -46,17 +45,10 @@ def get_args():
     )
 
     parser.add_argument(
-        "--llm-prefill",
+        "--llm",
         type=str,
         required=True,
-        help="Path to llm_prefill.onnx (KV cache mode)",
-    )
-
-    parser.add_argument(
-        "--llm-decode",
-        type=str,
-        required=True,
-        help="Path to llm_decode.onnx (KV cache mode)",
+        help="Path to llm.onnx (unified KV cache model)",
     )
 
     parser.add_argument(
@@ -83,7 +75,7 @@ def get_args():
     parser.add_argument(
         "--user-prompt",
         type=str,
-        default="Transcription:",
+        default=":",
         help="User prompt template for FunASR-nano",
     )
 
@@ -97,7 +89,7 @@ def get_args():
     parser.add_argument(
         "--temperature",
         type=float,
-        default=0.3,
+        default=1e-6,
         help="Sampling temperature",
     )
 
@@ -151,8 +143,7 @@ def get_args():
 def create_recognizer(args) -> sherpa_onnx.OfflineRecognizer:
     return sherpa_onnx.OfflineRecognizer.from_funasr_nano(
         encoder_adaptor=args.encoder_adaptor,
-        llm_prefill=args.llm_prefill,
-        llm_decode=args.llm_decode,
+        llm=args.llm,
         embedding=args.embedding,
         tokenizer=args.tokenizer,
         num_threads=args.num_threads,
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index de2797b3..1f17094c 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -454,8 +454,7 @@ type OfflineFireRedAsrModelConfig struct {
 
 type OfflineFunASRNanoModelConfig struct {
 	EncoderAdaptor string
-	LlmPreFill     string
-	LlmDecoder     string
+	Llm            string
 	Embedding      string
 	Tokenizer      string
 	SystemPrompt   string
@@ -598,8 +597,7 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 	c.model_config.fire_red_asr.decoder = C.CString(config.ModelConfig.FireRedAsr.Decoder)
 
 	c.model_config.funasr_nano.encoder_adaptor = C.CString(config.ModelConfig.FunASRNano.EncoderAdaptor)
-	c.model_config.funasr_nano.llm_prefill = C.CString(config.ModelConfig.FunASRNano.LlmPreFill)
-	c.model_config.funasr_nano.llm_decode = C.CString(config.ModelConfig.FunASRNano.LlmDecoder)
+	c.model_config.funasr_nano.llm = C.CString(config.ModelConfig.FunASRNano.Llm)
 	c.model_config.funasr_nano.embedding = C.CString(config.ModelConfig.FunASRNano.Embedding)
 	c.model_config.funasr_nano.tokenizer = C.CString(config.ModelConfig.FunASRNano.Tokenizer)
 	c.model_config.funasr_nano.system_prompt = C.CString(config.ModelConfig.FunASRNano.SystemPrompt)
@@ -679,8 +677,7 @@ func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig)
 		&c.model_config.fire_red_asr.encoder,
 		&c.model_config.fire_red_asr.decoder,
 		&c.model_config.funasr_nano.encoder_adaptor,
-		&c.model_config.funasr_nano.llm_prefill,
-		&c.model_config.funasr_nano.llm_decode,
+		&c.model_config.funasr_nano.llm,
 		&c.model_config.funasr_nano.embedding,
 		&c.model_config.funasr_nano.tokenizer,
 		&c.model_config.funasr_nano.system_prompt,
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 41c9a0fd..ef12a60c 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -513,10 +513,8 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
 
   recognizer_config.model_config.funasr_nano.encoder_adaptor =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.encoder_adaptor, "");
-  recognizer_config.model_config.funasr_nano.llm_prefill =
-      SHERPA_ONNX_OR(config->model_config.funasr_nano.llm_prefill, "");
-  recognizer_config.model_config.funasr_nano.llm_decode =
-      SHERPA_ONNX_OR(config->model_config.funasr_nano.llm_decode, "");
+  recognizer_config.model_config.funasr_nano.llm =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.llm, "");
   recognizer_config.model_config.funasr_nano.embedding =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.embedding, "");
   recognizer_config.model_config.funasr_nano.tokenizer =
@@ -530,7 +528,7 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.funasr_nano.max_new_tokens =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.max_new_tokens, 512);
   recognizer_config.model_config.funasr_nano.temperature =
-      SHERPA_ONNX_OR(config->model_config.funasr_nano.temperature, 0.3f);
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.temperature, 1e-6f);
   recognizer_config.model_config.funasr_nano.top_p =
       SHERPA_ONNX_OR(config->model_config.funasr_nano.top_p, 0.8f);
   recognizer_config.model_config.funasr_nano.seed =
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 768380a8..337f3855 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -486,8 +486,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
 
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineFunASRNanoModelConfig {
   const char *encoder_adaptor;
-  const char *llm_prefill;
-  const char *llm_decode;
+  const char *llm;
   const char *embedding;
   const char *tokenizer;
   const char *system_prompt;
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index 947b5e40..772d6fe3 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -274,10 +274,8 @@ static SherpaOnnxOfflineRecognizerConfig Convert(
 
   c.model_config.funasr_nano.encoder_adaptor =
       config.model_config.funasr_nano.encoder_adaptor.c_str();
-  c.model_config.funasr_nano.llm_prefill =
-      config.model_config.funasr_nano.llm_prefill.c_str();
-  c.model_config.funasr_nano.llm_decode =
-      config.model_config.funasr_nano.llm_decode.c_str();
+  c.model_config.funasr_nano.llm =
+      config.model_config.funasr_nano.llm.c_str();
   c.model_config.funasr_nano.embedding =
       config.model_config.funasr_nano.embedding.c_str();
   c.model_config.funasr_nano.tokenizer =
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index 9ba5c985..e1291fc6 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -285,14 +285,13 @@ struct SHERPA_ONNX_API OfflineMoonshineModelConfig {
 
 struct SHERPA_ONNX_API OfflineFunASRNanoModelConfig {
   std::string encoder_adaptor;
-  std::string llm_prefill;
-  std::string llm_decode;
+  std::string llm;
   std::string embedding;
   std::string tokenizer;
   std::string system_prompt = "You are a helpful assistant.";
   std::string user_prompt = "";
   int32_t max_new_tokens = 512;
-  float temperature = 0.3f;
+  float temperature = 1e-6f;
   float top_p = 0.8f;
   int32_t seed = 42;
 };
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
index c0e2a845..84f1fbaa 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
@@ -15,11 +15,8 @@ void OfflineFunASRNanoModelConfig::Register(ParseOptions *po) {
   po->Register("funasr-nano-encoder-adaptor", &encoder_adaptor,
                "Path to encoder_adaptor.onnx for FunASR-nano");
 
-  po->Register("funasr-nano-llm-prefill", &llm_prefill,
-               "Path to llm_prefill.onnx for FunASR-nano (KV cache mode)");
-
-  po->Register("funasr-nano-llm-decode", &llm_decode,
-               "Path to llm_decode.onnx for FunASR-nano (KV cache mode)");
+  po->Register("funasr-nano-llm", &llm,
+               "Path to llm.onnx for FunASR-nano (KV cache mode)");
 
   po->Register("funasr-nano-embedding", &embedding,
                "Path to embedding.onnx for FunASR-nano");
@@ -58,18 +55,13 @@ bool OfflineFunASRNanoModelConfig::Validate() const {
     return false;
   }
 
-  // KV cache mode (prefill + decode) is required
-  if (llm_prefill.empty() || llm_decode.empty()) {
-    SHERPA_ONNX_LOGE("Both --funasr-nano-llm-prefill and --funasr-nano-llm-decode are required");
+  if (llm.empty()) {
+    SHERPA_ONNX_LOGE("--funasr-nano-llm is required");
     return false;
   }
 
-  if (!FileExists(llm_prefill)) {
-    SHERPA_ONNX_LOGE("--funasr-nano-llm-prefill: '%s' does not exist", llm_prefill.c_str());
-    return false;
-  }
-  if (!FileExists(llm_decode)) {
-    SHERPA_ONNX_LOGE("--funasr-nano-llm-decode: '%s' does not exist", llm_decode.c_str());
+  if (!FileExists(llm)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-llm: '%s' does not exist", llm.c_str());
     return false;
   }
 
@@ -121,8 +113,7 @@ std::string OfflineFunASRNanoModelConfig::ToString() const {
 
   os << "OfflineFunASRNanoModelConfig(";
   os << "encoder_adaptor=\"" << encoder_adaptor << "\", ";
-  os << "llm_prefill=\"" << llm_prefill << "\", ";
-  os << "llm_decode=\"" << llm_decode << "\", ";
+  os << "llm=\"" << llm << "\", ";
   os << "embedding=\"" << embedding << "\", ";
   os << "tokenizer=\"" << tokenizer << "\", ";
   os << "system_prompt=\"" << system_prompt << "\", ";
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model-config.h b/sherpa-onnx/csrc/offline-funasr-nano-model-config.h
index dc7e4e34..060ea704 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model-config.h
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model-config.h
@@ -15,11 +15,8 @@ struct OfflineFunASRNanoModelConfig {
   // Path to encoder_adaptor.onnx
   std::string encoder_adaptor;
 
-  // Path to llm_prefill.onnx (KV cache prefill)
-  std::string llm_prefill;
-
-  // Path to llm_decode.onnx (KV cache decode)
-  std::string llm_decode;
+  // Path to llm.onnx (KV cache model)
+  std::string llm;
 
   // Path to embedding.onnx
   std::string embedding;
@@ -37,7 +34,7 @@ struct OfflineFunASRNanoModelConfig {
   int32_t max_new_tokens = 512;
 
   // Sampling temperature
-  float temperature = 0.3f;
+  float temperature = 1e-6f;
 
   // Top-p (nucleus) sampling threshold
   float top_p = 0.8f;
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.cc b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
index 261891dd..e888f6d6 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
@@ -9,6 +9,7 @@
 #include <cmath>
 #include <cstdint>
 #include <cstring>
+#include <filesystem>
 #include <memory>
 #include <sstream>
 #include <string>
@@ -115,12 +116,6 @@ static inline bool IsCudaProvider(const std::string &provider) {
   return p == "cuda" || (p.size() > 4 && p.find("cuda") == 0);
 }
 
-// Check if a tensor element type is FP16-IO (float16 or uint16).
-static inline bool IsFloat16IO(ONNXTensorElementDataType t) {
-  return t == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
-         t == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16;
-}
-
 // Get the element type of a session input tensor.
 static inline ONNXTensorElementDataType GetSessionInputElemType(
     Ort::Session *sess, size_t input_index) {
@@ -150,6 +145,22 @@ Ort::Value AllocTensor<uint16_t>(OrtAllocator *alloc,
                                   ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
 }
 
+// Allocate tensor by ONNX elem type (float/float16 only).
+static inline Ort::Value AllocTensorByElemType(OrtAllocator *alloc,
+                                               const std::vector<int64_t> &shape,
+                                               ONNXTensorElementDataType t) {
+  if (t == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+    return AllocTensor<float>(alloc, shape);
+  }
+  if (t == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+      t == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16) {
+    return AllocTensor<uint16_t>(alloc, shape);
+  }
+  SHERPA_ONNX_LOGE("AllocTensorByElemType: unsupported elem_type=%d", (int)t);
+  SHERPA_ONNX_EXIT(-1);
+  return AllocTensor<float>(alloc, shape);
+}
+
 // Convert tensor to float32, handling both float16 and float32 inputs.
 // NOTE: This helper assumes the input tensor is on CPU memory.
 // The caller must ensure the tensor is on CPU (e.g., via IO Binding).
@@ -211,7 +222,6 @@ static Ort::Value CastToFloat16(Ort::Value in, OrtAllocator *alloc) {
 
 // Cast tensor to the expected element type (float16 or float32).
 // Returns the input unchanged if it already matches the expected type.
-// NOTE: This helper assumes the input tensor is on CPU memory.
 static Ort::Value CastFloatLikeForExpected(Ort::Value in,
                                           ONNXTensorElementDataType expected,
                                           OrtAllocator *alloc) {
@@ -266,15 +276,55 @@ static Ort::Value CastMaskToInt64IfNeeded(Ort::Value in, OrtAllocator *alloc) {
   return in;
 }
 
-// Create a non-owning tensor view that preserves the underlying memory info.
-static inline Ort::Value ViewConst(const Ort::Value &v) {
-  return View(const_cast<Ort::Value *>(&v));
+// Ensure attention_mask is [batch, target_len] on CPU, int64.
+// If shorter: pad with 0. If longer: truncate.
+static Ort::Value NormalizeAttentionMask(Ort::Value mask, int64_t target_len,
+                                        OrtAllocator *alloc) {
+  if (!mask.IsTensor()) return mask;
+  AssertTensorIsCpu(mask, "NormalizeAttentionMask");
+
+  auto info = mask.GetTensorTypeAndShapeInfo();
+  auto shape = info.GetShape();
+  if (shape.size() != 2) return mask;
+
+  int64_t b = shape[0];
+  int64_t l = shape[1];
+  if (b <= 0 || l <= 0) return mask;
+
+  if (static_cast<ONNXTensorElementDataType>(info.GetElementType()) !=
+      ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
+    mask = CastMaskToInt64IfNeeded(std::move(mask), alloc);
+    info = mask.GetTensorTypeAndShapeInfo();
+    shape = info.GetShape();
+    if (shape.size() != 2) return mask;
+    b = shape[0];
+    l = shape[1];
+  }
+
+  if (l == target_len) return mask;
+
+  std::vector<int64_t> new_shape = {b, target_len};
+  Ort::Value out = AllocTensor<int64_t>(alloc, new_shape);
+  int64_t *dst = out.GetTensorMutableData<int64_t>();
+  const int64_t *src = mask.GetTensorData<int64_t>();
+
+  std::memset(dst, 0, static_cast<size_t>(b) * static_cast<size_t>(target_len) *
+                          sizeof(int64_t));
+
+  int64_t copy_len = std::min<int64_t>(l, target_len);
+  for (int64_t bi = 0; bi < b; ++bi) {
+    const int64_t *srow = src + bi * l;
+    int64_t *drow = dst + bi * target_len;
+    std::memcpy(drow, srow, static_cast<size_t>(copy_len) * sizeof(int64_t));
+  }
+
+  return out;
 }
 
 }  // namespace
 
 // Implementation class for OfflineFunASRNanoModel.
-// Manages ONNX sessions for encoder, LLM prefill/decode, and embedding models.
+// Manages ONNX sessions for encoder, KV cache LLM, and embedding models.
 class OfflineFunASRNanoModel::Impl {
  public:
   explicit Impl(const OfflineModelConfig &config)
@@ -293,44 +343,27 @@ class OfflineFunASRNanoModel::Impl {
       SHERPA_ONNX_LOGE("funasr_nano.encoder_adaptor is empty");
       SHERPA_ONNX_EXIT(-1);
     }
-    if (c.llm_prefill.empty() || c.llm_decode.empty()) {
-      SHERPA_ONNX_LOGE(
-          "funasr_nano.llm_prefill/llm_decode are required for KV-cache mode");
+
+    if (c.llm.empty()) {
+      SHERPA_ONNX_LOGE("funasr_nano.llm is required for KV cache mode");
       SHERPA_ONNX_EXIT(-1);
     }
-
+    
     InitEncoderAdaptor(c.encoder_adaptor);
-    InitLLMPrefill(c.llm_prefill);
-    InitLLMDecode(c.llm_decode);
+    InitLLM(c.llm);
     InitEmbedding(c.embedding);
     has_embedding_model_ = true;
 
     // FunASR-nano uses CPU-side sampling. When running on CUDA, we bind
-    // logits to CPU (so sampling can read it safely), while keeping KV cache
-    // on GPU to avoid large device<->host copies.
+    // logits to CPU (so sampling can read it safely).
     use_cuda_iobinding_ = (!is_cpu_provider_ && IsCudaProvider(config_.provider));
     if (use_cuda_iobinding_) {
       // Use device 0 by default. SessionOptions() in sherpa-onnx usually
       // configures the CUDA EP device; binding here only affects output memory.
       cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
           "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
-
-      // Check if prefill/decode models have FP16-IO, which is not supported on CUDA yet.
-      ONNXTensorElementDataType prefill_in_type = prefill_embeds_in_type_;
-      ONNXTensorElementDataType prefill_out_type =
-          GetSessionOutputElemType(prefill_sess_.get(), 0);
-      ONNXTensorElementDataType decode_in_type = decode_embeds_in_type_;
-      ONNXTensorElementDataType decode_out_type =
-          GetSessionOutputElemType(decode_sess_.get(), 0);
-
-      if (IsFloat16IO(prefill_in_type) || IsFloat16IO(prefill_out_type) ||
-          IsFloat16IO(decode_in_type) || IsFloat16IO(decode_out_type)) {
-        SHERPA_ONNX_LOGE(
-            "fp16-IO LLM models are not supported on CUDA yet. Please use "
-            "fp32/int8 models.");
-        SHERPA_ONNX_EXIT(-1);
-      }
     }
+    CheckFp16OnCuda();
   }
 
   void InitEncoderAdaptorFromMemory(void *model_data,
@@ -358,63 +391,163 @@ class OfflineFunASRNanoModel::Impl {
     SHERPA_ONNX_READ_META_DATA(hidden_size_, "llm_dim");
   }
 
-  void InitLLMPrefillFromMemory(void *model_data, size_t model_data_length) {
-    prefill_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_llm_);
-    GetInputNames(prefill_sess_.get(), &prefill_input_names_,
-                  &prefill_input_names_ptr_);
-    GetOutputNames(prefill_sess_.get(), &prefill_output_names_,
-                   &prefill_output_names_ptr_);
-    prefill_embeds_in_type_ = GetSessionInputElemType(prefill_sess_.get(), 0);
-    Ort::ModelMetadata meta_data = prefill_sess_->GetModelMetadata();
+  void SetupLlmFromSession() {
+    GetInputNames(llm_sess_.get(), &llm_input_names_, &llm_input_names_ptr_);
+    GetOutputNames(llm_sess_.get(), &llm_output_names_, &llm_output_names_ptr_);
+
+    llm_embeds_in_type_ = GetSessionInputElemType(llm_sess_.get(), 0);
+    if (llm_embeds_in_type_ != ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+      SHERPA_ONNX_LOGE("LLM inputs_embeds must be float32, got elem_type=%d",
+                       (int)llm_embeds_in_type_);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    Ort::ModelMetadata meta_data = llm_sess_->GetModelMetadata();
     if (config_.debug) {
       std::ostringstream os;
       PrintModelMetadata(os, meta_data);
 #if __OHOS__
-      SHERPA_ONNX_LOGE("Prefill model metadata:\n%{public}s\n", os.str().c_str());
+      SHERPA_ONNX_LOGE("LLM model metadata:\n%{public}s\n", os.str().c_str());
 #else
-      SHERPA_ONNX_LOGE("Prefill model metadata:\n%s\n", os.str().c_str());
+      SHERPA_ONNX_LOGE("LLM model metadata:\n%s\n", os.str().c_str());
 #endif
     }
-    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+
+    Ort::AllocatorWithDefaultOptions allocator;
     SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
     if (hidden_size_ == 0) {
       SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
     }
-  }
 
-  void InitLLMDecodeFromMemory(void *model_data, size_t model_data_length) {
-    decode_sess_ = std::make_unique<Ort::Session>(
-        env_, model_data, model_data_length, sess_opts_llm_);
-    GetInputNames(decode_sess_.get(), &decode_input_names_,
-                  &decode_input_names_ptr_);
-    GetOutputNames(decode_sess_.get(), &decode_output_names_,
-                   &decode_output_names_ptr_);
-    decode_embeds_in_type_ = GetSessionInputElemType(decode_sess_.get(), 0);
-    Ort::ModelMetadata meta_data = decode_sess_->GetModelMetadata();
-    if (config_.debug) {
-      std::ostringstream os;
-      PrintModelMetadata(os, meta_data);
-#if __OHOS__
-      SHERPA_ONNX_LOGE("Decode model metadata:\n%{public}s\n", os.str().c_str());
-#else
-      SHERPA_ONNX_LOGE("Decode model metadata:\n%s\n", os.str().c_str());
-#endif
-    }
-    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
-    int32_t decode_vocab_size = 0;
-    SHERPA_ONNX_READ_META_DATA(decode_vocab_size, "vocab_size");
-    if (vocab_size_ > 0 && decode_vocab_size != vocab_size_) {
+    // Detect KV delta model type (model_type metadata should contain "kv_delta")
+    auto model_type_value =
+        LookupCustomModelMetaData(meta_data, "model_type", allocator);
+    is_kv_delta_model_ = (!model_type_value.empty() &&
+                          model_type_value.find("kv_delta") != std::string::npos);
+
+    int32_t num_outputs = static_cast<int32_t>(llm_output_names_.size());
+    if (num_outputs < 1 || (num_outputs - 1) % 2 != 0) {
       SHERPA_ONNX_LOGE(
-          "Decode model vocab_size (%d) != prefill vocab_size (%d)",
-          decode_vocab_size, vocab_size_);
+          "LLM model must have 1 logits output + 2*num_layers KV outputs, got %d outputs",
+          num_outputs);
+      SHERPA_ONNX_EXIT(-1);
     }
-    if (vocab_size_ == 0) vocab_size_ = decode_vocab_size;
-    int32_t decode_hidden_size = 0;
-    if (hidden_size_ == 0) {
-      SHERPA_ONNX_READ_META_DATA(decode_hidden_size, "hidden_size");
-      hidden_size_ = decode_hidden_size;
+    int32_t inferred_layers = (num_outputs - 1) / 2;
+
+    auto num_layers_value =
+        LookupCustomModelMetaData(meta_data, "num_layers", allocator);
+    if (!num_layers_value.empty()) {
+      num_layers_ = atoi(num_layers_value.c_str());
+      if (num_layers_ <= 0) {
+        SHERPA_ONNX_LOGE("Invalid num_layers=%d from metadata", num_layers_);
+        SHERPA_ONNX_EXIT(-1);
+      }
+      if (num_layers_ != inferred_layers) {
+        SHERPA_ONNX_LOGE("LLM num_layers mismatch: metadata=%d, inferred=%d",
+                         num_layers_, inferred_layers);
+        SHERPA_ONNX_EXIT(-1);
+      }
+    } else {
+      num_layers_ = inferred_layers;
+    }
+
+    // Read KV cache capacity from metadata.
+    auto max_total_len_value =
+        LookupCustomModelMetaData(meta_data, "max_total_len", allocator);
+    if (!max_total_len_value.empty()) {
+      max_total_len_ = atoi(max_total_len_value.c_str());
+    } else {
+      auto attn_len_value =
+          LookupCustomModelMetaData(meta_data, "attention_mask_len", allocator);
+      if (!attn_len_value.empty()) max_total_len_ = atoi(attn_len_value.c_str());
     }
+    if (max_total_len_ <= 0) {
+      // Fallback: use input[1] shape
+      auto ti = llm_sess_->GetInputTypeInfo(1);
+      auto shp = ti.GetTensorTypeAndShapeInfo().GetShape();
+      if (shp.size() == 2 && shp[1] > 0) {
+        max_total_len_ = static_cast<int32_t>(shp[1]);
+      }
+      if (max_total_len_ <= 0) {
+        SHERPA_ONNX_LOGE("Failed to determine max_total_len from metadata or input shape");
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+
+    // Only KV delta models are supported
+    if (!is_kv_delta_model_) {
+      SHERPA_ONNX_LOGE("Only KV delta models are supported, but model_type does not contain 'kv_delta'");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    // Validate input layout: 0 embeds, 1 attention_mask, 2 cache_position, 3+ KV cache
+    if (llm_input_names_.size() < 3u) {
+      SHERPA_ONNX_LOGE("LLM model inputs must be >=3 (embeds,mask,cache_position)");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    cache_position_input_index_ = 2;
+    past_kv_input_start_index_ = 3;
+
+    int32_t expected_inputs = 3 + 2 * num_layers_;
+    int32_t actual_inputs = static_cast<int32_t>(llm_input_names_.size());
+    if (actual_inputs != expected_inputs) {
+      if (actual_inputs == 2 + 2 * num_layers_) {
+        SHERPA_ONNX_LOGE(
+            "LLM model inputs mismatch: expected %d (=3+2*num_layers with cache_position) "
+            "got %d (=2+2*num_layers without cache_position). "
+            "Please use a model exported with cache_position support.",
+            expected_inputs, actual_inputs);
+      } else {
+        SHERPA_ONNX_LOGE(
+            "LLM model inputs mismatch: expected %d (=3+2*num_layers) got %d",
+            expected_inputs, actual_inputs);
+      }
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    // KV input element type (should be float16 or float32).
+    kv_in_type_ = GetSessionInputElemType(llm_sess_.get(), past_kv_input_start_index_);
+    kv_in_type_v_ = GetSessionInputElemType(llm_sess_.get(), past_kv_input_start_index_ + 1);
+    if (!(kv_in_type_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT ||
+          kv_in_type_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+          kv_in_type_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16)) {
+      SHERPA_ONNX_LOGE("LLM past_key elem_type=%d not supported", (int)kv_in_type_);
+      SHERPA_ONNX_EXIT(-1);
+    }
+    if (!(kv_in_type_v_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT ||
+          kv_in_type_v_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+          kv_in_type_v_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16)) {
+      SHERPA_ONNX_LOGE("LLM past_value elem_type=%d not supported", (int)kv_in_type_v_);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    // Templates for KV shapes from session inputs.
+    auto past_key_ti = llm_sess_->GetInputTypeInfo(past_kv_input_start_index_);
+    past_key_shape_tpl_ = past_key_ti.GetTensorTypeAndShapeInfo().GetShape();
+
+    auto past_value_ti = llm_sess_->GetInputTypeInfo(past_kv_input_start_index_ + 1);
+    past_value_shape_tpl_ = past_value_ti.GetTensorTypeAndShapeInfo().GetShape();
+  }
+
+  void InitLLMFromMemory(void *model_data, size_t model_data_length) {
+    try {
+      llm_sess_ = std::make_unique<Ort::Session>(env_, model_data,
+                                                 model_data_length,
+                                                 sess_opts_llm_);
+    } catch (const Ort::Exception &e) {
+      SHERPA_ONNX_LOGE("InitLLMFromMemory: failed to create session: %s", e.what());
+      if (std::string(e.what()).find("external data") != std::string::npos ||
+          std::string(e.what()).find("External data") != std::string::npos) {
+        SHERPA_ONNX_LOGE(
+            "LLM model requires external data (.data file) but loaded from memory. "
+            "Please use fp16/int8 single-file model or load by file path instead.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+      throw;
+    }
+
+    SetupLlmFromSession();
   }
 
   void InitEmbeddingFromMemory(void *model_data, size_t model_data_length) {
@@ -457,22 +590,17 @@ class OfflineFunASRNanoModel::Impl {
       SHERPA_ONNX_LOGE("funasr_nano.encoder_adaptor is empty");
       SHERPA_ONNX_EXIT(-1);
     }
-    if (c.llm_prefill.empty() || c.llm_decode.empty()) {
-      SHERPA_ONNX_LOGE(
-          "funasr_nano.llm_prefill/llm_decode are required for KV-cache mode");
+
+    if (c.llm.empty()) {
+      SHERPA_ONNX_LOGE("funasr_nano.llm is required for KV cache mode");
       SHERPA_ONNX_EXIT(-1);
     }
 
     auto buf_encoder = ReadFile(mgr, c.encoder_adaptor);
     InitEncoderAdaptorFromMemory(buf_encoder.data(), buf_encoder.size());
 
-    {
-      auto buf_prefill = ReadFile(mgr, c.llm_prefill);
-      InitLLMPrefillFromMemory(buf_prefill.data(), buf_prefill.size());
-    }
-
-    auto buf_decode = ReadFile(mgr, c.llm_decode);
-    InitLLMDecodeFromMemory(buf_decode.data(), buf_decode.size());
+    auto buf_llm = ReadFile(mgr, c.llm);
+    InitLLMFromMemory(buf_llm.data(), buf_llm.size());
 
     auto buf_embedding = ReadFile(mgr, c.embedding);
     InitEmbeddingFromMemory(buf_embedding.data(), buf_embedding.size());
@@ -482,23 +610,8 @@ class OfflineFunASRNanoModel::Impl {
     if (use_cuda_iobinding_) {
       cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
           "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
-
-      // Check if prefill/decode models have FP16-IO, which is not supported on CUDA yet.
-      ONNXTensorElementDataType prefill_in_type = prefill_embeds_in_type_;
-      ONNXTensorElementDataType prefill_out_type =
-          GetSessionOutputElemType(prefill_sess_.get(), 0);
-      ONNXTensorElementDataType decode_in_type = decode_embeds_in_type_;
-      ONNXTensorElementDataType decode_out_type =
-          GetSessionOutputElemType(decode_sess_.get(), 0);
-
-      if (IsFloat16IO(prefill_in_type) || IsFloat16IO(prefill_out_type) ||
-          IsFloat16IO(decode_in_type) || IsFloat16IO(decode_out_type)) {
-        SHERPA_ONNX_LOGE(
-            "fp16-IO LLM models are not supported on CUDA yet. Please use "
-            "fp32/int8 models.");
-        SHERPA_ONNX_EXIT(-1);
-      }
     }
+    CheckFp16OnCuda();
   }
 
   // Forward pass through encoder adaptor model.
@@ -534,174 +647,283 @@ class OfflineFunASRNanoModel::Impl {
     return std::move(outputs[0]);
   }
 
-  // Forward pass through LLM prefill model with full context.
-  // Returns logits and initial KV cache states for all layers.
-  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-  ForwardLLMPrefill(Ort::Value inputs_embeds, Ort::Value attention_mask) {
-    if (NeedsTypeConversion(inputs_embeds, prefill_embeds_in_type_)) {
-      inputs_embeds = CastFloatLikeForExpected(
-          std::move(inputs_embeds), prefill_embeds_in_type_, allocator_);
+  std::vector<std::pair<Ort::Value, Ort::Value>> CreateEmptyKVCache(int64_t batch) {
+    std::vector<std::pair<Ort::Value, Ort::Value>> kv_cache;
+    kv_cache.reserve(num_layers_);
+
+    // Read kv_h, hd from input shape template (dim2, dim3)
+    auto &tpl = past_key_shape_tpl_;
+    if (tpl.size() < 4) {
+      SHERPA_ONNX_LOGE("Invalid KV cache shape template, expected >=4 dims");
+      SHERPA_ONNX_EXIT(-1);
     }
-    if (attention_mask.IsTensor()) {
-      auto mask_info = attention_mask.GetTensorTypeAndShapeInfo();
-      auto mask_type =
-          static_cast<ONNXTensorElementDataType>(mask_info.GetElementType());
-      if (mask_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
-        attention_mask =
-            CastMaskToInt64IfNeeded(std::move(attention_mask), allocator_);
+    int64_t kv_h = tpl[2];
+    int64_t hd = tpl[3];
+    std::vector<int64_t> key_shape = {batch, static_cast<int64_t>(max_total_len_), kv_h, hd};
+    std::vector<int64_t> value_shape = key_shape;
+
+    size_t key_numel = NumelFromShape(key_shape);
+    size_t value_numel = NumelFromShape(value_shape);
+
+    for (int32_t i = 0; i < num_layers_; ++i) {
+      Ort::Value key_tensor =
+          AllocTensorByElemType(allocator_, key_shape, kv_in_type_);
+      Ort::Value value_tensor =
+          AllocTensorByElemType(allocator_, value_shape, kv_in_type_);
+
+      // Zero-initialize cache
+      if (key_numel > 0) {
+        if (kv_in_type_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+          std::memset(key_tensor.GetTensorMutableData<float>(), 0, key_numel * sizeof(float));
+        } else {
+          std::memset(key_tensor.GetTensorMutableData<uint16_t>(), 0, key_numel * sizeof(uint16_t));
+        }
       }
-    }
 
-    std::vector<Ort::Value> outputs;
-
-    if (use_cuda_iobinding_) {
-      // CPU-side sampling needs logits on CPU, while KV cache should remain on
-      // GPU to avoid large device<->host copies between decode steps.
-      Ort::IoBinding binding(*prefill_sess_);
-      binding.BindInput(prefill_input_names_ptr_[0], inputs_embeds);
-      binding.BindInput(prefill_input_names_ptr_[1], attention_mask);
-
-      binding.BindOutput(prefill_output_names_ptr_[0], cpu_mem_info_);
-      for (size_t i = 1; i < prefill_output_names_ptr_.size(); ++i) {
-        binding.BindOutput(prefill_output_names_ptr_[i], *cuda_mem_info_);
+      if (value_numel > 0) {
+        if (kv_in_type_ == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+          std::memset(value_tensor.GetTensorMutableData<float>(), 0, value_numel * sizeof(float));
+        } else {
+          std::memset(value_tensor.GetTensorMutableData<uint16_t>(), 0, value_numel * sizeof(uint16_t));
+        }
       }
 
-      binding.SynchronizeInputs();
-      prefill_sess_->Run(Ort::RunOptions{nullptr}, binding);
-      binding.SynchronizeOutputs();
-      outputs = binding.GetOutputValues();
-    } else {
-      std::array<Ort::Value, 2> inputs = {std::move(inputs_embeds),
-                                          std::move(attention_mask)};
-      outputs = prefill_sess_->Run(
-          {}, prefill_input_names_ptr_.data(), inputs.data(), inputs.size(),
-          prefill_output_names_ptr_.data(), prefill_output_names_ptr_.size());
+      kv_cache.emplace_back(std::move(key_tensor), std::move(value_tensor));
     }
+    return kv_cache;
+  }
 
-    // First output is logits, remaining outputs are past_key_values
-    if (outputs.empty()) {
-      SHERPA_ONNX_LOGE("ForwardLLMPrefill: empty outputs");
+  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>> ForwardLLM(
+      Ort::Value inputs_embeds, Ort::Value attention_mask,
+      const Ort::Value &cache_position,
+      const std::vector<std::pair<Ort::Value, Ort::Value>> &cache_kv) {
+    if (static_cast<int32_t>(cache_kv.size()) != num_layers_) {
+      SHERPA_ONNX_LOGE("ForwardLLM: cache_kv size (%zu) != num_layers (%d)",
+                       cache_kv.size(), num_layers_);
       SHERPA_ONNX_EXIT(-1);
     }
 
-    Ort::Value logits = std::move(outputs[0]);
-
-    if (!logits.IsTensor()) {
-      SHERPA_ONNX_LOGE("ForwardLLMPrefill: logits is not a tensor");
+    if (!inputs_embeds.IsTensor()) {
+      SHERPA_ONNX_LOGE("ForwardLLM: inputs_embeds is not a tensor");
       SHERPA_ONNX_EXIT(-1);
     }
-    AssertTensorIsCpu(logits, "ForwardLLMPrefill logits");
 
-    if ((outputs.size() - 1) % 2 != 0) {
-      SHERPA_ONNX_LOGE("ForwardLLMPrefill: invalid KV cache outputs size=%d",
-                       static_cast<int>(outputs.size()));
+    auto embeds_info = inputs_embeds.GetTensorTypeAndShapeInfo();
+    auto embeds_type =
+        static_cast<ONNXTensorElementDataType>(embeds_info.GetElementType());
+    if (embeds_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+      SHERPA_ONNX_LOGE("ForwardLLM: inputs_embeds must be float32, got elem_type=%d",
+                       (int)embeds_type);
       SHERPA_ONNX_EXIT(-1);
     }
 
-    std::vector<std::pair<Ort::Value, Ort::Value>> past_kv;
-    int num_layers = static_cast<int>((outputs.size() - 1) / 2);
-    past_kv.reserve(num_layers);
-    for (int i = 0; i < num_layers; ++i) {
-      past_kv.emplace_back(std::move(outputs[1 + 2 * i]),
-                           std::move(outputs[1 + 2 * i + 1]));
-    }
-    return {std::move(logits), std::move(past_kv)};
-  }
-
-  // Forward pass through LLM decode model with KV cache.
-  // Takes a single token embedding and past KV cache, returns logits and
-  // updated KV cache states.
-  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-  ForwardLLMDecode(Ort::Value inputs_embeds, Ort::Value attention_mask,
-                   const std::vector<std::pair<Ort::Value, Ort::Value>>
-                       &past_key_values) {
-    if (NeedsTypeConversion(inputs_embeds, decode_embeds_in_type_)) {
-      inputs_embeds = CastFloatLikeForExpected(
-          std::move(inputs_embeds), decode_embeds_in_type_, allocator_);
-    }
+    // Prepare attention_mask: int64, ensure length <= max_total_len
     if (attention_mask.IsTensor()) {
       auto mask_info = attention_mask.GetTensorTypeAndShapeInfo();
       auto mask_type =
           static_cast<ONNXTensorElementDataType>(mask_info.GetElementType());
       if (mask_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
-        attention_mask =
-            CastMaskToInt64IfNeeded(std::move(attention_mask), allocator_);
+        attention_mask = CastMaskToInt64IfNeeded(std::move(attention_mask), allocator_);
+        mask_info = attention_mask.GetTensorTypeAndShapeInfo();
+      }
+
+      auto mask_shape = mask_info.GetShape();
+      if (mask_shape.size() == 2 && mask_shape[1] > max_total_len_) {
+        // Truncate attention_mask if it exceeds max_total_len
+        attention_mask = NormalizeAttentionMask(std::move(attention_mask),
+                                                max_total_len_, allocator_);
       }
     }
 
-    // Build inputs: [inputs_embeds, attention_mask, past_key_0, past_value_0, ...]
-    // NOTE: We create non-owning Ort::Value views that reference existing buffers.
     std::vector<Ort::Value> inputs;
-    inputs.reserve(2 + 2 * past_key_values.size());
+    inputs.reserve(3 + 2 * cache_kv.size());
     inputs.push_back(std::move(inputs_embeds));
     inputs.push_back(std::move(attention_mask));
-    for (const auto &kv : past_key_values) {
-      inputs.push_back(ViewConst(kv.first));
-      inputs.push_back(ViewConst(kv.second));
+    inputs.push_back(View(const_cast<Ort::Value *>(&cache_position)));
+
+    for (const auto &kv : cache_kv) {
+      inputs.push_back(View(const_cast<Ort::Value *>(&kv.first)));
+      inputs.push_back(View(const_cast<Ort::Value *>(&kv.second)));
     }
 
-    // Build input names: [inputs_embeds, attention_mask, past_key_0, past_value_0, ...]
     std::vector<const char *> input_names_ptr;
-    input_names_ptr.reserve(2 + 2 * past_key_values.size());
-    input_names_ptr.push_back(decode_input_names_ptr_[0]);
-    input_names_ptr.push_back(decode_input_names_ptr_[1]);
-    for (size_t i = 0; i < past_key_values.size(); ++i) {
-      input_names_ptr.push_back(decode_input_names_ptr_[2 + 2 * i]);
-      input_names_ptr.push_back(decode_input_names_ptr_[2 + 2 * i + 1]);
+    input_names_ptr.reserve(3 + 2 * cache_kv.size());
+    input_names_ptr.push_back(llm_input_names_ptr_[0]);  // inputs_embeds
+    input_names_ptr.push_back(llm_input_names_ptr_[1]);  // attention_mask
+    input_names_ptr.push_back(llm_input_names_ptr_[2]);  // cache_position
+    for (size_t i = 0; i < cache_kv.size(); ++i) {
+      input_names_ptr.push_back(llm_input_names_ptr_[past_kv_input_start_index_ + 2 * i]);
+      input_names_ptr.push_back(llm_input_names_ptr_[past_kv_input_start_index_ + 2 * i + 1]);
     }
 
     std::vector<Ort::Value> outputs;
 
     if (use_cuda_iobinding_) {
-      // Bind logits to CPU for sampling; keep KV cache on GPU for next steps.
-      Ort::IoBinding binding(*decode_sess_);
+      Ort::IoBinding binding(*llm_sess_);
       for (size_t i = 0; i < inputs.size(); ++i) {
         binding.BindInput(input_names_ptr[i], inputs[i]);
       }
 
-      binding.BindOutput(decode_output_names_ptr_[0], cpu_mem_info_);
-      for (size_t i = 1; i < decode_output_names_ptr_.size(); ++i) {
-        binding.BindOutput(decode_output_names_ptr_[i], *cuda_mem_info_);
+      // logits must be CPU (we will read it on CPU).
+      binding.BindOutput(llm_output_names_ptr_[0], cpu_mem_info_);
+
+      // KV outputs: bind to CPU so ApplyKvDeltaInplace can work with CPU cache
+      for (size_t i = 1; i < llm_output_names_ptr_.size(); ++i) {
+        binding.BindOutput(llm_output_names_ptr_[i], cpu_mem_info_);
       }
 
       binding.SynchronizeInputs();
-      decode_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      llm_sess_->Run(Ort::RunOptions{nullptr}, binding);
       binding.SynchronizeOutputs();
       outputs = binding.GetOutputValues();
     } else {
-      outputs = decode_sess_->Run(
-          {}, input_names_ptr.data(), inputs.data(), inputs.size(),
-          decode_output_names_ptr_.data(), decode_output_names_ptr_.size());
+      outputs = llm_sess_->Run({}, input_names_ptr.data(), inputs.data(),
+                               inputs.size(), llm_output_names_ptr_.data(),
+                               llm_output_names_ptr_.size());
     }
 
-    // First output is logits, remaining outputs are updated past_key_values
     if (outputs.empty()) {
-      SHERPA_ONNX_LOGE("ForwardLLMDecode: empty outputs");
+      SHERPA_ONNX_LOGE("ForwardLLM: empty outputs");
       SHERPA_ONNX_EXIT(-1);
     }
 
     Ort::Value logits = std::move(outputs[0]);
-
     if (!logits.IsTensor()) {
-      SHERPA_ONNX_LOGE("ForwardLLMDecode: logits is not a tensor");
+      SHERPA_ONNX_LOGE("ForwardLLM: logits is not a tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    AssertTensorIsCpu(logits, "ForwardLLM logits");
+
+    auto logits_info = logits.GetTensorTypeAndShapeInfo();
+    auto logits_type =
+        static_cast<ONNXTensorElementDataType>(logits_info.GetElementType());
+    if (logits_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+      SHERPA_ONNX_LOGE("ForwardLLM: logits must be float32, got elem_type=%d",
+                       (int)logits_type);
       SHERPA_ONNX_EXIT(-1);
     }
-    AssertTensorIsCpu(logits, "ForwardLLMDecode logits");
 
     if ((outputs.size() - 1) % 2 != 0) {
-      SHERPA_ONNX_LOGE("ForwardLLMDecode: invalid KV cache outputs size=%d",
+      SHERPA_ONNX_LOGE("ForwardLLM: invalid KV cache outputs size=%d",
                        static_cast<int>(outputs.size()));
       SHERPA_ONNX_EXIT(-1);
     }
 
-    std::vector<std::pair<Ort::Value, Ort::Value>> updated_past_kv;
-    int num_layers = static_cast<int>((outputs.size() - 1) / 2);
-    updated_past_kv.reserve(num_layers);
-    for (int i = 0; i < num_layers; ++i) {
-      updated_past_kv.emplace_back(std::move(outputs[1 + 2 * i]),
-                                   std::move(outputs[1 + 2 * i + 1]));
+    int32_t inferred_layers = static_cast<int32_t>((outputs.size() - 1) / 2);
+    if (inferred_layers != num_layers_) {
+      SHERPA_ONNX_LOGE("ForwardLLM: KV outputs layers mismatch: expected=%d, got=%d",
+                       num_layers_, inferred_layers);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<std::pair<Ort::Value, Ort::Value>> kv_outputs;
+    kv_outputs.reserve(num_layers_);
+    for (int32_t i = 0; i < num_layers_; ++i) {
+      kv_outputs.emplace_back(std::move(outputs[1 + 2 * i]),
+                              std::move(outputs[1 + 2 * i + 1]));
+    }
+
+    return {std::move(logits), std::move(kv_outputs)};
+  }
+
+  // Apply KV delta in-place to the KV cache.
+  // Copy key_delta/value_delta into cache_key/value at positions [pos0:pos0+S)
+  void ApplyKvDeltaInplace(std::vector<std::pair<Ort::Value, Ort::Value>> *cache_kv,
+                          const std::vector<std::pair<Ort::Value, Ort::Value>> &kv_delta,
+                          const Ort::Value &cache_position) const {
+    if (!cache_kv || cache_kv->size() != static_cast<size_t>(num_layers_) ||
+        kv_delta.size() != static_cast<size_t>(num_layers_)) {
+      SHERPA_ONNX_LOGE("ApplyKvDeltaInplace: invalid kv sizes: cache=%zu delta=%zu",
+                       cache_kv ? cache_kv->size() : 0, kv_delta.size());
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    // cache_position: [S], first element is pos0 (contiguous write)
+    auto pos_info = cache_position.GetTensorTypeAndShapeInfo();
+    auto pos_shape = pos_info.GetShape();
+    int64_t S = pos_shape.empty() ? 0 : pos_shape[0];
+    if (S <= 0) {
+      SHERPA_ONNX_LOGE("ApplyKvDeltaInplace: cache_position has invalid shape");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    const int64_t *pos_data = cache_position.GetTensorData<int64_t>();
+    int64_t pos0 = pos_data[0];
+
+    if (pos0 < 0) {
+      SHERPA_ONNX_LOGE("ApplyKvDeltaInplace: pos0 < 0 (%lld)", static_cast<long long>(pos0));
+      SHERPA_ONNX_EXIT(-1);
+    }
+    if (pos0 + S > max_total_len_) {
+      SHERPA_ONNX_LOGE("ApplyKvDeltaInplace: pos0+S exceeds max_total_len_ (%lld + %lld > %d), clamping S",
+                       static_cast<long long>(pos0), static_cast<long long>(S), max_total_len_);
+      S = max_total_len_ - pos0;
+      if (S <= 0) return;
+    }
+
+    for (int32_t layer = 0; layer < num_layers_; ++layer) {
+      Ort::Value &cache_key = (*cache_kv)[layer].first;
+      Ort::Value &cache_val = (*cache_kv)[layer].second;
+
+      const Ort::Value &delta_key = kv_delta[layer].first;
+      const Ort::Value &delta_val = kv_delta[layer].second;
+
+      auto ck_info = cache_key.GetTensorTypeAndShapeInfo();
+      auto dk_info = delta_key.GetTensorTypeAndShapeInfo();
+
+      auto ck_shape = ck_info.GetShape();  // [B, max_total_len, kv_h, hd]
+      auto dk_shape = dk_info.GetShape();  // [B, S, kv_h, hd]
+
+      int64_t B = ck_shape[0];
+      int64_t kv_h = ck_shape[2];
+      int64_t hd = ck_shape[3];
+
+      // bytes per element
+      auto elem_type = ck_info.GetElementType();
+      size_t elem_bytes = 0;
+      switch (elem_type) {
+        case ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT:
+          elem_bytes = 4;
+          break;
+        case ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16:
+        case ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16:
+          elem_bytes = 2;
+          break;
+        default:
+          SHERPA_ONNX_LOGE("ApplyKvDeltaInplace: unsupported elem_type=%d",
+                           elem_type);
+          SHERPA_ONNX_EXIT(-1);
+      }
+
+      size_t bytes_per_pos =
+          static_cast<size_t>(kv_h) * static_cast<size_t>(hd) * elem_bytes;
+
+      void *dst_k = cache_key.GetTensorMutableData<void>();
+      void *dst_v = cache_val.GetTensorMutableData<void>();
+      const void *src_k = delta_key.GetTensorData<void>();
+      const void *src_v = delta_val.GetTensorData<void>();
+
+      for (int64_t b = 0; b < B; ++b) {
+        size_t dst_off =
+            (static_cast<size_t>(b) * static_cast<size_t>(max_total_len_) +
+             static_cast<size_t>(pos0)) *
+            bytes_per_pos;
+        size_t src_off =
+            (static_cast<size_t>(b) * static_cast<size_t>(dk_shape[1])) *
+            bytes_per_pos;
+
+        size_t copy_bytes = static_cast<size_t>(S) * bytes_per_pos;
+
+        uint8_t *dst_k_ptr = static_cast<uint8_t *>(dst_k) + dst_off;
+        uint8_t *dst_v_ptr = static_cast<uint8_t *>(dst_v) + dst_off;
+        const uint8_t *src_k_ptr = static_cast<const uint8_t *>(src_k) + src_off;
+        const uint8_t *src_v_ptr = static_cast<const uint8_t *>(src_v) + src_off;
+
+        std::memcpy(dst_k_ptr, src_k_ptr, copy_bytes);
+        std::memcpy(dst_v_ptr, src_v_ptr, copy_bytes);
+      }
     }
-    return {std::move(logits), std::move(updated_past_kv)};
   }
 
   // Forward pass through embedding model.
@@ -734,20 +956,30 @@ class OfflineFunASRNanoModel::Impl {
 
   int32_t VocabSize() const { return vocab_size_; }
   int32_t HiddenSize() const { return hidden_size_; }
+  int32_t GetMaxTotalLen() const { return max_total_len_; }
   int32_t LfrWindowSize() const { return lfr_window_size_; }
   int32_t LfrWindowShift() const { return lfr_window_shift_; }
   OrtAllocator *Allocator() { return allocator_; }
   bool HasEmbeddingModel() const { return has_embedding_model_; }
   bool UseKVCache() const { return true; }
-  ONNXTensorElementDataType GetPrefillInputType() const {
-    return prefill_embeds_in_type_;
-  }
-  ONNXTensorElementDataType GetDecodeInputType() const {
-    return decode_embeds_in_type_;
-  }
   bool IsCpuProvider() const { return is_cpu_provider_; }
 
  private:
+  void CheckFp16OnCuda() {
+    if (use_cuda_iobinding_) {
+      Ort::ModelMetadata meta_data = llm_sess_->GetModelMetadata();
+      Ort::AllocatorWithDefaultOptions allocator;
+      auto quant_type = LookupCustomModelMetaData(meta_data, "quantization_type", allocator);
+
+      if (!quant_type.empty() && quant_type == "fp16") {
+        SHERPA_ONNX_LOGE(
+            "fp16 LLM models are not supported on CUDA yet. Please use "
+            "fp32/int8 models.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
   void InitEncoderAdaptor(const std::string &model_path) {
     encoder_sess_ = std::make_unique<Ort::Session>(
         env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_encoder_);
@@ -772,63 +1004,38 @@ class OfflineFunASRNanoModel::Impl {
     SHERPA_ONNX_READ_META_DATA(hidden_size_, "llm_dim");
   }
 
-  void InitLLMPrefill(const std::string &model_path) {
-    prefill_sess_ = std::make_unique<Ort::Session>(
-        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_llm_);
-    GetInputNames(prefill_sess_.get(), &prefill_input_names_,
-                  &prefill_input_names_ptr_);
-    GetOutputNames(prefill_sess_.get(), &prefill_output_names_,
-                   &prefill_output_names_ptr_);
-    prefill_embeds_in_type_ = GetSessionInputElemType(prefill_sess_.get(), 0);
-    Ort::ModelMetadata meta_data = prefill_sess_->GetModelMetadata();
-    if (config_.debug) {
-      std::ostringstream os;
-      PrintModelMetadata(os, meta_data);
-#if __OHOS__
-      SHERPA_ONNX_LOGE("Prefill model metadata:\n%{public}s\n", os.str().c_str());
-#else
-      SHERPA_ONNX_LOGE("Prefill model metadata:\n%s\n", os.str().c_str());
-#endif
-    }
-    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
-    SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
-    if (hidden_size_ == 0) {
-      SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
+  void InitLLM(const std::string &model_path) {
+    // For fp32 models: check for .data file by replacing .onnx with .data
+    // int8 and fp16 models don't have .data files, so no need to check
+    std::string data_path = model_path;
+    if (data_path.size() >= 5 && data_path.substr(data_path.size() - 5) == ".onnx") {
+      data_path = data_path.substr(0, data_path.size() - 5) + ".data";
+    } else {
+      data_path = model_path + ".data";
     }
-  }
+    bool has_external_data = FileExists(data_path);
 
-  void InitLLMDecode(const std::string &model_path) {
-    decode_sess_ = std::make_unique<Ort::Session>(
-        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_llm_);
-    GetInputNames(decode_sess_.get(), &decode_input_names_,
-                  &decode_input_names_ptr_);
-    GetOutputNames(decode_sess_.get(), &decode_output_names_,
-                   &decode_output_names_ptr_);
-    decode_embeds_in_type_ = GetSessionInputElemType(decode_sess_.get(), 0);
-    Ort::ModelMetadata meta_data = decode_sess_->GetModelMetadata();
-    if (config_.debug) {
-      std::ostringstream os;
-      PrintModelMetadata(os, meta_data);
-#if __OHOS__
-      SHERPA_ONNX_LOGE("Decode model metadata:\n%{public}s\n", os.str().c_str());
-#else
-      SHERPA_ONNX_LOGE("Decode model metadata:\n%s\n", os.str().c_str());
-#endif
+    // Resolve absolute path for model file
+    std::string abs_model_path = model_path;
+    if (!model_path.empty() && !std::filesystem::path(model_path).is_absolute()) {
+      abs_model_path = std::filesystem::absolute(model_path).string();
     }
-    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
-    int32_t decode_vocab_size = 0;
-    SHERPA_ONNX_READ_META_DATA(decode_vocab_size, "vocab_size");
-    if (vocab_size_ > 0 && decode_vocab_size != vocab_size_) {
-      SHERPA_ONNX_LOGE(
-          "Decode model vocab_size (%d) != prefill vocab_size (%d)",
-          decode_vocab_size, vocab_size_);
-    }
-    if (vocab_size_ == 0) vocab_size_ = decode_vocab_size;
-    int32_t decode_hidden_size = 0;
-    if (hidden_size_ == 0) {
-      SHERPA_ONNX_READ_META_DATA(decode_hidden_size, "hidden_size");
-      hidden_size_ = decode_hidden_size;
+
+    if (has_external_data) {
+      // When external data exists, use absolute file path to create session.
+      // ONNX Runtime will automatically find .data file in the same directory
+      // as the model file when using absolute path.
+      llm_sess_ =
+          std::make_unique<Ort::Session>(env_, SHERPA_ONNX_TO_ORT_PATH(abs_model_path), sess_opts_llm_);
+    } else {
+      // No external data: load entire model into memory
+      std::vector<char> model_data = ReadFile(model_path);
+      llm_sess_ = std::make_unique<Ort::Session>(env_, model_data.data(),
+                                                 model_data.size(),
+                                                 sess_opts_llm_);
     }
+
+    SetupLlmFromSession();
   }
 
   void InitEmbedding(const std::string &model_path) {
@@ -867,8 +1074,7 @@ class OfflineFunASRNanoModel::Impl {
   bool use_cuda_iobinding_ = false;
 
   std::unique_ptr<Ort::Session> encoder_sess_;
-  std::unique_ptr<Ort::Session> prefill_sess_;
-  std::unique_ptr<Ort::Session> decode_sess_;
+  std::unique_ptr<Ort::Session> llm_sess_;
   std::unique_ptr<Ort::Session> embedding_sess_;
 
   std::vector<std::string> encoder_input_names_;
@@ -876,15 +1082,10 @@ class OfflineFunASRNanoModel::Impl {
   std::vector<std::string> encoder_output_names_;
   std::vector<const char *> encoder_output_names_ptr_;
 
-  std::vector<std::string> prefill_input_names_;
-  std::vector<const char *> prefill_input_names_ptr_;
-  std::vector<std::string> prefill_output_names_;
-  std::vector<const char *> prefill_output_names_ptr_;
-
-  std::vector<std::string> decode_input_names_;
-  std::vector<const char *> decode_input_names_ptr_;
-  std::vector<std::string> decode_output_names_;
-  std::vector<const char *> decode_output_names_ptr_;
+  std::vector<std::string> llm_input_names_;
+  std::vector<const char *> llm_input_names_ptr_;
+  std::vector<std::string> llm_output_names_;
+  std::vector<const char *> llm_output_names_ptr_;
 
   std::vector<std::string> embedding_input_names_;
   std::vector<const char *> embedding_input_names_ptr_;
@@ -896,16 +1097,28 @@ class OfflineFunASRNanoModel::Impl {
   int32_t lfr_window_size_ = 0;
   int32_t lfr_window_shift_ = 0;
 
+  int32_t num_layers_ = 0;
+  int32_t max_total_len_ = 0;  // attention_mask length / cache capacity
   bool has_embedding_model_ = false;
 
   ONNXTensorElementDataType encoder_in_type_ =
       ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
-  ONNXTensorElementDataType prefill_embeds_in_type_ =
-      ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
-  ONNXTensorElementDataType decode_embeds_in_type_ =
+  ONNXTensorElementDataType llm_embeds_in_type_ =
       ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
 
+  // KV input element types (for CreateEmptyKVCache).
+  ONNXTensorElementDataType kv_in_type_ = ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;
+  ONNXTensorElementDataType kv_in_type_v_ = ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT;
+
+  // Input indices for KV cache LLM.
+  size_t cache_position_input_index_ = 2;
+  size_t past_kv_input_start_index_ = 3;
+
+  std::vector<int64_t> past_key_shape_tpl_;
+  std::vector<int64_t> past_value_shape_tpl_;
+
   bool is_cpu_provider_ = false;
+  bool is_kv_delta_model_ = false;
 };
 
 OfflineFunASRNanoModel::OfflineFunASRNanoModel(const OfflineModelConfig &config)
@@ -923,34 +1136,36 @@ Ort::Value OfflineFunASRNanoModel::ForwardEncoderAdaptor(Ort::Value features) {
 }
 
 std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-OfflineFunASRNanoModel::ForwardLLMPrefill(Ort::Value inputs_embeds,
-                                          Ort::Value attention_mask) {
-  return impl_->ForwardLLMPrefill(std::move(inputs_embeds),
-                                  std::move(attention_mask));
+OfflineFunASRNanoModel::ForwardLLM(
+    Ort::Value inputs_embeds, Ort::Value attention_mask,
+    const Ort::Value &cache_position,
+    const std::vector<std::pair<Ort::Value, Ort::Value>> &cache_kv) {
+  return impl_->ForwardLLM(std::move(inputs_embeds), std::move(attention_mask),
+                           std::move(cache_position), cache_kv);
 }
 
-std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-OfflineFunASRNanoModel::ForwardLLMDecode(
-    Ort::Value inputs_embeds, Ort::Value attention_mask,
-    const std::vector<std::pair<Ort::Value, Ort::Value>> &past_key_values) {
-  return impl_->ForwardLLMDecode(std::move(inputs_embeds),
-                                 std::move(attention_mask), past_key_values);
+std::vector<std::pair<Ort::Value, Ort::Value>>
+OfflineFunASRNanoModel::CreateEmptyKVCache(int64_t batch) {
+  return impl_->CreateEmptyKVCache(batch);
 }
 
-bool OfflineFunASRNanoModel::UseKVCache() const {
-  return impl_->UseKVCache();
+void OfflineFunASRNanoModel::ApplyKvDeltaInplace(
+    std::vector<std::pair<Ort::Value, Ort::Value>> *cache_kv,
+    const std::vector<std::pair<Ort::Value, Ort::Value>> &kv_delta,
+    const Ort::Value &cache_position) {
+  return impl_->ApplyKvDeltaInplace(cache_kv, kv_delta, cache_position);
 }
 
+bool OfflineFunASRNanoModel::UseKVCache() const { return impl_->UseKVCache(); }
+
 Ort::Value OfflineFunASRNanoModel::ForwardEmbedding(Ort::Value input_ids) {
   return impl_->ForwardEmbedding(std::move(input_ids));
 }
 
-int32_t OfflineFunASRNanoModel::VocabSize() const {
-  return impl_->VocabSize();
-}
-int32_t OfflineFunASRNanoModel::HiddenSize() const {
-  return impl_->HiddenSize();
-}
+int32_t OfflineFunASRNanoModel::VocabSize() const { return impl_->VocabSize(); }
+int32_t OfflineFunASRNanoModel::HiddenSize() const { return impl_->HiddenSize(); }
+int32_t OfflineFunASRNanoModel::GetMaxTotalLen() const { return impl_->GetMaxTotalLen(); }
+
 int32_t OfflineFunASRNanoModel::LfrWindowSize() const {
   return impl_->LfrWindowSize();
 }
@@ -966,14 +1181,6 @@ bool OfflineFunASRNanoModel::HasEmbeddingModel() const {
   return impl_->HasEmbeddingModel();
 }
 
-ONNXTensorElementDataType OfflineFunASRNanoModel::GetPrefillInputType() const {
-  return impl_->GetPrefillInputType();
-}
-
-ONNXTensorElementDataType OfflineFunASRNanoModel::GetDecodeInputType() const {
-  return impl_->GetDecodeInputType();
-}
-
 #if __ANDROID_API__ >= 9
 template OfflineFunASRNanoModel::OfflineFunASRNanoModel(
     AAssetManager *mgr, const OfflineModelConfig &config);
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.h b/sherpa-onnx/csrc/offline-funasr-nano-model.h
index 084d1e6b..835e4f05 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model.h
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.h
@@ -5,6 +5,7 @@
 #ifndef SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_H_
 #define SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_H_
 
+#include <cstdint>
 #include <memory>
 #include <vector>
 
@@ -30,26 +31,38 @@ class OfflineFunASRNanoModel {
    */
   Ort::Value ForwardEncoderAdaptor(Ort::Value features);
 
-  /** Run the LLM prefill model (KV cache mode).
+  /** Run the LLM model (KV cache mode).
    *
-   * @param inputs_embeds  A tensor of shape (N, T, hidden_size).
-   * @param attention_mask  A tensor of shape (N, T) containing attention mask.
-   * @return Return tuple (logits, past_key_values...). Logits shape (N, T, vocab_size).
-   *         past_key_values is a vector of (key, value) pairs for each layer.
+   * @param inputs_embeds  A tensor of shape (N, T, hidden_size), float32.
+   * @param attention_mask  A tensor of shape (N, T) containing attention mask, int64.
+   * @param cache_position  A tensor of shape (T,) containing cache positions, int64.
+   * @param cache_kv  Fixed-size KV cache, vector of (key, value) pairs.
+   * @return Return tuple (logits, kv_outputs...). Logits shape (N, T, vocab_size), float32.
+   *         kv_outputs is a vector of (key_delta, value_delta) pairs for each layer.
    */
-  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-  ForwardLLMPrefill(Ort::Value inputs_embeds, Ort::Value attention_mask);
+   std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+   ForwardLLM(Ort::Value inputs_embeds,
+              Ort::Value attention_mask,
+              const Ort::Value &cache_position,
+              const std::vector<std::pair<Ort::Value, Ort::Value>> &cache_kv);
+  
+  /** Create fixed-size KV cache buffer.
+   *
+   * @param batch  Batch size (usually 1).
+   * @return Return vector of (key, value) pairs with fixed cache dimensions [B, max_total_len, kv_h, hd].
+   */
+  std::vector<std::pair<Ort::Value, Ort::Value>>
+  CreateEmptyKVCache(int64_t batch);
 
-  /** Run the LLM decode model (KV cache mode).
+  /** Apply KV delta in-place to KV cache buffer.
    *
-   * @param inputs_embeds  A tensor of shape (N, 1, hidden_size) for the next token.
-   * @param attention_mask  A tensor of shape (N, total_seq_len) containing attention mask.
-   * @param past_key_values  KV cache from previous steps, vector of (key, value) pairs.
-   * @return Return tuple (logits, updated_past_key_values...). Logits shape (N, 1, vocab_size).
+   * @param cache_kv  Fixed-size KV cache to update, vector of (key, value) pairs.
+   * @param kv_delta  KV deltas from current step, vector of (key_delta, value_delta) pairs.
+   * @param cache_position  Cache position tensor indicating where to write deltas.
    */
-  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
-  ForwardLLMDecode(Ort::Value inputs_embeds, Ort::Value attention_mask,
-                   const std::vector<std::pair<Ort::Value, Ort::Value>> &past_key_values);
+  void ApplyKvDeltaInplace(std::vector<std::pair<Ort::Value, Ort::Value>> *cache_kv,
+                          const std::vector<std::pair<Ort::Value, Ort::Value>> &kv_delta,
+                          const Ort::Value &cache_position);
 
   /** Check if using KV cache mode. Always returns true for FunASR-nano.
    */
@@ -70,6 +83,10 @@ class OfflineFunASRNanoModel {
    */
   int32_t HiddenSize() const;
 
+  /** Return the maximum total sequence length (from metadata)
+   */
+  int32_t GetMaxTotalLen() const;
+
   /** It is lfr_window_size in metadata
    */
   int32_t LfrWindowSize() const;
@@ -86,14 +103,6 @@ class OfflineFunASRNanoModel {
    */
   bool HasEmbeddingModel() const;
 
-  /** Get expected input type for prefill model
-   */
-  ONNXTensorElementDataType GetPrefillInputType() const;
-
-  /** Get expected input type for decode model
-   */
-  ONNXTensorElementDataType GetDecodeInputType() const;
-
  private:
   class Impl;
   std::unique_ptr<Impl> impl_;
diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
index b80f60e2..29f2d360 100644
--- a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
@@ -8,6 +8,7 @@
 #include <cmath>
 #include <cstdint>
 #include <cstring>
+#include <limits>
 #include <memory>
 #include <random>
 #include <string>
@@ -30,6 +31,43 @@
 
 namespace sherpa_onnx {
 
+namespace {
+// Build cache_position tensor from attention_mask.
+// Creates a [S] int64_t tensor where the first element is the starting position (pos0)
+// for writing KV deltas. The remaining elements are consecutive positions [pos0, pos0+1, ..., pos0+S-1].
+// For prefill: pos0 = 0, S = context_len
+// For decode: pos0 = valid_len - 1, S = 1
+static Ort::Value BuildCachePositionFromMask(const Ort::Value &attention_mask,
+                                             int32_t seq_len,
+                                             OrtAllocator *allocator) {
+  auto mask_info = attention_mask.GetTensorTypeAndShapeInfo();
+  auto mask_shape = mask_info.GetShape();
+  
+  // Get the current position from attention_mask length
+  // mask_shape is [1, mask_len], where mask_len = past_len + seq_len
+  int64_t pos0 = 0;
+  if (mask_shape.size() == 2 && mask_shape[1] > 0) {
+    // pos0 is the current position in cache (past length = mask_len - seq_len)
+    pos0 = static_cast<int64_t>(mask_shape[1]) - seq_len;
+  }
+  if (pos0 < 0) pos0 = 0;
+  
+  // Create tensor using allocator
+  std::array<int64_t, 1> pos_shape{seq_len};
+  Ort::Value cache_position = Ort::Value::CreateTensor<int64_t>(
+      allocator, pos_shape.data(), pos_shape.size());
+  
+  // Fill the tensor with position values
+  int64_t *p = cache_position.GetTensorMutableData<int64_t>();
+  for (int32_t i = 0; i < seq_len; ++i) {
+    p[i] = pos0 + i;
+  }
+  
+  return cache_position;
+}
+
+}  // namespace
+
 OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
     const OfflineRecognizerConfig &config)
     : OfflineRecognizerImpl(config),
@@ -153,9 +191,134 @@ int64_t OfflineRecognizerFunASRNanoImpl::SampleTokenFromLogitsFp16OrFp32(
   return static_cast<int64_t>(best);
 }
 
-// Generate text from encoder output using autoregressive decoding with KV cache.
-// Combines text embeddings (from prompts) and audio embeddings (from encoder)
-// to form the input sequence, then generates tokens autoregressively.
+// Sample token from logits using temperature and top-p (nucleus) sampling.
+// Handles both FP16 and FP32 logits.
+// Returns token ID 0 as fallback if all logits are invalid.
+// If temperature is very small (<= 1e-6) or invalid, falls back to greedy decoding.
+// If top_p >= 1.0, samples from all tokens without sorting (full vocabulary).
+int64_t OfflineRecognizerFunASRNanoImpl::SampleTokenWithTemperatureAndTopP(
+    const void *logits, bool is_fp16, int32_t vocab_size, float temperature,
+    float top_p) const {
+  if (temperature <= 1e-6f || !std::isfinite(temperature)) {
+    return SampleTokenFromLogitsFp16OrFp32(logits, is_fp16, vocab_size);
+  }
+
+  // top_p <= 0: treat as greedy (minimal candidate set)
+  if (!std::isfinite(top_p) || top_p <= 0.0f) {
+    return SampleTokenFromLogitsFp16OrFp32(logits, is_fp16, vocab_size);
+  }
+  if (top_p > 1.0f) top_p = 1.0f;
+
+  // Reuse buffers to avoid per-token allocations
+  thread_local std::vector<float> probs;
+  thread_local std::vector<int32_t> idx;
+
+  probs.resize(vocab_size);
+  idx.resize(vocab_size);
+
+  // Scale logits & find max for numerical stability
+  float max_logit = -std::numeric_limits<float>::infinity();
+  bool found_valid = false;
+
+  if (is_fp16) {
+    const uint16_t *p = reinterpret_cast<const uint16_t *>(logits);
+    for (int32_t i = 0; i < vocab_size; ++i) {
+      float v = HalfBitsToFloat(p[i]);
+      if (std::isfinite(v)) {
+        v /= temperature;
+        probs[i] = v;
+        if (v > max_logit) max_logit = v;
+        found_valid = true;
+      } else {
+        probs[i] = -1e30f;
+      }
+      idx[i] = i;
+    }
+  } else {
+    const float *p = reinterpret_cast<const float *>(logits);
+    for (int32_t i = 0; i < vocab_size; ++i) {
+      float v = p[i];
+      if (std::isfinite(v)) {
+        v /= temperature;
+        probs[i] = v;
+        if (v > max_logit) max_logit = v;
+        found_valid = true;
+      } else {
+        probs[i] = -1e30f;
+      }
+      idx[i] = i;
+    }
+  }
+
+  if (!found_valid) return 0;
+
+  // Compute softmax probabilities
+  float sum_exp = 0.0f;
+  for (int32_t i = 0; i < vocab_size; ++i) {
+    float e = std::exp(probs[i] - max_logit);
+    probs[i] = e;
+    sum_exp += e;
+  }
+  if (sum_exp <= 0.0f || !std::isfinite(sum_exp)) return 0;
+  for (int32_t i = 0; i < vocab_size; ++i) {
+    probs[i] /= sum_exp;
+  }
+
+  // top_p >= 1: sample from full distribution (no sorting)
+  if (top_p >= 1.0f) {
+    std::uniform_real_distribution<float> dist(0.0f, 1.0f);
+    float sample = dist(rng_);
+    float cumsum = 0.0f;
+    for (int32_t i = 0; i < vocab_size; ++i) {
+      cumsum += probs[i];
+      if (sample <= cumsum) return static_cast<int64_t>(i);
+    }
+    return static_cast<int64_t>(vocab_size - 1);
+  }
+
+  // Fast path: partial_sort top-K, increase K only if needed
+  int32_t k = std::min<int32_t>(256, vocab_size);
+  float cum_k = 0.0f;
+
+  while (true) {
+    std::partial_sort(idx.begin(), idx.begin() + k, idx.end(),
+                      [&](int32_t a, int32_t b) { return probs[a] > probs[b]; });
+
+    cum_k = 0.0f;
+    for (int32_t i = 0; i < k; ++i) cum_k += probs[idx[i]];
+
+    if (cum_k >= top_p || k == vocab_size) break;
+
+    int32_t new_k = std::min(vocab_size, k * 2);
+    if (new_k == k) break;
+    k = new_k;
+  }
+
+  // Find cutoff inside sorted top-K
+  float cumsum = 0.0f;
+  int32_t cutoff = k;
+  for (int32_t i = 0; i < k; ++i) {
+    cumsum += probs[idx[i]];
+    if (cumsum >= top_p) {
+      cutoff = i + 1;
+      break;
+    }
+  }
+
+  float renorm_sum = 0.0f;
+  for (int32_t i = 0; i < cutoff; ++i) renorm_sum += probs[idx[i]];
+  if (renorm_sum <= 0.0f) return 0;
+
+  std::uniform_real_distribution<float> dist(0.0f, renorm_sum);
+  float sample = dist(rng_);
+  float cumsum_sample = 0.0f;
+  for (int32_t i = 0; i < cutoff; ++i) {
+    cumsum_sample += probs[idx[i]];
+    if (sample <= cumsum_sample) return static_cast<int64_t>(idx[i]);
+  }
+  return static_cast<int64_t>(idx[cutoff - 1]);
+}
+
 OfflineRecognitionResult OfflineRecognizerFunASRNanoImpl::GenerateText(
     Ort::Value encoder_out, const std::string &system_prompt,
     const std::string &user_prompt) const {
@@ -168,228 +331,334 @@ OfflineRecognitionResult OfflineRecognizerFunASRNanoImpl::GenerateText(
   int32_t hidden_size = static_cast<int32_t>(enc_shape[2]);
   int32_t fbank_beg_idx = 0;
   int32_t fake_token_len = 0;
-  std::vector<int64_t> source_ids = BuildSourceIds(
-      system_prompt, user_prompt, audio_token_len, fbank_beg_idx,
-      fake_token_len);
+  std::vector<int64_t> source_ids =
+      BuildSourceIds(system_prompt, user_prompt, audio_token_len, fbank_beg_idx, fake_token_len);
   int32_t context_len = static_cast<int32_t>(source_ids.size());
-  const int32_t max_seq_len = 2048;
+
+  // Create KV cache buffer [B, max_total_len, kv_h, hd].
+  // This stores the accumulated KV cache. Model outputs are deltas that get applied in-place.
+  std::vector<std::pair<Ort::Value, Ort::Value>> cache_kv =
+      model_->CreateEmptyKVCache(1);
+    int32_t max_seq_len = model_->GetMaxTotalLen();
+    if (max_seq_len <= 0) {
+      SHERPA_ONNX_LOGE("Invalid max_seq_len=%d", max_seq_len);
+      result.text = "";
+      return result;
+    }
+
+  // If context exceeds KV capacity: prioritize truncating audio placeholders
+  // (keep prompt scaffold intact).
   if (context_len > max_seq_len) {
-    SHERPA_ONNX_LOGE(
-        "Input sequence length (%d) exceeds maximum sequence length (%d). "
-        "Truncating to %d tokens. Recognition result may be incomplete due to "
-        "truncated input.",
-        context_len, max_seq_len, max_seq_len);
-    source_ids.resize(max_seq_len);
-    context_len = max_seq_len;
+    int32_t before_len = fbank_beg_idx;
+    int32_t after_len = context_len - before_len - fake_token_len;
+    if (after_len < 0) after_len = 0;
+
+    int32_t keep_audio = max_seq_len - before_len - after_len;
+    if (keep_audio < 0) {
+      SHERPA_ONNX_LOGE(
+          "Context_len (%d) too large for KV capacity (%d) and prompts already exceed capacity. "
+          "Falling back to keep last %d tokens.",
+          context_len, max_seq_len, max_seq_len);
+      // Fallback: keep the suffix.
+      source_ids.erase(source_ids.begin(), source_ids.end() - max_seq_len);
+      // Audio alignment is no longer controllable, skip injecting audio embeddings.
+      fbank_beg_idx = -1;
+      fake_token_len = 0;
+      context_len = static_cast<int32_t>(source_ids.size());
+    } else {
+      if (keep_audio > audio_token_len) keep_audio = audio_token_len;
+
+      SHERPA_ONNX_LOGE(
+          "Context_len (%d) exceeds KV capacity (%d). Truncating audio placeholders: "
+          "audio_token_len=%d -> keep_audio=%d (before=%d after=%d).",
+          context_len, max_seq_len, audio_token_len, keep_audio, before_len, after_len);
+
+      // Rebuild ids_before/ids_after using slices.
+      std::vector<int64_t> ids_before(source_ids.begin(),
+                                      source_ids.begin() + before_len);
+      std::vector<int64_t> ids_after(source_ids.end() - after_len,
+                                     source_ids.end());
+
+      int64_t pad_id = tokenizer_->GetPadTokenId();
+      if (pad_id < 0) pad_id = tokenizer_->GetEosTokenId();
+
+      source_ids.clear();
+      source_ids.reserve(before_len + keep_audio + after_len);
+      source_ids.insert(source_ids.end(), ids_before.begin(), ids_before.end());
+      source_ids.insert(source_ids.end(), keep_audio, pad_id);
+      source_ids.insert(source_ids.end(), ids_after.begin(), ids_after.end());
+
+      fake_token_len = keep_audio;
+      fbank_beg_idx = before_len;
+      context_len = static_cast<int32_t>(source_ids.size());
+    }
   }
 
   // Get text embeddings for the prompt tokens
   std::vector<int64_t> input_ids = source_ids;
   std::array<int64_t, 2> ids_shape{1, context_len};
   Ort::Value input_ids_tensor = Ort::Value::CreateTensor(
-      memory_info, input_ids.data(), input_ids.size(), ids_shape.data(),
-      ids_shape.size());
-  Ort::Value text_embeds =
-      model_->ForwardEmbedding(std::move(input_ids_tensor));
+      memory_info, input_ids.data(), input_ids.size(),
+      ids_shape.data(), ids_shape.size());
+
+  Ort::Value text_embeds = model_->ForwardEmbedding(std::move(input_ids_tensor));
+
   auto te_info = text_embeds.GetTensorTypeAndShapeInfo();
   auto te_shape = te_info.GetShape();
-  if (static_cast<int32_t>(te_shape[2]) != hidden_size) {
-    SHERPA_ONNX_LOGE("Embedding hidden mismatch: %d vs %d",
-                     static_cast<int32_t>(te_shape[2]), hidden_size);
-    result.text = "";
-    return result;
-  }
+
   const auto te_type = te_info.GetElementType();
   const bool te_fp16 = (te_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
-  // Pre-allocate full inputs_embeds buffer to max_seq_len
-  std::vector<uint16_t> inputs_embeds_fp16(
-      static_cast<size_t>(max_seq_len) * hidden_size, FloatToHalfBits(0.0f));
-  std::vector<int64_t> attention_mask(static_cast<size_t>(max_seq_len), 0);
-  // Copy text embeddings into inputs_embeds buffer
+
+  // Allocate inputs_embeds only for prefill (context_len * hidden_size).
+  // Decode steps will use a separate reusable buffer.
+  std::vector<float> inputs_embeds_fp32(
+      static_cast<size_t>(context_len) * hidden_size, 0.0f);
+
+  // Copy text embeddings.
   if (te_fp16) {
     const uint16_t *p = text_embeds.GetTensorData<uint16_t>();
-    std::copy(p, p + static_cast<size_t>(context_len) * hidden_size,
-              inputs_embeds_fp16.data());
+    const size_t total = static_cast<size_t>(context_len) * hidden_size;
+    for (size_t i = 0; i < total; ++i) {
+      inputs_embeds_fp32[i] = HalfBitsToFloat(p[i]);
+    }
   } else {
     const float *p = text_embeds.GetTensorData<float>();
-    for (int64_t i = 0; i < static_cast<int64_t>(context_len) * hidden_size;
-         ++i) {
-      inputs_embeds_fp16[static_cast<size_t>(i)] = FloatToHalfBits(p[i]);
-    }
+    const size_t total = static_cast<size_t>(context_len) * hidden_size;
+    std::memcpy(inputs_embeds_fp32.data(), p, total * sizeof(float));
   }
-  // Inject audio embeddings into inputs_embeds at the position of audio tokens
+
+  // Inject audio embeddings into placeholder region (if alignment is still possible).
   auto enc_info2 = encoder_out.GetTensorTypeAndShapeInfo();
-  auto enc_et =
-      static_cast<ONNXTensorElementDataType>(enc_info2.GetElementType());
+  auto enc_et = static_cast<ONNXTensorElementDataType>(enc_info2.GetElementType());
   int32_t copy_len = std::min(fake_token_len, audio_token_len);
-  if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
-    const uint16_t *enc = encoder_out.GetTensorData<uint16_t>();
-    for (int32_t t = 0; t < copy_len; ++t) {
-      const uint16_t *src = enc + static_cast<int64_t>(t) * hidden_size;
-      uint16_t *dst = inputs_embeds_fp16.data() +
-                      static_cast<int64_t>(fbank_beg_idx + t) * hidden_size;
-      std::copy(src, src + hidden_size, dst);
-    }
-  } else if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
-    const float *enc = encoder_out.GetTensorData<float>();
-    for (int32_t t = 0; t < copy_len; ++t) {
-      const float *src = enc + static_cast<int64_t>(t) * hidden_size;
-      uint16_t *dst = inputs_embeds_fp16.data() +
-                      static_cast<int64_t>(fbank_beg_idx + t) * hidden_size;
-      for (int32_t d = 0; d < hidden_size; ++d) {
-        dst[d] = FloatToHalfBits(src[d]);
+
+  if (copy_len > 0 && fbank_beg_idx >= 0) {
+    if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
+      const uint16_t *enc = encoder_out.GetTensorData<uint16_t>();
+      const size_t hidden_size_u = static_cast<size_t>(hidden_size);
+      for (int32_t t = 0; t < copy_len; ++t) {
+        const uint16_t *src = enc + static_cast<size_t>(t) * hidden_size_u;
+        float *dst = inputs_embeds_fp32.data() +
+                     static_cast<size_t>(fbank_beg_idx + t) * hidden_size_u;
+        for (size_t d = 0; d < hidden_size_u; ++d) {
+          dst[d] = HalfBitsToFloat(src[d]);
+        }
       }
+    } else if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+      const float *enc = encoder_out.GetTensorData<float>();
+      const size_t hidden_size_u = static_cast<size_t>(hidden_size);
+      for (int32_t t = 0; t < copy_len; ++t) {
+        const float *src = enc + static_cast<size_t>(t) * hidden_size_u;
+        float *dst = inputs_embeds_fp32.data() +
+                     static_cast<size_t>(fbank_beg_idx + t) * hidden_size_u;
+        std::memcpy(dst, src, hidden_size_u * sizeof(float));
+      }
+    } else {
+      SHERPA_ONNX_LOGE("encoder_out elem_type=%d not supported", (int)enc_et);
+      result.text = "";
+      return result;
     }
-  } else {
-    SHERPA_ONNX_LOGE("encoder_out elem_type=%d not supported", (int)enc_et);
-    result.text = "";
-    return result;
   }
-  // Set attention mask for context tokens
-  for (int32_t i = 0; i < context_len; ++i) attention_mask[i] = 1;
+
+  // Pre-allocate attention_mask buffer to avoid per-step allocations
+  // All values are 1, we'll use a slice of this buffer for each step
+  std::vector<int64_t> attention_mask(static_cast<size_t>(max_seq_len), 1);
+
+  // Pre-allocate reusable buffer for decode step embeddings (hidden_size)
+  std::vector<float> next_embed_fp32(static_cast<size_t>(hidden_size));
+
   int32_t valid_len = context_len;
+
   std::vector<int64_t> generated_ids;
-  
   generated_ids.reserve(funasr_config.max_new_tokens);
+
   const int64_t eos_id = tokenizer_->GetEosTokenId();
   const int64_t im_end_id = tokenizer_->GetImEndTokenId();
   const int32_t max_new_tokens = funasr_config.max_new_tokens;
-  std::vector<std::pair<Ort::Value, Ort::Value>> past_key_values;
+
   bool is_first_step = true;
-  // Autoregressive generation loop
+
   for (int32_t step = 0; step < max_new_tokens; ++step) {
+    // valid_len represents the mask_len for the next decode step (= past + current).
     if (valid_len >= max_seq_len) break;
-    Ort::Value logits(nullptr);
-    
+
+    Ort::Value logits{nullptr};
+
     if (is_first_step) {
-      // First step: use prefill model with full context
+      // Prefill: seq = context_len, mask_len = context_len.
+      if (config_.model_config.debug) {
+        SHERPA_ONNX_LOGE("GenerateText: starting prefill with context_len=%d, inputs_embeds_fp32.size()=%zu",
+           context_len, inputs_embeds_fp32.size());
+      }
+
       std::array<int64_t, 3> embeds_shape{1, context_len, hidden_size};
-      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor(
-          memory_info, inputs_embeds_fp16.data(),
-          static_cast<size_t>(context_len) * hidden_size * sizeof(uint16_t),
-          embeds_shape.data(), embeds_shape.size(),
-          ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor<float>(
+          memory_info, inputs_embeds_fp32.data(),
+          static_cast<size_t>(context_len) * hidden_size,
+          embeds_shape.data(), embeds_shape.size());
 
+      // Use pre-allocated attention_mask buffer (first context_len elements)
       std::array<int64_t, 2> mask_shape{1, context_len};
-      Ort::Value attention_mask_tensor =
-          Ort::Value::CreateTensor<int64_t>(
-              memory_info, attention_mask.data(),
-              static_cast<size_t>(context_len), mask_shape.data(),
-              mask_shape.size());
-      auto tmp = model_->ForwardLLMPrefill(std::move(inputs_embeds_tensor),
-                                           std::move(attention_mask_tensor));
+      Ort::Value attention_mask_tensor = Ort::Value::CreateTensor<int64_t>(
+          memory_info, attention_mask.data(),
+          static_cast<size_t>(context_len),
+          mask_shape.data(), mask_shape.size());
+
+      Ort::Value cache_position = BuildCachePositionFromMask(
+          attention_mask_tensor, context_len, model_->Allocator());
+
+      auto tmp = model_->ForwardLLM(std::move(inputs_embeds_tensor),
+                                    std::move(attention_mask_tensor),
+                                    cache_position,
+                                    cache_kv);
       logits = std::move(tmp.first);
-      past_key_values = std::move(tmp.second);
+      auto kv_outputs = std::move(tmp.second);
+
+      // Apply KV deltas to cache buffer in-place.
+      // kv_outputs contains deltas that update cache_kv at positions specified by cache_position.
+      model_->ApplyKvDeltaInplace(&cache_kv, kv_outputs, cache_position);
+
     } else {
-      // Subsequent steps: use decode model with KV cache
+      // Decode: seq = 1, mask_len = valid_len (= past + 1).
       int64_t last_token_id = generated_ids.back();
       std::vector<int64_t> one_id{last_token_id};
       std::array<int64_t, 2> one_shape{1, 1};
       Ort::Value one_tensor = Ort::Value::CreateTensor(
-          memory_info, one_id.data(), one_id.size(), one_shape.data(),
-          one_shape.size());
-      Ort::Value next_embed =
-          model_->ForwardEmbedding(std::move(one_tensor));
+          memory_info, one_id.data(), one_id.size(),
+          one_shape.data(), one_shape.size());
+
+      Ort::Value next_embed = model_->ForwardEmbedding(std::move(one_tensor));
       auto ne_info = next_embed.GetTensorTypeAndShapeInfo();
-      bool ne_fp16 = (ne_info.GetElementType() ==
-                      ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
-      std::vector<uint16_t> next_embed_fp16(hidden_size);
+      bool ne_fp16 = (ne_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+
+      // Reuse pre-allocated buffer for decode step embedding
       if (ne_fp16) {
         const uint16_t *src = next_embed.GetTensorData<uint16_t>();
-        std::copy(src, src + hidden_size, next_embed_fp16.data());
+        for (size_t d = 0; d < static_cast<size_t>(hidden_size); ++d) {
+          next_embed_fp32[d] = HalfBitsToFloat(src[d]);
+        }
       } else {
         const float *src = next_embed.GetTensorData<float>();
-        for (int32_t d = 0; d < hidden_size; ++d) {
-          next_embed_fp16[d] = FloatToHalfBits(src[d]);
-        }
+        std::memcpy(next_embed_fp32.data(), src, static_cast<size_t>(hidden_size) * sizeof(float));
       }
+
       std::array<int64_t, 3> embeds_shape{1, 1, hidden_size};
-      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor(
-          memory_info, next_embed_fp16.data(),
-          static_cast<size_t>(hidden_size) * sizeof(uint16_t),
-          embeds_shape.data(), embeds_shape.size(),
-          ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
-      std::vector<int64_t> decode_mask(valid_len, 1);
+      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor<float>(
+          memory_info, next_embed_fp32.data(),
+          static_cast<size_t>(hidden_size),
+          embeds_shape.data(), embeds_shape.size());
+
+      // mask_len must equal kv_seq_len (= past + current).
+      // Use pre-allocated attention_mask buffer (first valid_len elements)
       std::array<int64_t, 2> mask_shape{1, valid_len};
-      Ort::Value attention_mask_tensor =
-          Ort::Value::CreateTensor<int64_t>(
-              memory_info, decode_mask.data(), decode_mask.size(),
-              mask_shape.data(), mask_shape.size());
-      auto tmp = model_->ForwardLLMDecode(std::move(inputs_embeds_tensor),
-                                          std::move(attention_mask_tensor),
-                                          past_key_values);
+      Ort::Value attention_mask_tensor = Ort::Value::CreateTensor<int64_t>(
+          memory_info, attention_mask.data(), static_cast<size_t>(valid_len),
+          mask_shape.data(), mask_shape.size());
+
+      Ort::Value cache_position = BuildCachePositionFromMask(
+          attention_mask_tensor, 1, model_->Allocator());
+
+      auto tmp = model_->ForwardLLM(std::move(inputs_embeds_tensor),
+                                    std::move(attention_mask_tensor),
+                                    cache_position,
+                                    cache_kv);
       logits = std::move(tmp.first);
-      past_key_values = std::move(tmp.second);
+      auto kv_outputs = std::move(tmp.second);
+
+      // Apply KV deltas to cache buffer in-place.
+      model_->ApplyKvDeltaInplace(&cache_kv, kv_outputs, cache_position);
     }
-    // Extract logits for the last position
+
     auto log_info = logits.GetTensorTypeAndShapeInfo();
     auto log_shape = log_info.GetShape();
+
+    // logits are [B, S, V]. Always pick the last available step.
+    if (log_shape.size() < 3) {
+      SHERPA_ONNX_LOGE("Unexpected logits rank=%zu", log_shape.size());
+      result.text = "";
+      return result;
+    }
+
+    int32_t time_dim = static_cast<int32_t>(log_shape[1]);
     int32_t vocab_size = static_cast<int32_t>(log_shape[2]);
-    const bool log_fp16 =
-        (log_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
-    // In KV cache mode: prefill uses last position, decode uses position 0
-    int32_t last_idx = is_first_step ? (context_len - 1) : 0;
-    const void *base = nullptr;
-    if (log_fp16) {
-      base = logits.GetTensorData<uint16_t>();
-    } else {
-      base = logits.GetTensorData<float>();
+    if (time_dim <= 0 || vocab_size <= 0) {
+      SHERPA_ONNX_LOGE("Invalid logits shape [%lld,%lld,%lld]",
+                       (long long)log_shape[0], (long long)log_shape[1],
+                       (long long)log_shape[2]);
+      result.text = "";
+      return result;
     }
+
+    const bool log_fp16 = (log_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+
+    int32_t last_idx = time_dim - 1;
+
+    const void *base = nullptr;
+    if (log_fp16) base = logits.GetTensorData<uint16_t>();
+    else base = logits.GetTensorData<float>();
+
     const size_t offset = static_cast<size_t>(last_idx) * vocab_size;
     const void *last_logits =
         log_fp16
-            ? static_cast<const void *>(
-                  reinterpret_cast<const uint16_t *>(base) + offset)
-            : static_cast<const void *>(
-                  reinterpret_cast<const float *>(base) + offset);
-    // Sample next token using greedy decoding
-    int64_t next_id =
-        SampleTokenFromLogitsFp16OrFp32(last_logits, log_fp16, vocab_size);
-    if (next_id == eos_id || next_id == im_end_id) {
-      break;
-    }
+            ? static_cast<const void *>(reinterpret_cast<const uint16_t *>(base) + offset)
+            : static_cast<const void *>(reinterpret_cast<const float *>(base) + offset);
+
+    int64_t next_id = SampleTokenWithTemperatureAndTopP(
+        last_logits, log_fp16, vocab_size,
+        funasr_config.temperature, funasr_config.top_p);
+
+    if (next_id == eos_id || next_id == im_end_id) break;
+
     generated_ids.push_back(next_id);
 
-    // After sampling the first token from prefill, switch to decode mode
-    if (is_first_step) {
-      is_first_step = false;
-    }
+    if (is_first_step) is_first_step = false;
+
+    // valid_len represents the kv_seq_len for the next decode step.
     valid_len += 1;
   }
-  // Decode generated token IDs to text
+
   result.text = tokenizer_->Decode(generated_ids);
   result.text = ApplyInverseTextNormalization(std::move(result.text));
   result.text = ApplyHomophoneReplacer(std::move(result.text));
+
+  if (config_.model_config.debug) {
+    SHERPA_ONNX_LOGE("GenerateText: generated %zu tokens: %s",
+                     generated_ids.size(), result.text.c_str());
+    std::string token_str;
+    for (size_t i = 0; i < generated_ids.size() && i < 10; ++i) {
+      if (i > 0) token_str += ",";
+      token_str += std::to_string(generated_ids[i]);
+    }
+    SHERPA_ONNX_LOGE("GenerateText: token ids: %s%s",
+                     token_str.c_str(), generated_ids.size() > 10 ? "..." : "");
+  }
+
   if (!generated_ids.empty()) {
-    // Fill tokens: decode each token individually
     result.tokens.reserve(generated_ids.size());
     for (int64_t token_id : generated_ids) {
       std::vector<int64_t> single_token{token_id};
-      std::string token_str = tokenizer_->Decode(single_token);
-      result.tokens.push_back(token_str);
+      result.tokens.push_back(tokenizer_->Decode(single_token));
     }
-    // Fill timestamps: estimate based on audio duration and token count
+
     auto enc_shape2 = encoder_out.GetTensorTypeAndShapeInfo().GetShape();
     int32_t audio_token_len2 = static_cast<int32_t>(enc_shape2[1]);
     int32_t lfr_window_size = model_->LfrWindowSize();
     int32_t lfr_window_shift = model_->LfrWindowShift();
-    int32_t sampling_rate = config_.feat_config.sampling_rate;
-    // Reverse LFR to get original feature frames
+
     int32_t original_feature_frames =
         (audio_token_len2 > 0)
             ? ((audio_token_len2 - 1) * lfr_window_shift + lfr_window_size)
             : 0;
+
     float frame_shift_ms = config_.feat_config.frame_shift_ms;
     float audio_duration =
         (original_feature_frames > 0 && frame_shift_ms > 0)
-            ? static_cast<float>(original_feature_frames) * frame_shift_ms /
-                  1000.0f
+            ? static_cast<float>(original_feature_frames) * frame_shift_ms / 1000.0f
             : 0.0f;
+
     result.timestamps.reserve(generated_ids.size());
-    // Linear interpolation: distribute tokens evenly over audio duration
     if (generated_ids.size() > 1 && audio_duration > 0) {
-      float time_per_token =
-          audio_duration / static_cast<float>(generated_ids.size());
+      float time_per_token = audio_duration / static_cast<float>(generated_ids.size());
       for (size_t i = 0; i < generated_ids.size(); ++i) {
         result.timestamps.push_back(static_cast<float>(i) * time_per_token);
       }
@@ -397,6 +666,7 @@ OfflineRecognitionResult OfflineRecognizerFunASRNanoImpl::GenerateText(
       result.timestamps.push_back(audio_duration / 2.0f);
     }
   }
+
   return result;
 }
 
@@ -418,16 +688,22 @@ void OfflineRecognizerFunASRNanoImpl::DecodeStreams(OfflineStream **ss,
       ss[i]->SetResult(r);
       continue;
     }
-    std::array<int64_t, 3> shape{1, num_frames,
-                                  static_cast<int64_t>(f.size() / num_frames)};
+
+    std::array<int64_t, 3> shape{
+        1, num_frames, static_cast<int64_t>(f.size() / num_frames)};
+
     Ort::Value features = Ort::Value::CreateTensor<float>(
-        memory_info, const_cast<float *>(f.data()), f.size(), shape.data(),
-        shape.size());
+        memory_info, const_cast<float *>(f.data()), f.size(),
+        shape.data(), shape.size());
+
     Ort::Value encoder_out =
         model_->ForwardEncoderAdaptor(std::move(features));
+
     OfflineRecognitionResult r = GenerateText(
-        std::move(encoder_out), funasr_config.system_prompt,
+        std::move(encoder_out),
+        funasr_config.system_prompt,
         funasr_config.user_prompt);
+
     ss[i]->SetResult(r);
   }
 }
diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
index 5917dae7..ceada9fb 100644
--- a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
@@ -39,17 +39,26 @@ class OfflineRecognizerFunASRNanoImpl : public OfflineRecognizerImpl {
  private:
   void InitFeatConfig();
   std::vector<float> ApplyLFR(const std::vector<float> &in) const;
-  std::vector<int64_t> BuildSourceIds(
-      const std::string &system_prompt, const std::string &user_prompt,
-      int32_t audio_token_len, int32_t &fbank_beg_idx,
-      int32_t &fake_token_len) const;
-  int64_t SampleToken(const float *logits, int32_t vocab_size, int32_t step,
-                     int64_t eos_token_id, int64_t im_end_token_id) const;
-  int64_t SampleTokenFromLogitsFp16OrFp32(const void *logits, bool is_fp16,
-                                          int32_t vocab_size) const;
-  OfflineRecognitionResult GenerateText(
-      Ort::Value encoder_out, const std::string &system_prompt,
-      const std::string &user_prompt) const;
+
+  std::vector<int64_t> BuildSourceIds(const std::string &system_prompt,
+                                      const std::string &user_prompt,
+                                      int32_t audio_token_len,
+                                      int32_t &fbank_beg_idx,
+                                      int32_t &fake_token_len) const;
+
+  int64_t SampleTokenFromLogitsFp16OrFp32(const void *logits,
+                                         bool is_fp16,
+                                         int32_t vocab_size) const;
+
+  int64_t SampleTokenWithTemperatureAndTopP(const void *logits,
+                                            bool is_fp16,
+                                            int32_t vocab_size,
+                                            float temperature,
+                                            float top_p) const;
+
+  OfflineRecognitionResult GenerateText(Ort::Value encoder_out,
+                                       const std::string &system_prompt,
+                                       const std::string &user_prompt) const;
 
   OfflineRecognizerConfig config_;
   std::unique_ptr<OfflineFunASRNanoModel> model_;
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
index e4d278ca..1db1b414 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
@@ -100,8 +100,7 @@ See https://github.com/FunAudioLLM/Fun-ASR-Nano-2512
 
   ./bin/sherpa-onnx-offline \
     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
-    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-llm=/path/to/llm.onnx \
     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
     --funasr-nano-embedding=/path/to/embedding.onnx \
     /path/to/foo.wav [bar.wav foobar.wav ...]
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
index 5a3fc2bf..386e426e 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
@@ -124,13 +124,12 @@ See https://github.com/FunAudioLLM/Fun-ASR-Nano-2512
   ./bin/sherpa-onnx-vad-with-offline-asr \
     --silero-vad-model=/path/to/silero_vad.onnx \
     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
-    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
-    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-llm=/path/to/llm.onnx \
     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
     --funasr-nano-embedding=/path/to/embedding.onnx \
     [--funasr-nano-user-prompt="Transcription:"] \
     [--funasr-nano-max-new-tokens=512] \
-    [--funasr-nano-temperature=0.3] \
+    [--funasr-nano-temperature=1e-6] \
     [--funasr-nano-top-p=0.8] \
     --num-threads=4 \
     /path/to/foo.wav
diff --git a/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
index 87fab249..2fb02b79 100644
--- a/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
@@ -15,8 +15,7 @@ void PybindOfflineFunASRNanoModelConfig(py::module *m) {
   py::class_<PyClass>(*m, "OfflineFunASRNanoModelConfig")
       .def(py::init<>())
       .def_readwrite("encoder_adaptor", &PyClass::encoder_adaptor)
-      .def_readwrite("llm_prefill", &PyClass::llm_prefill)
-      .def_readwrite("llm_decode", &PyClass::llm_decode)
+      .def_readwrite("llm", &PyClass::llm)
       .def_readwrite("embedding", &PyClass::embedding)
       .def_readwrite("tokenizer", &PyClass::tokenizer)
       .def_readwrite("system_prompt", &PyClass::system_prompt)
diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
index 5fa42334..ae11276a 100644
--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
@@ -301,8 +301,7 @@ class OfflineRecognizer(object):
     def from_funasr_nano(
         cls,
         encoder_adaptor: str,
-        llm_prefill: str,
-        llm_decode: str,
+        llm: str,
         embedding: str,
         tokenizer: str,
         num_threads: int = 1,
@@ -312,9 +311,9 @@ class OfflineRecognizer(object):
         debug: bool = False,
         provider: str = "cpu",
         system_prompt: str = "You are a helpful assistant.",
-        user_prompt: str = ":",
+        user_prompt: str = ":",
         max_new_tokens: int = 512,
-        temperature: float = 0.3,
+        temperature: float = 1e-6,
         top_p: float = 0.8,
         seed: int = 42,
     ):
@@ -324,10 +323,8 @@ class OfflineRecognizer(object):
         Args:
           encoder_adaptor:
             Path to ``encoder_adaptor.onnx``.
-          llm_prefill:
-            Path to ``llm_prefill.onnx`` (KV cache mode).
-          llm_decode:
-            Path to ``llm_decode.onnx`` (KV cache mode).
+          llm:
+            Path to ``llm.onnx`` (KV cache model).
           embedding:
             Path to ``embedding.onnx``.
           tokenizer:
@@ -361,8 +358,7 @@ class OfflineRecognizer(object):
         # Create OfflineFunASRNanoModelConfig and set attributes
         funasr_nano_config = OfflineFunASRNanoModelConfig()
         funasr_nano_config.encoder_adaptor = encoder_adaptor
-        funasr_nano_config.llm_prefill = llm_prefill
-        funasr_nano_config.llm_decode = llm_decode
+        funasr_nano_config.llm = llm
         funasr_nano_config.embedding = embedding
         funasr_nano_config.tokenizer = tokenizer
         funasr_nano_config.system_prompt = system_prompt

commit c3053357ec3fa69f38c650e3e8f50930976df91b
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Tue Jan 6 07:52:26 2026 +0800

    Test Whisper on Ascend NPU using ACL Python API (#2986)

diff --git a/scripts/whisper/ascend-npu/export_onnx.py b/scripts/whisper/ascend-npu/export_onnx.py
new file mode 120000
index 00000000..25ea5731
--- /dev/null
+++ b/scripts/whisper/ascend-npu/export_onnx.py
@@ -0,0 +1 @@
+../rknn/export_onnx.py
\ No newline at end of file
diff --git a/scripts/whisper/ascend-npu/test_om.py b/scripts/whisper/ascend-npu/test_om.py
new file mode 100755
index 00000000..306cfc27
--- /dev/null
+++ b/scripts/whisper/ascend-npu/test_om.py
@@ -0,0 +1,265 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import argparse
+import base64
+from typing import List
+
+import kaldi_native_fbank as knf
+import librosa
+import numpy as np
+from ais_bench.infer.interface import InferSession
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--encoder",
+        type=str,
+        required=True,
+        help="Path to the encoder",
+    )
+
+    parser.add_argument(
+        "--decoder",
+        type=str,
+        required=True,
+        help="Path to the decoder",
+    )
+
+    parser.add_argument(
+        "--tokens",
+        type=str,
+        required=True,
+        help="Path to the tokens",
+    )
+
+    parser.add_argument(
+        "--wav",
+        type=str,
+        required=True,
+        help="Path to the test wav",
+    )
+
+    return parser.parse_args()
+
+
+def causal_mask_1d(n: int, L: int):
+    """
+    Returns a 1-D int mask of shape (L,) with:
+      0 -> allowed
+      1 -> masked (will be converted to -inf later)
+    """
+    mask = np.ones((L,), dtype=np.int32)
+    if n > 0:
+        mask[:n] = 0
+    return mask
+
+
+def load_audio(filename: str) -> np.ndarray:
+    samples, _ = librosa.load(filename, sr=16000)
+
+    samples = np.ascontiguousarray(samples)
+    return samples
+
+
+def compute_features(samples: np.ndarray, dim: int = 80) -> np.ndarray:
+    """
+    Returns:
+      Return a 1-D float32 tensor of shape (1, 80, 3000) containing the features.
+    """
+    features = []
+    opts = knf.WhisperFeatureOptions()
+    opts.dim = dim
+    online_whisper_fbank = knf.OnlineWhisperFbank(opts)
+    online_whisper_fbank.accept_waveform(16000, samples)
+    online_whisper_fbank.input_finished()
+
+    features = np.stack(
+        [
+            online_whisper_fbank.get_frame(i)
+            for i in range(online_whisper_fbank.num_frames_ready)
+        ]
+    )
+    log_spec = np.log10(np.clip(features, a_min=1e-10, a_max=None))
+    log_spec = np.maximum(log_spec, log_spec.max() - 8.0)
+    mel = (log_spec + 4.0) / 4.0
+    num_frames = mel.shape[0]
+    target = 3000
+    if num_frames < target:
+        mel = np.pad(
+            mel,
+            pad_width=((0, target - num_frames), (0, 0)),
+            mode="constant",
+            constant_values=0,
+        )
+
+    mel = np.expand_dims(mel.T, axis=0)
+    mel = np.ascontiguousarray(mel)
+
+    return mel
+
+
+def load_tokens(filename):
+    tokens = dict()
+    with open(filename, "r") as f:
+        for line in f:
+            t, i = line.split()
+            tokens[int(i)] = t
+    return tokens
+
+
+class OmModel:
+    def __init__(self, encoder: str, decoder: str):
+        self.encoder = InferSession(device_id=0, model_path=encoder, debug=False)
+        self.decoder = InferSession(device_id=0, model_path=decoder, debug=False)
+
+        name = self.encoder.get_inputs()[0].name
+
+        if ".en" in name:
+            self.sot_sequence = [50257, 50362]
+            self.eot = 50256
+        else:
+            self.sot_sequence = [50258, 50259, 50359, 50363]
+            self.eot = 50257
+
+        if "tiny" in name:
+            self.n_text_layer = 4
+            self.n_text_ctx = 448
+            self.n_text_state = 384
+        elif "base" in name:
+            self.n_text_layer = 6
+            self.n_text_ctx = 448
+            self.n_text_state = 512
+        elif "small" in name:
+            self.n_text_layer = 12
+            self.n_text_ctx = 448
+            self.n_text_state = 768
+        elif "medium" in name:
+            self.n_text_layer = 24
+            self.n_text_ctx = 448
+            self.n_text_state = 1024
+        else:
+            assert False, f"Unsupported encoder input {name}"
+
+        print("---encoder---")
+        for i in self.encoder.get_inputs():
+            print(i.name, i.datatype, i.shape)
+
+        print("-----")
+
+        for i in self.encoder.get_outputs():
+            print(i.name, i.datatype, i.shape)
+
+        print("---decoder---")
+        for i in self.decoder.get_inputs():
+            print(i.name, i.datatype, i.shape)
+
+        print("-----")
+
+        for i in self.decoder.get_outputs():
+            print(i.name, i.datatype, i.shape)
+
+    def get_self_cache(self) -> List[np.ndarray]:
+        self_cache = []
+        batch_size = 1
+        for i in range(self.n_text_layer):
+            k = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            v = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            self_cache.extend([k, v])
+        return self_cache
+
+    def run_encoder(self, x: np.ndarray):
+        """
+        Args:
+          x: (1, 80, 3000), np.float32
+        Returns:
+          cross_kv:
+           - (k, v) for layer 0
+           - (k, v) for layer 1
+           - (k, v) for layer 2
+           - (k, v) for layer 3
+        """
+        out = self.encoder.infer([x])
+        return out
+
+    def run_decoder(self, tokens: np.ndarray, self_kv, cross_kv, offset, mask):
+        """
+        Args:
+          tokens: (1, 1), np.int32
+          offset: (1,), np.int32
+          mask: (model.n_text_ctx,), np.int32
+        Returns:
+          logit: (1, 1, vocab_size)
+          this_self_kv
+        """
+        return self.decoder.infer([tokens] + self_kv + cross_kv + [offset, mask])
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+    samples = load_audio(args.wav)
+    features = compute_features(samples)
+    print("features", features.shape)
+
+    model = OmModel(args.encoder, args.decoder)
+
+    cross_kv = model.run_encoder(features)
+
+    self_kv = model.get_self_cache()
+
+    offset = np.array([0], dtype=np.int32)
+    for t in model.sot_sequence:
+        token = np.array([[t]], dtype=np.int32)  # sot
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx)
+
+        out = model.run_decoder(
+            tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask
+        )
+
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        offset += 1
+
+    idx = out[0][0, 0].argmax()
+
+    eot = model.eot
+
+    ans = []
+
+    while idx != eot and offset.item() < 100:
+        ans.append(idx)
+        token = np.array([[idx]], dtype=np.int32)
+
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx)
+
+        out = model.run_decoder(
+            tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask
+        )
+
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        offset += 1
+        idx = out[0][0, 0].argmax()
+
+    print(ans)
+    id2token = load_tokens(args.tokens)
+
+    s = b""
+    for i in ans:
+        if i in id2token:
+            s += base64.b64decode(id2token[i])
+
+    print(s.decode().strip())
+    return
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/test_on_rk3588_board.py b/scripts/whisper/rknn/test_on_rk3588_board.py
index 6fb74608..b837aaed 100755
--- a/scripts/whisper/rknn/test_on_rk3588_board.py
+++ b/scripts/whisper/rknn/test_on_rk3588_board.py
@@ -201,7 +201,6 @@ class RKNNModel:
            - (k, v) for layer 3
         """
         out = self.encoder.inference(inputs=[x.numpy()])
-        print("after running encoder", len(out))
         return out
 
     def get_self_cache(self) -> List[np.ndarray]:
@@ -239,11 +238,9 @@ def main():
     id2token = load_tokens(args.tokens)
 
     if ".en" in args.encoder:
-        print("here", args.encoder)
         sot_sequence = [50257, 50362]
         eot = 50256
     else:
-        print("not here", args.encoder)
         sot_sequence = [50258, 50259, 50359, 50363]
         eot = 50257
 
@@ -296,7 +293,6 @@ def test(model, id2token):
     for t in model.sot_sequence:
         token = np.array([[t]], dtype=np.int32)  # sot
         mask = causal_mask_1d(offset.item(), model.n_text_ctx)
-        print(t, model.sot_sequence, token, mask.shape, len(cross_kv), len(self_kv))
 
         out = model.run_decoder(
             tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask

commit b1db3eaa8d33c4334a56fa23a18a30dc65973e56
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Jan 5 19:46:30 2026 +0800

    Export Whisper to RK NPU (#2983)

diff --git a/.gitignore b/.gitignore
index 2ee313d3..1989ce74 100755
--- a/.gitignore
+++ b/.gitignore
@@ -166,3 +166,5 @@ spacemit-toolchain*
 sherpa-onnx-qnn-*
 matcha-icefall-*
 sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+*.raw
+*-input-list.txt
diff --git a/README.md b/README.md
index 6cd6d6f4..470fd8d2 100644
--- a/README.md
+++ b/README.md
@@ -424,10 +424,6 @@ It uses Swift for iOS and Java for Android.
 Flet ASR/STT component based on sherpa-onnx.
 Example [a chat box agent](https://github.com/SamYuan1990/i18n-agent-action)
 
-### [elderly-companion](https://github.com/SearocIsMe/elderly-companion)
-
-It uses sherpa-onnx's Python API for real-time speech recognition in ROS2 with RK NPU.
-
 ### [achatbot-go](https://github.com/ai-bot-pro/achatbot-go)
 
 a multimodal chatbot based on go with sherpa-onnx's speech lib api.
diff --git a/scripts/whisper/export-onnx.py b/scripts/whisper/export-onnx.py
index 1fdd5fe6..13a807e0 100755
--- a/scripts/whisper/export-onnx.py
+++ b/scripts/whisper/export-onnx.py
@@ -32,9 +32,6 @@ from whisper.model import (
     TextDecoder,
 )
 
-torch.set_num_threads(1)
-torch.set_num_interop_threads(1)
-
 
 def get_args():
     parser = argparse.ArgumentParser()
@@ -321,7 +318,7 @@ def main():
     print(args)
     print(name)
 
-    opset_version = 13
+    opset_version = 17
 
     if name == "distil-medium.en":
         filename = "./distil-medium-en-original-model.bin"
@@ -640,4 +637,12 @@ def main():
 
 
 if __name__ == "__main__":
-    main()
+    torch.set_num_threads(1)
+    torch.set_num_interop_threads(1)
+    # To fix
+    # TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor
+    # See also https://github.com/k2-fsa/sherpa-onnx/issues/1764
+    from whisper.model import disable_sdpa
+
+    with disable_sdpa():
+        main()
diff --git a/scripts/whisper/export_onnx.py b/scripts/whisper/export_onnx.py
new file mode 120000
index 00000000..34533bc4
--- /dev/null
+++ b/scripts/whisper/export_onnx.py
@@ -0,0 +1 @@
+export-onnx.py
\ No newline at end of file
diff --git a/scripts/whisper/model-info.md b/scripts/whisper/model-info.md
new file mode 100644
index 00000000..2ae1cc70
--- /dev/null
+++ b/scripts/whisper/model-info.md
@@ -0,0 +1,81 @@
+# tiny/tiny.en
+```
+ModelDimensions(
+    n_mels=80,
+    n_audio_ctx=1500,
+    n_audio_state=384,
+    n_audio_head=6,
+    n_audio_layer=4,
+    n_vocab=51865,
+    n_text_ctx=448,
+    n_text_state=384,
+    n_text_head=6,
+    n_text_layer=4
+)
+```
+
+# base/base.en
+```
+ModelDimensions(
+    n_mels=80,
+    n_audio_ctx=1500,
+    n_audio_state=512,
+    n_audio_head=8,
+    n_audio_layer=6,
+    n_vocab=51865,
+    n_text_ctx=448,
+    n_text_state=512,
+    n_text_head=8,
+    n_text_layer=6
+)
+```
+
+# small/small.en
+```
+ModelDimensions(
+    n_mels=80,
+    n_audio_ctx=1500,
+    n_audio_state=768,
+    n_audio_head=12,
+    n_audio_layer=12,
+    n_vocab=51865,
+    n_text_ctx=448,
+    n_text_state=768,
+    n_text_head=12,
+    n_text_layer=12
+)
+```
+
+
+# medium/medium.en
+```
+ModelDimensions(
+    n_mels=80,
+    n_audio_ctx=1500,
+    n_audio_state=1024,
+    n_audio_head=16,
+    n_audio_layer=24,
+    n_vocab=51865,
+    n_text_ctx=448,
+    n_text_state=1024,
+    n_text_head=16,
+    n_text_layer=24
+)
+```
+
+# large
+```
+ModelDimensions(
+    n_mels=80,
+    n_audio_ctx=1500,
+    n_audio_state=1280,
+    n_audio_head=20,
+    n_audio_layer=32,
+    n_vocab=51865,
+    n_text_ctx=448,
+    n_text_state=1280,
+    n_text_head=20,
+    n_text_layer=32
+)
+```
+
diff --git a/scripts/whisper/rknn/README.md b/scripts/whisper/rknn/README.md
new file mode 100644
index 00000000..247b739f
--- /dev/null
+++ b/scripts/whisper/rknn/README.md
@@ -0,0 +1,48 @@
+# Usage
+
+You can find pre-exported rknn models for rk3588 at
+
+https://modelscope.cn/models/csukuangfj/2026-01-05-rknn/files
+
+
+# Download test wave
+
+```
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en.wav
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en-16k.wav
+```
+
+## Export to onnx
+
+```
+./export_onnx.py --model tiny.en
+```
+
+## Test onnx
+
+```
+./test_onnx.py --model tiny.en
+```
+
+## Export to rknn
+
+```
+python3 ./export_rknn.py --target-platform rk3588  --in-model ./tiny.en-encoder.onnx --out-model ./tiny.en-encoder.rknn
+
+python3 ./export_rknn.py --target-platform rk3588  --in-model ./tiny.en-decoder.onnx --out-model ./tiny.en-decoder.rknn
+```
+
+```
+ls -lh tiny.en-*.rknn
+
+-rw-r--r-- 1 kuangfangjun root 95M Jan  5 16:16 tiny.en-decoder.rknn
+-rw-r--r-- 1 kuangfangjun root 22M Jan  5 16:15 tiny.en-encoder.rknn
+```
+
+## Run it on your rk3588 board
+
+```
+wget https://huggingface.co/csukuangfj/sherpa-onnx-whisper-tiny.en/resolve/main/tiny.en-tokens.txt
+
+./test_on_rk3588_board.py  --encoder ./tiny.en-encoder.rknn --decoder ./tiny.en-decoder.rknn --tokens ./tiny.en-tokens.txt --wav ./en-16k.wav
+```
diff --git a/scripts/whisper/rknn/export_onnx.py b/scripts/whisper/rknn/export_onnx.py
new file mode 100755
index 00000000..587b50d9
--- /dev/null
+++ b/scripts/whisper/rknn/export_onnx.py
@@ -0,0 +1,739 @@
+#!/usr/bin/env python3
+# Copyright    2023  Xiaomi Corp.        (authors: Fangjun Kuang)
+# flake8: noqa
+
+"""
+Note: Code in this file is modified from
+https://github.com/TadaoYamaoka/whisper/blob/main/to_onnx.py
+
+Thanks to https://github.com/TadaoYamaoka
+for making the onnx export script public.
+
+Note that we have removed the 30 seconds constraint from whisper. You can
+use any T <= 30.
+"""
+
+import argparse
+import inspect
+import os
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Tuple
+
+import onnx
+import torch
+import torch.nn.functional as F
+import whisper
+from onnxruntime.quantization import QuantType, quantize_dynamic
+from torch import Tensor, nn
+from whisper.model import (
+    AudioEncoder,
+    MultiHeadAttention,
+    ResidualAttentionBlock,
+    TextDecoder,
+)
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--model",
+        type=str,
+        required=True,
+        # fmt: off
+        choices=[
+            "tiny", "tiny.en", "base", "base.en",
+            "small", "small.en", "medium", "medium.en",
+            "large-v1", "large-v2",
+            "large", "large-v3", "turbo", # these three have feature dim 128
+            "distil-medium.en", "distil-small.en", "distil-large-v2",
+            "distil-large-v3",
+            "distil-large-v3.5",
+            # for fine-tuned models from icefall
+            "medium-aishell",
+            ],
+        # fmt: on
+    )
+    return parser.parse_args()
+
+
+def causal_mask_1d(n: int, L: int, device=None, dtype=torch.int32):
+    """
+    Returns a 1-D int mask of shape (L,) with:
+      0 -> allowed
+      1 -> masked (will be converted to -inf later)
+    """
+    mask = torch.ones((L,), device=device, dtype=dtype)
+    if n > 0:
+        mask[:n] = 0
+    return mask
+
+
+def add_meta_data(filename: str, meta_data: Dict[str, Any]):
+    """Add meta data to an ONNX model. It is changed in-place.
+
+    Args:
+      filename:
+        Filename of the ONNX model to be changed.
+      meta_data:
+        Key-value pairs.
+    """
+    model = onnx.load(filename)
+
+    while len(model.metadata_props):
+        model.metadata_props.pop()
+
+    for key, value in meta_data.items():
+        meta = model.metadata_props.add()
+        meta.key = key
+        meta.value = str(value)
+
+    if "large" in filename or "turbo" in filename:
+        external_filename = filename.split(".onnx")[0]
+        onnx.save(
+            model,
+            filename,
+            save_as_external_data=True,
+            all_tensors_to_one_file=True,
+            location=external_filename + ".weights",
+        )
+    else:
+        onnx.save(model, filename)
+
+
+def modified_self_qkv_attention(
+    self,
+    q: Tensor,
+    k_cache: Tensor,
+    v_cache: Tensor,
+    k1: Tensor,
+    v1: Tensor,
+    mask: Tensor,
+) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+    assert mask is not None
+
+    n_batch, n_ctx, n_state = q.shape
+
+    scale = (n_state // self.n_head) ** -0.25
+    q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)
+    k_cache = k_cache.view(*k_cache.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)
+    v_cache = v_cache.view(*v_cache.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)
+
+    k1 = k1.view(*k1.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)
+    v1 = v1.view(*v1.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)
+
+    qk = (q * scale) @ (k_cache * scale).transpose(-1, -2)  # (1, 6, 1, 448)
+
+    qk1 = (q * scale) @ (k1 * scale).transpose(-1, -2)  # (1, 6, 1, 1)
+
+    #  qk = qk + mask
+    #  qk.masked_fill_(mask.to(torch.bool), float("-inf"))
+    qk.masked_fill_(mask.to(torch.bool), -60000)
+
+    qk = qk.float()
+    qk1 = qk1.float()
+
+    qk_total = torch.cat([qk, qk1], dim=-1)
+
+    w_total = F.softmax(qk_total, dim=-1).to(q.dtype)
+    w = w_total[:, :, :, :-1]
+    w1 = w_total[:, :, :, -1:]
+
+    out = (w @ v_cache).permute(0, 2, 1, 3).flatten(start_dim=2)
+    out1 = (w1 @ v1).permute(0, 2, 1, 3).flatten(start_dim=2)
+    out = out + out1
+
+    qk = qk.detach()
+
+    return out, qk
+
+
+MultiHeadAttention.qkv_attention_self = modified_self_qkv_attention
+
+
+def modified_audio_encoder_forward(self: AudioEncoder, x: torch.Tensor):
+    """
+    x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)
+        the mel spectrogram of the audio
+    """
+    x = F.gelu(self.conv1(x))
+    x = F.gelu(self.conv2(x))
+    x = x.permute(0, 2, 1)
+
+    if False:
+        # This branch contains the original code
+        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
+        x = (x + self.positional_embedding).to(x.dtype)
+    else:
+        #  print(x.shape, self.positional_embedding.shape)
+        # This branch contains the actual changes
+        assert (
+            x.shape[2] == self.positional_embedding.shape[1]
+        ), f"incorrect audio shape: {x.shape}, {self.positional_embedding.shape}"
+        assert (
+            x.shape[1] == self.positional_embedding.shape[0]
+        ), f"incorrect audio shape: {x.shape}, {self.positional_embedding.shape}"
+        x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)
+
+    for block in self.blocks:
+        x = block(x)
+
+    x = self.ln_post(x)
+    return x
+
+
+AudioEncoder.forward = modified_audio_encoder_forward
+
+
+class AudioEncoderTensorCache(nn.Module):
+    def __init__(self, inAudioEncoder: AudioEncoder, inTextDecoder: TextDecoder):
+        super().__init__()
+        self.audioEncoder = inAudioEncoder
+        self.textDecoder = inTextDecoder
+
+    def forward(self, x: Tensor) -> List[Tuple[Tensor, Tensor]]:
+        """
+        Args:
+          x: (1, 80, 3000)
+          cross_kv_pair:
+            - the i-th entry contains kv cache for the i-th layer
+        """
+        audio_features = self.audioEncoder(x)
+
+        n_layer_cross_k_list = []
+        n_layer_cross_v_list = []
+
+        cross_kv_pair = []
+        for block in self.textDecoder.blocks:
+            k = block.cross_attn.key(audio_features)  # (batch_size, 1500, 384)
+            v = block.cross_attn.value(audio_features)  # (batch_size, 1500, 384)
+
+            cross_kv_pair.append((k, v))
+
+        return cross_kv_pair
+
+
+class MultiHeadAttentionCross(nn.Module):
+    def __init__(self, inMultiHeadAttention: MultiHeadAttention):
+        super().__init__()
+        self.multiHeadAttention = inMultiHeadAttention
+
+    def forward(
+        self,
+        x: Tensor,
+        k: Tensor,
+        v: Tensor,
+        mask: Optional[Tensor] = None,
+    ):
+        q = self.multiHeadAttention.query(x)
+        wv, qk = self.multiHeadAttention.qkv_attention(q, k, v, mask)
+        return self.multiHeadAttention.out(wv)
+
+
+class MultiHeadAttentionSelf(nn.Module):
+    def __init__(self, inMultiHeadAttention: MultiHeadAttention):
+        super().__init__()
+        self.multiHeadAttention = inMultiHeadAttention
+
+    def forward(
+        self,
+        x: Tensor,  # (1, 1      , 384)
+        k_cache: Tensor,  # (1, 448, 384)
+        v_cache: Tensor,  # (1, 448, 384)
+        mask: Tensor,  # (448,)
+    ):
+        q = self.multiHeadAttention.query(x)  # (1, 1, 384)
+        k = self.multiHeadAttention.key(x)  # (1, 1, 384)
+        v = self.multiHeadAttention.value(x)  # (1, 1, 384)
+
+        #  k_cache[:, offset : offset + 1, :] = k  # (b, n_ctx_cache + n_ctx, n_state)
+        #  v_cache[:, offset : offset + 1, :] = v  # (b, n_ctx_cache + n_ctx, n_state)
+
+        wv, qk = self.multiHeadAttention.qkv_attention_self(
+            q,
+            k_cache=k_cache,
+            v_cache=v_cache,
+            k1=k,
+            v1=v,
+            mask=mask,
+        )
+
+        return self.multiHeadAttention.out(wv), k, v
+
+
+class ResidualAttentionBlockTensorCache(nn.Module):
+    def __init__(self, inResidualAttentionBlock: ResidualAttentionBlock):
+        super().__init__()
+        self.originalBlock = inResidualAttentionBlock
+        self.attn = MultiHeadAttentionSelf(inResidualAttentionBlock.attn)
+        self.cross_attn = (
+            MultiHeadAttentionCross(inResidualAttentionBlock.cross_attn)
+            if inResidualAttentionBlock.cross_attn
+            else None
+        )
+
+    def forward(
+        self,
+        x: Tensor,
+        self_k_cache: Tensor,
+        self_v_cache: Tensor,
+        cross_k: Tensor,
+        cross_v: Tensor,
+        offset: Tensor,
+        mask: Tensor,
+    ):
+        self_attn_x, self_k, self_v = self.attn(
+            self.originalBlock.attn_ln(x),
+            self_k_cache,
+            self_v_cache,
+            mask=mask,
+        )
+        x = x + self_attn_x
+
+        if self.cross_attn:
+            x = x + self.cross_attn(
+                self.originalBlock.cross_attn_ln(x), cross_k, cross_v
+            )
+
+        x = x + self.originalBlock.mlp(self.originalBlock.mlp_ln(x))
+        return x, self_k, self_v
+
+
+class TextDecoderTensorCache(nn.Module):
+    def __init__(self, inTextDecoder: TextDecoder, in_n_ctx: int):
+        super().__init__()
+        self.textDecoder = inTextDecoder
+        self.n_ctx = in_n_ctx
+
+        self.blocks = []
+        for orginal_block in self.textDecoder.blocks:
+            self.blocks.append(ResidualAttentionBlockTensorCache(orginal_block))
+
+    def forward(
+        self,
+        tokens: Tensor,
+        self_kv_pair: List[Tuple[Tensor, Tensor]],
+        cross_kv_pair: List[Tuple[Tensor, Tensor]],
+        offset: Tensor,
+        mask: Tensor,
+    ) -> Tuple[Tensor, List[Tuple[Tensor, Tensor]]]:
+        """
+        tokens: (batch_size, 1)
+        self_kv_pair:
+            - [i][0]: layer_i_self_k_cache, (batch_size, 448, dim)
+            - [i][1]: layer_i_self_v_cache, (batch_size, 448, dim)
+        Returns:
+          - logits
+          - this_self_kv_pair
+        """
+        assert tokens.shape == (1, 1), tokens.shape
+        x = self.textDecoder.token_embedding(
+            tokens
+        ) + self.textDecoder.positional_embedding[offset.to(torch.int64)].unsqueeze(0)
+
+        i = 0
+        this_self_kv_pair = []
+        for block in self.blocks:
+            self_k_cache = self_kv_pair[i][0]
+            self_v_cache = self_kv_pair[i][1]
+
+            x, self_k, self_v = block(
+                x,
+                #  self_k_cache=self_k_cache[:, : offset + 1],
+                #  self_v_cache=self_v_cache[:, : offset + 1],
+                self_k_cache=self_k_cache,
+                self_v_cache=self_v_cache,
+                cross_k=cross_kv_pair[i][0],
+                cross_v=cross_kv_pair[i][1],
+                offset=offset,
+                #  mask=self.textDecoder.mask,
+                mask=mask,
+            )
+            #  self_k_cache[:, : offset + 1] = updated_self_k_cache
+            #  self_v_cache[:, : offset + 1] = updated_self_v_cache
+            #  updated_self_kv_pair.append((self_k_cache, self_v_cache))
+            this_self_kv_pair.append((self_k, self_v))
+
+            i += 1
+
+        x = self.textDecoder.ln(x)
+
+        if False:
+            # x.shape (1, 3, 384)
+            # weight.shape (51684, 384)
+
+            logits = (
+                x
+                @ torch.transpose(
+                    self.textDecoder.token_embedding.weight.to(x.dtype), 0, 1
+                )
+            ).float()
+        else:
+            logits = (
+                torch.matmul(
+                    self.textDecoder.token_embedding.weight.to(x.dtype),
+                    x.permute(0, 2, 1),
+                )
+                .permute(0, 2, 1)
+                .float()
+            )
+
+        return logits, this_self_kv_pair
+
+
+# ref: https://github.com/ggerganov/whisper.cpp/blob/master/models/convert-pt-to-ggml.py#L232
+def convert_tokens(name, model):
+    whisper_dir = Path(whisper.__file__).parent
+    multilingual = model.is_multilingual
+    tokenizer = (
+        whisper_dir
+        / "assets"
+        / (multilingual and "multilingual.tiktoken" or "gpt2.tiktoken")
+    )
+    if not tokenizer.is_file():
+        raise ValueError(f"Cannot find {tokenizer}")
+
+    #  import base64
+
+    with open(tokenizer, "r") as f:
+        contents = f.read()
+        #  tokens = {
+        #      base64.b64decode(token): int(rank)
+        #      for token, rank in (line.split() for line in contents.splitlines() if line)
+        #  }
+        tokens = {
+            token: int(rank)
+            for token, rank in (line.split() for line in contents.splitlines() if line)
+        }
+
+    with open(f"{name}-tokens.txt", "w") as f:
+        for t, i in tokens.items():
+            f.write(f"{t} {i}\n")
+
+
+@torch.no_grad()
+def main():
+    args = get_args()
+    name = args.model
+    print(args)
+    print(name)
+
+    opset_version = 17
+
+    if name == "distil-medium.en":
+        filename = "./distil-medium-en-original-model.bin"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/distil-whisper/distil-medium.en
+                to download original-model.bin
+                You can use the following command to do that:
+
+                wget -O distil-medium-en-original-model.bin https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/original-model.bin
+            """
+            )
+        model = whisper.load_model(filename)
+    elif name == "distil-large-v2":
+        filename = "./distil-large-v2-original-model.bin"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/distil-whisper/distil-large-v2
+                to download original-model.bin
+                You can use the following command to do that:
+
+                wget -O distil-large-v2-original-model.bin https://huggingface.co/distil-whisper/distil-large-v2/resolve/main/original-model.bin
+            """
+            )
+        model = whisper.load_model(filename)
+    elif name == "distil-large-v3":
+        filename = "./distil-large-v3-original-model.bin"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/distil-whisper/distil-large-v3-openai
+                to download model.bin
+                You can use the following command to do that:
+
+                wget -O distil-large-v3-original-model.bin https://huggingface.co/distil-whisper/distil-large-v3-openai/resolve/main/model.bin
+            """
+            )
+        model = whisper.load_model(filename)
+    elif name == "distil-large-v3.5":
+        filename = "./distil-large-v3.5-original-model.bin"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/distil-whisper/distil-large-v3.5-openai/
+                to download model.bin
+                You can use the following command to do that:
+
+                wget -O distil-large-v3.5-original-model.bin https://huggingface.co/distil-whisper/distil-large-v3.5-openai/resolve/main/model.bin
+            """
+            )
+        model = whisper.load_model(filename)
+    elif name == "distil-small.en":
+        filename = "./distil-small-en-original-model.bin"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/distil-whisper/distil-small.en
+                to download original-model.bin
+                You can use the following command to do that:
+
+                wget -O distil-small-en-original-model.bin https://huggingface.co/distil-whisper/distil-small.en/resolve/main/original-model.bin
+            """
+            )
+        model = whisper.load_model(filename)
+    elif name == "medium-aishell":
+        filename = "./medium-aishell.pt"
+        if not Path(filename).is_file():
+            raise ValueError(
+                """
+                Please go to https://huggingface.co/yuekai/icefall_asr_aishell_whisper/tree/main/exp_medium
+                to download whisper-medium-aishell1-epoch-10-avg-4.pt
+                You can use the following command to do that:
+
+                wget -O medium-aishell.pt https://huggingface.co/yuekai/icefall_asr_aishell_whisper/resolve/main/exp_medium/whisper-medium-aishell1-epoch-10-avg-4.pt
+            """
+            )
+        model = whisper.load_model(filename)
+    else:
+        model = whisper.load_model(name)
+    model.to("cpu")
+
+    num_params = sum(p.numel() for p in model.parameters())
+    num_encoder_params = sum(p.numel() for p in model.encoder.parameters())
+    num_decoder_params = sum(p.numel() for p in model.decoder.parameters())
+    print(f"{name} model parameters: {num_params} (or {num_params/1000/1000} M)")
+    print(
+        f"{name} encoder parameters: {num_encoder_params} (or {num_encoder_params/1000/1000} M)"
+    )
+    print(
+        f"{name} decoder parameters: {num_decoder_params} (or {num_decoder_params/1000/1000} M)"
+    )
+
+    convert_tokens(name=name, model=model)
+
+    # write tokens
+
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        model.is_multilingual, num_languages=model.num_languages
+    )
+    # tiny: <|startoftranscript|><|en|><|transcribe|> (50258, 50259, 50359)
+    # base: <|startoftranscript|><|en|><|transcribe|> (50258, 50259, 50359)
+    # tiny.en: <|startoftranscript|> (50257,)
+    print(tokenizer.decode(tokenizer.sot_sequence), tokenizer.sot_sequence)
+
+    # tiny: <|notimestamps|> 50363
+    # base: <|notimestamps|> 50363
+    # tiny.en: <|notimestamps|> 50362
+    print(tokenizer.decode([tokenizer.no_timestamps]), tokenizer.no_timestamps)
+
+    model.eval()
+    print(model.dims)
+    audio = torch.rand(16000 * 2)
+    audio = whisper.pad_or_trim(audio)
+    assert audio.shape == (16000 * 30,), audio.shape
+
+    if args.model in ("distil-large-v3", "distil-large-v3.5"):
+        n_mels = 128
+    elif args.model in (
+        "large",
+        "large-v3",
+        "turbo",
+    ):
+        n_mels = 128
+    else:
+        n_mels = 80
+
+    mel = (
+        whisper.log_mel_spectrogram(audio, n_mels=n_mels).to(model.device).unsqueeze(0)
+    )
+    batch_size = 1
+    assert mel.shape == (batch_size, n_mels, 30 * 100), mel.shape
+
+    encoder = AudioEncoderTensorCache(model.encoder, model.decoder)
+
+    cross_kv_pair = encoder(mel)
+    assert len(cross_kv_pair) == model.dims.n_text_layer, (
+        len(cross_kv_pair),
+        model.dims.n_text_layer,
+    )
+
+    output_names = []
+    for i in range(model.dims.n_text_layer):
+        k = f"cross_k_{i}"
+        v = f"cross_v_{i}"
+        output_names.append(k)
+        output_names.append(v)
+
+    export_sig = inspect.signature(torch.onnx.export)
+
+    kwargs = dict()
+    if "dynamo" in export_sig.parameters:
+        kwargs["dynamo"] = False
+
+    if "external_data" in export_sig.parameters:
+        kwargs["external_data"] = False
+
+    encoder_filename = f"{name}-encoder.onnx"
+    torch.onnx.export(
+        encoder,
+        mel,
+        encoder_filename,
+        opset_version=opset_version,
+        input_names=[f"{name}-mel"],
+        output_names=output_names,
+        **kwargs,
+    )
+
+    encoder_meta_data = {
+        "model_type": f"whisper-{name}",
+        "version": "1",
+        "maintainer": "k2-fsa",
+        "n_mels": model.dims.n_mels,
+        "n_audio_ctx": model.dims.n_audio_ctx,
+        "n_audio_state": model.dims.n_audio_state,
+        "n_audio_head": model.dims.n_audio_head,
+        "n_audio_layer": model.dims.n_audio_layer,
+        "n_vocab": model.dims.n_vocab,
+        "n_text_ctx": model.dims.n_text_ctx,
+        "n_text_state": model.dims.n_text_state,
+        "n_text_head": model.dims.n_text_head,
+        "n_text_layer": model.dims.n_text_layer,
+        "sot_sequence": ",".join(list(map(str, tokenizer.sot_sequence))),
+        #  "all_language_tokens": ",".join(
+        #      list(map(str, tokenizer.all_language_tokens))
+        #  ),  # a list of ids
+        #  "all_language_codes": ",".join(
+        #      tokenizer.all_language_codes
+        #  ),  # e.g., en, de, zh, fr
+        "sot": tokenizer.sot,
+        "sot_index": tokenizer.sot_sequence.index(tokenizer.sot),
+        "eot": tokenizer.eot,
+        "blank_id": tokenizer.encode(" ")[0],
+        "is_multilingual": int(model.is_multilingual),
+        "no_speech": tokenizer.no_speech,
+        "non_speech_tokens": ",".join(list(map(str, tokenizer.non_speech_tokens))),
+        "transcribe": tokenizer.transcribe,
+        "translate": tokenizer.translate,
+        "sot_prev": tokenizer.sot_prev,
+        "sot_lm": tokenizer.sot_lm,
+        "no_timestamps": tokenizer.no_timestamps,
+    }
+    print(f"encoder_meta_data: {encoder_meta_data}")
+    add_meta_data(filename=encoder_filename, meta_data=encoder_meta_data)
+
+    tokens = torch.tensor([[tokenizer.sot]], dtype=torch.int32)
+    decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
+
+    self_kv_pair = []
+    batch_size = 1
+    for i in range(model.dims.n_text_layer):
+        k = torch.zeros(batch_size, model.dims.n_text_ctx, model.dims.n_text_state)
+        v = torch.zeros(batch_size, model.dims.n_text_ctx, model.dims.n_text_state)
+        self_kv_pair.append((k, v))
+
+    offset = torch.zeros(1, dtype=torch.int32)
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    logits, this_self_kv_pair = decoder(
+        tokens,
+        self_kv_pair,
+        cross_kv_pair,
+        offset,
+        mask,
+    )
+
+    assert logits.shape == (batch_size, tokens.shape[1], model.dims.n_vocab)
+    assert len(this_self_kv_pair) == model.dims.n_text_layer, (
+        len(this_self_kv_pair),
+        model.dims.n_text_layer,
+    )
+
+    input_names = [f"{name}-tokens"]
+    for i in range(model.dims.n_text_layer):
+        k = f"{name}-self_k_{i}"
+        v = f"{name}-self_v_{i}"
+        input_names.append(k)
+        input_names.append(v)
+
+    for i in range(model.dims.n_text_layer):
+        k = f"{name}-cross_k_{i}"
+        v = f"{name}-cross_v_{i}"
+        input_names.append(k)
+        input_names.append(v)
+    input_names.append(f"{name}-offset")
+    input_names.append(f"{name}-mask")
+
+    output_names = [f"{name}-logits"]
+    for i in range(model.dims.n_text_layer):
+        k = f"{name}-this_self_k_{i}"
+        v = f"{name}-this_self_v_{i}"
+        output_names.append(k)
+        output_names.append(v)
+
+    decoder_filename = f"{name}-decoder.onnx"
+    torch.onnx.export(
+        decoder,
+        (
+            tokens,
+            self_kv_pair,
+            cross_kv_pair,
+            offset,
+            mask,
+        ),
+        decoder_filename,
+        opset_version=opset_version,
+        input_names=input_names,
+        output_names=output_names,
+        **kwargs,
+    )
+
+    if "large" in args.model:
+        decoder_external_filename = decoder_filename.split(".onnx")[0]
+        decoder_model = onnx.load(decoder_filename)
+        onnx.save(
+            decoder_model,
+            decoder_filename,
+            save_as_external_data=True,
+            all_tensors_to_one_file=True,
+            location=decoder_external_filename + ".weights",
+        )
+
+    if False:
+        # Generate int8 quantization models
+        # See https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#data-type-selection
+
+        print("Generate int8 quantization models")
+
+        encoder_filename_int8 = f"{name}-encoder.int8.onnx"
+        quantize_dynamic(
+            model_input=encoder_filename,
+            model_output=encoder_filename_int8,
+            op_types_to_quantize=["MatMul"],
+            weight_type=QuantType.QInt8,
+        )
+
+        decoder_filename_int8 = f"{name}-decoder.int8.onnx"
+        quantize_dynamic(
+            model_input=decoder_filename,
+            model_output=decoder_filename_int8,
+            op_types_to_quantize=["MatMul"],
+            weight_type=QuantType.QInt8,
+        )
+
+
+if __name__ == "__main__":
+    torch.set_num_threads(1)
+    torch.set_num_interop_threads(1)
+    try:
+        # To fix
+        # TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor
+        # See also https://github.com/k2-fsa/sherpa-onnx/issues/1764
+        from whisper.model import disable_sdpa
+
+        with disable_sdpa():
+            main()
+    except:
+        main()
diff --git a/scripts/whisper/rknn/export_rknn.py b/scripts/whisper/rknn/export_rknn.py
new file mode 100755
index 00000000..dcf2af50
--- /dev/null
+++ b/scripts/whisper/rknn/export_rknn.py
@@ -0,0 +1,152 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+
+import argparse
+import logging
+from pathlib import Path
+
+from rknn.api import RKNN
+
+logging.basicConfig(level=logging.WARNING)
+
+g_platforms = [
+    "rk3562",
+    "rk3566",
+    "rk3568",
+    "rk3576",
+    "rk3588",
+]
+
+
+def get_parser():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+
+    parser.add_argument(
+        "--target-platform",
+        type=str,
+        required=True,
+        help=f"Supported values are: {','.join(g_platforms)}",
+    )
+
+    parser.add_argument(
+        "--in-model",
+        type=str,
+        required=True,
+        help="Path to the input onnx model",
+    )
+
+    parser.add_argument(
+        "--out-model",
+        type=str,
+        required=True,
+        help="Path to the output rknn model",
+    )
+
+    return parser
+
+
+def get_meta_data(model: str):
+    import onnxruntime
+
+    session_opts = onnxruntime.SessionOptions()
+    session_opts.inter_op_num_threads = 1
+    session_opts.intra_op_num_threads = 1
+
+    m = onnxruntime.InferenceSession(
+        model,
+        sess_options=session_opts,
+        providers=["CPUExecutionProvider"],
+    )
+
+    for i in m.get_inputs():
+        print(i)
+
+    print("-----")
+
+    for i in m.get_outputs():
+        print(i)
+    print()
+
+    meta = m.get_modelmeta().custom_metadata_map
+    s = ""
+    sep = ""
+    for key, value in meta.items():
+        s = s + sep + f"{key}={value}"
+        sep = ";"
+    assert len(s) < 1024, len(s)
+
+    print("len(s)", len(s), s)
+
+    return s
+
+
+def export_rknn(rknn, filename):
+    ret = rknn.export_rknn(filename)
+    if ret != 0:
+        exit(f"Export rknn model to {filename} failed!")
+
+
+def init_model(filename: str, target_platform: str, custom_string=None):
+    rknn = RKNN(verbose=False)
+
+    rknn.config(
+        optimization_level=0,
+        target_platform=target_platform,
+        custom_string=custom_string,
+    )
+    if not Path(filename).is_file():
+        exit(f"{filename} does not exist")
+
+    ret = rknn.load_onnx(model=filename)
+    if ret != 0:
+        exit(f"Load model {filename} failed!")
+
+    ret = rknn.build(do_quantization=False)
+    if ret != 0:
+        exit(f"Build model {filename} failed!")
+
+    return rknn
+
+
+class RKNNModel:
+    def __init__(
+        self,
+        model: str,
+        target_platform: str,
+    ):
+        meta = get_meta_data(model)
+        print(meta)
+
+        self.model = init_model(
+            model,
+            target_platform=target_platform,
+            custom_string=meta,
+        )
+
+    def export_rknn(self, model):
+        export_rknn(self.model, model)
+
+    def release(self):
+        self.model.release()
+
+
+def main():
+    args = get_parser().parse_args()
+    print(vars(args))
+
+    model = RKNNModel(
+        model=args.in_model,
+        target_platform=args.target_platform,
+    )
+
+    model.export_rknn(
+        model=args.out_model,
+    )
+
+    model.release()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/generate_decoder_data.py b/scripts/whisper/rknn/generate_decoder_data.py
new file mode 100755
index 00000000..b467a56f
--- /dev/null
+++ b/scripts/whisper/rknn/generate_decoder_data.py
@@ -0,0 +1,207 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import glob
+from dataclasses import dataclass
+from pathlib import Path
+from typing import List, Tuple
+
+import numpy as np
+import torch
+import whisper
+
+from export_onnx import AudioEncoderTensorCache, TextDecoderTensorCache, causal_mask_1d
+from test_torch import compute_feat
+
+# we need to transpose cross_kv to (1, 384, 1500) when using it as an input
+# we need to transpose self_kv to (1, 384, 448) when using it as an input
+
+
+def deepcopy_pair(pair):
+    return [(a.clone(), b.clone()) for a, b in pair]
+
+
+def to_file(tensor, filename, debug):
+    if debug:
+        print(filename, tensor.shape, tensor.dtype)
+    tensor.numpy().tofile(filename)
+
+
+@dataclass
+class DecoderInput:
+    tokens: torch.Tensor
+    self_kv_pair: List[Tuple[torch.Tensor, torch.Tensor]]
+    cross_kv_pair: List[Tuple[torch.Tensor, torch.Tensor]]
+    offset: torch.Tensor
+    mask: torch.Tensor
+
+    def save_to_file(self, prefix, debug):
+        ans = []
+        to_file(self.tokens.to(torch.int32), f"{prefix}-tokens.raw", debug)
+        ans.append(f"{prefix}-tokens.raw")
+
+        for i, (k, v) in enumerate(self.self_kv_pair):
+            to_file(k.permute(0, 2, 1), f"{prefix}-self_k_{i}.raw", debug)
+            ans.append(f"{prefix}-self_k_{i}.raw")
+
+            to_file(v.permute(0, 2, 1), f"{prefix}-self_v_{i}.raw", debug)
+            ans.append(f"{prefix}-self_v_{i}.raw")
+
+        for i, (k, v) in enumerate(self.cross_kv_pair):
+            to_file(k.permute(0, 2, 1), f"{prefix}-cross_k_{i}.raw", debug)
+            ans.append(f"{prefix}-cross_k_{i}.raw")
+
+            to_file(v.permute(0, 2, 1), f"{prefix}-cross_v_{i}.raw", debug)
+            ans.append(f"{prefix}-cross_v_{i}.raw")
+
+        to_file(self.offset.to(torch.int32), f"{prefix}-offset.raw", debug)
+        ans.append(f"{prefix}-offset.raw")
+
+        to_file(self.mask.to(torch.int32), f"{prefix}-mask.raw", debug)
+        ans.append(f"{prefix}-mask.raw")
+
+        return ans
+
+
+def process(model, tokenizer, w):
+    mel = compute_feat(w)
+
+    encoder = AudioEncoderTensorCache(model.encoder, model.decoder)
+    cross_kv_pair = encoder(mel)
+
+    # cross_kv_pair[0][0]: (1, 1500, 384)
+    # cross_kv_pair[0][1]: (1, 1500, 384)
+
+    ans = []
+
+    decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
+
+    batch_size = 1
+    self_kv_pair = []
+    for i in range(model.dims.n_text_layer):
+        k = torch.zeros(batch_size, model.dims.n_text_ctx, model.dims.n_text_state)
+        v = torch.zeros(batch_size, model.dims.n_text_ctx, model.dims.n_text_state)
+
+        self_kv_pair.append((k, v))
+    # self_kv_pair[0][0]: (1, 448, 384)
+    # self_kv_pair[0][1]: (1, 448, 384)
+
+    offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.sot]])
+
+    ans.append(
+        DecoderInput(
+            tokens=tokens.clone(),
+            self_kv_pair=deepcopy_pair(self_kv_pair),
+            cross_kv_pair=deepcopy_pair(cross_kv_pair),
+            offset=offset.clone(),
+            mask=mask.clone(),
+        )
+    )
+
+    logits, this_self_kv_pair = decoder(
+        tokens,
+        self_kv_pair,
+        cross_kv_pair,
+        offset,
+        mask,
+    )
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    offset += 1
+
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.no_timestamps]])
+    logits, this_self_kv_pair = decoder(
+        tokens, self_kv_pair, cross_kv_pair, offset, mask
+    )
+
+    ans.append(
+        DecoderInput(
+            tokens=tokens.clone(),
+            self_kv_pair=deepcopy_pair(self_kv_pair),
+            cross_kv_pair=deepcopy_pair(cross_kv_pair),
+            offset=offset.clone(),
+            mask=mask.clone(),
+        )
+    )
+
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    assert logits.shape == (1, tokens.shape[1], model.dims.n_vocab)
+
+    print("logits.shape", logits.shape)  # (1, 3, 51864)
+    idx = logits[0, -1].argmax().item()
+
+    steps = 0
+    results = []
+    while idx != tokenizer.eot and steps < 50:
+        results.append(idx)
+        tokens = torch.tensor([[results[-1]]])
+
+        offset += 1
+        mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+        logits, this_self_kv_pair = decoder(
+            tokens, self_kv_pair, cross_kv_pair, offset, mask
+        )
+
+        ans.append(
+            DecoderInput(
+                tokens=tokens.clone(),
+                self_kv_pair=deepcopy_pair(self_kv_pair),
+                cross_kv_pair=deepcopy_pair(cross_kv_pair),
+                offset=offset.clone(),
+                mask=mask.clone(),
+            )
+        )
+
+        for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+            k_cache[:, offset : offset + 1] = k
+            v_cache[:, offset : offset + 1] = v
+
+        idx = logits[0, -1].argmax().item()
+        steps += 1
+
+    print(results)
+    print(tokenizer.decode(results))
+    return ans
+
+
+@torch.no_grad()
+def main():
+    model = whisper.load_model("tiny.en")
+    model.eval()
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        model.is_multilingual, num_languages=model.num_languages
+    )
+
+    wav_files = glob.glob("*.wav")
+    features_name = []
+    for w in wav_files:
+        decoder_input_list = process(model, tokenizer, w)
+        print(len(decoder_input_list))
+
+        name = Path(w).stem
+        files = [
+            d.save_to_file(f"{name}-decoder-iter-{k:02d}", k == 0)
+            for k, d in enumerate(decoder_input_list)
+        ]
+
+        features_name.extend(files)
+
+    with open("decoder-input-list.txt", "w") as f:
+        for line in features_name:
+            line = " ".join(line)
+            f.write(f"{line}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/generate_encoder_data.py b/scripts/whisper/rknn/generate_encoder_data.py
new file mode 100755
index 00000000..e2e2c2f1
--- /dev/null
+++ b/scripts/whisper/rknn/generate_encoder_data.py
@@ -0,0 +1,36 @@
+#!/usr/bin/env python3
+# Copyright (c)  2025  Xiaomi Corporation
+
+import glob
+from pathlib import Path
+
+import numpy as np
+
+from test_torch import compute_feat
+
+
+@torch.no_grad()
+def main():
+    wav_files = glob.glob("*.wav")
+    features_name = []
+    for w in wav_files:
+        f = compute_feat(w)
+
+        # Note: qnn expects (1, 3000, 80) as input
+        f = f.permute(0, 2, 1)  # (1, 80, 3000) -> (1, 3000, 80)
+
+        f = f.numpy()
+        print(w, f.shape)
+        name = Path(w).stem
+
+        s = f"encoder-input-{name}.raw"
+        f.tofile(s)
+        features_name.append(s)
+
+    with open("encoder-input-list.txt", "w") as f:
+        for line in features_name:
+            f.write(f"{line}\n")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/notes.md b/scripts/whisper/rknn/notes.md
new file mode 100644
index 00000000..a4148837
--- /dev/null
+++ b/scripts/whisper/rknn/notes.md
@@ -0,0 +1,51 @@
+# Note
+
+## Encoder
+```
+=========./tiny.en-encoder.onnx==========
+NodeArg(name='tiny.en-mel', type='tensor(float)', shape=[1, 80, 3000])
+-----
+NodeArg(name='cross_k_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_3', type='tensor(float)', shape=[1, 1500, 384])
+```
+
+## Decoder
+
+```
+=========./tiny.en-decoder.onnx==========
+NodeArg(name='tiny.en-tokens', type='tensor(int32)', shape=[1, 1])
+NodeArg(name='tiny.en-self_k_0', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_0', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_1', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_1', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_2', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_2', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_3', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_3', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-cross_k_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-offset', type='tensor(int64)', shape=[1])
+NodeArg(name='tiny.en-mask', type='tensor(float)', shape=[448])
+-----
+NodeArg(name='tiny.en-logits', type='tensor(float)', shape=['Casttiny.en-logits_dim_0', 'Casttiny.en-logits_dim_1', 51864])
+NodeArg(name='tiny.en-this_self_k_0', type='tensor(float)', shape=[1, 'MatMultiny.en-this_self_k_0_dim_1', 384])
+NodeArg(name='tiny.en-this_self_v_0', type='tensor(float)', shape=[1, 'MatMultiny.en-this_self_k_0_dim_1', 384])
+NodeArg(name='tiny.en-this_self_k_1', type='tensor(float)', shape=['MatMultiny.en-this_self_k_1_dim_0', 'MatMultiny.en-this_self_k_1_dim_1', 384])
+NodeArg(name='tiny.en-this_self_v_1', type='tensor(float)', shape=['MatMultiny.en-this_self_k_1_dim_0', 'MatMultiny.en-this_self_k_1_dim_1', 384])
+NodeArg(name='tiny.en-this_self_k_2', type='tensor(float)', shape=['MatMultiny.en-this_self_k_2_dim_0', 'MatMultiny.en-this_self_k_2_dim_1', 384])
+NodeArg(name='tiny.en-this_self_v_2', type='tensor(float)', shape=['MatMultiny.en-this_self_k_2_dim_0', 'MatMultiny.en-this_self_k_2_dim_1', 384])
+NodeArg(name='tiny.en-this_self_k_3', type='tensor(float)', shape=['MatMultiny.en-this_self_k_3_dim_0', 'MatMultiny.en-this_self_k_3_dim_1', 384])
+NodeArg(name='tiny.en-this_self_v_3', type='tensor(float)', shape=['MatMultiny.en-this_self_k_3_dim_0', 'MatMultiny.en-this_self_k_3_dim_1', 384])
+```
diff --git a/scripts/whisper/rknn/test_on_rk3588_board.py b/scripts/whisper/rknn/test_on_rk3588_board.py
new file mode 100755
index 00000000..6fb74608
--- /dev/null
+++ b/scripts/whisper/rknn/test_on_rk3588_board.py
@@ -0,0 +1,343 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+"""
+usage:
+
+./test_on_rk3588_board.py  --encoder ./base-encoder.rknn --decoder ./base-decoder.rknn --tokens ./base-tokens.txt --wav ./en-16k.wav
+
+./test_on_rk3588_board.py  --encoder ./base.en-encoder.rknn --decoder ./base.en-decoder.rknn --tokens ./base.en-tokens.txt --wav ./en-16k.wav
+"""
+
+try:
+    from rknnlite.api import RKNNLite
+except:
+    print("Please run this file on your board (linux + aarch64 + npu)")
+    print("You need to install rknn_toolkit_lite2")
+    print(
+        " from https://github.com/airockchip/rknn-toolkit2/tree/master/rknn-toolkit-lite2/packages"
+    )
+    print(
+        "https://github.com/airockchip/rknn-toolkit2/blob/v2.1.0/rknn-toolkit-lite2/packages/rknn_toolkit_lite2-2.1.0-cp310-cp310-linux_aarch64.whl"
+    )
+    print("is known to work")
+    raise
+
+import argparse
+import base64
+import time
+from pathlib import Path
+from typing import List, Tuple
+
+import kaldi_native_fbank as knf
+import numpy as np
+import soundfile as sf
+import torch
+
+
+def get_args():
+    parser = argparse.ArgumentParser()
+    parser.add_argument(
+        "--encoder",
+        type=str,
+        required=True,
+        help="Path to the encoder",
+    )
+
+    parser.add_argument(
+        "--decoder",
+        type=str,
+        required=True,
+        help="Path to the decoder",
+    )
+
+    parser.add_argument(
+        "--tokens",
+        type=str,
+        required=True,
+        help="Path to the tokens",
+    )
+
+    parser.add_argument(
+        "--wav",
+        type=str,
+        required=True,
+        help="Path to the test wav",
+    )
+
+    return parser.parse_args()
+
+
+def causal_mask_1d(n: int, L: int):
+    """
+    Returns a 1-D int mask of shape (L,) with:
+      0 -> allowed
+      1 -> masked (will be converted to -inf later)
+    """
+    mask = np.ones((L,), dtype=np.int32)
+    if n > 0:
+        mask[:n] = 0
+    return mask
+
+
+def load_audio(filename: str) -> Tuple[np.ndarray, int]:
+    data, sample_rate = sf.read(
+        filename,
+        always_2d=True,
+        dtype="float32",
+    )
+    data = data[:, 0]  # use only the first channel
+
+    samples = np.ascontiguousarray(data)
+    return samples, sample_rate
+
+
+def compute_features(samples: np.ndarray, dim: int = 80) -> np.ndarray:
+    """
+    Returns:
+      Return a 1-D float32 tensor of shape (1, 80, 3000) containing the features.
+    """
+    features = []
+    opts = knf.WhisperFeatureOptions()
+    opts.dim = dim
+    online_whisper_fbank = knf.OnlineWhisperFbank(opts)
+    online_whisper_fbank.accept_waveform(16000, samples)
+    online_whisper_fbank.input_finished()
+    for i in range(online_whisper_fbank.num_frames_ready):
+        f = online_whisper_fbank.get_frame(i)
+        f = torch.from_numpy(f)
+        features.append(f)
+
+    features = torch.stack(features)
+
+    log_spec = torch.clamp(features, min=1e-10).log10()
+    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
+    mel = (log_spec + 4.0) / 4.0
+    # mel (T, 80)
+
+    # We pad 1500 frames at the end so that it is able to detect eot
+    # You can use another value instead of 1500.
+    mel = torch.nn.functional.pad(mel, (0, 0, 0, 1500), "constant", 0)
+    # Note that if it throws for a multilingual model,
+    # please use a larger value, say 300
+
+    target = 3000
+    if mel.shape[0] > target:
+        # -50 so that there are some zero tail paddings.
+        mel = mel[: target - 50]
+        mel = torch.nn.functional.pad(mel, (0, 0, 0, 50), "constant", 0)
+    elif mel.shape[0] < target:
+        mel = torch.nn.functional.pad(
+            mel, (0, 0, 0, target - mel.shape[0]), "constant", 0
+        )
+
+    mel = mel.t().unsqueeze(0)
+
+    return mel
+
+
+def load_tokens(filename):
+    tokens = dict()
+    with open(filename, "r") as f:
+        for line in f:
+            t, i = line.split()
+            tokens[int(i)] = t
+    return tokens
+
+
+def init_model(filename, target_platform="rk3588"):
+
+    if not Path(filename).is_file():
+        exit(f"{filename} does not exist")
+
+    rknn_lite = RKNNLite(verbose=False)
+    ret = rknn_lite.load_rknn(path=filename)
+    if ret != 0:
+        exit(f"Load model {filename} failed!")
+
+    ret = rknn_lite.init_runtime(core_mask=RKNNLite.NPU_CORE_0)
+    if ret != 0:
+        exit(f"Failed to init rknn runtime for {filename}")
+    return rknn_lite
+
+
+class RKNNModel:
+    def __init__(
+        self,
+        encoder: str,
+        decoder: str,
+        sot_sequence: List[int],
+        eot: int,
+        n_text_layer: int,
+        n_text_ctx: int,
+        n_text_state: int,
+        target_platform="rk3588",
+    ):
+        self.sot_sequence = sot_sequence
+        self.eot = eot
+        self.n_text_layer = n_text_layer
+        self.n_text_ctx = n_text_ctx
+        self.n_text_state = n_text_state
+
+        print("sot_sequence", self.sot_sequence)
+        print("eot", self.eot)
+
+        self.encoder = init_model(encoder)
+        self.decoder = init_model(decoder)
+
+    def release(self):
+        self.encoder.release()
+        self.decoder.release()
+
+    def run_encoder(self, x: np.ndarray):
+        """
+        Args:
+          x: (1, 80, 3000), np.float32
+        Returns:
+          cross_kv:
+           - (k, v) for layer 0
+           - (k, v) for layer 1
+           - (k, v) for layer 2
+           - (k, v) for layer 3
+        """
+        out = self.encoder.inference(inputs=[x.numpy()])
+        print("after running encoder", len(out))
+        return out
+
+    def get_self_cache(self) -> List[np.ndarray]:
+        self_cache = []
+        batch_size = 1
+        for i in range(self.n_text_layer):
+            k = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            v = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            self_cache.extend([k, v])
+        return self_cache
+
+    def run_decoder(self, tokens: np.ndarray, self_kv, cross_kv, offset, mask):
+        """
+        Args:
+          tokens: (1, 1), np.int32
+          offset: (1,), np.int32
+          mask: (model.n_text_ctx,), np.int32
+        Returns:
+          logit: (1, 1, vocab_size)
+          this_self_kv
+        """
+        return self.decoder.inference(
+            inputs=[tokens] + self_kv + cross_kv + [offset, mask]
+        )
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+
+    id2token = load_tokens(args.tokens)
+
+    if ".en" in args.encoder:
+        print("here", args.encoder)
+        sot_sequence = [50257, 50362]
+        eot = 50256
+    else:
+        print("not here", args.encoder)
+        sot_sequence = [50258, 50259, 50359, 50363]
+        eot = 50257
+
+    if "tiny" in args.encoder:
+        n_text_layer = 4
+        n_text_ctx = 448
+        n_text_state = 384
+    elif "base" in args.encoder:
+        n_text_layer = 6
+        n_text_ctx = 448
+        n_text_state = 512
+    elif "small" in args.encoder:
+        n_text_layer = 12
+        n_text_ctx = 448
+        n_text_state = 768
+    elif "medium" in args.encoder:
+        n_text_layer = 24
+        n_text_ctx = 448
+        n_text_state = 1024
+    else:
+        assert False, f"Unsupported encoder {args.encoder}"
+
+    model = RKNNModel(
+        encoder=args.encoder,
+        decoder=args.decoder,
+        sot_sequence=sot_sequence,
+        eot=eot,
+        n_text_layer=n_text_layer,
+        n_text_ctx=n_text_ctx,
+        n_text_state=n_text_state,
+    )
+
+    for i in range(1):
+        test(model, id2token)
+
+
+def test(model, id2token):
+
+    start = time.time()
+    samples, sample_rate = load_audio("./en-16k.wav")
+    assert sample_rate == 16000, sample_rate
+
+    features = compute_features(samples)
+    print(features.shape)
+    cross_kv = model.run_encoder(features)
+
+    self_kv = model.get_self_cache()
+
+    offset = np.array([0], dtype=np.int32)
+    for t in model.sot_sequence:
+        token = np.array([[t]], dtype=np.int32)  # sot
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx)
+        print(t, model.sot_sequence, token, mask.shape, len(cross_kv), len(self_kv))
+
+        out = model.run_decoder(
+            tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask
+        )
+
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        offset += 1
+
+    idx = out[0][0, 0].argmax()
+
+    eot = model.eot
+
+    ans = []
+
+    while idx != eot and offset.item() < 100:
+        ans.append(idx)
+        token = np.array([[idx]], dtype=np.int32)
+
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx)
+
+        out = model.run_decoder(
+            tokens=token, self_kv=self_kv, cross_kv=cross_kv, offset=offset, mask=mask
+        )
+
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        offset += 1
+        idx = out[0][0, 0].argmax()
+
+    print(ans)
+
+    s = b""
+    for i in ans:
+        if i in id2token:
+            s += base64.b64decode(id2token[i])
+
+    print(s.decode().strip())
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/test_onnx.py b/scripts/whisper/rknn/test_onnx.py
new file mode 100755
index 00000000..18a95627
--- /dev/null
+++ b/scripts/whisper/rknn/test_onnx.py
@@ -0,0 +1,180 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from typing import List, Tuple
+
+import numpy as np
+import onnxruntime as ort
+import torch
+import whisper
+
+from test_torch import compute_feat
+from export_onnx import causal_mask_1d, get_args
+
+
+class OnnxModel:
+    def __init__(
+        self,
+        encoder: str,
+        decoder: str,
+    ):
+        session_opts = ort.SessionOptions()
+        session_opts.inter_op_num_threads = 1
+        session_opts.intra_op_num_threads = 4
+
+        self.session_opts = session_opts
+
+        self.init_encoder(encoder)
+        self.init_decoder(decoder)
+
+    def init_encoder(self, encoder: str):
+        self.encoder = ort.InferenceSession(
+            encoder,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+
+        self.encoder_input_names = []
+        self.encoder_output_names = []
+
+        print(f"-----{encoder}-----")
+        print(f"----input----")
+        for i in self.encoder.get_inputs():
+            print(i)
+            self.encoder_input_names.append(i.name)
+
+        print("-----output-----")
+
+        for i in self.encoder.get_outputs():
+            print(i)
+            self.encoder_output_names.append(i.name)
+
+        meta = self.encoder.get_modelmeta().custom_metadata_map
+        self.n_text_layer = int(meta["n_text_layer"])
+        self.n_text_ctx = int(meta["n_text_ctx"])
+        self.n_text_state = int(meta["n_text_state"])
+
+    def init_decoder(self, decoder: str):
+        self.decoder = ort.InferenceSession(
+            decoder,
+            sess_options=self.session_opts,
+            providers=["CPUExecutionProvider"],
+        )
+
+        self.decoder_input_names = []
+        self.decoder_output_names = []
+
+        print(f"-----{decoder}-----")
+        print(f"----input----")
+        for i in self.decoder.get_inputs():
+            print(i)
+            self.decoder_input_names.append(i.name)
+
+        print("-----output-----")
+
+        for i in self.decoder.get_outputs():
+            print(i)
+            self.decoder_output_names.append(i.name)
+
+    def run_encoder(
+        self,
+        mel: np.ndarray,
+    ) -> List[np.ndarray]:
+        cross_kv = self.encoder.run(
+            self.encoder_output_names,
+            {
+                self.encoder.get_inputs()[0].name: mel,
+            },
+        )
+        return cross_kv
+
+    def run_decoder(self, inputs: List[np.ndarray]) -> List[np.ndarray]:
+        feed = {
+            self.decoder.get_inputs()[i].name: inputs[i] for i in range(len(inputs))
+        }
+
+        out = self.decoder.run(
+            self.decoder_output_names,
+            feed,
+        )
+        return out
+
+    def get_self_cache(self) -> List[np.ndarray]:
+        self_cache = []
+        batch_size = 1
+        for i in range(self.n_text_layer):
+            k = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            v = np.zeros(
+                (batch_size, self.n_text_ctx, self.n_text_state), dtype=np.float32
+            )
+            self_cache.extend([k, v])
+        return self_cache
+
+
+def main():
+    args = get_args()
+    print(vars(args))
+
+    torch_model = whisper.load_model(args.model)
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        torch_model.is_multilingual, num_languages=torch_model.num_languages
+    )
+
+    mel = compute_feat("./en-16k.wav").numpy()
+    print(mel.shape)  # (1, 80. 3000)
+    model = OnnxModel(f"./{args.model}-encoder.onnx", f"./{args.model}-decoder.onnx")
+
+    sot_sequence = list(tokenizer.sot_sequence) + [tokenizer.no_timestamps]
+
+    # tiny.en: [50257, 50362]
+    # tiny: [50258, 50259, 50359, 50363]
+    print("sot sequence", sot_sequence)
+
+    cross_kv = model.run_encoder(mel)
+    print(len(cross_kv))  # 8
+
+    self_kv = model.get_self_cache()
+
+    # tiny.en: 50256
+    # tiny: 50257
+    eot = tokenizer.eot
+    print("eot", eot)
+
+    offset = np.array([0], dtype=np.int32)
+    for t in sot_sequence:
+        token = np.array([[t]], dtype=np.int32)  # sot
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx).numpy()
+
+        out = model.run_decoder([token] + self_kv + cross_kv + [offset, mask])
+
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        offset += 1
+
+    idx = out[0][0, 0].argmax()
+
+    ans = []
+
+    while idx != eot and offset.item() < 200:
+        ans.append(idx)
+        token = np.array([[idx]], dtype=np.int32)  # no_timestamps
+        for i in range(1, len(out)):
+            self_kv[i - 1][:, offset.item() : offset.item() + 1, :] = out[i]
+
+        mask = causal_mask_1d(offset.item(), model.n_text_ctx).numpy()
+
+        out = model.run_decoder([token] + self_kv + cross_kv + [offset, mask])
+        idx = out[0][0, 0].argmax()
+
+        offset += 1
+
+    print(ans)
+    text = "".join(tokenizer.decode(ans)).strip()
+    print(text)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/whisper/rknn/test_qnn.py b/scripts/whisper/rknn/test_qnn.py
new file mode 100755
index 00000000..f14a8e5f
--- /dev/null
+++ b/scripts/whisper/rknn/test_qnn.py
@@ -0,0 +1,117 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from typing import Tuple
+
+import numpy as np
+import soundfile as sf
+import torch
+import whisper
+
+from export_onnx import AudioEncoderTensorCache, TextDecoderTensorCache, causal_mask_1d
+from test_torch import compute_feat
+
+
+@torch.no_grad()
+def main():
+    mel = compute_feat("en.wav")
+
+    model = whisper.load_model("tiny.en")
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        model.is_multilingual, num_languages=model.num_languages
+    )
+
+    model.eval()
+
+    cross_kv_pair = []
+    for i in range(4):
+        k = features = np.fromfile(f"./cross_k_{i}.raw", dtype=np.float32).reshape(
+            1, 1500, 384
+        )
+        v = features = np.fromfile(f"./cross_v_{i}.raw", dtype=np.float32).reshape(
+            1, 1500, 384
+        )
+
+        k = torch.from_numpy(k)
+        v = torch.from_numpy(v)
+
+        cross_kv_pair.append((k, v))
+
+    n_audio = mel.shape[0]
+
+    decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
+
+    self_kv_pair = []
+    for i in range(model.dims.n_text_layer):
+        k = torch.zeros(n_audio, model.dims.n_text_ctx, model.dims.n_text_state)
+        v = torch.zeros(n_audio, model.dims.n_text_ctx, model.dims.n_text_state)
+        self_kv_pair.append((k, v))
+
+    offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
+
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.sot]])
+    logits, this_self_kv_pair = decoder(
+        tokens,
+        self_kv_pair,
+        cross_kv_pair,
+        offset,
+        mask,
+    )
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    offset += 1
+
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.no_timestamps]])
+    logits, this_self_kv_pair = decoder(
+        tokens, self_kv_pair, cross_kv_pair, offset, mask
+    )
+
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    assert logits.shape == (n_audio, tokens.shape[1], model.dims.n_vocab)
+
+    print("logits.shape", logits.shape)  # (1, 3, 51864)
+    idx = logits[0, -1].argmax().item()
+
+    steps = 0
+    results = []
+    while idx != tokenizer.eot and steps < 50:
+        results.append(idx)
+        tokens = torch.tensor([[results[-1]]])
+
+        offset += 1
+        mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+        logits, this_self_kv_pair = decoder(
+            tokens, self_kv_pair, cross_kv_pair, offset, mask
+        )
+
+        for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+            k_cache[:, offset : offset + 1] = k
+            v_cache[:, offset : offset + 1] = v
+
+        idx = logits[0, -1].argmax().item()
+        steps += 1
+
+    print(results)
+    print(tokenizer.decode(results))
+
+
+if __name__ == "__main__":
+    torch.set_num_threads(1)
+    torch.set_num_interop_threads(1)
+    # To fix
+    # TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor
+    # See also https://github.com/k2-fsa/sherpa-onnx/issues/1764
+    from whisper.model import disable_sdpa
+
+    with disable_sdpa():
+        main()
diff --git a/scripts/whisper/rknn/test_torch.py b/scripts/whisper/rknn/test_torch.py
new file mode 100755
index 00000000..812b2e4f
--- /dev/null
+++ b/scripts/whisper/rknn/test_torch.py
@@ -0,0 +1,144 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+from typing import Tuple
+
+import numpy as np
+import soundfile as sf
+import torch
+import whisper
+
+from export_onnx import (
+    AudioEncoderTensorCache,
+    TextDecoderTensorCache,
+    causal_mask_1d,
+    get_args,
+)
+
+
+def load_audio(filename: str) -> Tuple[np.ndarray, int]:
+    data, sample_rate = sf.read(
+        filename,
+        always_2d=True,
+        dtype="float32",
+    )
+    data = data[:, 0]  # use only the first channel
+    samples = np.ascontiguousarray(data)
+    return samples, sample_rate
+
+
+def compute_feat(filename: str):
+    wave, sample_rate = load_audio(filename)
+    if sample_rate != 16000:
+        import librosa
+
+        wave = librosa.resample(wave, orig_sr=sample_rate, target_sr=16000)
+        sample_rate = 16000
+
+    audio = whisper.pad_or_trim(wave)
+    assert audio.shape == (16000 * 30,), audio.shape
+
+    mel = whisper.log_mel_spectrogram(audio, n_mels=80).unsqueeze(0)
+    assert mel.shape == (1, 80, 3000), mel.shape
+
+    return mel
+
+
+@torch.no_grad()
+def main():
+    args = get_args()
+    print(vars(args))
+    mel = compute_feat("en.wav")
+
+    model = whisper.load_model(args.model, device="cpu")
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        model.is_multilingual, num_languages=model.num_languages
+    )
+
+    model.eval()
+
+    encoder = AudioEncoderTensorCache(model.encoder, model.decoder)
+
+    cross_kv_pair = encoder(mel)
+
+    n_audio = mel.shape[0]
+
+    decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
+
+    self_kv_pair = []
+    for i in range(model.dims.n_text_layer):
+        k = torch.zeros(n_audio, model.dims.n_text_ctx, model.dims.n_text_state)
+        v = torch.zeros(n_audio, model.dims.n_text_ctx, model.dims.n_text_state)
+        self_kv_pair.append((k, v))
+
+    offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
+
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.sot]])
+    logits, this_self_kv_pair = decoder(
+        tokens,
+        self_kv_pair,
+        cross_kv_pair,
+        offset,
+        mask,
+    )
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    offset += 1
+
+    mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+    tokens = torch.tensor([[tokenizer.no_timestamps]])
+    logits, this_self_kv_pair = decoder(
+        tokens, self_kv_pair, cross_kv_pair, offset, mask
+    )
+
+    for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+        k_cache[:, offset : offset + 1] = k
+        v_cache[:, offset : offset + 1] = v
+
+    assert logits.shape == (n_audio, tokens.shape[1], model.dims.n_vocab)
+
+    print("logits.shape", logits.shape)  # (1, 3, 51864)
+    idx = logits[0, -1].argmax().item()
+
+    steps = 0
+    results = []
+    while idx != tokenizer.eot and steps < 50:
+        results.append(idx)
+        tokens = torch.tensor([[results[-1]]])
+
+        offset += 1
+        mask = causal_mask_1d(offset.item(), model.dims.n_text_ctx)
+
+        logits, this_self_kv_pair = decoder(
+            tokens, self_kv_pair, cross_kv_pair, offset, mask
+        )
+
+        for (k_cache, v_cache), (k, v) in zip(self_kv_pair, this_self_kv_pair):
+            k_cache[:, offset : offset + 1] = k
+            v_cache[:, offset : offset + 1] = v
+
+        idx = logits[0, -1].argmax().item()
+        steps += 1
+
+    print(results)
+    print(tokenizer.decode(results))
+
+
+if __name__ == "__main__":
+    torch.set_num_threads(1)
+    torch.set_num_interop_threads(1)
+    try:
+        # To fix
+        # TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor
+        # See also https://github.com/k2-fsa/sherpa-onnx/issues/1764
+        from whisper.model import disable_sdpa
+
+        with disable_sdpa():
+            main()
+    except:
+        main()
diff --git a/scripts/whisper/rknn/tiny-en-onnx-info.md b/scripts/whisper/rknn/tiny-en-onnx-info.md
new file mode 100644
index 00000000..68551d5a
--- /dev/null
+++ b/scripts/whisper/rknn/tiny-en-onnx-info.md
@@ -0,0 +1,53 @@
+# tiny.en encoder
+
+```
+----input----
+NodeArg(name='tiny.en-mel', type='tensor(float)', shape=[1, 80, 3000])
+
+-----output-----
+NodeArg(name='cross_k_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_k_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='cross_v_3', type='tensor(float)', shape=[1, 1500, 384])
+```
+
+# tiny.en decoder
+
+```
+----input----
+NodeArg(name='tiny.en-tokens', type='tensor(int32)', shape=[1, 1])
+NodeArg(name='tiny.en-self_k_0', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_0', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_1', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_1', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_2', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_2', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_k_3', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-self_v_3', type='tensor(float)', shape=[1, 448, 384])
+NodeArg(name='tiny.en-cross_k_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_0', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_1', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_2', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_k_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-cross_v_3', type='tensor(float)', shape=[1, 1500, 384])
+NodeArg(name='tiny.en-offset', type='tensor(int32)', shape=[1])
+NodeArg(name='tiny.en-mask', type='tensor(int32)', shape=[448])
+
+-----output-----
+
+NodeArg(name='tiny.en-logits', type='tensor(float)', shape=[1, 1, 51864])
+NodeArg(name='tiny.en-this_self_k_0', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_v_0', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_k_1', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_v_1', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_k_2', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_v_2', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_k_3', type='tensor(float)', shape=[1, 1, 384])
+NodeArg(name='tiny.en-this_self_v_3', type='tensor(float)', shape=[1, 1, 384])
+```
diff --git a/scripts/whisper/test_torch.py b/scripts/whisper/test_torch.py
new file mode 100755
index 00000000..1f8e5851
--- /dev/null
+++ b/scripts/whisper/test_torch.py
@@ -0,0 +1,127 @@
+#!/usr/bin/env python3
+# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+
+import torch
+
+
+from export_onnx import AudioEncoderTensorCache, TextDecoderTensorCache
+from test import load_audio
+
+import whisper
+
+
+@torch.no_grad()
+def main():
+    wave, sample_rate = load_audio("en.wav")
+    if sample_rate != 16000:
+        import librosa
+
+        wave = librosa.resample(wave, orig_sr=sample_rate, target_sr=16000)
+        sample_rate = 16000
+
+    audio = whisper.pad_or_trim(wave)
+    assert audio.shape == (16000 * 30,), audio.shape
+
+    mel = whisper.log_mel_spectrogram(audio, n_mels=80).unsqueeze(0)
+    assert mel.shape == (1, 80, 3000), mel.shape
+
+    model = whisper.load_model("tiny.en")
+    tokenizer = whisper.tokenizer.get_tokenizer(
+        model.is_multilingual, num_languages=model.num_languages
+    )
+
+    model.eval()
+
+    encoder = AudioEncoderTensorCache(model.encoder, model.decoder)
+
+    n_layer_cross_k, n_layer_cross_v = encoder(mel)
+    print("n_layer_cross_k", n_layer_cross_k.shape)  # (4, 1, 1500, 384)
+    print("n_layer_cross_v", n_layer_cross_v.shape)  # (4, 1, 1500, 384)
+
+    n_audio = mel.shape[0]
+    tokens = torch.tensor([[tokenizer.sot, tokenizer.sot, tokenizer.sot]] * n_audio).to(
+        mel.device
+    )  # [n_audio, 3]
+
+    decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
+
+    n_layer_self_k_cache = torch.zeros(
+        (
+            len(model.decoder.blocks),
+            n_audio,
+            model.dims.n_text_ctx,
+            model.dims.n_text_state,
+        ),
+        device=mel.device,
+    )
+    n_layer_self_v_cache = torch.zeros(
+        (
+            len(model.decoder.blocks),
+            n_audio,
+            model.dims.n_text_ctx,
+            model.dims.n_text_state,
+        ),
+        device=mel.device,
+    )
+    offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
+    logits, n_layer_self_k_cache, n_layer_self_v_cache = decoder(
+        tokens,
+        n_layer_self_k_cache,
+        n_layer_self_v_cache,
+        n_layer_cross_k,
+        n_layer_cross_v,
+        offset,
+    )
+    assert logits.shape == (n_audio, tokens.shape[1], model.dims.n_vocab)
+    assert n_layer_self_k_cache.shape == (
+        model.dims.n_text_layer,
+        n_audio,
+        model.dims.n_text_ctx,
+        model.dims.n_text_state,
+    )
+    assert n_layer_self_v_cache.shape == (
+        model.dims.n_text_layer,
+        n_audio,
+        model.dims.n_text_ctx,
+        model.dims.n_text_state,
+    )
+
+    offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
+
+    offset += len(tokenizer.sot_sequence)
+    print("logits.shape", logits.shape)  # (1, 3, 51864)
+    idx = logits[0, -1].argmax().item()
+
+    steps = 0
+    results = []
+    while idx != tokenizer.eot and steps < 50:
+        results.append(idx)
+        tokens = torch.tensor([[results[-1]]])
+        offset += 1
+
+        logits, n_layer_self_k_cache, n_layer_self_v_cache = decoder(
+            tokens,
+            n_layer_self_k_cache,
+            n_layer_self_v_cache,
+            n_layer_cross_k,
+            n_layer_cross_v,
+            offset,
+        )
+        idx = logits[0, -1].argmax().item()
+        print("idx", idx, "step", steps)
+        steps += 1
+
+    print(results)
+    print(tokenizer.decode(results))
+
+
+if __name__ == "__main__":
+    torch.set_num_threads(1)
+    torch.set_num_interop_threads(1)
+    # To fix
+    # TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor
+    # See also https://github.com/k2-fsa/sherpa-onnx/issues/1764
+    from whisper.model import disable_sdpa
+
+    with disable_sdpa():
+        main()

commit e2bf3dcbf267020f38eb29f3f044dcefbd39c7d4
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Jan 5 14:46:55 2026 +0800

    Fix building for onnxruntime >= 1.11.0 (#2981)

diff --git a/.github/workflows/test-onnxruntime-version.yaml b/.github/workflows/test-onnxruntime-version.yaml
new file mode 100644
index 00000000..b55b88ff
--- /dev/null
+++ b/.github/workflows/test-onnxruntime-version.yaml
@@ -0,0 +1,290 @@
+name: test-onnxruntime-version
+
+on:
+  push:
+    branches:
+      - master
+      - test-onnxruntime
+    tags:
+      - 'v[0-9]+.[0-9]+.[0-9]+*'
+
+  workflow_dispatch:
+
+concurrency:
+  group: test-onnxrntime-version-${{ github.ref }}
+  cancel-in-progress: true
+
+jobs:
+  macos:
+    runs-on: ${{ matrix.os }}
+    name: onnxruntime ${{ matrix.version }}
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [macos-latest]
+        version: ["1.11.0", "1.11.1", "1.12.0", "1.12.1", "1.13.1", "1.14.0", "1.14.1", "1.15.0", "1.15.1", "1.16.1", "1.16.2", "1.17.0", "1.17.1", "1.17.3", "1.18.0", "1.18.1", "1.19.0", "1.19.2", "1.20.0", "1.20.1", "1.20.2", "1.21.0", "1.21.1", "1.22.0", "1.22.1", "1.22.2", "1.23.0", "1.23.1", "1.23.2"]
+
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          fetch-depth: 0
+
+      - name: Update version
+        shell: bash
+        run: |
+          ./new-release.sh
+          git diff .
+
+      - name: ccache
+        uses: hendrikmuhs/ccache-action@v1.2
+        with:
+          key: ${{ matrix.os }}-onnxruntime-${{ matrix.version }}
+
+      - name: Download onnxruntime ${{ matrix.version }}
+        shell: bash
+        run: |
+          version=${{ matrix.version }}
+          curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v${version}/onnxruntime-osx-universal2-${version}.tgz
+          tar xvf onnxruntime-osx-universal2-${version}.tgz
+          ls -lh onnxruntime-osx-universal2-${version}
+
+          ls -lh onnxruntime-osx-universal2-${version}
+          echo "---"
+          ls -lh onnxruntime-osx-universal2-${version}/include
+          echo "---"
+          ls -lh onnxruntime-osx-universal2-${version}/lib
+
+      - name: Configure CMake
+        shell: bash
+        run: |
+          version=${{ matrix.version }}
+          onnxruntime_dir=$PWD/onnxruntime-osx-universal2-${version}
+          export SHERPA_ONNXRUNTIME_LIB_DIR=$onnxruntime_dir/lib/
+          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$onnxruntime_dir/include/
+
+          export CMAKE_CXX_COMPILER_LAUNCHER=ccache
+          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
+          cmake --version
+
+          mkdir build
+          cd build
+
+          cmake \
+            -D BUILD_SHARED_LIBS=ON \
+            -D CMAKE_OSX_ARCHITECTURES='arm64;x86_64' \
+            -D CMAKE_INSTALL_PREFIX=./install \
+            ..
+
+      - name: Build sherpa-onnx for macos
+        shell: bash
+        run: |
+          version=${{ matrix.version }}
+          onnxruntime_dir=$PWD/onnxruntime-osx-universal2-${version}
+          export SHERPA_ONNXRUNTIME_LIB_DIR=$onnxruntime_dir/lib/
+          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$onnxruntime_dir/include/
+
+          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
+
+          cd build
+          make -j2
+          make install
+
+          ls -lh lib
+          ls -lh bin
+
+          file ./bin/sherpa-onnx
+
+          rm -fv ./install/include/cargs.h
+          rm -fv ./install/lib/cargs.h
+          rm -fv ./install/lib/libcargs.dylib
+          rm -fv ./install/lib/libcargs.a
+          rm -rfv ./install/lib/pkgconfig
+
+      - name: Display dependencies of sherpa-onnx for macos
+        shell: bash
+        run: |
+          file bin/sherpa-onnx
+          otool -L build/bin/sherpa-onnx
+          otool -l build/bin/sherpa-onnx
+
+      - name: Copy files
+        shell: bash
+        run: |
+          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+
+          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-onnxruntime-${{ matrix.version }}-osx-universal2-shared
+          mkdir $dst
+
+          cp -a build/install/bin $dst/
+          mkdir $dst/lib
+          cp -a build/install/lib/*.dylib* $dst/lib/
+          cp -a build/install/include $dst/
+
+          brew install tree
+          tree $dst
+
+          tar cjvf ${dst}.tar.bz2 $dst
+
+      - name: Release pre-compiled binaries and libs for macOS
+        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+        uses: svenstaro/upload-release-action@v2
+        with:
+          file_glob: true
+          overwrite: true
+          file: sherpa-onnx-*osx-universal2*.tar.bz2
+
+      - name: Test offline CTC
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline
+
+          .github/scripts/test-offline-ctc.sh
+
+      - name: Test offline speech denoiser
+        shell: bash
+        run: |
+          du -h -d1 .
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-denoiser
+
+          .github/scripts/test-offline-speech-denoiser.sh
+
+      - name: Test offline TTS
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-tts
+
+          .github/scripts/test-offline-tts.sh
+
+      - name: Test offline Moonshine
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline
+
+          .github/scripts/test-offline-moonshine.sh
+
+      - name: Test C++ API
+        shell: bash
+        run: |
+          du -h -d1 .
+          export PATH=$PWD/build/bin:$PATH
+          export CXX_STREAMING_ZIPFORMER_EXE=streaming-zipformer-cxx-api
+          export CXX_WHISPER_EXE=whisper-cxx-api
+          export CXX_SENSE_VOICE_EXE=sense-voice-cxx-api
+
+          .github/scripts/test-cxx-api.sh
+          du -h -d1 .
+
+      - name: Test offline speaker diarization
+        shell: bash
+        run: |
+          du -h -d1 .
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-speaker-diarization
+
+          .github/scripts/test-speaker-diarization.sh
+
+      - name: Test offline transducer
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline
+
+          .github/scripts/test-offline-transducer.sh
+
+      - name: Test online punctuation
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-online-punctuation
+
+          .github/scripts/test-online-punctuation.sh
+
+      - name: Test online CTC
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx
+
+          .github/scripts/test-online-ctc.sh
+
+      - name: Test offline punctuation
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-punctuation
+
+          .github/scripts/test-offline-punctuation.sh
+
+      - name: Test C API
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export SLID_EXE=spoken-language-identification-c-api
+          export SID_EXE=speaker-identification-c-api
+          export AT_EXE=audio-tagging-c-api
+          export PUNCT_EXE=add-punctuation-c-api
+
+          .github/scripts/test-c-api.sh
+
+      - name: Test Audio tagging
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-audio-tagging
+
+          .github/scripts/test-audio-tagging.sh
+
+      - name: Test spoken language identification (C++ API)
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline-language-identification
+
+          .github/scripts/test-spoken-language-identification.sh
+
+      - name: Test transducer kws
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-keyword-spotter
+
+          .github/scripts/test-kws.sh
+
+      - name: Test online paraformer
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx
+
+          .github/scripts/test-online-paraformer.sh
+
+      - name: Test offline Whisper
+        if: matrix.build_type != 'Debug'
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx-offline
+
+          .github/scripts/test-offline-whisper.sh
+
+      - name: Test online transducer
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=sherpa-onnx
+
+          .github/scripts/test-online-transducer.sh
+
+      - name: Test online transducer (C API)
+        shell: bash
+        run: |
+          export PATH=$PWD/build/bin:$PATH
+          export EXE=decode-file-c-api
+
+          .github/scripts/test-online-transducer.sh
+
+
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 2e709b60..d9abc9dc 100755
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -213,7 +213,14 @@ if(location_onnxruntime_header_dir AND location_onnxruntime_lib)
       if(DEFINED ANDROID_ABI)
         file(GLOB onnxruntime_lib_files "$ENV{SHERPA_ONNXRUNTIME_LIB_DIR}/libonnxruntime.so")
       else()
-        file(GLOB onnxruntime_lib_files "$ENV{SHERPA_ONNXRUNTIME_LIB_DIR}/libonnxruntime*")
+        file(GLOB _onnxruntime_all "$ENV{SHERPA_ONNXRUNTIME_LIB_DIR}/libonnxruntime*")
+        set(onnxruntime_lib_files "")
+
+        foreach(f ${_onnxruntime_all})
+          if (NOT IS_DIRECTORY "${f}")
+            list(APPEND onnxruntime_lib_files "${f}")
+          endif()
+        endforeach()
       endif()
     endif()
 
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.cc b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
index ae463c96..261891dd 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
@@ -46,6 +46,7 @@ static inline size_t NumelFromShape(const std::vector<int64_t> &shape) {
   return n;
 }
 
+#if ORT_API_VERSION >= 14
 static inline void AssertTensorIsCpu(const Ort::Value &v, const char *what) {
   if (!v.IsTensor()) return;
   auto mi = v.GetTensorMemoryInfo();
@@ -55,6 +56,49 @@ static inline void AssertTensorIsCpu(const Ort::Value &v, const char *what) {
     SHERPA_ONNX_EXIT(-1);
   }
 }
+#else
+static inline void AssertTensorIsCpu(const Ort::Value &v, const char *what) {
+  if (!v.IsTensor()) return;
+
+  const OrtValue* v_ptr = reinterpret_cast<const OrtValue*>(&v);
+  const OrtMemoryInfo* memory_info = nullptr;
+
+  // 1. Get memory info
+  OrtStatus* status = Ort::GetApi().GetTensorMemoryInfo(v_ptr, &memory_info);
+  if (status) {
+    const char* msg = Ort::GetApi().GetErrorMessage(status);
+    Ort::GetApi().ReleaseStatus(status);
+    SHERPA_ONNX_LOGE("%s: failed to get tensor memory info: %s", what, msg);
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  // 2. Get memory type (OrtMemType)
+  OrtMemType mem_type;
+  status = Ort::GetApi().MemoryInfoGetMemType(memory_info, &mem_type);
+  if (status) {
+    const char* msg = Ort::GetApi().GetErrorMessage(status);
+    Ort::GetApi().ReleaseStatus(status);
+    SHERPA_ONNX_LOGE("%s: failed to get mem type: %s", what, msg);
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  // 3. Check CPU
+  if (mem_type != OrtMemTypeCPU) {
+    int device_id = 0;
+    status = Ort::GetApi().MemoryInfoGetId(memory_info, &device_id);
+    if (status) {
+      const char* msg = Ort::GetApi().GetErrorMessage(status);
+      Ort::GetApi().ReleaseStatus(status);
+      SHERPA_ONNX_LOGE("%s: failed to get device id: %s", what, msg);
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    SHERPA_ONNX_LOGE("%s: expected CPU tensor but got mem_type=%d device_id=%d",
+                     what, static_cast<int>(mem_type), device_id);
+    SHERPA_ONNX_EXIT(-1);
+  }
+}
+#endif
 
 static inline std::string ToLower(std::string s) {
   std::transform(s.begin(), s.end(), s.begin(),
diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
index fc060ff3..626af008 100644
--- a/sherpa-onnx/csrc/onnx-utils.cc
+++ b/sherpa-onnx/csrc/onnx-utils.cc
@@ -191,7 +191,18 @@ Ort::Value View(Ort::Value *v) {
   auto type_and_shape = v->GetTensorTypeAndShapeInfo();
   std::vector<int64_t> shape = type_and_shape.GetShape();
 
+#if ORT_API_VERSION >= 14
   auto memory_info = v->GetTensorMemoryInfo();
+#else
+  const OrtMemoryInfo *memory_info = nullptr;
+  OrtStatus *status = Ort::GetApi().GetTensorMemoryInfo(*v, &memory_info);
+  if (status != nullptr) {
+    const char *msg = Ort::GetApi().GetErrorMessage(status);
+    Ort::GetApi().ReleaseStatus(status);
+    SHERPA_ONNX_LOGE("Failed to get tensor memory info with error: '%s'", msg);
+    SHERPA_ONNX_EXIT(-1);
+  }
+#endif
 
   switch (type_and_shape.GetElementType()) {
     case ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32:
diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
index cba83f1a..0588a20b 100755
--- a/sherpa-onnx/csrc/session.cc
+++ b/sherpa-onnx/csrc/session.cc
@@ -12,7 +12,7 @@
 
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/provider.h"
-#if defined(__APPLE__)
+#if defined(__APPLE__) && (ORT_API_VERSION >= 15)
 #include "coreml_provider_factory.h"  // NOLINT
 #endif
 
@@ -204,12 +204,13 @@ Ort::SessionOptions GetSessionOptionsImpl(
       break;
     }
     case Provider::kCoreML: {
-#if defined(__APPLE__)
+#if defined(__APPLE__) && (ORT_API_VERSION >= 15)
       uint32_t coreml_flags = 0;
       (void)OrtSessionOptionsAppendExecutionProvider_CoreML(sess_opts,
                                                             coreml_flags);
 #else
-      SHERPA_ONNX_LOGE("CoreML is for Apple only. Fallback to cpu!");
+      SHERPA_ONNX_LOGE(
+          "CoreML is for Apple only since onnxruntime>=1.15. Fallback to cpu!");
 #endif
       break;
     }

commit 57a8d88cccaefb18444ecab805c151704d13e795
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Jan 5 10:37:51 2026 +0800

    Reformat Go API code (#2979)

diff --git a/scripts/go/generate.py b/scripts/go/generate.py
index d0750a0a..24fbf82e 100644
--- a/scripts/go/generate.py
+++ b/scripts/go/generate.py
@@ -1,11 +1,12 @@
 #!/usr/bin/env python3
 
+import argparse
 import os
 import re
-import sys
-import argparse
+
 import jinja2
 
+
 def parse_args():
     # set the source code file
     # -s sherpa_onnx.go
@@ -20,38 +21,40 @@ def parse_args():
     parser.add_argument("-o", "--output", type=str, required=True)
     return parser.parse_args()
 
+
 def parse_golang(target):
-    with open(target, 'r') as file:
+    with open(target, "r") as file:
         content = file.read()
     defines = []
-    struct_pattern = re.compile(r'type\s+([A-Z]\w+)\s+struct', re.DOTALL)
+    struct_pattern = re.compile(r"type\s+([A-Z]\w+)\s+struct", re.DOTALL)
     struct_matches = struct_pattern.findall(content)
     for name in struct_matches:
         c_define = {
-            'type': 'struct',
-            'name': name,
+            "type": "struct",
+            "name": name,
         }
         defines.append(c_define)
-    struct_pattern = re.compile(r'type\s+([A-Z][^ =]+)\s*=', re.DOTALL)
+    struct_pattern = re.compile(r"type\s+([A-Z][^ =]+)\s*=", re.DOTALL)
     struct_matches = struct_pattern.findall(content)
     for name in struct_matches:
         c_define = {
-            'type': 'struct',
-            'name': name,
+            "type": "struct",
+            "name": name,
         }
         defines.append(c_define)
-    func_pattern = re.compile(r'func\s+([A-Z][^ \(]+)\s*\(', re.DOTALL)
+    func_pattern = re.compile(r"func\s+([A-Z][^ \(]+)\s*\(", re.DOTALL)
     func_matches = func_pattern.findall(content)
     for name in func_matches:
         c_define = {
-            'type': 'function',
-            'name': name,
+            "type": "function",
+            "name": name,
         }
         defines.append(c_define)
     return defines
 
+
 def render(output, defines, platform):
-    build_info = ''
+    build_info = ""
     if platform == "windows":
         build_info = "//go:build (windows && amd64) || (windows && 386)"
     elif platform == "linux":
@@ -71,14 +74,12 @@ def render(output, defines, platform):
     folder = os.path.dirname(output)
     if not os.path.exists(folder):
         os.makedirs(folder)
-    with open(output, 'w') as f:
+    with open(output, "w") as f:
         print(rendered, file=f)
 
+
 def generate(src, output):
-    # print("source: ", src)
-    # print("output: ", output)
     defines = parse_golang(src)
-    # print("defines: ", defines)
     platform = "linux"
     render(f"{output}/sherpa_onnx/sherpa_onnx_{platform}.go", defines, platform)
     platform = "windows"
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 90ceb866..de2797b3 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -660,61 +660,61 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 }
 func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig) {
 	stringFields := []*(*C.char){
-	    &c.model_config.transducer.encoder,
-	    &c.model_config.transducer.decoder,
-	    &c.model_config.transducer.joiner,
-	    &c.model_config.paraformer.model,
-	    &c.model_config.nemo_ctc.model,
-	    &c.model_config.whisper.encoder,
-	    &c.model_config.whisper.decoder,
-	    &c.model_config.whisper.language,
-	    &c.model_config.whisper.task,
-	    &c.model_config.tdnn.model,
-	    &c.model_config.sense_voice.model,
-	    &c.model_config.sense_voice.language,
-	    &c.model_config.moonshine.preprocessor,
-	    &c.model_config.moonshine.encoder,
-	    &c.model_config.moonshine.uncached_decoder,
-	    &c.model_config.moonshine.cached_decoder,
-	    &c.model_config.fire_red_asr.encoder,
-	    &c.model_config.fire_red_asr.decoder,
-        &c.model_config.funasr_nano.encoder_adaptor,
-        &c.model_config.funasr_nano.llm_prefill,
-        &c.model_config.funasr_nano.llm_decode,
-        &c.model_config.funasr_nano.embedding,
-        &c.model_config.funasr_nano.tokenizer,
-        &c.model_config.funasr_nano.system_prompt,
-        &c.model_config.funasr_nano.user_prompt,
-        &c.model_config.dolphin.model,
-        &c.model_config.zipformer_ctc.model,
-        &c.model_config.canary.encoder,
-        &c.model_config.canary.decoder,
-        &c.model_config.canary.src_lang,
-        &c.model_config.canary.tgt_lang,
-        &c.model_config.wenet_ctc.model,
-        &c.model_config.medasr.model,
-        &c.model_config.omnilingual.model,
-        &c.model_config.tokens,
-        &c.model_config.provider,
-        &c.model_config.model_type,
-        &c.model_config.modeling_unit,
-        &c.model_config.bpe_vocab,
-        &c.model_config.telespeech_ctc,
-        &c.lm_config.model,
-        &c.decoding_method,
-        &c.hotwords_file,
-        &c.rule_fsts,
-        &c.rule_fars,
-        &c.hr.lexicon,
-        &c.hr.rule_fsts,
-    }
-
-    for _, field := range stringFields {
-        if *field != nil {
-            C.free(unsafe.Pointer(*field))
-            *field = nil
-        }
-    }
+		&c.model_config.transducer.encoder,
+		&c.model_config.transducer.decoder,
+		&c.model_config.transducer.joiner,
+		&c.model_config.paraformer.model,
+		&c.model_config.nemo_ctc.model,
+		&c.model_config.whisper.encoder,
+		&c.model_config.whisper.decoder,
+		&c.model_config.whisper.language,
+		&c.model_config.whisper.task,
+		&c.model_config.tdnn.model,
+		&c.model_config.sense_voice.model,
+		&c.model_config.sense_voice.language,
+		&c.model_config.moonshine.preprocessor,
+		&c.model_config.moonshine.encoder,
+		&c.model_config.moonshine.uncached_decoder,
+		&c.model_config.moonshine.cached_decoder,
+		&c.model_config.fire_red_asr.encoder,
+		&c.model_config.fire_red_asr.decoder,
+		&c.model_config.funasr_nano.encoder_adaptor,
+		&c.model_config.funasr_nano.llm_prefill,
+		&c.model_config.funasr_nano.llm_decode,
+		&c.model_config.funasr_nano.embedding,
+		&c.model_config.funasr_nano.tokenizer,
+		&c.model_config.funasr_nano.system_prompt,
+		&c.model_config.funasr_nano.user_prompt,
+		&c.model_config.dolphin.model,
+		&c.model_config.zipformer_ctc.model,
+		&c.model_config.canary.encoder,
+		&c.model_config.canary.decoder,
+		&c.model_config.canary.src_lang,
+		&c.model_config.canary.tgt_lang,
+		&c.model_config.wenet_ctc.model,
+		&c.model_config.medasr.model,
+		&c.model_config.omnilingual.model,
+		&c.model_config.tokens,
+		&c.model_config.provider,
+		&c.model_config.model_type,
+		&c.model_config.modeling_unit,
+		&c.model_config.bpe_vocab,
+		&c.model_config.telespeech_ctc,
+		&c.lm_config.model,
+		&c.decoding_method,
+		&c.hotwords_file,
+		&c.rule_fsts,
+		&c.rule_fars,
+		&c.hr.lexicon,
+		&c.hr.rule_fsts,
+	}
+
+	for _, field := range stringFields {
+		if *field != nil {
+			C.free(unsafe.Pointer(*field))
+			*field = nil
+		}
+	}
 }
 
 // Frees the internal pointer of the recognition to avoid memory leak.

commit d987ce43656cfb80ddc580cf3265e4af0a8f1330
Author: ilibx <i.lufei@qq.com>
Date:   Mon Jan 5 10:26:55 2026 +0800

    [feature] use jinja2 to generate sherpa-onnx-go lib (#2976)

diff --git a/scripts/go/defines.go.jinja b/scripts/go/defines.go.jinja
new file mode 100644
index 00000000..4b39cd64
--- /dev/null
+++ b/scripts/go/defines.go.jinja
@@ -0,0 +1,21 @@
+{{ golang_header }}
+package sherpa_onnx
+
+// ============================================================
+// Code Generated Automatically for {{ platform }} platform, DO NOT EDIT MANUALLY!!
+// ============================================================
+
+import (
+	sherpa "github.com/k2-fsa/sherpa-onnx-go-{{ platform }}"
+)
+
+// ============================================================
+// Struct/Function Defines
+// ============================================================
+{% for define in defines -%}
+{%- if define.type == 'function' %}
+var {{ define.name }} = sherpa.{{ define.name }}
+{%- else %}
+type {{ define.name }} = sherpa.{{ define.name }}
+{%- endif %}
+{%- endfor %}
\ No newline at end of file
diff --git a/scripts/go/generate.py b/scripts/go/generate.py
new file mode 100644
index 00000000..d0750a0a
--- /dev/null
+++ b/scripts/go/generate.py
@@ -0,0 +1,92 @@
+#!/usr/bin/env python3
+
+import os
+import re
+import sys
+import argparse
+import jinja2
+
+def parse_args():
+    # set the source code file
+    # -s sherpa_onnx.go
+    # set the output folder
+    # -o ./sherpa-onnx-go
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.ArgumentDefaultsHelpFormatter
+    )
+    # add argv to set source code file
+    parser.add_argument("-s", "--source", type=str, required=True)
+    # add argv to set output folder
+    parser.add_argument("-o", "--output", type=str, required=True)
+    return parser.parse_args()
+
+def parse_golang(target):
+    with open(target, 'r') as file:
+        content = file.read()
+    defines = []
+    struct_pattern = re.compile(r'type\s+([A-Z]\w+)\s+struct', re.DOTALL)
+    struct_matches = struct_pattern.findall(content)
+    for name in struct_matches:
+        c_define = {
+            'type': 'struct',
+            'name': name,
+        }
+        defines.append(c_define)
+    struct_pattern = re.compile(r'type\s+([A-Z][^ =]+)\s*=', re.DOTALL)
+    struct_matches = struct_pattern.findall(content)
+    for name in struct_matches:
+        c_define = {
+            'type': 'struct',
+            'name': name,
+        }
+        defines.append(c_define)
+    func_pattern = re.compile(r'func\s+([A-Z][^ \(]+)\s*\(', re.DOTALL)
+    func_matches = func_pattern.findall(content)
+    for name in func_matches:
+        c_define = {
+            'type': 'function',
+            'name': name,
+        }
+        defines.append(c_define)
+    return defines
+
+def render(output, defines, platform):
+    build_info = ''
+    if platform == "windows":
+        build_info = "//go:build (windows && amd64) || (windows && 386)"
+    elif platform == "linux":
+        build_info = "//go:build (!android && linux && arm64) || (!android && linux && amd64 && !musl) || (!android && linux && arm && !arm7) || (!android && arm7) || (!android && linux && 386 && !musl) || (!android && musl) || (!android && linux && mips) || (!android && linux && mips64) || (!android && linux && mips64le) || (!android && linux && mipsle)"
+    elif platform == "macos":
+        build_info = "//go:build (darwin && amd64 && !ios) || (darwin && arm64 && !ios)"
+    with open("./defines.go.jinja") as f:
+        content = f.read()
+    environment = jinja2.Environment()
+    template = environment.from_string(content)
+    context = {
+        "platform": platform,
+        "defines": defines,
+        "golang_header": build_info,
+    }
+    rendered = template.render(**context)
+    folder = os.path.dirname(output)
+    if not os.path.exists(folder):
+        os.makedirs(folder)
+    with open(output, 'w') as f:
+        print(rendered, file=f)
+
+def generate(src, output):
+    # print("source: ", src)
+    # print("output: ", output)
+    defines = parse_golang(src)
+    # print("defines: ", defines)
+    platform = "linux"
+    render(f"{output}/sherpa_onnx/sherpa_onnx_{platform}.go", defines, platform)
+    platform = "windows"
+    render(f"{output}/sherpa_onnx/sherpa_onnx_{platform}.go", defines, platform)
+    platform = "macos"
+    render(f"{output}/sherpa_onnx/sherpa_onnx_{platform}.go", defines, platform)
+
+
+if __name__ == "__main__":
+    args = parse_args()
+    generate(args.source, args.output)
diff --git a/scripts/go/release.sh b/scripts/go/release.sh
index c0dbeb83..e696c058 100755
--- a/scripts/go/release.sh
+++ b/scripts/go/release.sh
@@ -172,7 +172,25 @@ function windows() {
   rm -rf sherpa-onnx-go-windows
 }
 
+function basic() {
+  echo "Process sherpa-onnx-go"
+  git clone git@github.com:k2-fsa/sherpa-onnx-go.git
 
+  python3 ./generate.py -s ./sherpa_onnx.go -o ./sherpa-onnx-go
+
+  echo "------------------------------"
+  cd sherpa-onnx-go
+  git status
+  git add .
+  git commit -m "Release v$SHERPA_ONNX_VERSION" && \
+    git push && \
+    git tag v$SHERPA_ONNX_VERSION && \
+    git push origin v$SHERPA_ONNX_VERSION
+  cd ..
+  rm -rf sherpa-onnx-go
+}
+
+basic
 windows
 linux
 osx

commit 53ff28774601979b72beebb355ccf0d0cb3072ca
Author: ilibx <i.lufei@qq.com>
Date:   Mon Jan 5 10:25:59 2026 +0800

    [opt] opt free pointer function in Go API (#2975)

diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index cbfd1b82..90ceb866 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -659,226 +659,62 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 	return &c
 }
 func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig) {
-	if c.model_config.transducer.encoder != nil {
-		C.free(unsafe.Pointer(c.model_config.transducer.encoder))
-		c.model_config.transducer.encoder = nil
-	}
-	if c.model_config.transducer.decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.transducer.decoder))
-		c.model_config.transducer.decoder = nil
-	}
-	if c.model_config.transducer.joiner != nil {
-		C.free(unsafe.Pointer(c.model_config.transducer.joiner))
-		c.model_config.transducer.joiner = nil
-	}
-
-	if c.model_config.paraformer.model != nil {
-		C.free(unsafe.Pointer(c.model_config.paraformer.model))
-		c.model_config.paraformer.model = nil
-	}
-
-	if c.model_config.nemo_ctc.model != nil {
-		C.free(unsafe.Pointer(c.model_config.nemo_ctc.model))
-		c.model_config.nemo_ctc.model = nil
-	}
-
-	if c.model_config.whisper.encoder != nil {
-		C.free(unsafe.Pointer(c.model_config.whisper.encoder))
-		c.model_config.whisper.encoder = nil
-	}
-	if c.model_config.whisper.decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.whisper.decoder))
-		c.model_config.whisper.decoder = nil
-	}
-	if c.model_config.whisper.language != nil {
-		C.free(unsafe.Pointer(c.model_config.whisper.language))
-		c.model_config.whisper.language = nil
-	}
-	if c.model_config.whisper.task != nil {
-		C.free(unsafe.Pointer(c.model_config.whisper.task))
-		c.model_config.whisper.task = nil
-	}
-
-	if c.model_config.tdnn.model != nil {
-		C.free(unsafe.Pointer(c.model_config.tdnn.model))
-		c.model_config.tdnn.model = nil
-	}
-
-	if c.model_config.sense_voice.model != nil {
-		C.free(unsafe.Pointer(c.model_config.sense_voice.model))
-		c.model_config.sense_voice.model = nil
-	}
-	if c.model_config.sense_voice.language != nil {
-		C.free(unsafe.Pointer(c.model_config.sense_voice.language))
-		c.model_config.sense_voice.language = nil
-	}
-
-	if c.model_config.moonshine.preprocessor != nil {
-		C.free(unsafe.Pointer(c.model_config.moonshine.preprocessor))
-		c.model_config.moonshine.preprocessor = nil
-	}
-	if c.model_config.moonshine.encoder != nil {
-		C.free(unsafe.Pointer(c.model_config.moonshine.encoder))
-		c.model_config.moonshine.encoder = nil
-	}
-	if c.model_config.moonshine.uncached_decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.moonshine.uncached_decoder))
-		c.model_config.moonshine.uncached_decoder = nil
-	}
-	if c.model_config.moonshine.cached_decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.moonshine.cached_decoder))
-		c.model_config.moonshine.cached_decoder = nil
-	}
-
-	if c.model_config.fire_red_asr.encoder != nil {
-		C.free(unsafe.Pointer(c.model_config.fire_red_asr.encoder))
-		c.model_config.fire_red_asr.encoder = nil
-	}
-
-	if c.model_config.fire_red_asr.decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.fire_red_asr.decoder))
-		c.model_config.fire_red_asr.decoder = nil
-	}
-
-	if c.model_config.funasr_nano.encoder_adaptor != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.encoder_adaptor))
-		c.model_config.funasr_nano.encoder_adaptor = nil
-	}
-
-	if c.model_config.funasr_nano.llm_prefill != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.llm_prefill))
-		c.model_config.funasr_nano.llm_prefill = nil
-	}
-
-	if c.model_config.funasr_nano.llm_decode != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.llm_decode))
-		c.model_config.funasr_nano.llm_decode = nil
-	}
-
-	if c.model_config.funasr_nano.embedding != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.embedding))
-		c.model_config.funasr_nano.embedding = nil
-	}
-
-	if c.model_config.funasr_nano.tokenizer != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.tokenizer))
-		c.model_config.funasr_nano.tokenizer = nil
-	}
-
-	if c.model_config.funasr_nano.system_prompt != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.system_prompt))
-		c.model_config.funasr_nano.system_prompt = nil
-	}
-
-	if c.model_config.funasr_nano.user_prompt != nil {
-		C.free(unsafe.Pointer(c.model_config.funasr_nano.user_prompt))
-		c.model_config.funasr_nano.user_prompt = nil
-	}
-
-	if c.model_config.dolphin.model != nil {
-		C.free(unsafe.Pointer(c.model_config.dolphin.model))
-		c.model_config.dolphin.model = nil
-	}
-
-	if c.model_config.zipformer_ctc.model != nil {
-		C.free(unsafe.Pointer(c.model_config.zipformer_ctc.model))
-		c.model_config.zipformer_ctc.model = nil
-	}
-
-	if c.model_config.canary.encoder != nil {
-		C.free(unsafe.Pointer(c.model_config.canary.encoder))
-		c.model_config.canary.encoder = nil
-	}
-
-	if c.model_config.canary.decoder != nil {
-		C.free(unsafe.Pointer(c.model_config.canary.decoder))
-		c.model_config.canary.decoder = nil
-	}
-
-	if c.model_config.canary.src_lang != nil {
-		C.free(unsafe.Pointer(c.model_config.canary.src_lang))
-		c.model_config.canary.src_lang = nil
-	}
-
-	if c.model_config.canary.tgt_lang != nil {
-		C.free(unsafe.Pointer(c.model_config.canary.tgt_lang))
-		c.model_config.canary.tgt_lang = nil
-	}
-
-	if c.model_config.wenet_ctc.model != nil {
-		C.free(unsafe.Pointer(c.model_config.wenet_ctc.model))
-		c.model_config.wenet_ctc.model = nil
-	}
-
-	if c.model_config.medasr.model != nil {
-		C.free(unsafe.Pointer(c.model_config.medasr.model))
-		c.model_config.medasr.model = nil
-	}
-
-	if c.model_config.omnilingual.model != nil {
-		C.free(unsafe.Pointer(c.model_config.omnilingual.model))
-		c.model_config.omnilingual.model = nil
-	}
-
-	if c.model_config.tokens != nil {
-		C.free(unsafe.Pointer(c.model_config.tokens))
-		c.model_config.tokens = nil
-	}
-	if c.model_config.provider != nil {
-		C.free(unsafe.Pointer(c.model_config.provider))
-		c.model_config.provider = nil
-	}
-	if c.model_config.model_type != nil {
-		C.free(unsafe.Pointer(c.model_config.model_type))
-		c.model_config.model_type = nil
-	}
-	if c.model_config.modeling_unit != nil {
-		C.free(unsafe.Pointer(c.model_config.modeling_unit))
-		c.model_config.modeling_unit = nil
-	}
-	if c.model_config.bpe_vocab != nil {
-		C.free(unsafe.Pointer(c.model_config.bpe_vocab))
-		c.model_config.bpe_vocab = nil
-	}
-	if c.model_config.telespeech_ctc != nil {
-		C.free(unsafe.Pointer(c.model_config.telespeech_ctc))
-		c.model_config.telespeech_ctc = nil
-	}
-
-	if c.lm_config.model != nil {
-		C.free(unsafe.Pointer(c.lm_config.model))
-		c.lm_config.model = nil
-	}
-
-	if c.decoding_method != nil {
-		C.free(unsafe.Pointer(c.decoding_method))
-		c.decoding_method = nil
-	}
-
-	if c.hotwords_file != nil {
-		C.free(unsafe.Pointer(c.hotwords_file))
-		c.hotwords_file = nil
-	}
-
-	if c.rule_fsts != nil {
-		C.free(unsafe.Pointer(c.rule_fsts))
-		c.rule_fsts = nil
-	}
-
-	if c.rule_fars != nil {
-		C.free(unsafe.Pointer(c.rule_fars))
-		c.rule_fars = nil
-	}
-
-	if c.hr.lexicon != nil {
-		C.free(unsafe.Pointer(c.hr.lexicon))
-		c.hr.lexicon = nil
-	}
-
-	if c.hr.rule_fsts != nil {
-		C.free(unsafe.Pointer(c.hr.rule_fsts))
-		c.hr.rule_fsts = nil
-	}
+	stringFields := []*(*C.char){
+	    &c.model_config.transducer.encoder,
+	    &c.model_config.transducer.decoder,
+	    &c.model_config.transducer.joiner,
+	    &c.model_config.paraformer.model,
+	    &c.model_config.nemo_ctc.model,
+	    &c.model_config.whisper.encoder,
+	    &c.model_config.whisper.decoder,
+	    &c.model_config.whisper.language,
+	    &c.model_config.whisper.task,
+	    &c.model_config.tdnn.model,
+	    &c.model_config.sense_voice.model,
+	    &c.model_config.sense_voice.language,
+	    &c.model_config.moonshine.preprocessor,
+	    &c.model_config.moonshine.encoder,
+	    &c.model_config.moonshine.uncached_decoder,
+	    &c.model_config.moonshine.cached_decoder,
+	    &c.model_config.fire_red_asr.encoder,
+	    &c.model_config.fire_red_asr.decoder,
+        &c.model_config.funasr_nano.encoder_adaptor,
+        &c.model_config.funasr_nano.llm_prefill,
+        &c.model_config.funasr_nano.llm_decode,
+        &c.model_config.funasr_nano.embedding,
+        &c.model_config.funasr_nano.tokenizer,
+        &c.model_config.funasr_nano.system_prompt,
+        &c.model_config.funasr_nano.user_prompt,
+        &c.model_config.dolphin.model,
+        &c.model_config.zipformer_ctc.model,
+        &c.model_config.canary.encoder,
+        &c.model_config.canary.decoder,
+        &c.model_config.canary.src_lang,
+        &c.model_config.canary.tgt_lang,
+        &c.model_config.wenet_ctc.model,
+        &c.model_config.medasr.model,
+        &c.model_config.omnilingual.model,
+        &c.model_config.tokens,
+        &c.model_config.provider,
+        &c.model_config.model_type,
+        &c.model_config.modeling_unit,
+        &c.model_config.bpe_vocab,
+        &c.model_config.telespeech_ctc,
+        &c.lm_config.model,
+        &c.decoding_method,
+        &c.hotwords_file,
+        &c.rule_fsts,
+        &c.rule_fars,
+        &c.hr.lexicon,
+        &c.hr.rule_fsts,
+    }
+
+    for _, field := range stringFields {
+        if *field != nil {
+            C.free(unsafe.Pointer(*field))
+            *field = nil
+        }
+    }
 }
 
 // Frees the internal pointer of the recognition to avoid memory leak.

commit c59f21d03854fe72b5ab002cf64ad0c50655dd0e
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Mon Jan 5 10:24:18 2026 +0800

    Update FunAsr-Nano CTC model (#2978)

diff --git a/.github/workflows/export-sense-voice-to-onnx.yaml b/.github/workflows/export-sense-voice-to-onnx.yaml
index f094f780..f9d2d893 100644
--- a/.github/workflows/export-sense-voice-to-onnx.yaml
+++ b/.github/workflows/export-sense-voice-to-onnx.yaml
@@ -101,7 +101,7 @@ jobs:
             mv $m ../../
           done
 
-      - name: Publish v3 to huggingface
+      - name: Publish to huggingface
         if: true
         env:
           HF_TOKEN: ${{ secrets.HF_TOKEN }}
diff --git a/scripts/sense-voice/rknn/test_nano_torch.py b/scripts/sense-voice/rknn/test_nano_torch.py
index 7e150742..d41e1c9f 100755
--- a/scripts/sense-voice/rknn/test_nano_torch.py
+++ b/scripts/sense-voice/rknn/test_nano_torch.py
@@ -36,6 +36,12 @@ def load_torch_model():
     model = nano.Nano()
 
     state_dict = torch.load("./model.pt", map_location="cpu")
+
+    to_delete = [k for k in state_dict if "llm" in k or "audio_adaptor" in k]
+
+    for k in to_delete:
+        del state_dict[k]
+
     model.load_state_dict(state_dict, strict=True)
     model.eval()
 

commit 1d56a3fd388376fee306ee4d93e8507ad1cf3d12
Author: ilibx <i.lufei@qq.com>
Date:   Sun Jan 4 18:18:07 2026 +0800

    [feature] add FunASRNano config into golang api (#2974)

diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 47c7864c..cbfd1b82 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -452,6 +452,20 @@ type OfflineFireRedAsrModelConfig struct {
 	Decoder string
 }
 
+type OfflineFunASRNanoModelConfig struct {
+	EncoderAdaptor string
+	LlmPreFill     string
+	LlmDecoder     string
+	Embedding      string
+	Tokenizer      string
+	SystemPrompt   string
+	UserPrompt     string
+	MaxNewTokens   int
+	Temperature    float32
+	TopP           float32
+	Seed           int
+}
+
 type OfflineMoonshineModelConfig struct {
 	Preprocessor    string
 	Encoder         string
@@ -484,6 +498,7 @@ type OfflineModelConfig struct {
 	SenseVoice   OfflineSenseVoiceModelConfig
 	Moonshine    OfflineMoonshineModelConfig
 	FireRedAsr   OfflineFireRedAsrModelConfig
+	FunASRNano   OfflineFunASRNanoModelConfig
 	Dolphin      OfflineDolphinModelConfig
 	ZipformerCtc OfflineZipformerCtcModelConfig
 	Canary       OfflineCanaryModelConfig
@@ -582,6 +597,18 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 	c.model_config.fire_red_asr.encoder = C.CString(config.ModelConfig.FireRedAsr.Encoder)
 	c.model_config.fire_red_asr.decoder = C.CString(config.ModelConfig.FireRedAsr.Decoder)
 
+	c.model_config.funasr_nano.encoder_adaptor = C.CString(config.ModelConfig.FunASRNano.EncoderAdaptor)
+	c.model_config.funasr_nano.llm_prefill = C.CString(config.ModelConfig.FunASRNano.LlmPreFill)
+	c.model_config.funasr_nano.llm_decode = C.CString(config.ModelConfig.FunASRNano.LlmDecoder)
+	c.model_config.funasr_nano.embedding = C.CString(config.ModelConfig.FunASRNano.Embedding)
+	c.model_config.funasr_nano.tokenizer = C.CString(config.ModelConfig.FunASRNano.Tokenizer)
+	c.model_config.funasr_nano.system_prompt = C.CString(config.ModelConfig.FunASRNano.SystemPrompt)
+	c.model_config.funasr_nano.user_prompt = C.CString(config.ModelConfig.FunASRNano.UserPrompt)
+	c.model_config.funasr_nano.max_new_tokens = C.int(config.ModelConfig.FunASRNano.MaxNewTokens)
+	c.model_config.funasr_nano.temperature = C.float(config.ModelConfig.FunASRNano.Temperature)
+	c.model_config.funasr_nano.top_p = C.float(config.ModelConfig.FunASRNano.TopP)
+	c.model_config.funasr_nano.seed = C.int(config.ModelConfig.FunASRNano.Seed)
+
 	c.model_config.dolphin.model = C.CString(config.ModelConfig.Dolphin.Model)
 	c.model_config.zipformer_ctc.model = C.CString(config.ModelConfig.ZipformerCtc.Model)
 
@@ -713,6 +740,41 @@ func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig)
 		c.model_config.fire_red_asr.decoder = nil
 	}
 
+	if c.model_config.funasr_nano.encoder_adaptor != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.encoder_adaptor))
+		c.model_config.funasr_nano.encoder_adaptor = nil
+	}
+
+	if c.model_config.funasr_nano.llm_prefill != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.llm_prefill))
+		c.model_config.funasr_nano.llm_prefill = nil
+	}
+
+	if c.model_config.funasr_nano.llm_decode != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.llm_decode))
+		c.model_config.funasr_nano.llm_decode = nil
+	}
+
+	if c.model_config.funasr_nano.embedding != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.embedding))
+		c.model_config.funasr_nano.embedding = nil
+	}
+
+	if c.model_config.funasr_nano.tokenizer != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.tokenizer))
+		c.model_config.funasr_nano.tokenizer = nil
+	}
+
+	if c.model_config.funasr_nano.system_prompt != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.system_prompt))
+		c.model_config.funasr_nano.system_prompt = nil
+	}
+
+	if c.model_config.funasr_nano.user_prompt != nil {
+		C.free(unsafe.Pointer(c.model_config.funasr_nano.user_prompt))
+		c.model_config.funasr_nano.user_prompt = nil
+	}
+
 	if c.model_config.dolphin.model != nil {
 		C.free(unsafe.Pointer(c.model_config.dolphin.model))
 		c.model_config.dolphin.model = nil

commit bb941f544b0877f42bde10a894cb0c670c534cbb
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Sun Jan 4 17:40:05 2026 +0800

    Fix building for HarmonyOS (#2972)

diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
index c418008a..b80f60e2 100644
--- a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
@@ -14,6 +14,15 @@
 #include <utility>
 #include <vector>
 
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
 #include "onnxruntime_cxx_api.h"
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
index e2b19d72..5917dae7 100644
--- a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
@@ -57,16 +57,6 @@ class OfflineRecognizerFunASRNanoImpl : public OfflineRecognizerImpl {
   mutable std::mt19937 rng_;
 };
 
-#if __ANDROID_API__ >= 9
-template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
-    AAssetManager *mgr, const OfflineRecognizerConfig &config);
-#endif
-
-#if __OHOS__
-template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
-    NativeResourceManager *mgr, const OfflineRecognizerConfig &config);
-#endif
-
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_FUNASR_NANO_IMPL_H_

commit e61d692f8122f04c196b27badafc172e2fe59450
Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
Date:   Tue Dec 30 21:48:27 2025 +0800

    Fix building for Windows (#2964)

diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.cc b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
index e4a5ecac..ae463c96 100644
--- a/sherpa-onnx/csrc/offline-funasr-nano-model.cc
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
@@ -29,6 +29,7 @@
 #include "sherpa-onnx/csrc/macros.h"
 #include "sherpa-onnx/csrc/onnx-utils.h"
 #include "sherpa-onnx/csrc/session.h"
+#include "sherpa-onnx/csrc/text-utils.h"
 
 namespace sherpa_onnx {
 
@@ -705,7 +706,7 @@ class OfflineFunASRNanoModel::Impl {
  private:
   void InitEncoderAdaptor(const std::string &model_path) {
     encoder_sess_ = std::make_unique<Ort::Session>(
-        env_, model_path.c_str(), sess_opts_encoder_);
+        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_encoder_);
     GetInputNames(encoder_sess_.get(), &encoder_input_names_,
                   &encoder_input_names_ptr_);
     GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
@@ -729,7 +730,7 @@ class OfflineFunASRNanoModel::Impl {
 
   void InitLLMPrefill(const std::string &model_path) {
     prefill_sess_ = std::make_unique<Ort::Session>(
-        env_, model_path.c_str(), sess_opts_llm_);
+        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_llm_);
     GetInputNames(prefill_sess_.get(), &prefill_input_names_,
                   &prefill_input_names_ptr_);
     GetOutputNames(prefill_sess_.get(), &prefill_output_names_,
@@ -754,7 +755,7 @@ class OfflineFunASRNanoModel::Impl {
 
   void InitLLMDecode(const std::string &model_path) {
     decode_sess_ = std::make_unique<Ort::Session>(
-        env_, model_path.c_str(), sess_opts_llm_);
+        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_llm_);
     GetInputNames(decode_sess_.get(), &decode_input_names_,
                   &decode_input_names_ptr_);
     GetOutputNames(decode_sess_.get(), &decode_output_names_,
@@ -788,7 +789,7 @@ class OfflineFunASRNanoModel::Impl {
 
   void InitEmbedding(const std::string &model_path) {
     embedding_sess_ = std::make_unique<Ort::Session>(
-        env_, model_path.c_str(), sess_opts_embedding_);
+        env_, SHERPA_ONNX_TO_ORT_PATH(model_path), sess_opts_embedding_);
     GetInputNames(embedding_sess_.get(), &embedding_input_names_,
                   &embedding_input_names_ptr_);
     GetOutputNames(embedding_sess_.get(), &embedding_output_names_,

commit 4ea060ab499a9cc408821d22e702d98dc7651b58
Author: Wasser1462 <150865334+Wasser1462@users.noreply.github.com>
Date:   Tue Dec 30 12:04:29 2025 +0800

    Add funASR-Nano with LLM support (#2936)

diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
index 88b3add8..07c891aa 100644
--- a/cxx-api-examples/CMakeLists.txt
+++ b/cxx-api-examples/CMakeLists.txt
@@ -145,6 +145,9 @@ target_link_libraries(dolphin-ctc-cxx-api sherpa-onnx-cxx-api)
 add_executable(vad-cxx-api ./vad-cxx-api.cc)
 target_link_libraries(vad-cxx-api sherpa-onnx-cxx-api)
 
+add_executable(funasr-nano-cxx-api ./funasr-nano-cxx-api.cc)
+target_link_libraries(funasr-nano-cxx-api sherpa-onnx-cxx-api)
+
 if(SHERPA_ONNX_ENABLE_TTS)
   add_executable(matcha-tts-zh-cxx-api ./matcha-tts-zh-cxx-api.cc)
   target_link_libraries(matcha-tts-zh-cxx-api sherpa-onnx-cxx-api)
diff --git a/cxx-api-examples/funasr-nano-cxx-api.cc b/cxx-api-examples/funasr-nano-cxx-api.cc
new file mode 100644
index 00000000..c01b8fc0
--- /dev/null
+++ b/cxx-api-examples/funasr-nano-cxx-api.cc
@@ -0,0 +1,205 @@
+// cxx-api-examples/funasr-nano-cxx-api.cc
+//
+// Copyright (c)  2025  zengyw
+//
+// This file demonstrates how to use FunASR-nano with sherpa-onnx's C++ API.
+//
+// clang-format off
+//
+// Example usage:
+//   ./bin/funasr-nano-cxx-api \
+//     --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
+//     --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
+//     --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+//     --funasr-nano-embedding=/path/to/embedding.onnx \
+//     --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
+//     /path/to/audio.wav
+//
+// clang-format on
+
+#include <chrono>
+#include <cstring>
+#include <iostream>
+#include <string>
+
+#include "sherpa-onnx/c-api/cxx-api.h"
+
+int32_t main(int32_t argc, char *argv[]) {
+  using namespace sherpa_onnx::cxx;
+
+  const char *kUsageMessage = R"usage(
+FunASR-nano speech recognition example using sherpa-onnx C++ API.
+
+Usage:
+  ./bin/funasr-nano-cxx-api \
+    --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
+    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
+    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
+    --funasr-nano-embedding=/path/to/embedding.onnx \
+    [--funasr-nano-user-prompt=""] \
+    [--funasr-nano-max-new-tokens=512] \
+    [--funasr-nano-temperature=0.3] \
+    [--funasr-nano-top-p=0.8] \
+    /path/to/audio.wav
+
+Required arguments:
+  --funasr-nano-encoder-adaptor: Path to encoder_adaptor.onnx
+  --funasr-nano-llm-prefill: Path to llm_prefill.onnx
+  --funasr-nano-llm-decode: Path to llm_decode.onnx
+  --funasr-nano-tokenizer: Path to tokenizer directory (e.g., Qwen3-0.6B)
+  --funasr-nano-embedding: Path to embedding.onnx
+
+Optional arguments:
+  --funasr-nano-user-prompt: User prompt template (default: "")
+  --funasr-nano-max-new-tokens: Maximum tokens to generate (default: 512)
+  --funasr-nano-temperature: Sampling temperature (default: 0.3)
+  --funasr-nano-top-p: Top-p sampling threshold (default: 0.8)
+  --num-threads: Number of threads (default: 2)
+  --provider: cpu (default) or cuda
+
+Example:
+  ./bin/funasr-nano-cxx-api \
+    --funasr-nano-encoder-adaptor=./models/encoder_adaptor.onnx \
+    --funasr-nano-llm-prefill=./models/llm_prefill.onnx \
+    --funasr-nano-llm-decode=./models/llm_decode.onnx \
+    --funasr-nano-tokenizer=./models/Qwen3-0.6B \
+    --funasr-nano-embedding=./models/embedding.onnx \
+    ./test.wav
+)usage";
+
+  if (argc < 6) {
+    std::cerr << kUsageMessage << "\n";
+    return -1;
+  }
+
+  OfflineRecognizerConfig config;
+  config.model_config.num_threads = 2;
+  config.model_config.debug = false;
+  config.model_config.provider = "cpu";
+
+  // Parse command line arguments
+  const char kEncoderAdaptor[] = "--funasr-nano-encoder-adaptor=";
+  const char kLlmPrefill[] = "--funasr-nano-llm-prefill=";
+  const char kLlmDecode[] = "--funasr-nano-llm-decode=";
+  const char kEmbedding[] = "--funasr-nano-embedding=";
+  const char kTokenizer[] = "--funasr-nano-tokenizer=";
+  const char kUserPrompt[] = "--funasr-nano-user-prompt=";
+  const char kMaxNewTokens[] = "--funasr-nano-max-new-tokens=";
+  const char kTemperature[] = "--funasr-nano-temperature=";
+  const char kTopP[] = "--funasr-nano-top-p=";
+  const char kNumThreads[] = "--num-threads=";
+  const char kProvider[] = "--provider=";
+
+  for (int32_t i = 1; i < argc; ++i) {
+    std::string arg = argv[i];
+    if (arg.find(kEncoderAdaptor) == 0) {
+      config.model_config.funasr_nano.encoder_adaptor =
+          arg.substr(sizeof(kEncoderAdaptor) - 1);
+    } else if (arg.find(kLlmPrefill) == 0) {
+      config.model_config.funasr_nano.llm_prefill =
+          arg.substr(sizeof(kLlmPrefill) - 1);
+    } else if (arg.find(kLlmDecode) == 0) {
+      config.model_config.funasr_nano.llm_decode =
+          arg.substr(sizeof(kLlmDecode) - 1);
+    } else if (arg.find(kEmbedding) == 0) {
+      config.model_config.funasr_nano.embedding =
+          arg.substr(sizeof(kEmbedding) - 1);
+    } else if (arg.find(kTokenizer) == 0) {
+      config.model_config.funasr_nano.tokenizer =
+          arg.substr(sizeof(kTokenizer) - 1);
+    } else if (arg.find(kUserPrompt) == 0) {
+      config.model_config.funasr_nano.user_prompt =
+          arg.substr(sizeof(kUserPrompt) - 1);
+    } else if (arg.find(kMaxNewTokens) == 0) {
+      config.model_config.funasr_nano.max_new_tokens =
+          std::stoi(arg.substr(sizeof(kMaxNewTokens) - 1));
+    } else if (arg.find(kTemperature) == 0) {
+      config.model_config.funasr_nano.temperature =
+          std::stof(arg.substr(sizeof(kTemperature) - 1));
+    } else if (arg.find(kTopP) == 0) {
+      config.model_config.funasr_nano.top_p =
+          std::stof(arg.substr(sizeof(kTopP) - 1));
+    } else if (arg.find(kNumThreads) == 0) {
+      config.model_config.num_threads =
+          std::stoi(arg.substr(sizeof(kNumThreads) - 1));
+    } else if (arg.find(kProvider) == 0) {
+      config.model_config.provider = arg.substr(sizeof(kProvider) - 1);
+    } else if (arg[0] != '-') {
+      // This should be the audio file
+      std::string wave_filename = arg;
+
+      std::cout << "Loading model...\n";
+      std::cout << "  encoder_adaptor: "
+                << config.model_config.funasr_nano.encoder_adaptor << "\n";
+      std::cout << "  llm_prefill: "
+                << config.model_config.funasr_nano.llm_prefill << "\n";
+      std::cout << "  llm_decode: "
+                << config.model_config.funasr_nano.llm_decode << "\n";
+      std::cout << "  tokenizer: " << config.model_config.funasr_nano.tokenizer
+                << "\n";
+      std::cout << "  embedding: "
+                << config.model_config.funasr_nano.embedding << "\n";
+
+      const auto begin_init = std::chrono::steady_clock::now();
+
+      OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
+      if (!recognizer.Get()) {
+        std::cerr << "Failed to create recognizer. Please check your config.\n";
+        return -1;
+      }
+
+      const auto end_init = std::chrono::steady_clock::now();
+      float elapsed_seconds_init =
+          std::chrono::duration_cast<std::chrono::milliseconds>(end_init -
+                                                                begin_init)
+              .count() /
+          1000.;
+      std::cout << "Model loaded in " << elapsed_seconds_init << " seconds\n";
+
+      Wave wave = ReadWave(wave_filename);
+      if (wave.samples.empty()) {
+        std::cerr << "Failed to read: '" << wave_filename << "'\n";
+        return -1;
+      }
+
+      std::cout << "Audio file: " << wave_filename << "\n";
+      std::cout << "Sample rate: " << wave.sample_rate << " Hz\n";
+      std::cout << "Duration: "
+                << wave.samples.size() / static_cast<float>(wave.sample_rate)
+                << " seconds\n";
+
+      std::cout << "\nStart recognition...\n";
+      const auto begin = std::chrono::steady_clock::now();
+
+      OfflineStream stream = recognizer.CreateStream();
+      stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
+                            wave.samples.size());
+
+      recognizer.Decode(&stream);
+
+      OfflineRecognizerResult result = recognizer.GetResult(&stream);
+
+      const auto end = std::chrono::steady_clock::now();
+      const float elapsed_seconds =
+          std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
+              .count() /
+          1000.;
+      float duration =
+          wave.samples.size() / static_cast<float>(wave.sample_rate);
+      float rtf = elapsed_seconds / duration;
+
+      std::cout << "Text: " << result.text << "\n";
+      std::cout << "Audio duration: " << duration << "s\n";
+      std::cout << "Processing time: " << elapsed_seconds << "s\n";
+      std::cout << "Real-time factor (RTF): " << rtf << "\n";
+
+      return 0;
+    }
+  }
+
+  std::cerr << "Error: Please provide an audio file.\n";
+  std::cerr << kUsageMessage << "\n";
+  return -1;
+}
+
diff --git a/python-api-examples/offline-funasr-nano-decode-files.py b/python-api-examples/offline-funasr-nano-decode-files.py
new file mode 100644
index 00000000..31e1c22f
--- /dev/null
+++ b/python-api-examples/offline-funasr-nano-decode-files.py
@@ -0,0 +1,211 @@
+#!/usr/bin/env python3
+#
+# Copyright (c)  2025  zengyw
+#
+"""
+Decode audio files using FunASR-nano models with sherpa-onnx Python API.
+
+This script demonstrates how to use FunASR-nano models for offline speech recognition.
+
+Usage:
+    python offline-funasr-nano-decode-files.py \
+        --encoder-adaptor=/path/to/encoder_adaptor.onnx \
+        --llm-prefill=/path/to/llm_prefill.onnx \
+        --llm-decode=/path/to/llm_decode.onnx \
+        --tokenizer=/path/to/Qwen3-0.6B \
+        --embedding=/path/to/embedding.onnx \
+        [--num-threads=4] \
+        [--provider=cpu] \
+        audio1.wav audio2.wav ...
+"""
+
+import argparse
+import sys
+from pathlib import Path
+
+import soundfile as sf
+
+try:
+    import sherpa_onnx
+except ImportError:
+    print("Please install sherpa-onnx: pip install sherpa-onnx")
+    sys.exit(1)
+
+
+def get_args():
+    parser = argparse.ArgumentParser(
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        description=__doc__,
+    )
+
+    parser.add_argument(
+        "--encoder-adaptor",
+        type=str,
+        required=True,
+        help="Path to encoder_adaptor.onnx",
+    )
+
+    parser.add_argument(
+        "--llm-prefill",
+        type=str,
+        required=True,
+        help="Path to llm_prefill.onnx (KV cache mode)",
+    )
+
+    parser.add_argument(
+        "--llm-decode",
+        type=str,
+        required=True,
+        help="Path to llm_decode.onnx (KV cache mode)",
+    )
+
+    parser.add_argument(
+        "--tokenizer",
+        type=str,
+        required=True,
+        help="Path to tokenizer directory (e.g., Qwen3-0.6B)",
+    )
+
+    parser.add_argument(
+        "--embedding",
+        type=str,
+        required=True,
+        help="Path to embedding.onnx",
+    )
+
+    parser.add_argument(
+        "--system-prompt",
+        type=str,
+        default="You are a helpful assistant.",
+        help="System prompt for FunASR-nano",
+    )
+
+    parser.add_argument(
+        "--user-prompt",
+        type=str,
+        default="Transcription:",
+        help="User prompt template for FunASR-nano",
+    )
+
+    parser.add_argument(
+        "--max-new-tokens",
+        type=int,
+        default=512,
+        help="Maximum number of new tokens to generate",
+    )
+
+    parser.add_argument(
+        "--temperature",
+        type=float,
+        default=0.3,
+        help="Sampling temperature",
+    )
+
+    parser.add_argument(
+        "--top-p",
+        type=float,
+        default=0.8,
+        help="Top-p (nucleus) sampling threshold",
+    )
+
+    parser.add_argument(
+        "--seed",
+        type=int,
+        default=42,
+        help="Random seed",
+    )
+
+    parser.add_argument(
+        "--num-threads",
+        type=int,
+        default=2,
+        help="Number of threads for neural network computation",
+    )
+
+    parser.add_argument(
+        "--provider",
+        type=str,
+        default="cpu",
+        choices=["cpu", "cuda"],
+        help="Provider: cpu or cuda",
+    )
+
+    parser.add_argument(
+        "--debug",
+        action="store_true",
+        help="True to print model information while loading",
+    )
+
+    parser.add_argument(
+        "sound_files",
+        type=str,
+        nargs="+",
+        help="The input sound file(s) to decode. "
+        "Each file must be of single channel, 16-bit PCM encoded wav file. "
+        "Its sample rate can be arbitrary and does not need to be 16kHz.",
+    )
+
+    return parser.parse_args()
+
+
+def create_recognizer(args) -> sherpa_onnx.OfflineRecognizer:
+    return sherpa_onnx.OfflineRecognizer.from_funasr_nano(
+        encoder_adaptor=args.encoder_adaptor,
+        llm_prefill=args.llm_prefill,
+        llm_decode=args.llm_decode,
+        embedding=args.embedding,
+        tokenizer=args.tokenizer,
+        num_threads=args.num_threads,
+        provider=args.provider,
+        debug=args.debug,
+        system_prompt=args.system_prompt,
+        user_prompt=args.user_prompt,
+        max_new_tokens=args.max_new_tokens,
+        temperature=args.temperature,
+        top_p=args.top_p,
+        seed=args.seed,
+    )
+
+
+def decode_file(
+    recognizer: sherpa_onnx.OfflineRecognizer,
+    filename: str,
+):
+    """Decode a single audio file."""
+    audio, sample_rate = sf.read(filename, dtype="float32", always_2d=True)
+    audio = audio[:, 0]  # only use the first channel
+
+    stream = recognizer.create_stream()
+    stream.accept_waveform(sample_rate, audio)
+    recognizer.decode_stream(stream)
+    result = stream.result
+    return result
+
+
+def main():
+    args = get_args()
+
+    print("Creating recognizer...")
+    recognizer = create_recognizer(args)
+    print("Recognizer created!")
+
+    print(f"\nDecoding {len(args.sound_files)} file(s)...\n")
+
+    for sound_file in args.sound_files:
+        if not Path(sound_file).exists():
+            print(f"Error: File not found: {sound_file}", file=sys.stderr)
+            continue
+
+        print(f"Processing: {sound_file}")
+        result = decode_file(recognizer, sound_file)
+
+        print(f"Text: {result.text}")
+        if result.tokens:
+            print(f"Tokens: {result.tokens}")
+        if result.timestamps:
+            print(f"Timestamps: {[f'{t:.2f}' for t in result.timestamps]}")
+        print()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index 8c293154..41c9a0fd 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -511,6 +511,30 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.omnilingual.model =
       SHERPA_ONNX_OR(config->model_config.omnilingual.model, "");
 
+  recognizer_config.model_config.funasr_nano.encoder_adaptor =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.encoder_adaptor, "");
+  recognizer_config.model_config.funasr_nano.llm_prefill =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.llm_prefill, "");
+  recognizer_config.model_config.funasr_nano.llm_decode =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.llm_decode, "");
+  recognizer_config.model_config.funasr_nano.embedding =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.embedding, "");
+  recognizer_config.model_config.funasr_nano.tokenizer =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.tokenizer, "");
+  recognizer_config.model_config.funasr_nano.system_prompt =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.system_prompt,
+                     "You are a helpful assistant.");
+  recognizer_config.model_config.funasr_nano.user_prompt =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.user_prompt,
+                     "");
+  recognizer_config.model_config.funasr_nano.max_new_tokens =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.max_new_tokens, 512);
+  recognizer_config.model_config.funasr_nano.temperature =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.temperature, 0.3f);
+  recognizer_config.model_config.funasr_nano.top_p =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.top_p, 0.8f);
+  recognizer_config.model_config.funasr_nano.seed =
+      SHERPA_ONNX_OR(config->model_config.funasr_nano.seed, 42);
   recognizer_config.model_config.medasr.model =
       SHERPA_ONNX_OR(config->model_config.medasr.model, "");
 
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index 15a464b9..768380a8 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -484,6 +484,19 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
   const char *model;
 } SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
 
+SHERPA_ONNX_API typedef struct SherpaOnnxOfflineFunASRNanoModelConfig {
+  const char *encoder_adaptor;
+  const char *llm_prefill;
+  const char *llm_decode;
+  const char *embedding;
+  const char *tokenizer;
+  const char *system_prompt;
+  const char *user_prompt;
+  int32_t max_new_tokens;
+  float temperature;
+  float top_p;
+  int32_t seed;
+} SherpaOnnxOfflineFunASRNanoModelConfig;
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineMedAsrCtcModelConfig {
   const char *model;
 } SherpaOnnxOfflineMedAsrCtcModelConfig;
@@ -515,6 +528,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineCanaryModelConfig canary;
   SherpaOnnxOfflineWenetCtcModelConfig wenet_ctc;
   SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+  SherpaOnnxOfflineFunASRNanoModelConfig funasr_nano;
   SherpaOnnxOfflineMedAsrCtcModelConfig medasr;
 } SherpaOnnxOfflineModelConfig;
 
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index 33b3c8a1..947b5e40 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -272,6 +272,28 @@ static SherpaOnnxOfflineRecognizerConfig Convert(
   c.model_config.omnilingual.model =
       config.model_config.omnilingual.model.c_str();
 
+  c.model_config.funasr_nano.encoder_adaptor =
+      config.model_config.funasr_nano.encoder_adaptor.c_str();
+  c.model_config.funasr_nano.llm_prefill =
+      config.model_config.funasr_nano.llm_prefill.c_str();
+  c.model_config.funasr_nano.llm_decode =
+      config.model_config.funasr_nano.llm_decode.c_str();
+  c.model_config.funasr_nano.embedding =
+      config.model_config.funasr_nano.embedding.c_str();
+  c.model_config.funasr_nano.tokenizer =
+      config.model_config.funasr_nano.tokenizer.c_str();
+  c.model_config.funasr_nano.system_prompt =
+      config.model_config.funasr_nano.system_prompt.c_str();
+  c.model_config.funasr_nano.user_prompt =
+      config.model_config.funasr_nano.user_prompt.c_str();
+  c.model_config.funasr_nano.max_new_tokens =
+      config.model_config.funasr_nano.max_new_tokens;
+  c.model_config.funasr_nano.temperature =
+      config.model_config.funasr_nano.temperature;
+  c.model_config.funasr_nano.top_p =
+      config.model_config.funasr_nano.top_p;
+  c.model_config.funasr_nano.seed =
+      config.model_config.funasr_nano.seed;
   c.model_config.medasr.model = config.model_config.medasr.model.c_str();
 
   c.lm_config.model = config.lm_config.model.c_str();
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index 42aaa73b..9ba5c985 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -283,6 +283,20 @@ struct SHERPA_ONNX_API OfflineMoonshineModelConfig {
   std::string cached_decoder;
 };
 
+struct SHERPA_ONNX_API OfflineFunASRNanoModelConfig {
+  std::string encoder_adaptor;
+  std::string llm_prefill;
+  std::string llm_decode;
+  std::string embedding;
+  std::string tokenizer;
+  std::string system_prompt = "You are a helpful assistant.";
+  std::string user_prompt = "";
+  int32_t max_new_tokens = 512;
+  float temperature = 0.3f;
+  float top_p = 0.8f;
+  int32_t seed = 42;
+};
+
 struct SHERPA_ONNX_API OfflineModelConfig {
   OfflineTransducerModelConfig transducer;
   OfflineParaformerModelConfig paraformer;
@@ -306,6 +320,7 @@ struct SHERPA_ONNX_API OfflineModelConfig {
   OfflineCanaryModelConfig canary;
   OfflineWenetCtcModelConfig wenet_ctc;
   OfflineOmnilingualAsrCtcModelConfig omnilingual;
+  OfflineFunASRNanoModelConfig funasr_nano;
   OfflineMedAsrCtcModelConfig medasr;
 };
 
diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
index d21995b5..a3221341 100755
--- a/sherpa-onnx/csrc/CMakeLists.txt
+++ b/sherpa-onnx/csrc/CMakeLists.txt
@@ -245,6 +245,13 @@ if(SHERPA_ONNX_ENABLE_QNN)
   )
 endif()
 
+list(APPEND sources
+  offline-funasr-nano-model-config.cc
+  offline-funasr-nano-model.cc
+  offline-recognizer-funasr-nano-impl.cc
+  funasr-nano-tokenizer.cc
+)
+
 if(SHERPA_ONNX_ENABLE_TTS)
   list(APPEND sources
     character-lexicon.cc
diff --git a/sherpa-onnx/csrc/funasr-nano-tokenizer.cc b/sherpa-onnx/csrc/funasr-nano-tokenizer.cc
new file mode 100644
index 00000000..81dacdd7
--- /dev/null
+++ b/sherpa-onnx/csrc/funasr-nano-tokenizer.cc
@@ -0,0 +1,1368 @@
+// sherpa-onnx/csrc/funasr-nano-tokenizer.cc
+//
+// Copyright (c)  2025  zengyw
+
+#include "sherpa-onnx/csrc/funasr-nano-tokenizer.h"
+
+#include <algorithm>
+#include <cctype>
+#include <cstdint>
+#include <cstring>
+#include <limits>
+#include <sstream>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+namespace {
+
+static std::string FindTokenizerJson(const std::string &tokenizer_dir) {
+  std::string p = tokenizer_dir + "/tokenizer.json";
+  if (FileExists(p)) return p;
+  return "";
+}
+
+static std::string FindVocabJson(const std::string &tokenizer_dir) {
+  std::string p = tokenizer_dir + "/vocab.json";
+  if (FileExists(p)) return p;
+  return "";
+}
+
+static std::string FindMergesTxt(const std::string &tokenizer_dir) {
+  std::string p = tokenizer_dir + "/merges.txt";
+  if (FileExists(p)) return p;
+  return "";
+}
+
+static std::string LoadBytesFromFile(const std::string &path) {
+  std::vector<char> data = ReadFile(path);
+  if (data.empty()) return "";
+  return std::string(data.data(), data.size());
+}
+
+#if __ANDROID_API__ >= 9
+static std::string LoadBytesFromFile(AAssetManager *mgr,
+                                     const std::string &path) {
+  std::vector<char> data = ReadFile(mgr, path);
+  if (data.empty()) return "";
+  return std::string(data.data(), data.size());
+}
+#endif
+
+#if __OHOS__
+static std::string LoadBytesFromFile(NativeResourceManager *mgr,
+                                     const std::string &path) {
+  std::vector<char> data = ReadFile(mgr, path);
+  if (data.empty()) return "";
+  return std::string(data.data(), data.size());
+}
+#endif
+
+static inline void TrimInPlace(std::string *s) {
+  if (!s) return;
+  auto &x = *s;
+  size_t b = x.find_first_not_of(" \t\r\n");
+  if (b == std::string::npos) {
+    x.clear();
+    return;
+  }
+  size_t e = x.find_last_not_of(" \t\r\n");
+  x = x.substr(b, e - b + 1);
+}
+
+static inline void AppendUtf8(uint32_t cp, std::string *out) {
+  if (!out) return;
+  if (cp <= 0x7Fu) {
+    out->push_back(static_cast<char>(cp));
+  } else if (cp <= 0x7FFu) {
+    out->push_back(static_cast<char>(0xC0u | ((cp >> 6) & 0x1Fu)));
+    out->push_back(static_cast<char>(0x80u | (cp & 0x3Fu)));
+  } else if (cp <= 0xFFFFu) {
+    out->push_back(static_cast<char>(0xE0u | ((cp >> 12) & 0x0Fu)));
+    out->push_back(static_cast<char>(0x80u | ((cp >> 6) & 0x3Fu)));
+    out->push_back(static_cast<char>(0x80u | (cp & 0x3Fu)));
+  } else {
+    out->push_back(static_cast<char>(0xF0u | ((cp >> 18) & 0x07u)));
+    out->push_back(static_cast<char>(0x80u | ((cp >> 12) & 0x3Fu)));
+    out->push_back(static_cast<char>(0x80u | ((cp >> 6) & 0x3Fu)));
+    out->push_back(static_cast<char>(0x80u | (cp & 0x3Fu)));
+  }
+}
+
+static inline bool Utf8Next(const std::string &s, size_t *i, uint32_t *cp,
+                            size_t *nbytes) {
+  if (!i || !cp || !nbytes) return false;
+  if (*i >= s.size()) return false;
+  const unsigned char c = static_cast<unsigned char>(s[*i]);
+  if (c < 0x80) {
+    *cp = c;
+    *nbytes = 1;
+    return true;
+  }
+  if ((c >> 5) == 0x6) {  // 110xxxxx
+    if (*i + 1 >= s.size()) return false;
+    const unsigned char c1 = static_cast<unsigned char>(s[*i + 1]);
+    if ((c1 >> 6) != 0x2) return false;
+    *cp = ((c & 0x1F) << 6) | (c1 & 0x3F);
+    *nbytes = 2;
+    return true;
+  }
+  if ((c >> 4) == 0xE) {  // 1110xxxx
+    if (*i + 2 >= s.size()) return false;
+    const unsigned char c1 = static_cast<unsigned char>(s[*i + 1]);
+    const unsigned char c2 = static_cast<unsigned char>(s[*i + 2]);
+    if ((c1 >> 6) != 0x2 || (c2 >> 6) != 0x2) return false;
+    *cp = ((c & 0x0F) << 12) | ((c1 & 0x3F) << 6) | (c2 & 0x3F);
+    *nbytes = 3;
+    return true;
+  }
+  if ((c >> 3) == 0x1E) {  // 11110xxx
+    if (*i + 3 >= s.size()) return false;
+    const unsigned char c1 = static_cast<unsigned char>(s[*i + 1]);
+    const unsigned char c2 = static_cast<unsigned char>(s[*i + 2]);
+    const unsigned char c3 = static_cast<unsigned char>(s[*i + 3]);
+    if ((c1 >> 6) != 0x2 || (c2 >> 6) != 0x2 || (c3 >> 6) != 0x2) return false;
+    *cp = ((c & 0x07) << 18) | ((c1 & 0x3F) << 12) | ((c2 & 0x3F) << 6) |
+          (c3 & 0x3F);
+    *nbytes = 4;
+    return true;
+  }
+  return false;
+}
+
+static inline bool IsNewline(uint32_t cp) { return cp == '\n' || cp == '\r'; }
+
+static inline bool IsAsciiSpace(uint32_t cp) { return cp == ' '; }
+
+static inline bool IsWhitespace(uint32_t cp) {
+  return cp == ' ' || cp == '\t' || cp == '\n' || cp == '\r' || cp == '\v' ||
+         cp == '\f';
+}
+
+static inline bool IsAsciiAlpha(uint32_t cp) {
+  return (cp >= 'a' && cp <= 'z') || (cp >= 'A' && cp <= 'Z');
+}
+
+static inline bool IsAsciiDigit(uint32_t cp) { return (cp >= '0' && cp <= '9'); }
+
+// A light-weight unicode letter/number approximation good enough for
+// Qwen3(English/Chinese/Japanese/Korean + common scripts).
+static inline bool IsLetter(uint32_t cp) {
+  if (IsAsciiAlpha(cp)) return true;
+
+  // CJK Unified Ideographs
+  if (cp >= 0x4E00 && cp <= 0x9FFF) return true;
+  // CJK Extension A
+  if (cp >= 0x3400 && cp <= 0x4DBF) return true;
+  // Hiragana/Katakana
+  if (cp >= 0x3040 && cp <= 0x30FF) return true;
+  // Hangul syllables
+  if (cp >= 0xAC00 && cp <= 0xD7AF) return true;
+  // Hangul Jamo
+  if (cp >= 0x1100 && cp <= 0x11FF) return true;
+
+  // Latin-1 Supplement + Latin Extended (covers most European letters)
+  if (cp >= 0x00C0 && cp <= 0x02AF) return true;
+
+  return false;
+}
+
+static inline bool IsNumber(uint32_t cp) {
+  if (IsAsciiDigit(cp)) return true;
+  // Fullwidth digits
+  if (cp >= 0xFF10 && cp <= 0xFF19) return true;
+  return false;
+}
+
+class JsonReader {
+ public:
+  explicit JsonReader(const std::string &s) : s_(s), p_(0) {}
+
+  bool SeekToKey(const std::string &key) {
+    std::string needle = "\"" + key + "\"";
+    size_t pos = s_.find(needle);
+    if (pos == std::string::npos) return false;
+    p_ = pos + needle.size();
+    return true;
+  }
+
+  void SkipWs() {
+    while (p_ < s_.size()) {
+      char c = s_[p_];
+      if (c == ' ' || c == '\t' || c == '\r' || c == '\n') {
+        ++p_;
+      } else {
+        break;
+      }
+    }
+  }
+
+  bool Consume(char c) {
+    SkipWs();
+    if (p_ < s_.size() && s_[p_] == c) {
+      ++p_;
+      return true;
+    }
+    return false;
+  }
+
+  bool Peek(char *c) const {
+    if (!c) return false;
+    size_t q = p_;
+    while (q < s_.size()) {
+      char x = s_[q];
+      if (x == ' ' || x == '\t' || x == '\r' || x == '\n') {
+        ++q;
+        continue;
+      }
+      *c = x;
+      return true;
+    }
+    return false;
+  }
+
+  bool ParseString(std::string *out) {
+    if (!out) return false;
+    SkipWs();
+    if (p_ >= s_.size() || s_[p_] != '"') return false;
+    ++p_;
+    std::string r;
+    while (p_ < s_.size()) {
+      char c = s_[p_++];
+      if (c == '"') {
+        *out = std::move(r);
+        return true;
+      }
+      if (c != '\\') {
+        r.push_back(c);
+        continue;
+      }
+      if (p_ >= s_.size()) return false;
+      char esc = s_[p_++];
+      switch (esc) {
+        case '"':
+          r.push_back('"');
+          break;
+        case '\\':
+          r.push_back('\\');
+          break;
+        case '/':
+          r.push_back('/');
+          break;
+        case 'b':
+          r.push_back('\b');
+          break;
+        case 'f':
+          r.push_back('\f');
+          break;
+        case 'n':
+          r.push_back('\n');
+          break;
+        case 'r':
+          r.push_back('\r');
+          break;
+        case 't':
+          r.push_back('\t');
+          break;
+        case 'u': {
+          if (p_ + 4 > s_.size()) return false;
+          uint32_t u = 0;
+          for (int i = 0; i < 4; ++i) {
+            char h = s_[p_++];
+            u <<= 4;
+            if (h >= '0' && h <= '9')
+              u |= (h - '0');
+            else if (h >= 'a' && h <= 'f')
+              u |= (h - 'a' + 10);
+            else if (h >= 'A' && h <= 'F')
+              u |= (h - 'A' + 10);
+            else
+              return false;
+          }
+          if (u >= 0xD800 && u <= 0xDBFF) {
+            size_t save = p_;
+            if (p_ + 6 <= s_.size() && s_[p_] == '\\' && s_[p_ + 1] == 'u') {
+              p_ += 2;
+              uint32_t v = 0;
+              for (int i = 0; i < 4; ++i) {
+                char h = s_[p_++];
+                v <<= 4;
+                if (h >= '0' && h <= '9')
+                  v |= (h - '0');
+                else if (h >= 'a' && h <= 'f')
+                  v |= (h - 'a' + 10);
+                else if (h >= 'A' && h <= 'F')
+                  v |= (h - 'A' + 10);
+                else
+                  return false;
+              }
+              if (v >= 0xDC00 && v <= 0xDFFF) {
+                uint32_t cp = 0x10000 + (((u - 0xD800) << 10) | (v - 0xDC00));
+                AppendUtf8(cp, &r);
+                break;
+              }
+            }
+            p_ = save;
+          }
+          AppendUtf8(u, &r);
+          break;
+        }
+        default:
+          return false;
+      }
+    }
+    return false;
+  }
+
+  bool ParseBool(bool *out) {
+    if (!out) return false;
+    SkipWs();
+    if (p_ + 4 <= s_.size() && s_.compare(p_, 4, "true") == 0) {
+      p_ += 4;
+      *out = true;
+      return true;
+    }
+    if (p_ + 5 <= s_.size() && s_.compare(p_, 5, "false") == 0) {
+      p_ += 5;
+      *out = false;
+      return true;
+    }
+    return false;
+  }
+
+  bool ParseInt64(int64_t *out) {
+    if (!out) return false;
+    SkipWs();
+    if (p_ >= s_.size()) return false;
+    bool neg = false;
+    if (s_[p_] == '-') {
+      neg = true;
+      ++p_;
+    }
+    if (p_ >= s_.size() || !std::isdigit(static_cast<unsigned char>(s_[p_]))) {
+      return false;
+    }
+    int64_t v = 0;
+    while (p_ < s_.size() &&
+           std::isdigit(static_cast<unsigned char>(s_[p_]))) {
+      int d = s_[p_] - '0';
+      if (v > (std::numeric_limits<int64_t>::max() - d) / 10) return false;
+      v = v * 10 + d;
+      ++p_;
+    }
+    *out = neg ? -v : v;
+    return true;
+  }
+
+  bool SkipValue() {
+    SkipWs();
+    if (p_ >= s_.size()) return false;
+    char c = s_[p_];
+    if (c == '"') {
+      std::string tmp;
+      return ParseString(&tmp);
+    }
+    if (c == '{') return SkipObject();
+    if (c == '[') return SkipArray();
+    if (c == 't' || c == 'f') {
+      bool b = false;
+      return ParseBool(&b);
+    }
+    if (c == 'n') {
+      if (p_ + 4 <= s_.size() && s_.compare(p_, 4, "null") == 0) {
+        p_ += 4;
+        return true;
+      }
+      return false;
+    }
+    int64_t v = 0;
+    return ParseInt64(&v);
+  }
+
+ private:
+  bool SkipObject() {
+    if (!Consume('{')) return false;
+    SkipWs();
+    if (Consume('}')) return true;
+    while (true) {
+      std::string k;
+      if (!ParseString(&k)) return false;
+      if (!Consume(':')) return false;
+      if (!SkipValue()) return false;
+      SkipWs();
+      if (Consume('}')) return true;
+      if (!Consume(',')) return false;
+    }
+  }
+
+  bool SkipArray() {
+    if (!Consume('[')) return false;
+    SkipWs();
+    if (Consume(']')) return true;
+    while (true) {
+      if (!SkipValue()) return false;
+      SkipWs();
+      if (Consume(']')) return true;
+      if (!Consume(',')) return false;
+    }
+  }
+
+ private:
+  const std::string &s_;
+  size_t p_;
+};
+
+namespace {
+static inline int64_t TokenToIdOrDefault(
+    const std::unordered_map<std::string, int32_t> &vocab,
+    const std::string &tok, int64_t def_val) {
+  auto it = vocab.find(tok);
+  if (it == vocab.end()) return def_val;
+  return static_cast<int64_t>(it->second);
+}
+}  // namespace
+
+// Build bytes_to_unicode mapping (ByteLevel encoder/decoder).
+static void BuildBytesToUnicode(
+    std::string byte_to_unicode[256],
+    std::unordered_map<std::string, uint8_t> *unicode_to_byte) {
+  std::vector<uint32_t> bs;
+  bs.reserve(256);
+  for (uint32_t c = 33; c <= 126; ++c) bs.push_back(c);
+  for (uint32_t c = 161; c <= 172; ++c) bs.push_back(c);
+  for (uint32_t c = 174; c <= 255; ++c) bs.push_back(c);
+
+  std::vector<uint32_t> cs = bs;
+  cs.reserve(256);
+  uint32_t n = 0;
+  auto contains = [&](uint32_t b) -> bool {
+    return std::find(bs.begin(), bs.end(), b) != bs.end();
+  };
+  for (uint32_t b = 0; b <= 255; ++b) {
+    if (!contains(b)) {
+      bs.push_back(b);
+      cs.push_back(256 + n);
+      ++n;
+    }
+  }
+
+  if (unicode_to_byte) unicode_to_byte->clear();
+  for (size_t i = 0; i < bs.size(); ++i) {
+    uint32_t b = bs[i];
+    uint32_t c = cs[i];
+    std::string u;
+    AppendUtf8(c, &u);
+    byte_to_unicode[b] = u;
+    if (unicode_to_byte) {
+      (*unicode_to_byte)[u] = static_cast<uint8_t>(b);
+    }
+  }
+}
+
+// Parse vocab.json: {"token": id, ...}
+static bool ParseVocabJson(const std::string &blob,
+                           std::unordered_map<std::string, int32_t> *out) {
+  if (!out) return false;
+  out->clear();
+  JsonReader r(blob);
+  r.SkipWs();
+  if (!r.Consume('{')) return false;
+  r.SkipWs();
+  if (r.Consume('}')) return true;
+
+  while (true) {
+    std::string key;
+    if (!r.ParseString(&key)) return false;
+    if (!r.Consume(':')) return false;
+    int64_t id64 = 0;
+    if (!r.ParseInt64(&id64)) return false;
+    if (id64 < 0 || id64 > std::numeric_limits<int32_t>::max()) return false;
+    (*out)[key] = static_cast<int32_t>(id64);
+
+    r.SkipWs();
+    if (r.Consume('}')) return true;
+    if (!r.Consume(',')) return false;
+  }
+}
+
+// Parse merges.txt: each non-comment line: "left right"
+static bool ParseMergesTxt(const std::string &blob,
+                           std::unordered_map<std::string, int32_t> *out) {
+  if (!out) return false;
+  out->clear();
+  std::istringstream is(blob);
+  std::string line;
+  int32_t rank = 0;
+  while (std::getline(is, line)) {
+    if (line.empty()) continue;
+    if (line.rfind("#version", 0) == 0) continue;
+    std::string left, right;
+    {
+      std::istringstream ls(line);
+      if (!(ls >> left >> right)) continue;
+    }
+    std::string key = left;
+    key.push_back('\t');
+    key.append(right);
+    (*out)[key] = rank++;
+  }
+  return true;
+}
+
+static inline bool IsWordChar(uint32_t cp) {
+  return IsLetter(cp) || IsNumber(cp) || cp == '_';
+}
+
+// A manual approximation for Qwen3 tokenizer Split regex.
+// The regex is in tokenizer.json pre_tokenizer Split. We avoid std::regex
+// due to missing \p{L}/\p{N} support in libc++/libstdc++ regex.
+static std::vector<std::string> SplitByQwen3Pattern(const std::string &text) {
+  std::vector<std::string> out;
+  out.reserve(text.size() / 2 + 1);
+
+  size_t i = 0;
+  while (i < text.size()) {
+    if (text[i] == '\'') {
+      auto lower = [](char c) -> char {
+        return static_cast<char>(std::tolower(static_cast<unsigned char>(c)));
+      };
+      if (i + 1 < text.size()) {
+        char c1 = lower(text[i + 1]);
+        if (c1 == 's' || c1 == 't' || c1 == 'm' || c1 == 'd') {
+          out.push_back(text.substr(i, 2));
+          i += 2;
+          continue;
+        }
+        if (i + 2 < text.size()) {
+          char c2 = lower(text[i + 2]);
+          if (c1 == 'r' && c2 == 'e') {
+            out.push_back(text.substr(i, 3));
+            i += 3;
+            continue;
+          }
+          if (c1 == 'v' && c2 == 'e') {
+            out.push_back(text.substr(i, 3));
+            i += 3;
+            continue;
+          }
+          if (c1 == 'l' && c2 == 'l') {
+            out.push_back(text.substr(i, 3));
+            i += 3;
+            continue;
+          }
+        }
+      }
+    }
+
+    size_t cur = i;
+    uint32_t cp = 0;
+    size_t n = 0;
+    if (!Utf8Next(text, &cur, &cp, &n) || n == 0) {
+      out.push_back(text.substr(i, 1));
+      i += 1;
+      continue;
+    }
+
+    auto peek_next_cp = [&](size_t pos, uint32_t *cp2, size_t *n2) -> bool {
+      size_t t = pos;
+      uint32_t x = 0;
+      size_t nn = 0;
+      if (!Utf8Next(text, &t, &x, &nn)) return false;
+      if (cp2) *cp2 = x;
+      if (n2) *n2 = nn;
+      return true;
+    };
+
+    {
+      uint32_t next_cp = 0;
+      size_t next_n = 0;
+      bool has_next = peek_next_cp(i + n, &next_cp, &next_n);
+
+      bool cur_ok_prefix = (!IsNewline(cp) && !IsLetter(cp) && !IsNumber(cp));
+      bool cur_is_letter = IsLetter(cp);
+
+      if (cur_is_letter || (cur_ok_prefix && has_next && IsLetter(next_cp))) {
+        size_t start = i;
+        size_t j = i;
+        if (!cur_is_letter) {
+          j += n;
+          while (j < text.size()) {
+            size_t t = j;
+            uint32_t cpl = 0;
+            size_t nl = 0;
+            if (!Utf8Next(text, &t, &cpl, &nl)) break;
+            if (!IsLetter(cpl)) break;
+            j += nl;
+          }
+        } else {
+          j = i;
+          while (j < text.size()) {
+            size_t t = j;
+            uint32_t cpl = 0;
+            size_t nl = 0;
+            if (!Utf8Next(text, &t, &cpl, &nl)) break;
+            if (!IsLetter(cpl)) break;
+            j += nl;
+          }
+        }
+        out.push_back(text.substr(start, j - start));
+        i = j;
+        continue;
+      }
+    }
+
+    if (IsNumber(cp)) {
+      out.push_back(text.substr(i, n));
+      i += n;
+      continue;
+    }
+
+    {
+      bool starts_with_space_prefix = IsAsciiSpace(cp);
+      size_t start = i;
+      size_t j = i;
+
+      auto is_punct_like = [&](uint32_t x) -> bool {
+        return (!IsWhitespace(x) && !IsLetter(x) && !IsNumber(x));
+      };
+
+      if (starts_with_space_prefix) {
+        uint32_t next_cp = 0;
+        size_t next_n = 0;
+        if (peek_next_cp(i + n, &next_cp, &next_n) && is_punct_like(next_cp)) {
+          j += n;
+          while (j < text.size()) {
+            size_t t = j;
+            uint32_t cx = 0;
+            size_t nx = 0;
+            if (!Utf8Next(text, &t, &cx, &nx)) break;
+            if (!is_punct_like(cx)) break;
+            j += nx;
+          }
+          while (j < text.size()) {
+            size_t t = j;
+            uint32_t cx = 0;
+            size_t nx = 0;
+            if (!Utf8Next(text, &t, &cx, &nx)) break;
+            if (!IsNewline(cx)) break;
+            j += nx;
+          }
+          out.push_back(text.substr(start, j - start));
+          i = j;
+          continue;
+        }
+      } else if (is_punct_like(cp)) {
+        while (j < text.size()) {
+          size_t t = j;
+          uint32_t cx = 0;
+          size_t nx = 0;
+          if (!Utf8Next(text, &t, &cx, &nx)) break;
+            if (!is_punct_like(cx)) break;
+            j += nx;
+          }
+          while (j < text.size()) {
+          size_t t = j;
+          uint32_t cx = 0;
+          size_t nx = 0;
+          if (!Utf8Next(text, &t, &cx, &nx)) break;
+          if (!IsNewline(cx)) break;
+          j += nx;
+        }
+        out.push_back(text.substr(start, j - start));
+        i = j;
+        continue;
+      }
+    }
+
+    {
+      if (IsWhitespace(cp)) {
+        size_t start = i;
+        size_t j = i;
+
+        bool saw_newline = false;
+        while (j < text.size()) {
+          size_t t = j;
+          uint32_t cx = 0;
+          size_t nx = 0;
+          if (!Utf8Next(text, &t, &cx, &nx)) break;
+          if (IsNewline(cx)) {
+            saw_newline = true;
+            break;
+          }
+          if (!IsWhitespace(cx)) break;
+          j += nx;
+        }
+
+        if (saw_newline) {
+          while (j < text.size()) {
+            size_t t = j;
+            uint32_t cx = 0;
+            size_t nx = 0;
+            if (!Utf8Next(text, &t, &cx, &nx)) break;
+            if (!IsNewline(cx)) break;
+            j += nx;
+          }
+          out.push_back(text.substr(start, j - start));
+          i = j;
+          continue;
+        }
+      }
+    }
+
+    if (IsWhitespace(cp)) {
+      bool only_ws_to_end = true;
+      size_t j = i;
+      while (j < text.size()) {
+        size_t t = j;
+        uint32_t cx = 0;
+        size_t nx = 0;
+        if (!Utf8Next(text, &t, &cx, &nx)) break;
+        if (!IsWhitespace(cx)) {
+          only_ws_to_end = false;
+          break;
+        }
+        j += nx;
+      }
+      if (only_ws_to_end) {
+        out.push_back(text.substr(i));
+        break;
+      }
+    }
+
+    if (IsWhitespace(cp)) {
+      size_t start = i;
+      size_t j = i;
+      while (j < text.size()) {
+        size_t t = j;
+        uint32_t cx = 0;
+        size_t nx = 0;
+        if (!Utf8Next(text, &t, &cx, &nx)) break;
+        if (!IsWhitespace(cx)) break;
+        j += nx;
+      }
+      out.push_back(text.substr(start, j - start));
+      i = j;
+      continue;
+    }
+
+    out.push_back(text.substr(i, n));
+    i += n;
+  }
+
+  return out;
+}
+
+static std::vector<std::string> SplitUtf8ToChars(const std::string &s) {
+  std::vector<std::string> out;
+  out.reserve(s.size());
+  size_t i = 0;
+  while (i < s.size()) {
+    size_t t = i;
+    uint32_t cp = 0;
+    size_t n = 0;
+    if (!Utf8Next(s, &t, &cp, &n) || n == 0) {
+      out.push_back(s.substr(i, 1));
+      i += 1;
+      continue;
+    }
+    out.push_back(s.substr(i, n));
+    i += n;
+  }
+  return out;
+}
+
+static inline std::string MakeMergeKey(const std::string &a,
+                                       const std::string &b) {
+  std::string k = a;
+  k.push_back('\t');
+  k.append(b);
+  return k;
+}
+
+}  // namespace
+
+// Parse tokenizer.json added_tokens: extract objects with {id, content, ...}
+static bool ParseAddedTokensFromTokenizerJson(
+    const std::string &blob, std::vector<FunASRNanoTokenizer::AddedToken> *out) {
+  if (!out) return false;
+  out->clear();
+
+  JsonReader r(blob);
+  if (!r.SeekToKey("added_tokens")) return true;
+  if (!r.Consume(':')) return false;
+  if (!r.Consume('[')) return false;
+
+  r.SkipWs();
+  if (r.Consume(']')) return true;
+
+  while (true) {
+    if (!r.Consume('{')) return false;
+    FunASRNanoTokenizer::AddedToken t;
+
+    r.SkipWs();
+    if (!r.Consume('}')) {
+      while (true) {
+        std::string k;
+        if (!r.ParseString(&k)) return false;
+        if (!r.Consume(':')) return false;
+
+        if (k == "id") {
+          int64_t v = 0;
+          if (!r.ParseInt64(&v)) return false;
+          t.id = static_cast<int32_t>(v);
+        } else if (k == "content") {
+          if (!r.ParseString(&t.content)) return false;
+        } else if (k == "single_word") {
+          if (!r.ParseBool(&t.single_word)) return false;
+        } else if (k == "lstrip") {
+          if (!r.ParseBool(&t.lstrip)) return false;
+        } else if (k == "rstrip") {
+          if (!r.ParseBool(&t.rstrip)) return false;
+        } else if (k == "normalized") {
+          if (!r.ParseBool(&t.normalized)) return false;
+        } else if (k == "special") {
+          if (!r.ParseBool(&t.special)) return false;
+        } else {
+          if (!r.SkipValue()) return false;
+        }
+
+        r.SkipWs();
+        if (r.Consume('}')) break;
+        if (!r.Consume(',')) return false;
+      }
+    }
+
+    if (t.id >= 0 && !t.content.empty()) {
+      out->push_back(std::move(t));
+    }
+
+    r.SkipWs();
+    if (r.Consume(']')) return true;
+    if (!r.Consume(',')) return false;
+  }
+}
+
+// Build trie for AddedTokens longest match (byte-wise).
+void BuildAddedTokensTrie(
+    const std::vector<FunASRNanoTokenizer::AddedToken> &tokens,
+    std::vector<FunASRNanoTokenizer::TrieNode> *trie) {
+  if (!trie) return;
+  trie->clear();
+  trie->push_back(FunASRNanoTokenizer::TrieNode{});
+  for (int32_t i = 0; i < static_cast<int32_t>(tokens.size()); ++i) {
+    const auto &tok = tokens[i];
+    int32_t node = 0;
+    for (uint8_t b : std::vector<uint8_t>(tok.content.begin(),
+                                          tok.content.end())) {
+      auto it = (*trie)[node].next.find(b);
+      if (it == (*trie)[node].next.end()) {
+        int32_t new_node = static_cast<int32_t>(trie->size());
+        trie->push_back(FunASRNanoTokenizer::TrieNode{});
+        (*trie)[node].next.emplace(b, new_node);
+        node = new_node;
+      } else {
+        node = it->second;
+      }
+    }
+    (*trie)[node].token_index = i;
+  }
+}
+
+static void MergeVocabAndAddedTokens(
+    std::unordered_map<std::string, int32_t> *vocab,
+    const std::vector<FunASRNanoTokenizer::AddedToken> &added,
+    std::unordered_set<std::string> *added_contents) {
+  if (!vocab) return;
+  if (added_contents) added_contents->clear();
+
+  int32_t overwritten = 0;
+  for (const auto &t : added) {
+    if (t.id < 0 || t.content.empty()) continue;
+    if (added_contents) added_contents->insert(t.content);
+
+    auto it = vocab->find(t.content);
+    if (it != vocab->end() && it->second != t.id) {
+      ++overwritten;
+    }
+    (*vocab)[t.content] = t.id;
+  }
+
+  if (overwritten > 0) {
+    SHERPA_ONNX_LOGE(
+        "AddedTokens overwrote %d vocab entries with different ids. "
+        "This is expected for some tokenizers; keeping added-token ids.",
+        overwritten);
+  }
+}
+
+void BuildIdToToken(
+    const std::unordered_map<std::string, int32_t> &vocab,
+    const std::unordered_set<std::string> &added_contents,
+    std::vector<std::string> *id2token) {
+  if (!id2token) return;
+  int32_t max_id = -1;
+  for (const auto &kv : vocab) {
+    max_id = std::max(max_id, kv.second);
+  }
+  if (max_id < 0) {
+    id2token->clear();
+    return;
+  }
+  id2token->assign(static_cast<size_t>(max_id) + 1, std::string{});
+
+  int32_t dup = 0;
+  for (const auto &kv : vocab) {
+    const std::string &tok = kv.first;
+    int32_t id = kv.second;
+    if (id < 0) continue;
+    std::string &slot = (*id2token)[static_cast<size_t>(id)];
+    if (slot.empty()) {
+      slot = tok;
+      continue;
+    }
+    if (slot == tok) continue;
+
+    bool slot_is_added = added_contents.count(slot) > 0;
+    bool tok_is_added = added_contents.count(tok) > 0;
+    if (!slot_is_added && tok_is_added) {
+      slot = tok;
+    }
+    ++dup;
+  }
+
+  if (dup > 0) {
+    SHERPA_ONNX_LOGE(
+        "Detected %d duplicated id->token collisions while building id2token. "
+        "Kept added_tokens' string when possible.",
+        dup);
+  }
+}
+
+
+// Try to match an AddedToken at byte-position `pos`.
+// Returns (matched_len_bytes, token_index) or (0, -1) if no match.
+std::pair<int32_t, int32_t> MatchAddedToken(
+    const std::string &text, size_t pos,
+    const std::vector<FunASRNanoTokenizer::TrieNode> &trie) {
+  if (trie.empty()) return {0, -1};
+  int32_t node = 0;
+  int32_t best_idx = -1;
+  int32_t best_len = 0;
+
+  size_t i = pos;
+  while (i < text.size()) {
+    uint8_t b = static_cast<uint8_t>(text[i]);
+    auto it = trie[node].next.find(b);
+    if (it == trie[node].next.end()) break;
+    node = it->second;
+    ++i;
+    if (trie[node].token_index >= 0) {
+      best_idx = trie[node].token_index;
+      best_len = static_cast<int32_t>(i - pos);
+    }
+  }
+  return {best_len, best_idx};
+}
+
+FunASRNanoTokenizer::FunASRNanoTokenizer(const std::string &tokenizer_dir) {
+  Init(tokenizer_dir);
+}
+
+#if __ANDROID_API__ >= 9
+FunASRNanoTokenizer::FunASRNanoTokenizer(AAssetManager *mgr,
+                                         const std::string &tokenizer_dir) {
+  Init(mgr, tokenizer_dir);
+}
+#endif
+
+#if __OHOS__
+FunASRNanoTokenizer::FunASRNanoTokenizer(NativeResourceManager *mgr,
+                                         const std::string &tokenizer_dir) {
+  Init(mgr, tokenizer_dir);
+}
+#endif
+
+
+void FunASRNanoTokenizer::Init(const std::string &tokenizer_dir) {
+  std::string tok_json = FindTokenizerJson(tokenizer_dir);
+  if (tok_json.empty()) {
+    SHERPA_ONNX_LOGE("Cannot find tokenizer.json in: %s",
+                     tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  std::string vocab_json = FindVocabJson(tokenizer_dir);
+  if (vocab_json.empty()) {
+    SHERPA_ONNX_LOGE("Cannot find vocab.json in: %s", tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  std::string merges_txt = FindMergesTxt(tokenizer_dir);
+  if (merges_txt.empty()) {
+    SHERPA_ONNX_LOGE("Cannot find merges.txt in: %s", tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  const std::string tok_blob = LoadBytesFromFile(tok_json);
+  const std::string vocab_blob = LoadBytesFromFile(vocab_json);
+  const std::string merges_blob = LoadBytesFromFile(merges_txt);
+
+  if (tok_blob.empty() || vocab_blob.empty() || merges_blob.empty()) {
+    SHERPA_ONNX_LOGE("Failed to read tokenizer files from: %s",
+                     tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  // Build ByteLevel bytes_to_unicode mapping
+  BuildBytesToUnicode(byte_to_unicode_, &unicode_to_byte_);
+
+  if (!ParseVocabJson(vocab_blob, &token2id_)) {
+    SHERPA_ONNX_LOGE("Failed to parse vocab.json: %s", vocab_json.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (!ParseMergesTxt(merges_blob, &merges_rank_)) {
+    SHERPA_ONNX_LOGE("Failed to parse merges.txt: %s", merges_txt.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (!ParseAddedTokensFromTokenizerJson(tok_blob, &added_tokens_)) {
+    SHERPA_ONNX_LOGE("Failed to parse added_tokens from tokenizer.json: %s",
+                     tok_json.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  MergeVocabAndAddedTokens(&token2id_, added_tokens_, &added_token_contents_);
+
+  BuildIdToToken(token2id_, added_token_contents_, &id2token_);
+
+  BuildAddedTokensTrie(added_tokens_, &trie_);
+
+  FinalizeSpecialIds();
+}
+
+#if __ANDROID_API__ >= 9
+void FunASRNanoTokenizer::Init(AAssetManager *mgr,
+                               const std::string &tokenizer_dir) {
+  std::string tok_json = tokenizer_dir + "/tokenizer.json";
+  std::string vocab_json = tokenizer_dir + "/vocab.json";
+  std::string merges_txt = tokenizer_dir + "/merges.txt";
+
+  const std::string tok_blob = LoadBytesFromFile(mgr, tok_json);
+  const std::string vocab_blob = LoadBytesFromFile(mgr, vocab_json);
+  const std::string merges_blob = LoadBytesFromFile(mgr, merges_txt);
+
+  if (tok_blob.empty() || vocab_blob.empty() || merges_blob.empty()) {
+    SHERPA_ONNX_LOGE("Failed to read tokenizer files from assets: %s",
+                     tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  BuildBytesToUnicode(byte_to_unicode_, &unicode_to_byte_);
+
+  if (!ParseVocabJson(vocab_blob, &token2id_)) {
+    SHERPA_ONNX_LOGE("Failed to parse vocab.json from assets: %s",
+                     vocab_json.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (!ParseMergesTxt(merges_blob, &merges_rank_)) {
+    SHERPA_ONNX_LOGE("Failed to parse merges.txt from assets: %s",
+                     merges_txt.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (!ParseAddedTokensFromTokenizerJson(tok_blob, &added_tokens_)) {
+    SHERPA_ONNX_LOGE("Failed to parse added_tokens from assets tokenizer.json");
+    SHERPA_ONNX_EXIT(-1);
+  }
+  MergeVocabAndAddedTokens(&token2id_, added_tokens_, &added_token_contents_);
+  BuildIdToToken(token2id_, added_token_contents_, &id2token_);
+  BuildAddedTokensTrie(added_tokens_, &trie_);
+  FinalizeSpecialIds();
+}
+#endif
+
+#if __OHOS__
+void FunASRNanoTokenizer::Init(NativeResourceManager *mgr,
+                               const std::string &tokenizer_dir) {
+  std::string tok_json = tokenizer_dir + "/tokenizer.json";
+  std::string vocab_json = tokenizer_dir + "/vocab.json";
+  std::string merges_txt = tokenizer_dir + "/merges.txt";
+
+  const std::string tok_blob = LoadBytesFromFile(mgr, tok_json);
+  const std::string vocab_blob = LoadBytesFromFile(mgr, vocab_json);
+  const std::string merges_blob = LoadBytesFromFile(mgr, merges_txt);
+
+  if (tok_blob.empty() || vocab_blob.empty() || merges_blob.empty()) {
+    SHERPA_ONNX_LOGE("Failed to read tokenizer files from rawfile: %s",
+                     tokenizer_dir.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  BuildBytesToUnicode(byte_to_unicode_, &unicode_to_byte_);
+
+  if (!ParseVocabJson(vocab_blob, &token2id_)) {
+    SHERPA_ONNX_LOGE("Failed to parse vocab.json from rawfile: %s",
+                     vocab_json.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (!ParseMergesTxt(merges_blob, &merges_rank_)) {
+    SHERPA_ONNX_LOGE("Failed to parse merges.txt from rawfile: %s",
+                     merges_txt.c_str());
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  if (!ParseAddedTokensFromTokenizerJson(tok_blob, &added_tokens_)) {
+    SHERPA_ONNX_LOGE("Failed to parse added_tokens from rawfile tokenizer.json");
+    SHERPA_ONNX_EXIT(-1);
+  }
+  MergeVocabAndAddedTokens(&token2id_, added_tokens_, &added_token_contents_);
+  BuildIdToToken(token2id_, added_token_contents_, &id2token_);
+  BuildAddedTokensTrie(added_tokens_, &trie_);
+  FinalizeSpecialIds();
+}
+#endif
+
+void FunASRNanoTokenizer::FinalizeSpecialIds() {
+  im_end_token_id_ = TokenToIdOrDefault(token2id_, "<|im_end|>", 151645);
+  eos_token_id_ = TokenToIdOrDefault(token2id_, "<|endoftext|>", -1);
+  if (eos_token_id_ < 0) eos_token_id_ = im_end_token_id_;
+
+  pad_token_id_ = TokenToIdOrDefault(token2id_, "<|pad|>", -1);
+  if (pad_token_id_ < 0) pad_token_id_ = eos_token_id_;
+
+  special_ids_.clear();
+  special_ids_.insert(static_cast<int32_t>(eos_token_id_));
+  special_ids_.insert(static_cast<int32_t>(im_end_token_id_));
+  special_ids_.insert(static_cast<int32_t>(pad_token_id_));
+
+  int64_t im_start = TokenToIdOrDefault(token2id_, "<|im_start|>", -1);
+  if (im_start >= 0) special_ids_.insert(static_cast<int32_t>(im_start));
+}
+
+static inline bool CheckSingleWordBoundary(const std::string &text, size_t pos,
+                                           size_t end) {
+  auto prev_is_word = [&]() -> bool {
+    if (pos == 0) return false;
+    size_t j = pos;
+    while (j > 0 && (static_cast<unsigned char>(text[j - 1]) & 0xC0) == 0x80)
+      --j;
+    if (j == 0) return false;
+    size_t t = j - 1;
+    while (t > 0 && (static_cast<unsigned char>(text[t]) & 0xC0) == 0x80) --t;
+    size_t k = t;
+    uint32_t cp = 0;
+    size_t nb = 0;
+    if (!Utf8Next(text, &k, &cp, &nb)) return false;
+    return IsWordChar(cp);
+  };
+
+  auto next_is_word = [&]() -> bool {
+    if (end >= text.size()) return false;
+    size_t k = end;
+    uint32_t cp = 0;
+    size_t nb = 0;
+    if (!Utf8Next(text, &k, &cp, &nb)) return false;
+    return IsWordChar(cp);
+  };
+
+  return !(prev_is_word() || next_is_word());
+}
+
+// ByteLevel encode: map each byte to unicode char (bytes_to_unicode).
+static inline std::string ByteLevelEncode(
+    const std::string &token,
+    const std::string byte_to_unicode[256]) {
+  std::string out;
+  out.reserve(token.size() * 2);
+  for (unsigned char b : token) {
+    out.append(byte_to_unicode[b]);
+  }
+  return out;
+}
+
+// BPE encode (with cache): bytelevel_word to merged token strings.
+static std::vector<std::string> BpeEncodeWithCache(
+    const std::string &word,
+    const std::unordered_map<std::string, int32_t> &merges_rank,
+    std::unordered_map<std::string, std::vector<std::string>> *cache) {
+  if (!cache) return {};
+  auto it = cache->find(word);
+  if (it != cache->end()) return it->second;
+
+  std::vector<std::string> symbols = SplitUtf8ToChars(word);
+  if (symbols.empty()) {
+    (*cache)[word] = {};
+    return {};
+  }
+  if (symbols.size() == 1) {
+    (*cache)[word] = symbols;
+    return symbols;
+  }
+
+  while (symbols.size() > 1) {
+    int32_t best_rank = std::numeric_limits<int32_t>::max();
+    int32_t best_pos = -1;
+
+    for (int32_t i = 0; i + 1 < static_cast<int32_t>(symbols.size()); ++i) {
+      std::string key = MakeMergeKey(symbols[i], symbols[i + 1]);
+      auto it2 = merges_rank.find(key);
+      if (it2 != merges_rank.end()) {
+        int32_t r = it2->second;
+        if (r < best_rank) {
+          best_rank = r;
+          best_pos = i;
+        }
+      }
+    }
+
+    if (best_pos < 0) break;
+
+    // Merge best pair
+    symbols[best_pos].append(symbols[best_pos + 1]);
+    symbols.erase(symbols.begin() + best_pos + 1);
+  }
+
+  (*cache)[word] = symbols;
+  return symbols;
+}
+
+std::vector<int64_t> FunASRNanoTokenizer::Encode(const std::string &text) {
+  if (token2id_.empty()) {
+    SHERPA_ONNX_LOGE("Tokenizer not initialized");
+    SHERPA_ONNX_EXIT(-1);
+  }
+
+  std::vector<int64_t> out;
+  if (text.empty()) return out;
+
+  size_t pos = 0;
+  size_t last = 0;
+  while (pos < text.size()) {
+    auto m = MatchAddedToken(text, pos, trie_);
+    int32_t mlen = m.first;
+    int32_t tidx = m.second;
+
+    if (mlen > 0 && tidx >= 0) {
+      const auto &tok = added_tokens_[static_cast<size_t>(tidx)];
+
+      if (tok.single_word) {
+        if (!CheckSingleWordBoundary(text, pos, pos + mlen)) {
+          mlen = 0;
+          tidx = -1;
+        }
+      }
+    }
+
+    if (mlen > 0 && tidx >= 0) {
+      if (pos > last) {
+        std::string seg = text.substr(last, pos - last);
+        auto pieces = SplitByQwen3Pattern(seg);
+        for (const auto &p : pieces) {
+          std::string bl = ByteLevelEncode(p, byte_to_unicode_);
+          auto bpe_toks =
+              BpeEncodeWithCache(bl, merges_rank_, &bpe_cache_);
+          for (const auto &bt : bpe_toks) {
+            auto it = token2id_.find(bt);
+            if (it == token2id_.end()) {
+              continue;
+            }
+            out.push_back(static_cast<int64_t>(it->second));
+          }
+        }
+      }
+
+      const auto &atok = added_tokens_[static_cast<size_t>(tidx)];
+      out.push_back(static_cast<int64_t>(atok.id));
+
+      pos += static_cast<size_t>(mlen);
+      last = pos;
+      continue;
+    }
+
+    ++pos;
+  }
+
+  if (last < text.size()) {
+    std::string seg = text.substr(last);
+    auto pieces = SplitByQwen3Pattern(seg);
+    for (const auto &p : pieces) {
+      std::string bl = ByteLevelEncode(p, byte_to_unicode_);
+      auto bpe_toks = BpeEncodeWithCache(bl, merges_rank_, &bpe_cache_);
+      for (const auto &bt : bpe_toks) {
+        auto it = token2id_.find(bt);
+        if (it == token2id_.end()) continue;
+        out.push_back(static_cast<int64_t>(it->second));
+      }
+    }
+  }
+
+  return out;
+}
+
+std::string FunASRNanoTokenizer::Decode(const std::vector<int64_t> &token_ids) {
+  if (id2token_.empty()) {
+    SHERPA_ONNX_LOGE("Tokenizer not initialized");
+    SHERPA_ONNX_EXIT(-1);
+  }
+  if (token_ids.empty()) return "";
+
+  std::vector<std::string> toks;
+  toks.reserve(token_ids.size());
+  for (int64_t v : token_ids) {
+    if (v < 0) continue;
+    if (v > static_cast<int64_t>(std::numeric_limits<int32_t>::max())) continue;
+    int32_t id = static_cast<int32_t>(v);
+    if (!special_ids_.empty() && special_ids_.count(id)) continue;
+    if (id < 0 || static_cast<size_t>(id) >= id2token_.size()) continue;
+    const std::string &t = id2token_[static_cast<size_t>(id)];
+    if (!t.empty()) toks.push_back(t);
+  }
+
+  std::string merged;
+  {
+    size_t total = 0;
+    for (const auto &t : toks) total += t.size();
+    merged.reserve(total);
+    for (const auto &t : toks) merged.append(t);
+  }
+
+  std::vector<uint8_t> bytes;
+  bytes.reserve(merged.size());
+
+  size_t i = 0;
+  while (i < merged.size()) {
+    size_t t = i;
+    uint32_t cp = 0;
+    size_t n = 0;
+    if (!Utf8Next(merged, &t, &cp, &n) || n == 0) {
+      bytes.push_back(static_cast<uint8_t>(merged[i]));
+      i += 1;
+      continue;
+    }
+    std::string ch = merged.substr(i, n);
+    auto it = unicode_to_byte_.find(ch);
+    if (it != unicode_to_byte_.end()) {
+      bytes.push_back(it->second);
+    } else {
+      for (unsigned char b : ch) bytes.push_back(b);
+    }
+    i += n;
+  }
+
+  std::string out(reinterpret_cast<const char *>(bytes.data()), bytes.size());
+
+  for (const char *sp : {"<|im_end|>", "<|im_start|>", "<|endoftext|>"}) {
+    std::string needle(sp);
+    size_t pos = 0;
+    while ((pos = out.find(needle, pos)) != std::string::npos) {
+      out.erase(pos, needle.size());
+    }
+  }
+
+  TrimInPlace(&out);
+  return out;
+}
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/funasr-nano-tokenizer.h b/sherpa-onnx/csrc/funasr-nano-tokenizer.h
new file mode 100644
index 00000000..26b34abb
--- /dev/null
+++ b/sherpa-onnx/csrc/funasr-nano-tokenizer.h
@@ -0,0 +1,109 @@
+// sherpa-onnx/csrc/funasr-nano-tokenizer.h
+//
+// Copyright (c)  2025  zengyw
+//
+// A self-contained Qwen3 ByteLevel-BPE tokenizer implementation.
+// - No dependency on tokenizers-cpp / HF tokenizers
+// - Loads vocab.json + merges.txt + tokenizer.json(added_tokens)
+// - Supports AddedTokens via Trie longest-match
+// - ByteLevel bytes_to_unicode encode/decode
+
+#ifndef SHERPA_ONNX_CSRC_FUNASR_NANO_TOKENIZER_H_
+#define SHERPA_ONNX_CSRC_FUNASR_NANO_TOKENIZER_H_
+
+#include <cstdint>
+#include <string>
+#include <unordered_map>
+#include <unordered_set>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include <android/asset_manager.h>
+#endif
+
+#if __OHOS__
+struct NativeResourceManager;
+#endif
+
+namespace sherpa_onnx {
+
+class FunASRNanoTokenizer {
+ public:
+  explicit FunASRNanoTokenizer(const std::string &tokenizer_dir);
+
+#if __ANDROID_API__ >= 9
+  FunASRNanoTokenizer(AAssetManager *mgr, const std::string &tokenizer_dir);
+#endif
+
+#if __OHOS__
+  FunASRNanoTokenizer(NativeResourceManager *mgr,
+                      const std::string &tokenizer_dir);
+#endif
+
+  std::vector<int64_t> Encode(const std::string &text);
+  std::string Decode(const std::vector<int64_t> &token_ids);
+
+  int64_t GetEosTokenId() const { return eos_token_id_; }
+  int64_t GetPadTokenId() const { return pad_token_id_; }
+  int64_t GetImEndTokenId() const { return im_end_token_id_; }
+
+  // Public structures for helper functions
+  struct AddedToken {
+    std::string content;
+    int32_t id = -1;
+    bool single_word = false;
+    bool lstrip = false;
+    bool rstrip = false;
+    bool normalized = false;
+    bool special = false;
+  };
+
+  struct TrieNode {
+    std::unordered_map<uint8_t, int32_t> next;
+    int32_t token_index = -1;  // index in added_tokens_ if terminal
+  };
+
+ private:
+  void Init(const std::string &tokenizer_dir);
+
+#if __ANDROID_API__ >= 9
+  void Init(AAssetManager *mgr, const std::string &tokenizer_dir);
+#endif
+
+#if __OHOS__
+  void Init(NativeResourceManager *mgr, const std::string &tokenizer_dir);
+#endif
+
+  void FinalizeSpecialIds();
+
+ private:
+  // Special ids
+  int64_t eos_token_id_ = -1;
+  int64_t pad_token_id_ = -1;
+  int64_t im_end_token_id_ = -1;
+
+  std::unordered_set<int32_t> special_ids_;
+
+  // Vocab: token <-> id
+  std::unordered_map<std::string, int32_t> token2id_;
+  std::vector<std::string> id2token_;
+
+  // merges ranks: "left\tright" -> rank
+  std::unordered_map<std::string, int32_t> merges_rank_;
+
+  // BPE cache: bytelevel_word -> list of merged tokens
+  std::unordered_map<std::string, std::vector<std::string>> bpe_cache_;
+
+  // bytes_to_unicode mapping (ByteLevel)
+  std::string byte_to_unicode_[256];
+  std::unordered_map<std::string, uint8_t> unicode_to_byte_;
+
+  // AddedTokens
+  std::vector<AddedToken> added_tokens_;
+  std::vector<TrieNode> trie_;
+  std::unordered_set<std::string> added_token_contents_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_FUNASR_NANO_TOKENIZER_H_
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
new file mode 100644
index 00000000..c0e2a845
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
@@ -0,0 +1,139 @@
+// sherpa-onnx/csrc/offline-funasr-nano-model-config.cc
+//
+// Copyright (c)  2025  zengyw
+
+#include "sherpa-onnx/csrc/offline-funasr-nano-model-config.h"
+
+#include <string>
+#include <sstream>
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+
+namespace sherpa_onnx {
+
+void OfflineFunASRNanoModelConfig::Register(ParseOptions *po) {
+  po->Register("funasr-nano-encoder-adaptor", &encoder_adaptor,
+               "Path to encoder_adaptor.onnx for FunASR-nano");
+
+  po->Register("funasr-nano-llm-prefill", &llm_prefill,
+               "Path to llm_prefill.onnx for FunASR-nano (KV cache mode)");
+
+  po->Register("funasr-nano-llm-decode", &llm_decode,
+               "Path to llm_decode.onnx for FunASR-nano (KV cache mode)");
+
+  po->Register("funasr-nano-embedding", &embedding,
+               "Path to embedding.onnx for FunASR-nano");
+
+  po->Register("funasr-nano-tokenizer", &tokenizer,
+               "Path to tokenizer directory (e.g., Qwen3-0.6B) for FunASR-nano");
+
+  po->Register("funasr-nano-system-prompt", &system_prompt,
+               "System prompt for FunASR-nano");
+
+  po->Register("funasr-nano-user-prompt", &user_prompt,
+               "User prompt template for FunASR-nano");
+
+  po->Register("funasr-nano-max-new-tokens", &max_new_tokens,
+               "Maximum number of new tokens to generate for FunASR-nano");
+
+  po->Register("funasr-nano-temperature", &temperature,
+               "Sampling temperature for FunASR-nano");
+
+  po->Register("funasr-nano-top-p", &top_p,
+               "Top-p (nucleus) sampling threshold for FunASR-nano");
+
+  po->Register("funasr-nano-seed", &seed,
+               "Random seed for FunASR-nano");
+}
+
+bool OfflineFunASRNanoModelConfig::Validate() const {
+  if (encoder_adaptor.empty()) {
+    SHERPA_ONNX_LOGE("--funasr-nano-encoder-adaptor is required");
+    return false;
+  }
+
+  if (!FileExists(encoder_adaptor)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-encoder-adaptor: '%s' does not exist",
+                     encoder_adaptor.c_str());
+    return false;
+  }
+
+  // KV cache mode (prefill + decode) is required
+  if (llm_prefill.empty() || llm_decode.empty()) {
+    SHERPA_ONNX_LOGE("Both --funasr-nano-llm-prefill and --funasr-nano-llm-decode are required");
+    return false;
+  }
+
+  if (!FileExists(llm_prefill)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-llm-prefill: '%s' does not exist", llm_prefill.c_str());
+    return false;
+  }
+  if (!FileExists(llm_decode)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-llm-decode: '%s' does not exist", llm_decode.c_str());
+    return false;
+  }
+
+  if (tokenizer.empty()) {
+    SHERPA_ONNX_LOGE("--funasr-nano-tokenizer is required");
+    return false;
+  }
+
+  if (!FileExists(tokenizer)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-tokenizer: '%s' does not exist",
+                     tokenizer.c_str());
+    return false;
+  }
+
+  if (embedding.empty()) {
+    SHERPA_ONNX_LOGE("--funasr-nano-embedding is required");
+    return false;
+  }
+
+  if (!FileExists(embedding)) {
+    SHERPA_ONNX_LOGE("--funasr-nano-embedding: '%s' does not exist",
+                     embedding.c_str());
+    return false;
+  }
+
+  if (max_new_tokens <= 0) {
+    SHERPA_ONNX_LOGE("--funasr-nano-max-new-tokens should be > 0. Given: %d",
+                     max_new_tokens);
+    return false;
+  }
+
+  if (temperature < 0.0f) {
+    SHERPA_ONNX_LOGE("--funasr-nano-temperature should be >= 0.0. Given: %f",
+                     temperature);
+    return false;
+  }
+
+  if (top_p < 0.0f || top_p > 1.0f) {
+    SHERPA_ONNX_LOGE("--funasr-nano-top-p should be in [0.0, 1.0]. Given: %f",
+                     top_p);
+    return false;
+  }
+
+  return true;
+}
+
+std::string OfflineFunASRNanoModelConfig::ToString() const {
+  std::ostringstream os;
+
+  os << "OfflineFunASRNanoModelConfig(";
+  os << "encoder_adaptor=\"" << encoder_adaptor << "\", ";
+  os << "llm_prefill=\"" << llm_prefill << "\", ";
+  os << "llm_decode=\"" << llm_decode << "\", ";
+  os << "embedding=\"" << embedding << "\", ";
+  os << "tokenizer=\"" << tokenizer << "\", ";
+  os << "system_prompt=\"" << system_prompt << "\", ";
+  os << "user_prompt=\"" << user_prompt << "\", ";
+  os << "max_new_tokens=" << max_new_tokens << ", ";
+  os << "temperature=" << temperature << ", ";
+  os << "top_p=" << top_p << ", ";
+  os << "seed=" << seed << ")";
+
+  return os.str();
+}
+
+}  // namespace sherpa_onnx
+
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model-config.h b/sherpa-onnx/csrc/offline-funasr-nano-model-config.h
new file mode 100644
index 00000000..dc7e4e34
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model-config.h
@@ -0,0 +1,59 @@
+// sherpa-onnx/csrc/offline-funasr-nano-model-config.h
+//
+// Copyright (c)  2025  zengyw
+
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+
+#include <string>
+
+#include "sherpa-onnx/csrc/parse-options.h"
+
+namespace sherpa_onnx {
+
+struct OfflineFunASRNanoModelConfig {
+  // Path to encoder_adaptor.onnx
+  std::string encoder_adaptor;
+
+  // Path to llm_prefill.onnx (KV cache prefill)
+  std::string llm_prefill;
+
+  // Path to llm_decode.onnx (KV cache decode)
+  std::string llm_decode;
+
+  // Path to embedding.onnx
+  std::string embedding;
+
+  // Path to tokenizer directory (e.g., Qwen3-0.6B)
+  std::string tokenizer;
+
+  // System prompt
+  std::string system_prompt = "You are a helpful assistant.";
+
+  // User prompt template (will be filled with audio tokens)
+  std::string user_prompt = "";
+
+  // Maximum number of new tokens to generate
+  int32_t max_new_tokens = 512;
+
+  // Sampling temperature
+  float temperature = 0.3f;
+
+  // Top-p (nucleus) sampling threshold
+  float top_p = 0.8f;
+
+  // Random seed for reproducibility
+  int32_t seed = 42;
+
+  OfflineFunASRNanoModelConfig() = default;
+
+  void Register(ParseOptions *po);
+  bool Validate() const;
+
+  std::string ToString() const;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.cc b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
new file mode 100644
index 00000000..e4a5ecac
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.cc
@@ -0,0 +1,942 @@
+// sherpa-onnx/csrc/offline-funasr-nano-model.cc
+//
+// Copyright (c)  2025  zengyw
+
+#include "sherpa-onnx/csrc/offline-funasr-nano-model.h"
+
+#include <algorithm>
+#include <cctype>
+#include <cmath>
+#include <cstdint>
+#include <cstring>
+#include <memory>
+#include <sstream>
+#include <string>
+#include <utility>
+#include <vector>
+
+#if __ANDROID_API__ >= 9
+#include "android/asset_manager.h"
+#include "android/asset_manager_jni.h"
+#endif
+
+#if __OHOS__
+#include "rawfile/raw_file_manager.h"
+#endif
+
+#include "onnxruntime_cxx_api.h"
+#include "sherpa-onnx/csrc/file-utils.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/onnx-utils.h"
+#include "sherpa-onnx/csrc/session.h"
+
+namespace sherpa_onnx {
+
+namespace {
+
+// Calculate the total number of elements from a tensor shape.
+static inline size_t NumelFromShape(const std::vector<int64_t> &shape) {
+  if (shape.empty()) return 0;
+  size_t n = 1;
+  for (auto d : shape) {
+    if (d <= 0) return 0;
+    n *= static_cast<size_t>(d);
+  }
+  return n;
+}
+
+static inline void AssertTensorIsCpu(const Ort::Value &v, const char *what) {
+  if (!v.IsTensor()) return;
+  auto mi = v.GetTensorMemoryInfo();
+  if (mi.GetDeviceType() != OrtMemoryInfoDeviceType_CPU) {
+    SHERPA_ONNX_LOGE("%s: expected CPU tensor but got device_type=%d device_id=%d",
+                     what, (int)mi.GetDeviceType(), mi.GetDeviceId());
+    SHERPA_ONNX_EXIT(-1);
+  }
+}
+
+static inline std::string ToLower(std::string s) {
+  std::transform(s.begin(), s.end(), s.begin(),
+                 [](unsigned char c) -> char {
+                   return static_cast<char>(std::tolower(c));
+                 });
+  return s;
+}
+
+static inline bool IsCudaProvider(const std::string &provider) {
+  auto p = ToLower(provider);
+  // Keep it conservative. We only enable IO binding policy below when we
+  // are on CUDA; other EPs keep the existing behavior.
+  return p == "cuda" || (p.size() > 4 && p.find("cuda") == 0);
+}
+
+// Check if a tensor element type is FP16-IO (float16 or uint16).
+static inline bool IsFloat16IO(ONNXTensorElementDataType t) {
+  return t == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+         t == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16;
+}
+
+// Get the element type of a session input tensor.
+static inline ONNXTensorElementDataType GetSessionInputElemType(
+    Ort::Session *sess, size_t input_index) {
+  auto ti = sess->GetInputTypeInfo(input_index);
+  auto t = ti.GetTensorTypeAndShapeInfo();
+  return static_cast<ONNXTensorElementDataType>(t.GetElementType());
+}
+
+// Get the element type of a session output tensor.
+static inline ONNXTensorElementDataType GetSessionOutputElemType(
+    Ort::Session *sess, size_t output_index) {
+  auto ti = sess->GetOutputTypeInfo(output_index);
+  auto t = ti.GetTensorTypeAndShapeInfo();
+  return static_cast<ONNXTensorElementDataType>(t.GetElementType());
+}
+
+template <typename T>
+static Ort::Value AllocTensor(OrtAllocator *alloc,
+                              const std::vector<int64_t> &shape) {
+  return Ort::Value::CreateTensor<T>(alloc, shape.data(), shape.size());
+}
+
+template <>
+Ort::Value AllocTensor<uint16_t>(OrtAllocator *alloc,
+                                 const std::vector<int64_t> &shape) {
+  return Ort::Value::CreateTensor(alloc, shape.data(), shape.size(),
+                                  ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+}
+
+// Convert tensor to float32, handling both float16 and float32 inputs.
+// NOTE: This helper assumes the input tensor is on CPU memory.
+// The caller must ensure the tensor is on CPU (e.g., via IO Binding).
+static Ort::Value CastToFloat32(Ort::Value in, OrtAllocator *alloc) {
+  if (!in.IsTensor()) return in;
+  auto info = in.GetTensorTypeAndShapeInfo();
+  auto shape = info.GetShape();
+  size_t n = NumelFromShape(shape);
+  if (n == 0) return in;
+  auto et = info.GetElementType();
+
+  AssertTensorIsCpu(in, "CastToFloat32");
+
+  Ort::Value out = AllocTensor<float>(alloc, shape);
+  float *dst = out.GetTensorMutableData<float>();
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+    const float *src = in.GetTensorData<float>();
+    std::memcpy(dst, src, n * sizeof(float));
+    return out;
+  }
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+      et == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16) {
+    const uint16_t *src = in.GetTensorData<uint16_t>();
+    for (size_t i = 0; i < n; ++i) dst[i] = HalfBitsToFloat(src[i]);
+    return out;
+  }
+  SHERPA_ONNX_LOGE("CastToFloat32: unsupported input elem_type=%d", (int)et);
+  return in;
+}
+
+// Convert tensor to float16, handling both float16 and float32 inputs.
+// NOTE: This helper assumes the input tensor is on CPU memory.
+static Ort::Value CastToFloat16(Ort::Value in, OrtAllocator *alloc) {
+  if (!in.IsTensor()) return in;
+  auto info = in.GetTensorTypeAndShapeInfo();
+  auto shape = info.GetShape();
+  size_t n = NumelFromShape(shape);
+  if (n == 0) return in;
+  auto et = static_cast<ONNXTensorElementDataType>(info.GetElementType());
+
+  AssertTensorIsCpu(in, "CastToFloat16");
+
+  Ort::Value out = AllocTensor<uint16_t>(alloc, shape);
+  uint16_t *dst = out.GetTensorMutableData<uint16_t>();
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 ||
+      et == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16) {
+    const uint16_t *src = in.GetTensorData<uint16_t>();
+    std::memcpy(dst, src, n * sizeof(uint16_t));
+    return out;
+  }
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+    const float *src = in.GetTensorData<float>();
+    for (size_t i = 0; i < n; ++i) dst[i] = FloatToHalfBits(src[i]);
+    return out;
+  }
+  SHERPA_ONNX_LOGE("CastToFloat16: unsupported input elem_type=%d", (int)et);
+  return in;
+}
+
+// Cast tensor to the expected element type (float16 or float32).
+// Returns the input unchanged if it already matches the expected type.
+// NOTE: This helper assumes the input tensor is on CPU memory.
+static Ort::Value CastFloatLikeForExpected(Ort::Value in,
+                                          ONNXTensorElementDataType expected,
+                                          OrtAllocator *alloc) {
+  if (!in.IsTensor()) return in;
+  auto info = in.GetTensorTypeAndShapeInfo();
+  auto actual = static_cast<ONNXTensorElementDataType>(info.GetElementType());
+  if (actual == expected) return in;
+  if (expected == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
+    return CastToFloat16(std::move(in), alloc);
+  }
+  if (expected == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+    return CastToFloat32(std::move(in), alloc);
+  }
+  SHERPA_ONNX_LOGE(
+      "CastFloatLikeForExpected: unsupported expected elem_type=%d",
+      (int)expected);
+  return in;
+}
+
+static inline bool NeedsTypeConversion(Ort::Value &in,
+                                       ONNXTensorElementDataType expected) {
+  if (!in.IsTensor()) return false;
+  auto info = in.GetTensorTypeAndShapeInfo();
+  auto actual = static_cast<ONNXTensorElementDataType>(info.GetElementType());
+  return actual != expected;
+}
+
+// Cast attention mask tensor to int64 if needed.
+// Supports int32 to int64 conversion.
+// NOTE: This helper assumes the input tensor is on CPU memory.
+static Ort::Value CastMaskToInt64IfNeeded(Ort::Value in, OrtAllocator *alloc) {
+  if (!in.IsTensor()) return in;
+  auto info = in.GetTensorTypeAndShapeInfo();
+  auto shape = info.GetShape();
+  size_t n = NumelFromShape(shape);
+  if (n == 0) return in;
+  auto et = static_cast<ONNXTensorElementDataType>(info.GetElementType());
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) return in;
+
+  AssertTensorIsCpu(in, "CastMaskToInt64IfNeeded");
+
+  if (et == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32) {
+    const int32_t *src = in.GetTensorData<int32_t>();
+    Ort::Value out = AllocTensor<int64_t>(alloc, shape);
+    int64_t *dst = out.GetTensorMutableData<int64_t>();
+    for (size_t i = 0; i < n; ++i) dst[i] = static_cast<int64_t>(src[i]);
+    return out;
+  }
+
+  SHERPA_ONNX_LOGE("attention_mask elem_type=%d not supported, expected int64",
+                   (int)et);
+  return in;
+}
+
+// Create a non-owning tensor view that preserves the underlying memory info.
+static inline Ort::Value ViewConst(const Ort::Value &v) {
+  return View(const_cast<Ort::Value *>(&v));
+}
+
+}  // namespace
+
+// Implementation class for OfflineFunASRNanoModel.
+// Manages ONNX sessions for encoder, LLM prefill/decode, and embedding models.
+class OfflineFunASRNanoModel::Impl {
+ public:
+  explicit Impl(const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR, "funasr-nano"),
+        sess_opts_encoder_(GetSessionOptions(config)),
+        sess_opts_llm_(GetSessionOptions(config)),
+        sess_opts_embedding_(GetSessionOptions(config)),
+        allocator_(),
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
+    const auto &c = config_.funasr_nano;
+
+    if (c.encoder_adaptor.empty()) {
+      SHERPA_ONNX_LOGE("funasr_nano.encoder_adaptor is empty");
+      SHERPA_ONNX_EXIT(-1);
+    }
+    if (c.llm_prefill.empty() || c.llm_decode.empty()) {
+      SHERPA_ONNX_LOGE(
+          "funasr_nano.llm_prefill/llm_decode are required for KV-cache mode");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    InitEncoderAdaptor(c.encoder_adaptor);
+    InitLLMPrefill(c.llm_prefill);
+    InitLLMDecode(c.llm_decode);
+    InitEmbedding(c.embedding);
+    has_embedding_model_ = true;
+
+    // FunASR-nano uses CPU-side sampling. When running on CUDA, we bind
+    // logits to CPU (so sampling can read it safely), while keeping KV cache
+    // on GPU to avoid large device<->host copies.
+    use_cuda_iobinding_ = (!is_cpu_provider_ && IsCudaProvider(config_.provider));
+    if (use_cuda_iobinding_) {
+      // Use device 0 by default. SessionOptions() in sherpa-onnx usually
+      // configures the CUDA EP device; binding here only affects output memory.
+      cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
+          "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
+
+      // Check if prefill/decode models have FP16-IO, which is not supported on CUDA yet.
+      ONNXTensorElementDataType prefill_in_type = prefill_embeds_in_type_;
+      ONNXTensorElementDataType prefill_out_type =
+          GetSessionOutputElemType(prefill_sess_.get(), 0);
+      ONNXTensorElementDataType decode_in_type = decode_embeds_in_type_;
+      ONNXTensorElementDataType decode_out_type =
+          GetSessionOutputElemType(decode_sess_.get(), 0);
+
+      if (IsFloat16IO(prefill_in_type) || IsFloat16IO(prefill_out_type) ||
+          IsFloat16IO(decode_in_type) || IsFloat16IO(decode_out_type)) {
+        SHERPA_ONNX_LOGE(
+            "fp16-IO LLM models are not supported on CUDA yet. Please use "
+            "fp32/int8 models.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
+  void InitEncoderAdaptorFromMemory(void *model_data,
+                                    size_t model_data_length) {
+    encoder_sess_ = std::make_unique<Ort::Session>(
+        env_, model_data, model_data_length, sess_opts_encoder_);
+    GetInputNames(encoder_sess_.get(), &encoder_input_names_,
+                  &encoder_input_names_ptr_);
+    GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
+                   &encoder_output_names_ptr_);
+    encoder_in_type_ = GetSessionInputElemType(encoder_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = encoder_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    SHERPA_ONNX_READ_META_DATA(lfr_window_size_, "lfr_window_size");
+    SHERPA_ONNX_READ_META_DATA(lfr_window_shift_, "lfr_window_shift");
+    SHERPA_ONNX_READ_META_DATA(hidden_size_, "llm_dim");
+  }
+
+  void InitLLMPrefillFromMemory(void *model_data, size_t model_data_length) {
+    prefill_sess_ = std::make_unique<Ort::Session>(
+        env_, model_data, model_data_length, sess_opts_llm_);
+    GetInputNames(prefill_sess_.get(), &prefill_input_names_,
+                  &prefill_input_names_ptr_);
+    GetOutputNames(prefill_sess_.get(), &prefill_output_names_,
+                   &prefill_output_names_ptr_);
+    prefill_embeds_in_type_ = GetSessionInputElemType(prefill_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = prefill_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("Prefill model metadata:\n%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("Prefill model metadata:\n%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
+    }
+  }
+
+  void InitLLMDecodeFromMemory(void *model_data, size_t model_data_length) {
+    decode_sess_ = std::make_unique<Ort::Session>(
+        env_, model_data, model_data_length, sess_opts_llm_);
+    GetInputNames(decode_sess_.get(), &decode_input_names_,
+                  &decode_input_names_ptr_);
+    GetOutputNames(decode_sess_.get(), &decode_output_names_,
+                   &decode_output_names_ptr_);
+    decode_embeds_in_type_ = GetSessionInputElemType(decode_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = decode_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("Decode model metadata:\n%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("Decode model metadata:\n%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    int32_t decode_vocab_size = 0;
+    SHERPA_ONNX_READ_META_DATA(decode_vocab_size, "vocab_size");
+    if (vocab_size_ > 0 && decode_vocab_size != vocab_size_) {
+      SHERPA_ONNX_LOGE(
+          "Decode model vocab_size (%d) != prefill vocab_size (%d)",
+          decode_vocab_size, vocab_size_);
+    }
+    if (vocab_size_ == 0) vocab_size_ = decode_vocab_size;
+    int32_t decode_hidden_size = 0;
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(decode_hidden_size, "hidden_size");
+      hidden_size_ = decode_hidden_size;
+    }
+  }
+
+  void InitEmbeddingFromMemory(void *model_data, size_t model_data_length) {
+    embedding_sess_ = std::make_unique<Ort::Session>(
+        env_, model_data, model_data_length, sess_opts_embedding_);
+    GetInputNames(embedding_sess_.get(), &embedding_input_names_,
+                  &embedding_input_names_ptr_);
+    GetOutputNames(embedding_sess_.get(), &embedding_output_names_,
+                   &embedding_output_names_ptr_);
+    Ort::ModelMetadata meta_data = embedding_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
+    }
+  }
+
+  template <typename Manager>
+  Impl(Manager *mgr, const OfflineModelConfig &config)
+      : config_(config),
+        env_(ORT_LOGGING_LEVEL_ERROR, "funasr-nano"),
+        sess_opts_encoder_(GetSessionOptions(config)),
+        sess_opts_llm_(GetSessionOptions(config)),
+        sess_opts_embedding_(GetSessionOptions(config)),
+        allocator_(),
+        cpu_mem_info_(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,
+                                                 OrtMemTypeDefault)),
+        is_cpu_provider_(config.provider == "cpu" || config.provider.empty()) {
+    const auto &c = config_.funasr_nano;
+
+    if (c.encoder_adaptor.empty()) {
+      SHERPA_ONNX_LOGE("funasr_nano.encoder_adaptor is empty");
+      SHERPA_ONNX_EXIT(-1);
+    }
+    if (c.llm_prefill.empty() || c.llm_decode.empty()) {
+      SHERPA_ONNX_LOGE(
+          "funasr_nano.llm_prefill/llm_decode are required for KV-cache mode");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    auto buf_encoder = ReadFile(mgr, c.encoder_adaptor);
+    InitEncoderAdaptorFromMemory(buf_encoder.data(), buf_encoder.size());
+
+    {
+      auto buf_prefill = ReadFile(mgr, c.llm_prefill);
+      InitLLMPrefillFromMemory(buf_prefill.data(), buf_prefill.size());
+    }
+
+    auto buf_decode = ReadFile(mgr, c.llm_decode);
+    InitLLMDecodeFromMemory(buf_decode.data(), buf_decode.size());
+
+    auto buf_embedding = ReadFile(mgr, c.embedding);
+    InitEmbeddingFromMemory(buf_embedding.data(), buf_embedding.size());
+    has_embedding_model_ = true;
+
+    use_cuda_iobinding_ = (!is_cpu_provider_ && IsCudaProvider(config_.provider));
+    if (use_cuda_iobinding_) {
+      cuda_mem_info_ = std::make_unique<Ort::MemoryInfo>(
+          "Cuda", OrtDeviceAllocator, 0, OrtMemTypeDefault);
+
+      // Check if prefill/decode models have FP16-IO, which is not supported on CUDA yet.
+      ONNXTensorElementDataType prefill_in_type = prefill_embeds_in_type_;
+      ONNXTensorElementDataType prefill_out_type =
+          GetSessionOutputElemType(prefill_sess_.get(), 0);
+      ONNXTensorElementDataType decode_in_type = decode_embeds_in_type_;
+      ONNXTensorElementDataType decode_out_type =
+          GetSessionOutputElemType(decode_sess_.get(), 0);
+
+      if (IsFloat16IO(prefill_in_type) || IsFloat16IO(prefill_out_type) ||
+          IsFloat16IO(decode_in_type) || IsFloat16IO(decode_out_type)) {
+        SHERPA_ONNX_LOGE(
+            "fp16-IO LLM models are not supported on CUDA yet. Please use "
+            "fp32/int8 models.");
+        SHERPA_ONNX_EXIT(-1);
+      }
+    }
+  }
+
+  // Forward pass through encoder adaptor model.
+  // Converts audio features to embeddings compatible with the LLM.
+  Ort::Value ForwardEncoderAdaptor(Ort::Value features) {
+    if (NeedsTypeConversion(features, encoder_in_type_)) {
+      features = CastFloatLikeForExpected(std::move(features), encoder_in_type_,
+                                          allocator_);
+    }
+
+    // Encoder output is consumed by CPU-side code (embedding packing), so we
+    // bind it to CPU when running on CUDA to avoid returning a CUDA pointer.
+    if (use_cuda_iobinding_) {
+      Ort::IoBinding binding(*encoder_sess_);
+      binding.BindInput(encoder_input_names_ptr_[0], features);
+      binding.BindOutput(encoder_output_names_ptr_[0], cpu_mem_info_);
+      binding.SynchronizeInputs();
+      encoder_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      auto outs = binding.GetOutputValues();
+
+      if (outs.empty()) {
+        SHERPA_ONNX_LOGE("ForwardEncoderAdaptor: empty outputs");
+        SHERPA_ONNX_EXIT(-1);
+      }
+      return std::move(outs[0]);
+    }
+
+    std::array<Ort::Value, 1> inputs = {std::move(features)};
+    auto outputs = encoder_sess_->Run(
+        {}, encoder_input_names_ptr_.data(), inputs.data(), inputs.size(),
+        encoder_output_names_ptr_.data(), encoder_output_names_ptr_.size());
+    return std::move(outputs[0]);
+  }
+
+  // Forward pass through LLM prefill model with full context.
+  // Returns logits and initial KV cache states for all layers.
+  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+  ForwardLLMPrefill(Ort::Value inputs_embeds, Ort::Value attention_mask) {
+    if (NeedsTypeConversion(inputs_embeds, prefill_embeds_in_type_)) {
+      inputs_embeds = CastFloatLikeForExpected(
+          std::move(inputs_embeds), prefill_embeds_in_type_, allocator_);
+    }
+    if (attention_mask.IsTensor()) {
+      auto mask_info = attention_mask.GetTensorTypeAndShapeInfo();
+      auto mask_type =
+          static_cast<ONNXTensorElementDataType>(mask_info.GetElementType());
+      if (mask_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
+        attention_mask =
+            CastMaskToInt64IfNeeded(std::move(attention_mask), allocator_);
+      }
+    }
+
+    std::vector<Ort::Value> outputs;
+
+    if (use_cuda_iobinding_) {
+      // CPU-side sampling needs logits on CPU, while KV cache should remain on
+      // GPU to avoid large device<->host copies between decode steps.
+      Ort::IoBinding binding(*prefill_sess_);
+      binding.BindInput(prefill_input_names_ptr_[0], inputs_embeds);
+      binding.BindInput(prefill_input_names_ptr_[1], attention_mask);
+
+      binding.BindOutput(prefill_output_names_ptr_[0], cpu_mem_info_);
+      for (size_t i = 1; i < prefill_output_names_ptr_.size(); ++i) {
+        binding.BindOutput(prefill_output_names_ptr_[i], *cuda_mem_info_);
+      }
+
+      binding.SynchronizeInputs();
+      prefill_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      outputs = binding.GetOutputValues();
+    } else {
+      std::array<Ort::Value, 2> inputs = {std::move(inputs_embeds),
+                                          std::move(attention_mask)};
+      outputs = prefill_sess_->Run(
+          {}, prefill_input_names_ptr_.data(), inputs.data(), inputs.size(),
+          prefill_output_names_ptr_.data(), prefill_output_names_ptr_.size());
+    }
+
+    // First output is logits, remaining outputs are past_key_values
+    if (outputs.empty()) {
+      SHERPA_ONNX_LOGE("ForwardLLMPrefill: empty outputs");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    Ort::Value logits = std::move(outputs[0]);
+
+    if (!logits.IsTensor()) {
+      SHERPA_ONNX_LOGE("ForwardLLMPrefill: logits is not a tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+    AssertTensorIsCpu(logits, "ForwardLLMPrefill logits");
+
+    if ((outputs.size() - 1) % 2 != 0) {
+      SHERPA_ONNX_LOGE("ForwardLLMPrefill: invalid KV cache outputs size=%d",
+                       static_cast<int>(outputs.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<std::pair<Ort::Value, Ort::Value>> past_kv;
+    int num_layers = static_cast<int>((outputs.size() - 1) / 2);
+    past_kv.reserve(num_layers);
+    for (int i = 0; i < num_layers; ++i) {
+      past_kv.emplace_back(std::move(outputs[1 + 2 * i]),
+                           std::move(outputs[1 + 2 * i + 1]));
+    }
+    return {std::move(logits), std::move(past_kv)};
+  }
+
+  // Forward pass through LLM decode model with KV cache.
+  // Takes a single token embedding and past KV cache, returns logits and
+  // updated KV cache states.
+  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+  ForwardLLMDecode(Ort::Value inputs_embeds, Ort::Value attention_mask,
+                   const std::vector<std::pair<Ort::Value, Ort::Value>>
+                       &past_key_values) {
+    if (NeedsTypeConversion(inputs_embeds, decode_embeds_in_type_)) {
+      inputs_embeds = CastFloatLikeForExpected(
+          std::move(inputs_embeds), decode_embeds_in_type_, allocator_);
+    }
+    if (attention_mask.IsTensor()) {
+      auto mask_info = attention_mask.GetTensorTypeAndShapeInfo();
+      auto mask_type =
+          static_cast<ONNXTensorElementDataType>(mask_info.GetElementType());
+      if (mask_type != ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
+        attention_mask =
+            CastMaskToInt64IfNeeded(std::move(attention_mask), allocator_);
+      }
+    }
+
+    // Build inputs: [inputs_embeds, attention_mask, past_key_0, past_value_0, ...]
+    // NOTE: We create non-owning Ort::Value views that reference existing buffers.
+    std::vector<Ort::Value> inputs;
+    inputs.reserve(2 + 2 * past_key_values.size());
+    inputs.push_back(std::move(inputs_embeds));
+    inputs.push_back(std::move(attention_mask));
+    for (const auto &kv : past_key_values) {
+      inputs.push_back(ViewConst(kv.first));
+      inputs.push_back(ViewConst(kv.second));
+    }
+
+    // Build input names: [inputs_embeds, attention_mask, past_key_0, past_value_0, ...]
+    std::vector<const char *> input_names_ptr;
+    input_names_ptr.reserve(2 + 2 * past_key_values.size());
+    input_names_ptr.push_back(decode_input_names_ptr_[0]);
+    input_names_ptr.push_back(decode_input_names_ptr_[1]);
+    for (size_t i = 0; i < past_key_values.size(); ++i) {
+      input_names_ptr.push_back(decode_input_names_ptr_[2 + 2 * i]);
+      input_names_ptr.push_back(decode_input_names_ptr_[2 + 2 * i + 1]);
+    }
+
+    std::vector<Ort::Value> outputs;
+
+    if (use_cuda_iobinding_) {
+      // Bind logits to CPU for sampling; keep KV cache on GPU for next steps.
+      Ort::IoBinding binding(*decode_sess_);
+      for (size_t i = 0; i < inputs.size(); ++i) {
+        binding.BindInput(input_names_ptr[i], inputs[i]);
+      }
+
+      binding.BindOutput(decode_output_names_ptr_[0], cpu_mem_info_);
+      for (size_t i = 1; i < decode_output_names_ptr_.size(); ++i) {
+        binding.BindOutput(decode_output_names_ptr_[i], *cuda_mem_info_);
+      }
+
+      binding.SynchronizeInputs();
+      decode_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      outputs = binding.GetOutputValues();
+    } else {
+      outputs = decode_sess_->Run(
+          {}, input_names_ptr.data(), inputs.data(), inputs.size(),
+          decode_output_names_ptr_.data(), decode_output_names_ptr_.size());
+    }
+
+    // First output is logits, remaining outputs are updated past_key_values
+    if (outputs.empty()) {
+      SHERPA_ONNX_LOGE("ForwardLLMDecode: empty outputs");
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    Ort::Value logits = std::move(outputs[0]);
+
+    if (!logits.IsTensor()) {
+      SHERPA_ONNX_LOGE("ForwardLLMDecode: logits is not a tensor");
+      SHERPA_ONNX_EXIT(-1);
+    }
+    AssertTensorIsCpu(logits, "ForwardLLMDecode logits");
+
+    if ((outputs.size() - 1) % 2 != 0) {
+      SHERPA_ONNX_LOGE("ForwardLLMDecode: invalid KV cache outputs size=%d",
+                       static_cast<int>(outputs.size()));
+      SHERPA_ONNX_EXIT(-1);
+    }
+
+    std::vector<std::pair<Ort::Value, Ort::Value>> updated_past_kv;
+    int num_layers = static_cast<int>((outputs.size() - 1) / 2);
+    updated_past_kv.reserve(num_layers);
+    for (int i = 0; i < num_layers; ++i) {
+      updated_past_kv.emplace_back(std::move(outputs[1 + 2 * i]),
+                                   std::move(outputs[1 + 2 * i + 1]));
+    }
+    return {std::move(logits), std::move(updated_past_kv)};
+  }
+
+  // Forward pass through embedding model.
+  // Converts token IDs to embeddings.
+  Ort::Value ForwardEmbedding(Ort::Value input_ids) {
+    // Embedding output is consumed by CPU-side packing code; bind it to CPU
+    // when running on CUDA to avoid returning a CUDA pointer.
+    if (use_cuda_iobinding_) {
+      Ort::IoBinding binding(*embedding_sess_);
+      binding.BindInput(embedding_input_names_ptr_[0], input_ids);
+      binding.BindOutput(embedding_output_names_ptr_[0], cpu_mem_info_);
+      binding.SynchronizeInputs();
+      embedding_sess_->Run(Ort::RunOptions{nullptr}, binding);
+      binding.SynchronizeOutputs();
+      auto outs = binding.GetOutputValues();
+
+      if (outs.empty()) {
+        SHERPA_ONNX_LOGE("ForwardEmbedding: empty outputs");
+        SHERPA_ONNX_EXIT(-1);
+      }
+      return std::move(outs[0]);
+    }
+
+    std::array<Ort::Value, 1> inputs = {std::move(input_ids)};
+    auto outputs = embedding_sess_->Run(
+        {}, embedding_input_names_ptr_.data(), inputs.data(), inputs.size(),
+        embedding_output_names_ptr_.data(), embedding_output_names_ptr_.size());
+    return std::move(outputs[0]);
+  }
+
+  int32_t VocabSize() const { return vocab_size_; }
+  int32_t HiddenSize() const { return hidden_size_; }
+  int32_t LfrWindowSize() const { return lfr_window_size_; }
+  int32_t LfrWindowShift() const { return lfr_window_shift_; }
+  OrtAllocator *Allocator() { return allocator_; }
+  bool HasEmbeddingModel() const { return has_embedding_model_; }
+  bool UseKVCache() const { return true; }
+  ONNXTensorElementDataType GetPrefillInputType() const {
+    return prefill_embeds_in_type_;
+  }
+  ONNXTensorElementDataType GetDecodeInputType() const {
+    return decode_embeds_in_type_;
+  }
+  bool IsCpuProvider() const { return is_cpu_provider_; }
+
+ private:
+  void InitEncoderAdaptor(const std::string &model_path) {
+    encoder_sess_ = std::make_unique<Ort::Session>(
+        env_, model_path.c_str(), sess_opts_encoder_);
+    GetInputNames(encoder_sess_.get(), &encoder_input_names_,
+                  &encoder_input_names_ptr_);
+    GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
+                   &encoder_output_names_ptr_);
+    encoder_in_type_ = GetSessionInputElemType(encoder_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = encoder_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    SHERPA_ONNX_READ_META_DATA(lfr_window_size_, "lfr_window_size");
+    SHERPA_ONNX_READ_META_DATA(lfr_window_shift_, "lfr_window_shift");
+    SHERPA_ONNX_READ_META_DATA(hidden_size_, "llm_dim");
+  }
+
+  void InitLLMPrefill(const std::string &model_path) {
+    prefill_sess_ = std::make_unique<Ort::Session>(
+        env_, model_path.c_str(), sess_opts_llm_);
+    GetInputNames(prefill_sess_.get(), &prefill_input_names_,
+                  &prefill_input_names_ptr_);
+    GetOutputNames(prefill_sess_.get(), &prefill_output_names_,
+                   &prefill_output_names_ptr_);
+    prefill_embeds_in_type_ = GetSessionInputElemType(prefill_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = prefill_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("Prefill model metadata:\n%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("Prefill model metadata:\n%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
+    }
+  }
+
+  void InitLLMDecode(const std::string &model_path) {
+    decode_sess_ = std::make_unique<Ort::Session>(
+        env_, model_path.c_str(), sess_opts_llm_);
+    GetInputNames(decode_sess_.get(), &decode_input_names_,
+                  &decode_input_names_ptr_);
+    GetOutputNames(decode_sess_.get(), &decode_output_names_,
+                   &decode_output_names_ptr_);
+    decode_embeds_in_type_ = GetSessionInputElemType(decode_sess_.get(), 0);
+    Ort::ModelMetadata meta_data = decode_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("Decode model metadata:\n%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("Decode model metadata:\n%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    int32_t decode_vocab_size = 0;
+    SHERPA_ONNX_READ_META_DATA(decode_vocab_size, "vocab_size");
+    if (vocab_size_ > 0 && decode_vocab_size != vocab_size_) {
+      SHERPA_ONNX_LOGE(
+          "Decode model vocab_size (%d) != prefill vocab_size (%d)",
+          decode_vocab_size, vocab_size_);
+    }
+    if (vocab_size_ == 0) vocab_size_ = decode_vocab_size;
+    int32_t decode_hidden_size = 0;
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(decode_hidden_size, "hidden_size");
+      hidden_size_ = decode_hidden_size;
+    }
+  }
+
+  void InitEmbedding(const std::string &model_path) {
+    embedding_sess_ = std::make_unique<Ort::Session>(
+        env_, model_path.c_str(), sess_opts_embedding_);
+    GetInputNames(embedding_sess_.get(), &embedding_input_names_,
+                  &embedding_input_names_ptr_);
+    GetOutputNames(embedding_sess_.get(), &embedding_output_names_,
+                   &embedding_output_names_ptr_);
+    Ort::ModelMetadata meta_data = embedding_sess_->GetModelMetadata();
+    if (config_.debug) {
+      std::ostringstream os;
+      PrintModelMetadata(os, meta_data);
+#if __OHOS__
+      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
+#else
+      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+#endif
+    }
+    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+    if (hidden_size_ == 0) {
+      SHERPA_ONNX_READ_META_DATA(hidden_size_, "hidden_size");
+    }
+  }
+
+ private:
+  OfflineModelConfig config_;
+  Ort::Env env_;
+  Ort::SessionOptions sess_opts_encoder_;
+  Ort::SessionOptions sess_opts_llm_;
+  Ort::SessionOptions sess_opts_embedding_;
+  Ort::AllocatorWithDefaultOptions allocator_;
+
+  Ort::MemoryInfo cpu_mem_info_;
+  std::unique_ptr<Ort::MemoryInfo> cuda_mem_info_;
+  bool use_cuda_iobinding_ = false;
+
+  std::unique_ptr<Ort::Session> encoder_sess_;
+  std::unique_ptr<Ort::Session> prefill_sess_;
+  std::unique_ptr<Ort::Session> decode_sess_;
+  std::unique_ptr<Ort::Session> embedding_sess_;
+
+  std::vector<std::string> encoder_input_names_;
+  std::vector<const char *> encoder_input_names_ptr_;
+  std::vector<std::string> encoder_output_names_;
+  std::vector<const char *> encoder_output_names_ptr_;
+
+  std::vector<std::string> prefill_input_names_;
+  std::vector<const char *> prefill_input_names_ptr_;
+  std::vector<std::string> prefill_output_names_;
+  std::vector<const char *> prefill_output_names_ptr_;
+
+  std::vector<std::string> decode_input_names_;
+  std::vector<const char *> decode_input_names_ptr_;
+  std::vector<std::string> decode_output_names_;
+  std::vector<const char *> decode_output_names_ptr_;
+
+  std::vector<std::string> embedding_input_names_;
+  std::vector<const char *> embedding_input_names_ptr_;
+  std::vector<std::string> embedding_output_names_;
+  std::vector<const char *> embedding_output_names_ptr_;
+
+  int32_t vocab_size_ = 0;
+  int32_t hidden_size_ = 0;
+  int32_t lfr_window_size_ = 0;
+  int32_t lfr_window_shift_ = 0;
+
+  bool has_embedding_model_ = false;
+
+  ONNXTensorElementDataType encoder_in_type_ =
+      ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
+  ONNXTensorElementDataType prefill_embeds_in_type_ =
+      ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
+  ONNXTensorElementDataType decode_embeds_in_type_ =
+      ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED;
+
+  bool is_cpu_provider_ = false;
+};
+
+OfflineFunASRNanoModel::OfflineFunASRNanoModel(const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(config)) {}
+
+template <typename Manager>
+OfflineFunASRNanoModel::OfflineFunASRNanoModel(Manager *mgr,
+                                               const OfflineModelConfig &config)
+    : impl_(std::make_unique<Impl>(mgr, config)) {}
+
+OfflineFunASRNanoModel::~OfflineFunASRNanoModel() = default;
+
+Ort::Value OfflineFunASRNanoModel::ForwardEncoderAdaptor(Ort::Value features) {
+  return impl_->ForwardEncoderAdaptor(std::move(features));
+}
+
+std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+OfflineFunASRNanoModel::ForwardLLMPrefill(Ort::Value inputs_embeds,
+                                          Ort::Value attention_mask) {
+  return impl_->ForwardLLMPrefill(std::move(inputs_embeds),
+                                  std::move(attention_mask));
+}
+
+std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+OfflineFunASRNanoModel::ForwardLLMDecode(
+    Ort::Value inputs_embeds, Ort::Value attention_mask,
+    const std::vector<std::pair<Ort::Value, Ort::Value>> &past_key_values) {
+  return impl_->ForwardLLMDecode(std::move(inputs_embeds),
+                                 std::move(attention_mask), past_key_values);
+}
+
+bool OfflineFunASRNanoModel::UseKVCache() const {
+  return impl_->UseKVCache();
+}
+
+Ort::Value OfflineFunASRNanoModel::ForwardEmbedding(Ort::Value input_ids) {
+  return impl_->ForwardEmbedding(std::move(input_ids));
+}
+
+int32_t OfflineFunASRNanoModel::VocabSize() const {
+  return impl_->VocabSize();
+}
+int32_t OfflineFunASRNanoModel::HiddenSize() const {
+  return impl_->HiddenSize();
+}
+int32_t OfflineFunASRNanoModel::LfrWindowSize() const {
+  return impl_->LfrWindowSize();
+}
+int32_t OfflineFunASRNanoModel::LfrWindowShift() const {
+  return impl_->LfrWindowShift();
+}
+
+OrtAllocator *OfflineFunASRNanoModel::Allocator() const {
+  return impl_->Allocator();
+}
+
+bool OfflineFunASRNanoModel::HasEmbeddingModel() const {
+  return impl_->HasEmbeddingModel();
+}
+
+ONNXTensorElementDataType OfflineFunASRNanoModel::GetPrefillInputType() const {
+  return impl_->GetPrefillInputType();
+}
+
+ONNXTensorElementDataType OfflineFunASRNanoModel::GetDecodeInputType() const {
+  return impl_->GetDecodeInputType();
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineFunASRNanoModel::OfflineFunASRNanoModel(
+    AAssetManager *mgr, const OfflineModelConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineFunASRNanoModel::OfflineFunASRNanoModel(
+    NativeResourceManager *mgr, const OfflineModelConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-funasr-nano-model.h b/sherpa-onnx/csrc/offline-funasr-nano-model.h
new file mode 100644
index 00000000..084d1e6b
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-funasr-nano-model.h
@@ -0,0 +1,105 @@
+// sherpa-onnx/csrc/offline-funasr-nano-model.h
+//
+// Copyright (c)  2025  zengyw
+
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_H_
+
+#include <memory>
+#include <vector>
+
+#include "onnxruntime_cxx_api.h"  // NOLINT
+#include "sherpa-onnx/csrc/offline-funasr-nano-model-config.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+
+namespace sherpa_onnx {
+
+class OfflineFunASRNanoModel {
+ public:
+  explicit OfflineFunASRNanoModel(const OfflineModelConfig &config);
+
+  template <typename Manager>
+  OfflineFunASRNanoModel(Manager *mgr, const OfflineModelConfig &config);
+
+  ~OfflineFunASRNanoModel();
+
+  /** Run the encoder+adaptor model.
+   *
+   * @param features  A tensor of shape (N, T, C). Audio features.
+   * @return Return embeddings of shape (N, T', hidden_size)
+   */
+  Ort::Value ForwardEncoderAdaptor(Ort::Value features);
+
+  /** Run the LLM prefill model (KV cache mode).
+   *
+   * @param inputs_embeds  A tensor of shape (N, T, hidden_size).
+   * @param attention_mask  A tensor of shape (N, T) containing attention mask.
+   * @return Return tuple (logits, past_key_values...). Logits shape (N, T, vocab_size).
+   *         past_key_values is a vector of (key, value) pairs for each layer.
+   */
+  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+  ForwardLLMPrefill(Ort::Value inputs_embeds, Ort::Value attention_mask);
+
+  /** Run the LLM decode model (KV cache mode).
+   *
+   * @param inputs_embeds  A tensor of shape (N, 1, hidden_size) for the next token.
+   * @param attention_mask  A tensor of shape (N, total_seq_len) containing attention mask.
+   * @param past_key_values  KV cache from previous steps, vector of (key, value) pairs.
+   * @return Return tuple (logits, updated_past_key_values...). Logits shape (N, 1, vocab_size).
+   */
+  std::pair<Ort::Value, std::vector<std::pair<Ort::Value, Ort::Value>>>
+  ForwardLLMDecode(Ort::Value inputs_embeds, Ort::Value attention_mask,
+                   const std::vector<std::pair<Ort::Value, Ort::Value>> &past_key_values);
+
+  /** Check if using KV cache mode. Always returns true for FunASR-nano.
+   */
+  bool UseKVCache() const;
+
+  /** Run the embedding model.
+   *
+   * @param input_ids  A tensor of shape (N, T) containing token IDs.
+   * @return Return embeddings of shape (N, T, hidden_size)
+   */
+  Ort::Value ForwardEmbedding(Ort::Value input_ids);
+
+  /** Return the vocabulary size of the model
+   */
+  int32_t VocabSize() const;
+
+  /** Return the hidden size of the model
+   */
+  int32_t HiddenSize() const;
+
+  /** It is lfr_window_size in metadata
+   */
+  int32_t LfrWindowSize() const;
+
+  /** It is lfr_window_shift in metadata
+   */
+  int32_t LfrWindowShift() const;
+
+  /** Return an allocator for allocating memory
+   */
+  OrtAllocator *Allocator() const;
+
+  /** Check if embedding model is available
+   */
+  bool HasEmbeddingModel() const;
+
+  /** Get expected input type for prefill model
+   */
+  ONNXTensorElementDataType GetPrefillInputType() const;
+
+  /** Get expected input type for decode model
+   */
+  ONNXTensorElementDataType GetDecodeInputType() const;
+
+ private:
+  class Impl;
+  std::unique_ptr<Impl> impl_;
+};
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_FUNASR_NANO_MODEL_H_
+
diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
index cae0ea78..0e03d671 100644
--- a/sherpa-onnx/csrc/offline-model-config.cc
+++ b/sherpa-onnx/csrc/offline-model-config.cc
@@ -25,6 +25,7 @@ void OfflineModelConfig::Register(ParseOptions *po) {
   dolphin.Register(po);
   canary.Register(po);
   omnilingual.Register(po);
+  funasr_nano.Register(po);
   medasr.Register(po);
 
   po->Register("telespeech-ctc", &telespeech_ctc,
@@ -94,9 +95,13 @@ bool OfflineModelConfig::Validate() const {
     }
   }
 
-  if (!FileExists(tokens)) {
-    SHERPA_ONNX_LOGE("tokens: '%s' does not exist", tokens.c_str());
-    return false;
+  // For FunASR-nano, tokens file is not required (tokenizer is loaded from directory)
+  // Check tokens file only if not using funasr_nano
+  if (funasr_nano.encoder_adaptor.empty()) {
+    if (!FileExists(tokens)) {
+      SHERPA_ONNX_LOGE("tokens: '%s' does not exist", tokens.c_str());
+      return false;
+    }
   }
 
   if (!modeling_unit.empty() &&
@@ -157,6 +162,10 @@ bool OfflineModelConfig::Validate() const {
     return omnilingual.Validate();
   }
 
+  if (!funasr_nano.encoder_adaptor.empty()) {
+    return funasr_nano.Validate();
+  }
+
   if (!medasr.model.empty()) {
     return medasr.Validate();
   }
@@ -191,6 +200,7 @@ std::string OfflineModelConfig::ToString() const {
   os << "dolphin=" << dolphin.ToString() << ", ";
   os << "canary=" << canary.ToString() << ", ";
   os << "omnilingual=" << omnilingual.ToString() << ", ";
+  os << "funasr_nano=" << funasr_nano.ToString() << ", ";
   os << "medasr=" << medasr.ToString() << ", ";
   os << "telespeech_ctc=\"" << telespeech_ctc << "\", ";
   os << "tokens=\"" << tokens << "\", ";
diff --git a/sherpa-onnx/csrc/offline-model-config.h b/sherpa-onnx/csrc/offline-model-config.h
index dec5c95e..fc520f67 100644
--- a/sherpa-onnx/csrc/offline-model-config.h
+++ b/sherpa-onnx/csrc/offline-model-config.h
@@ -9,6 +9,7 @@
 #include "sherpa-onnx/csrc/offline-canary-model-config.h"
 #include "sherpa-onnx/csrc/offline-dolphin-model-config.h"
 #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
+#include "sherpa-onnx/csrc/offline-funasr-nano-model-config.h"
 #include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
 #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
@@ -37,6 +38,7 @@ struct OfflineModelConfig {
   OfflineDolphinModelConfig dolphin;
   OfflineCanaryModelConfig canary;
   OfflineOmnilingualAsrCtcModelConfig omnilingual;
+  OfflineFunASRNanoModelConfig funasr_nano;
   OfflineMedAsrCtcModelConfig medasr;
   std::string telespeech_ctc;
 
@@ -73,6 +75,7 @@ struct OfflineModelConfig {
                      const OfflineDolphinModelConfig &dolphin,
                      const OfflineCanaryModelConfig &canary,
                      const OfflineOmnilingualAsrCtcModelConfig &omnilingual,
+                     const OfflineFunASRNanoModelConfig &funasr_nano,
                      const OfflineMedAsrCtcModelConfig &medasr,
                      const std::string &telespeech_ctc,
                      const std::string &tokens, int32_t num_threads, bool debug,
@@ -92,6 +95,7 @@ struct OfflineModelConfig {
         dolphin(dolphin),
         canary(canary),
         omnilingual(omnilingual),
+        funasr_nano(funasr_nano),
         medasr(medasr),
         telespeech_ctc(telespeech_ctc),
         tokens(tokens),
diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
new file mode 100644
index 00000000..c418008a
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
@@ -0,0 +1,436 @@
+// sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.cc
+//
+// Copyright (c)  2025  zengyw
+
+#include "sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h"
+
+#include <algorithm>
+#include <cmath>
+#include <cstdint>
+#include <cstring>
+#include <memory>
+#include <random>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "onnxruntime_cxx_api.h"
+#include "sherpa-onnx/csrc/macros.h"
+#include "sherpa-onnx/csrc/onnx-utils.h"
+#include "sherpa-onnx/csrc/text-utils.h"
+
+namespace sherpa_onnx {
+
+OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    const OfflineRecognizerConfig &config)
+    : OfflineRecognizerImpl(config),
+      config_(config),
+      model_(std::make_unique<OfflineFunASRNanoModel>(config.model_config)),
+      tokenizer_(std::make_unique<FunASRNanoTokenizer>(
+          config.model_config.funasr_nano.tokenizer)),
+      rng_(config.model_config.funasr_nano.seed) {
+  InitFeatConfig();
+}
+
+template <typename Manager>
+OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    Manager *mgr, const OfflineRecognizerConfig &config)
+    : OfflineRecognizerImpl(mgr, config),
+      config_(config),
+      model_(std::make_unique<OfflineFunASRNanoModel>(mgr, config.model_config)),
+      tokenizer_(std::make_unique<FunASRNanoTokenizer>(
+          mgr, config.model_config.funasr_nano.tokenizer)),
+      rng_(config.model_config.funasr_nano.seed) {
+  InitFeatConfig();
+}
+
+std::unique_ptr<OfflineStream>
+OfflineRecognizerFunASRNanoImpl::CreateStream() const {
+  return std::make_unique<OfflineStream>(config_.feat_config);
+}
+
+// Initialize feature extraction configuration for FunASR-nano.
+// Sets normalization, window type, and disables edge snipping and dithering
+// to match the model's expected input format.
+void OfflineRecognizerFunASRNanoImpl::InitFeatConfig() {
+  config_.feat_config.normalize_samples = false;
+  config_.feat_config.window_type = "hamming";
+  config_.feat_config.snip_edges = false;
+  config_.feat_config.dither = 0.0f;
+}
+
+// Apply Low Frame Rate (LFR) processing to reduce temporal resolution.
+// Concatenates multiple consecutive frames into a single frame.
+std::vector<float> OfflineRecognizerFunASRNanoImpl::ApplyLFR(
+    const std::vector<float> &in) const {
+  int32_t lfr_window_size = model_->LfrWindowSize();
+  int32_t lfr_window_shift = model_->LfrWindowShift();
+  int32_t in_feat_dim = config_.feat_config.feature_dim;
+  int32_t in_num_frames = static_cast<int32_t>(in.size() / in_feat_dim);
+  int32_t out_num_frames =
+      (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+  if (out_num_frames <= 0) return {};
+  int32_t out_feat_dim = in_feat_dim * lfr_window_size;
+  std::vector<float> out(out_num_frames * out_feat_dim);
+  const float *p_in = in.data();
+  float *p_out = out.data();
+  for (int32_t i = 0; i != out_num_frames; ++i) {
+    std::copy(p_in, p_in + out_feat_dim, p_out);
+    p_out += out_feat_dim;
+    p_in += lfr_window_shift * in_feat_dim;
+  }
+  return out;
+}
+
+// Build source token IDs with chat template format:
+// [system_prompt] [user_prompt] [audio_tokens] [assistant_prompt]
+// Returns the token sequence and sets fbank_beg_idx to the start position
+// of audio tokens in the sequence.
+std::vector<int64_t> OfflineRecognizerFunASRNanoImpl::BuildSourceIds(
+    const std::string &system_prompt, const std::string &user_prompt,
+    int32_t audio_token_len, int32_t &fbank_beg_idx,
+    int32_t &fake_token_len) const {
+  const std::string system_text =
+      "<|im_start|>system\n" + system_prompt + "<|im_end|>\n";
+  const std::string user_text = "<|im_start|>user\n" + user_prompt;
+  const std::string after_text = "<|im_end|>\n<|im_start|>assistant\n";
+  std::vector<int64_t> ids_before =
+      tokenizer_->Encode(system_text + user_text);
+  std::vector<int64_t> ids_after = tokenizer_->Encode(after_text);
+  fbank_beg_idx = static_cast<int32_t>(ids_before.size());
+  fake_token_len = audio_token_len;
+  int64_t pad_id = tokenizer_->GetPadTokenId();
+  if (pad_id < 0) pad_id = tokenizer_->GetEosTokenId();
+  std::vector<int64_t> source_ids;
+  source_ids.reserve(ids_before.size() + audio_token_len + ids_after.size());
+  source_ids.insert(source_ids.end(), ids_before.begin(), ids_before.end());
+  // Use pad tokens as placeholders for audio embeddings
+  source_ids.insert(source_ids.end(), audio_token_len, pad_id);
+  source_ids.insert(source_ids.end(), ids_after.begin(), ids_after.end());
+  return source_ids;
+}
+
+// Sample token from logits using greedy decoding (argmax).
+// Handles both FP16 and FP32 logits, skipping NaN/Inf values.
+// Returns token ID 0 as fallback if all logits are invalid.
+int64_t OfflineRecognizerFunASRNanoImpl::SampleTokenFromLogitsFp16OrFp32(
+    const void *logits, bool is_fp16, int32_t vocab_size) const {
+  int32_t best = 0;
+  float best_val = -1e30f;
+  bool found_valid = false;
+  if (is_fp16) {
+    const uint16_t *p = reinterpret_cast<const uint16_t *>(logits);
+    for (int32_t i = 0; i < vocab_size; ++i) {
+      float v = HalfBitsToFloat(p[i]);
+      if (std::isfinite(v) && v > best_val) {
+        best_val = v;
+        best = i;
+        found_valid = true;
+      }
+    }
+  } else {
+    const float *p = reinterpret_cast<const float *>(logits);
+    for (int32_t i = 0; i < vocab_size; ++i) {
+      if (std::isfinite(p[i]) && p[i] > best_val) {
+        best_val = p[i];
+        best = i;
+        found_valid = true;
+      }
+    }
+  }
+  if (!found_valid) {
+    return 0;
+  }
+  return static_cast<int64_t>(best);
+}
+
+// Generate text from encoder output using autoregressive decoding with KV cache.
+// Combines text embeddings (from prompts) and audio embeddings (from encoder)
+// to form the input sequence, then generates tokens autoregressively.
+OfflineRecognitionResult OfflineRecognizerFunASRNanoImpl::GenerateText(
+    Ort::Value encoder_out, const std::string &system_prompt,
+    const std::string &user_prompt) const {
+  OfflineRecognitionResult result;
+  auto memory_info =
+      Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+  const auto &funasr_config = config_.model_config.funasr_nano;
+  auto enc_shape = encoder_out.GetTensorTypeAndShapeInfo().GetShape();
+  int32_t audio_token_len = static_cast<int32_t>(enc_shape[1]);
+  int32_t hidden_size = static_cast<int32_t>(enc_shape[2]);
+  int32_t fbank_beg_idx = 0;
+  int32_t fake_token_len = 0;
+  std::vector<int64_t> source_ids = BuildSourceIds(
+      system_prompt, user_prompt, audio_token_len, fbank_beg_idx,
+      fake_token_len);
+  int32_t context_len = static_cast<int32_t>(source_ids.size());
+  const int32_t max_seq_len = 2048;
+  if (context_len > max_seq_len) {
+    SHERPA_ONNX_LOGE(
+        "Input sequence length (%d) exceeds maximum sequence length (%d). "
+        "Truncating to %d tokens. Recognition result may be incomplete due to "
+        "truncated input.",
+        context_len, max_seq_len, max_seq_len);
+    source_ids.resize(max_seq_len);
+    context_len = max_seq_len;
+  }
+
+  // Get text embeddings for the prompt tokens
+  std::vector<int64_t> input_ids = source_ids;
+  std::array<int64_t, 2> ids_shape{1, context_len};
+  Ort::Value input_ids_tensor = Ort::Value::CreateTensor(
+      memory_info, input_ids.data(), input_ids.size(), ids_shape.data(),
+      ids_shape.size());
+  Ort::Value text_embeds =
+      model_->ForwardEmbedding(std::move(input_ids_tensor));
+  auto te_info = text_embeds.GetTensorTypeAndShapeInfo();
+  auto te_shape = te_info.GetShape();
+  if (static_cast<int32_t>(te_shape[2]) != hidden_size) {
+    SHERPA_ONNX_LOGE("Embedding hidden mismatch: %d vs %d",
+                     static_cast<int32_t>(te_shape[2]), hidden_size);
+    result.text = "";
+    return result;
+  }
+  const auto te_type = te_info.GetElementType();
+  const bool te_fp16 = (te_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+  // Pre-allocate full inputs_embeds buffer to max_seq_len
+  std::vector<uint16_t> inputs_embeds_fp16(
+      static_cast<size_t>(max_seq_len) * hidden_size, FloatToHalfBits(0.0f));
+  std::vector<int64_t> attention_mask(static_cast<size_t>(max_seq_len), 0);
+  // Copy text embeddings into inputs_embeds buffer
+  if (te_fp16) {
+    const uint16_t *p = text_embeds.GetTensorData<uint16_t>();
+    std::copy(p, p + static_cast<size_t>(context_len) * hidden_size,
+              inputs_embeds_fp16.data());
+  } else {
+    const float *p = text_embeds.GetTensorData<float>();
+    for (int64_t i = 0; i < static_cast<int64_t>(context_len) * hidden_size;
+         ++i) {
+      inputs_embeds_fp16[static_cast<size_t>(i)] = FloatToHalfBits(p[i]);
+    }
+  }
+  // Inject audio embeddings into inputs_embeds at the position of audio tokens
+  auto enc_info2 = encoder_out.GetTensorTypeAndShapeInfo();
+  auto enc_et =
+      static_cast<ONNXTensorElementDataType>(enc_info2.GetElementType());
+  int32_t copy_len = std::min(fake_token_len, audio_token_len);
+  if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
+    const uint16_t *enc = encoder_out.GetTensorData<uint16_t>();
+    for (int32_t t = 0; t < copy_len; ++t) {
+      const uint16_t *src = enc + static_cast<int64_t>(t) * hidden_size;
+      uint16_t *dst = inputs_embeds_fp16.data() +
+                      static_cast<int64_t>(fbank_beg_idx + t) * hidden_size;
+      std::copy(src, src + hidden_size, dst);
+    }
+  } else if (enc_et == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
+    const float *enc = encoder_out.GetTensorData<float>();
+    for (int32_t t = 0; t < copy_len; ++t) {
+      const float *src = enc + static_cast<int64_t>(t) * hidden_size;
+      uint16_t *dst = inputs_embeds_fp16.data() +
+                      static_cast<int64_t>(fbank_beg_idx + t) * hidden_size;
+      for (int32_t d = 0; d < hidden_size; ++d) {
+        dst[d] = FloatToHalfBits(src[d]);
+      }
+    }
+  } else {
+    SHERPA_ONNX_LOGE("encoder_out elem_type=%d not supported", (int)enc_et);
+    result.text = "";
+    return result;
+  }
+  // Set attention mask for context tokens
+  for (int32_t i = 0; i < context_len; ++i) attention_mask[i] = 1;
+  int32_t valid_len = context_len;
+  std::vector<int64_t> generated_ids;
+  
+  generated_ids.reserve(funasr_config.max_new_tokens);
+  const int64_t eos_id = tokenizer_->GetEosTokenId();
+  const int64_t im_end_id = tokenizer_->GetImEndTokenId();
+  const int32_t max_new_tokens = funasr_config.max_new_tokens;
+  std::vector<std::pair<Ort::Value, Ort::Value>> past_key_values;
+  bool is_first_step = true;
+  // Autoregressive generation loop
+  for (int32_t step = 0; step < max_new_tokens; ++step) {
+    if (valid_len >= max_seq_len) break;
+    Ort::Value logits(nullptr);
+    
+    if (is_first_step) {
+      // First step: use prefill model with full context
+      std::array<int64_t, 3> embeds_shape{1, context_len, hidden_size};
+      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor(
+          memory_info, inputs_embeds_fp16.data(),
+          static_cast<size_t>(context_len) * hidden_size * sizeof(uint16_t),
+          embeds_shape.data(), embeds_shape.size(),
+          ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+
+      std::array<int64_t, 2> mask_shape{1, context_len};
+      Ort::Value attention_mask_tensor =
+          Ort::Value::CreateTensor<int64_t>(
+              memory_info, attention_mask.data(),
+              static_cast<size_t>(context_len), mask_shape.data(),
+              mask_shape.size());
+      auto tmp = model_->ForwardLLMPrefill(std::move(inputs_embeds_tensor),
+                                           std::move(attention_mask_tensor));
+      logits = std::move(tmp.first);
+      past_key_values = std::move(tmp.second);
+    } else {
+      // Subsequent steps: use decode model with KV cache
+      int64_t last_token_id = generated_ids.back();
+      std::vector<int64_t> one_id{last_token_id};
+      std::array<int64_t, 2> one_shape{1, 1};
+      Ort::Value one_tensor = Ort::Value::CreateTensor(
+          memory_info, one_id.data(), one_id.size(), one_shape.data(),
+          one_shape.size());
+      Ort::Value next_embed =
+          model_->ForwardEmbedding(std::move(one_tensor));
+      auto ne_info = next_embed.GetTensorTypeAndShapeInfo();
+      bool ne_fp16 = (ne_info.GetElementType() ==
+                      ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+      std::vector<uint16_t> next_embed_fp16(hidden_size);
+      if (ne_fp16) {
+        const uint16_t *src = next_embed.GetTensorData<uint16_t>();
+        std::copy(src, src + hidden_size, next_embed_fp16.data());
+      } else {
+        const float *src = next_embed.GetTensorData<float>();
+        for (int32_t d = 0; d < hidden_size; ++d) {
+          next_embed_fp16[d] = FloatToHalfBits(src[d]);
+        }
+      }
+      std::array<int64_t, 3> embeds_shape{1, 1, hidden_size};
+      Ort::Value inputs_embeds_tensor = Ort::Value::CreateTensor(
+          memory_info, next_embed_fp16.data(),
+          static_cast<size_t>(hidden_size) * sizeof(uint16_t),
+          embeds_shape.data(), embeds_shape.size(),
+          ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+      std::vector<int64_t> decode_mask(valid_len, 1);
+      std::array<int64_t, 2> mask_shape{1, valid_len};
+      Ort::Value attention_mask_tensor =
+          Ort::Value::CreateTensor<int64_t>(
+              memory_info, decode_mask.data(), decode_mask.size(),
+              mask_shape.data(), mask_shape.size());
+      auto tmp = model_->ForwardLLMDecode(std::move(inputs_embeds_tensor),
+                                          std::move(attention_mask_tensor),
+                                          past_key_values);
+      logits = std::move(tmp.first);
+      past_key_values = std::move(tmp.second);
+    }
+    // Extract logits for the last position
+    auto log_info = logits.GetTensorTypeAndShapeInfo();
+    auto log_shape = log_info.GetShape();
+    int32_t vocab_size = static_cast<int32_t>(log_shape[2]);
+    const bool log_fp16 =
+        (log_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16);
+    // In KV cache mode: prefill uses last position, decode uses position 0
+    int32_t last_idx = is_first_step ? (context_len - 1) : 0;
+    const void *base = nullptr;
+    if (log_fp16) {
+      base = logits.GetTensorData<uint16_t>();
+    } else {
+      base = logits.GetTensorData<float>();
+    }
+    const size_t offset = static_cast<size_t>(last_idx) * vocab_size;
+    const void *last_logits =
+        log_fp16
+            ? static_cast<const void *>(
+                  reinterpret_cast<const uint16_t *>(base) + offset)
+            : static_cast<const void *>(
+                  reinterpret_cast<const float *>(base) + offset);
+    // Sample next token using greedy decoding
+    int64_t next_id =
+        SampleTokenFromLogitsFp16OrFp32(last_logits, log_fp16, vocab_size);
+    if (next_id == eos_id || next_id == im_end_id) {
+      break;
+    }
+    generated_ids.push_back(next_id);
+
+    // After sampling the first token from prefill, switch to decode mode
+    if (is_first_step) {
+      is_first_step = false;
+    }
+    valid_len += 1;
+  }
+  // Decode generated token IDs to text
+  result.text = tokenizer_->Decode(generated_ids);
+  result.text = ApplyInverseTextNormalization(std::move(result.text));
+  result.text = ApplyHomophoneReplacer(std::move(result.text));
+  if (!generated_ids.empty()) {
+    // Fill tokens: decode each token individually
+    result.tokens.reserve(generated_ids.size());
+    for (int64_t token_id : generated_ids) {
+      std::vector<int64_t> single_token{token_id};
+      std::string token_str = tokenizer_->Decode(single_token);
+      result.tokens.push_back(token_str);
+    }
+    // Fill timestamps: estimate based on audio duration and token count
+    auto enc_shape2 = encoder_out.GetTensorTypeAndShapeInfo().GetShape();
+    int32_t audio_token_len2 = static_cast<int32_t>(enc_shape2[1]);
+    int32_t lfr_window_size = model_->LfrWindowSize();
+    int32_t lfr_window_shift = model_->LfrWindowShift();
+    int32_t sampling_rate = config_.feat_config.sampling_rate;
+    // Reverse LFR to get original feature frames
+    int32_t original_feature_frames =
+        (audio_token_len2 > 0)
+            ? ((audio_token_len2 - 1) * lfr_window_shift + lfr_window_size)
+            : 0;
+    float frame_shift_ms = config_.feat_config.frame_shift_ms;
+    float audio_duration =
+        (original_feature_frames > 0 && frame_shift_ms > 0)
+            ? static_cast<float>(original_feature_frames) * frame_shift_ms /
+                  1000.0f
+            : 0.0f;
+    result.timestamps.reserve(generated_ids.size());
+    // Linear interpolation: distribute tokens evenly over audio duration
+    if (generated_ids.size() > 1 && audio_duration > 0) {
+      float time_per_token =
+          audio_duration / static_cast<float>(generated_ids.size());
+      for (size_t i = 0; i < generated_ids.size(); ++i) {
+        result.timestamps.push_back(static_cast<float>(i) * time_per_token);
+      }
+    } else if (generated_ids.size() == 1 && audio_duration > 0) {
+      result.timestamps.push_back(audio_duration / 2.0f);
+    }
+  }
+  return result;
+}
+
+// Decode multiple audio streams in batch.
+// Applies LFR processing, runs encoder, and generates text for each stream.
+void OfflineRecognizerFunASRNanoImpl::DecodeStreams(OfflineStream **ss,
+                                                    int32_t n) const {
+  auto memory_info =
+      Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+  const auto &funasr_config = config_.model_config.funasr_nano;
+  for (int32_t i = 0; i != n; ++i) {
+    std::vector<float> f = ss[i]->GetFrames();
+    f = ApplyLFR(f);
+    int32_t num_frames = static_cast<int32_t>(
+        f.size() / (config_.feat_config.feature_dim * model_->LfrWindowSize()));
+    if (num_frames <= 0) {
+      OfflineRecognitionResult r;
+      r.text = "";
+      ss[i]->SetResult(r);
+      continue;
+    }
+    std::array<int64_t, 3> shape{1, num_frames,
+                                  static_cast<int64_t>(f.size() / num_frames)};
+    Ort::Value features = Ort::Value::CreateTensor<float>(
+        memory_info, const_cast<float *>(f.data()), f.size(), shape.data(),
+        shape.size());
+    Ort::Value encoder_out =
+        model_->ForwardEncoderAdaptor(std::move(features));
+    OfflineRecognitionResult r = GenerateText(
+        std::move(encoder_out), funasr_config.system_prompt,
+        funasr_config.user_prompt);
+    ss[i]->SetResult(r);
+  }
+}
+
+#if __ANDROID_API__ >= 9
+template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    AAssetManager *mgr, const OfflineRecognizerConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    NativeResourceManager *mgr, const OfflineRecognizerConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
new file mode 100644
index 00000000..e2b19d72
--- /dev/null
+++ b/sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
@@ -0,0 +1,73 @@
+// sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h
+//
+// Copyright (c)  2025  zengyw
+
+#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_FUNASR_NANO_IMPL_H_
+#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_FUNASR_NANO_IMPL_H_
+
+#include <algorithm>
+#include <memory>
+#include <random>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "sherpa-onnx/csrc/funasr-nano-tokenizer.h"
+#include "sherpa-onnx/csrc/offline-funasr-nano-model.h"
+#include "sherpa-onnx/csrc/offline-model-config.h"
+#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer.h"
+#include "sherpa-onnx/csrc/pad-sequence.h"
+
+namespace sherpa_onnx {
+
+class OfflineRecognizerFunASRNanoImpl : public OfflineRecognizerImpl {
+ public:
+  explicit OfflineRecognizerFunASRNanoImpl(
+      const OfflineRecognizerConfig &config);
+
+  template <typename Manager>
+  OfflineRecognizerFunASRNanoImpl(Manager *mgr,
+                                  const OfflineRecognizerConfig &config);
+
+  std::unique_ptr<OfflineStream> CreateStream() const override;
+
+  void DecodeStreams(OfflineStream **ss, int32_t n) const override;
+
+  OfflineRecognizerConfig GetConfig() const override { return config_; }
+
+ private:
+  void InitFeatConfig();
+  std::vector<float> ApplyLFR(const std::vector<float> &in) const;
+  std::vector<int64_t> BuildSourceIds(
+      const std::string &system_prompt, const std::string &user_prompt,
+      int32_t audio_token_len, int32_t &fbank_beg_idx,
+      int32_t &fake_token_len) const;
+  int64_t SampleToken(const float *logits, int32_t vocab_size, int32_t step,
+                     int64_t eos_token_id, int64_t im_end_token_id) const;
+  int64_t SampleTokenFromLogitsFp16OrFp32(const void *logits, bool is_fp16,
+                                          int32_t vocab_size) const;
+  OfflineRecognitionResult GenerateText(
+      Ort::Value encoder_out, const std::string &system_prompt,
+      const std::string &user_prompt) const;
+
+  OfflineRecognizerConfig config_;
+  std::unique_ptr<OfflineFunASRNanoModel> model_;
+  std::unique_ptr<FunASRNanoTokenizer> tokenizer_;
+  mutable std::mt19937 rng_;
+};
+
+#if __ANDROID_API__ >= 9
+template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    AAssetManager *mgr, const OfflineRecognizerConfig &config);
+#endif
+
+#if __OHOS__
+template OfflineRecognizerFunASRNanoImpl::OfflineRecognizerFunASRNanoImpl(
+    NativeResourceManager *mgr, const OfflineRecognizerConfig &config);
+#endif
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_FUNASR_NANO_IMPL_H_
+
diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
index d894f955..4cfcc8c3 100644
--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
+++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
@@ -28,6 +28,7 @@
 #include "sherpa-onnx/csrc/offline-recognizer-canary-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-ctc-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h"
+#include "sherpa-onnx/csrc/offline-recognizer-funasr-nano-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-moonshine-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h"
 #include "sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h"
@@ -208,6 +209,10 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(config);
   }
 
+  if (!config.model_config.funasr_nano.encoder_adaptor.empty()) {
+    return std::make_unique<OfflineRecognizerFunASRNanoImpl>(config);
+  }
+
   if (!config.model_config.paraformer.model.empty()) {
     return std::make_unique<OfflineRecognizerParaformerImpl>(config);
   }
@@ -539,6 +544,10 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(mgr, config);
   }
 
+  if (!config.model_config.funasr_nano.encoder_adaptor.empty()) {
+    return std::make_unique<OfflineRecognizerFunASRNanoImpl>(mgr, config);
+  }
+
   if (!config.model_config.paraformer.model.empty()) {
     return std::make_unique<OfflineRecognizerParaformerImpl>(mgr, config);
   }
diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
index 2abfc25a..fc060ff3 100644
--- a/sherpa-onnx/csrc/onnx-utils.cc
+++ b/sherpa-onnx/csrc/onnx-utils.cc
@@ -5,6 +5,8 @@
 #include "sherpa-onnx/csrc/onnx-utils.h"
 
 #include <algorithm>
+#include <cstdint>
+#include <cstring>
 #include <fstream>
 #include <functional>
 #include <memory>
@@ -423,4 +425,79 @@ std::string LookupCustomModelMetaData(const Ort::ModelMetadata &meta_data,
 #endif
 }
 
+// Convert IEEE 754 half-precision (16-bit) float to single-precision (32-bit)
+// float. Handles special cases: zero, subnormal, normal, infinity, and NaN.
+float HalfBitsToFloat(uint16_t h) {
+  const uint32_t sign = (static_cast<uint32_t>(h & 0x8000u)) << 16;
+  const uint32_t exp = (h & 0x7C00u) >> 10;
+  const uint32_t mant = (h & 0x03FFu);
+  uint32_t fbits = 0;
+  if (exp == 0) {
+    if (mant == 0) {
+      fbits = sign;
+    } else {
+      uint32_t m = mant;
+      uint32_t e = 127 - 15 + 1;
+      while ((m & 0x0400u) == 0) {
+        m <<= 1;
+        --e;
+      }
+      m &= 0x03FFu;
+      fbits = sign | (e << 23) | (m << 13);
+    }
+  } else if (exp == 31) {
+    fbits = sign | 0x7F800000u | (mant << 13);
+  } else {
+    const uint32_t e = exp + (127 - 15);
+    fbits = sign | (e << 23) | (mant << 13);
+  }
+  float out;
+  std::memcpy(&out, &fbits, sizeof(out));
+  return out;
+}
+
+// Convert IEEE 754 single-precision (32-bit) float to half-precision (16-bit)
+// float. Handles overflow (clamped to infinity), underflow (clamped to zero),
+// and normal values with proper rounding.
+uint16_t FloatToHalfBits(float f) {
+  uint32_t x;
+  std::memcpy(&x, &f, sizeof(x));
+  const uint32_t sign = (x >> 16) & 0x8000u;
+  const int32_t exp = static_cast<int32_t>((x >> 23) & 0xFFu);
+  const uint32_t mant = x & 0x007FFFFFu;
+  if (exp == 255) {
+    if (mant == 0) return static_cast<uint16_t>(sign | 0x7C00u);
+    return static_cast<uint16_t>(sign | 0x7C00u | (mant ? 0x1u : 0));
+  }
+  int32_t new_exp = exp - 127 + 15;
+  if (new_exp >= 31) {
+    return static_cast<uint16_t>(sign | 0x7C00u);
+  } else if (new_exp <= 0) {
+    if (new_exp < -10) {
+      return static_cast<uint16_t>(sign);
+    }
+    uint32_t m = mant | 0x00800000u;
+    int32_t shift = 14 - new_exp;
+    uint32_t half_m = m >> shift;
+    if ((m >> (shift - 1)) & 1u) {
+      half_m += 1;
+    }
+    return static_cast<uint16_t>(sign | (half_m & 0x03FFu));
+  } else {
+    uint16_t half_exp = static_cast<uint16_t>(new_exp << 10);
+    uint32_t half_m = mant >> 13;
+    if (mant & 0x00001000u) {
+      half_m += 1;
+      if (half_m == 0x0400u) {
+        half_m = 0;
+        half_exp = static_cast<uint16_t>((new_exp + 1) << 10);
+        if ((half_exp >> 10) >= 31) {
+          return static_cast<uint16_t>(sign | 0x7C00u);
+        }
+      }
+    }
+    return static_cast<uint16_t>(sign | half_exp | (half_m & 0x03FFu));
+  }
+}
+
 }  // namespace sherpa_onnx
diff --git a/sherpa-onnx/csrc/onnx-utils.h b/sherpa-onnx/csrc/onnx-utils.h
index 1e3fc52b..c98dcae1 100644
--- a/sherpa-onnx/csrc/onnx-utils.h
+++ b/sherpa-onnx/csrc/onnx-utils.h
@@ -118,6 +118,10 @@ std::vector<CopyableOrtValue> Convert(std::vector<Ort::Value> values);
 
 std::vector<Ort::Value> Convert(std::vector<CopyableOrtValue> values);
 
+float HalfBitsToFloat(uint16_t h);
+
+uint16_t FloatToHalfBits(float f);
+
 }  // namespace sherpa_onnx
 
 #endif  // SHERPA_ONNX_CSRC_ONNX_UTILS_H_
diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
index 8f4d1934..e4d278ca 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
@@ -94,6 +94,18 @@ See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/yesno/ind
     ./sherpa-onnx-tdnn-yesno/test_wavs/0_0_0_1_0_0_0_1.wav \
     ./sherpa-onnx-tdnn-yesno/test_wavs/0_0_1_0_0_0_1_0.wav
 
+(7) FunASR-nano models
+
+See https://github.com/FunAudioLLM/Fun-ASR-Nano-2512
+
+  ./bin/sherpa-onnx-offline \
+    --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
+    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
+    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
+    --funasr-nano-embedding=/path/to/embedding.onnx \
+    /path/to/foo.wav [bar.wav foobar.wav ...]
+
 Note: It supports decoding multiple files in batches
 
 foo.wav should be of single channel, 16-bit PCM encoded wave file; its
diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
index 32097856..5a3fc2bf 100644
--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
+++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
@@ -117,6 +117,24 @@ See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/yesno/ind
     --tdnn-model=./sherpa-onnx-tdnn-yesno/model-epoch-14-avg-2.onnx \
     ./sherpa-onnx-tdnn-yesno/test_wavs/0_0_0_1_0_0_0_1.wav
 
+(7) FunASR-nano models
+
+See https://github.com/FunAudioLLM/Fun-ASR-Nano-2512
+
+  ./bin/sherpa-onnx-vad-with-offline-asr \
+    --silero-vad-model=/path/to/silero_vad.onnx \
+    --funasr-nano-encoder-adaptor=/path/to/encoder_adaptor.onnx \
+    --funasr-nano-llm-prefill=/path/to/llm_prefill.onnx \
+    --funasr-nano-llm-decode=/path/to/llm_decode.onnx \
+    --funasr-nano-tokenizer=/path/to/Qwen3-0.6B \
+    --funasr-nano-embedding=/path/to/embedding.onnx \
+    [--funasr-nano-user-prompt="Transcription:"] \
+    [--funasr-nano-max-new-tokens=512] \
+    [--funasr-nano-temperature=0.3] \
+    [--funasr-nano-top-p=0.8] \
+    --num-threads=4 \
+    /path/to/foo.wav
+
 The input wav should be of single channel, 16-bit PCM encoded wave file; its
 sampling rate can be arbitrary and does not need to be 16kHz.
 
diff --git a/sherpa-onnx/python/csrc/CMakeLists.txt b/sherpa-onnx/python/csrc/CMakeLists.txt
index 69b47c50..3eff1b7b 100644
--- a/sherpa-onnx/python/csrc/CMakeLists.txt
+++ b/sherpa-onnx/python/csrc/CMakeLists.txt
@@ -13,6 +13,7 @@ set(srcs
   offline-ctc-fst-decoder-config.cc
   offline-dolphin-model-config.cc
   offline-fire-red-asr-model-config.cc
+  offline-funasr-nano-model-config.cc
   offline-lm-config.cc
   offline-medasr-ctc-model-config.cc
   offline-model-config.cc
diff --git a/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
new file mode 100644
index 00000000..87fab249
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
@@ -0,0 +1,32 @@
+// sherpa-onnx/python/csrc/offline-funasr-nano-model-config.cc
+//
+// Copyright (c)  2025  zengyw
+
+#include "sherpa-onnx/csrc/offline-funasr-nano-model-config.h"
+
+#include <string>
+
+#include "sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineFunASRNanoModelConfig(py::module *m) {
+  using PyClass = OfflineFunASRNanoModelConfig;
+  py::class_<PyClass>(*m, "OfflineFunASRNanoModelConfig")
+      .def(py::init<>())
+      .def_readwrite("encoder_adaptor", &PyClass::encoder_adaptor)
+      .def_readwrite("llm_prefill", &PyClass::llm_prefill)
+      .def_readwrite("llm_decode", &PyClass::llm_decode)
+      .def_readwrite("embedding", &PyClass::embedding)
+      .def_readwrite("tokenizer", &PyClass::tokenizer)
+      .def_readwrite("system_prompt", &PyClass::system_prompt)
+      .def_readwrite("user_prompt", &PyClass::user_prompt)
+      .def_readwrite("max_new_tokens", &PyClass::max_new_tokens)
+      .def_readwrite("temperature", &PyClass::temperature)
+      .def_readwrite("top_p", &PyClass::top_p)
+      .def_readwrite("seed", &PyClass::seed)
+      .def("__str__", &PyClass::ToString);
+}
+
+}  // namespace sherpa_onnx
+
diff --git a/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h
new file mode 100644
index 00000000..d4d8edfb
--- /dev/null
+++ b/sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h
@@ -0,0 +1,17 @@
+// sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h
+//
+// Copyright (c)  2025  zengyw
+
+#ifndef SHERPA_ONNX_PYTHON_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+#define SHERPA_ONNX_PYTHON_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+
+#include "sherpa-onnx/python/csrc/sherpa-onnx.h"
+
+namespace sherpa_onnx {
+
+void PybindOfflineFunASRNanoModelConfig(py::module *m);
+
+}  // namespace sherpa_onnx
+
+#endif  // SHERPA_ONNX_PYTHON_CSRC_OFFLINE_FUNASR_NANO_MODEL_CONFIG_H_
+
diff --git a/sherpa-onnx/python/csrc/offline-model-config.cc b/sherpa-onnx/python/csrc/offline-model-config.cc
index 59fa03ab..023c2d14 100644
--- a/sherpa-onnx/python/csrc/offline-model-config.cc
+++ b/sherpa-onnx/python/csrc/offline-model-config.cc
@@ -11,6 +11,7 @@
 #include "sherpa-onnx/python/csrc/offline-canary-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-dolphin-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-fire-red-asr-model-config.h"
+#include "sherpa-onnx/python/csrc/offline-funasr-nano-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-moonshine-model-config.h"
 #include "sherpa-onnx/python/csrc/offline-nemo-enc-dec-ctc-model-config.h"
@@ -39,6 +40,7 @@ void PybindOfflineModelConfig(py::module *m) {
   PybindOfflineDolphinModelConfig(m);
   PybindOfflineCanaryModelConfig(m);
   PybindOfflineOmnilingualAsrCtcModelConfig(m);
+  PybindOfflineFunASRNanoModelConfig(m);
   PybindOfflineMedAsrCtcModelConfig(m);
 
   using PyClass = OfflineModelConfig;
@@ -56,6 +58,7 @@ void PybindOfflineModelConfig(py::module *m) {
                     const OfflineDolphinModelConfig &,
                     const OfflineCanaryModelConfig &,
                     const OfflineOmnilingualAsrCtcModelConfig &,
+                    const OfflineFunASRNanoModelConfig &,
                     const OfflineMedAsrCtcModelConfig &, const std::string &,
                     const std::string &, int32_t, bool, const std::string &,
                     const std::string &, const std::string &,
@@ -73,6 +76,7 @@ void PybindOfflineModelConfig(py::module *m) {
            py::arg("dolphin") = OfflineDolphinModelConfig(),
            py::arg("canary") = OfflineCanaryModelConfig(),
            py::arg("omnilingual") = OfflineOmnilingualAsrCtcModelConfig(),
+           py::arg("funasr_nano") = OfflineFunASRNanoModelConfig(),
            py::arg("medasr") = OfflineMedAsrCtcModelConfig(),
            py::arg("telespeech_ctc") = "", py::arg("tokens") = "",
            py::arg("num_threads") = 1, py::arg("debug") = false,
@@ -91,6 +95,7 @@ void PybindOfflineModelConfig(py::module *m) {
       .def_readwrite("dolphin", &PyClass::dolphin)
       .def_readwrite("canary", &PyClass::canary)
       .def_readwrite("omnilingual", &PyClass::omnilingual)
+      .def_readwrite("funasr_nano", &PyClass::funasr_nano)
       .def_readwrite("medasr", &PyClass::medasr)
       .def_readwrite("telespeech_ctc", &PyClass::telespeech_ctc)
       .def_readwrite("tokens", &PyClass::tokens)
diff --git a/sherpa-onnx/python/sherpa_onnx/__init__.py b/sherpa-onnx/python/sherpa_onnx/__init__.py
index 48aec7e4..81aa1dd2 100644
--- a/sherpa-onnx/python/sherpa_onnx/__init__.py
+++ b/sherpa-onnx/python/sherpa_onnx/__init__.py
@@ -14,6 +14,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
     OfflineCtcFstDecoderConfig,
     OfflineDolphinModelConfig,
     OfflineFireRedAsrModelConfig,
+    OfflineFunASRNanoModelConfig,
     OfflineLMConfig,
     OfflineModelConfig,
     OfflineMoonshineModelConfig,
diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
index 7149f87a..5fa42334 100644
--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
@@ -7,6 +7,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
     FeatureExtractorConfig,
     HomophoneReplacerConfig,
     OfflineCanaryModelConfig,
+    OfflineFunASRNanoModelConfig,
     OfflineOmnilingualAsrCtcModelConfig,
     OfflineMedAsrCtcModelConfig,
     OfflineCtcFstDecoderConfig,
@@ -296,6 +297,102 @@ class OfflineRecognizer(object):
         self.config = recognizer_config
         return self
 
+    @classmethod
+    def from_funasr_nano(
+        cls,
+        encoder_adaptor: str,
+        llm_prefill: str,
+        llm_decode: str,
+        embedding: str,
+        tokenizer: str,
+        num_threads: int = 1,
+        sample_rate: int = 16000,
+        feature_dim: int = 80,
+        decoding_method: str = "greedy_search",
+        debug: bool = False,
+        provider: str = "cpu",
+        system_prompt: str = "You are a helpful assistant.",
+        user_prompt: str = ":",
+        max_new_tokens: int = 512,
+        temperature: float = 0.3,
+        top_p: float = 0.8,
+        seed: int = 42,
+    ):
+        """
+        Create an offline recognizer for FunASR-nano models.
+
+        Args:
+          encoder_adaptor:
+            Path to ``encoder_adaptor.onnx``.
+          llm_prefill:
+            Path to ``llm_prefill.onnx`` (KV cache mode).
+          llm_decode:
+            Path to ``llm_decode.onnx`` (KV cache mode).
+          embedding:
+            Path to ``embedding.onnx``.
+          tokenizer:
+            Path to tokenizer directory (e.g., Qwen3-0.6B).
+          num_threads:
+            Number of threads for neural network computation.
+          sample_rate:
+            Sample rate of the training data used to train the model.
+          feature_dim:
+            Dimension of the feature used to train the model.
+          decoding_method:
+            Valid values are greedy_search.
+          debug:
+            True to show debug messages.
+          provider:
+            onnxruntime execution providers. Valid values are: cpu, cuda.
+          system_prompt:
+            System prompt for FunASR-nano.
+          user_prompt:
+            User prompt template for FunASR-nano.
+          max_new_tokens:
+            Maximum number of new tokens to generate.
+          temperature:
+            Sampling temperature.
+          top_p:
+            Top-p (nucleus) sampling threshold.
+          seed:
+            Random seed.
+        """
+        self = cls.__new__(cls)
+        # Create OfflineFunASRNanoModelConfig and set attributes
+        funasr_nano_config = OfflineFunASRNanoModelConfig()
+        funasr_nano_config.encoder_adaptor = encoder_adaptor
+        funasr_nano_config.llm_prefill = llm_prefill
+        funasr_nano_config.llm_decode = llm_decode
+        funasr_nano_config.embedding = embedding
+        funasr_nano_config.tokenizer = tokenizer
+        funasr_nano_config.system_prompt = system_prompt
+        funasr_nano_config.user_prompt = user_prompt
+        funasr_nano_config.max_new_tokens = max_new_tokens
+        funasr_nano_config.temperature = temperature
+        funasr_nano_config.top_p = top_p
+        funasr_nano_config.seed = seed
+
+        model_config = OfflineModelConfig(
+            funasr_nano=funasr_nano_config,
+            num_threads=num_threads,
+            debug=debug,
+            provider=provider,
+        )
+
+        feat_config = FeatureExtractorConfig(
+            sampling_rate=sample_rate,
+            feature_dim=feature_dim,
+        )
+
+        recognizer_config = OfflineRecognizerConfig(
+            feat_config=feat_config,
+            model_config=model_config,
+            decoding_method=decoding_method,
+        )
+        self.recognizer = _Recognizer(recognizer_config)
+        self.config = recognizer_config
+        return self
+
     @classmethod
     def from_paraformer(
         cls,

commit e8db588fcc989e612c3c4d3919d48b234f304c8a
Author: wangqi <wang.qi@qikuyx.com>
Date:   Mon Dec 29 18:39:49 2025 -0500

    Upgrade it to latest version

diff --git a/build-swift-macos.sh b/build-swift-macos.sh
index 1e1e8e9c..7b7ce098 100755
--- a/build-swift-macos.sh
+++ b/build-swift-macos.sh
@@ -3,13 +3,25 @@
 set -ex
 
 dir=build-swift-macos
+
+# Clean existing build directory to avoid stale CMake cache issues
+if [ -d "$dir" ]; then
+  echo "Cleaning existing build directory..."
+  rm -rf "$dir"
+fi
+
 mkdir -p $dir
 cd $dir
 
+# Get the correct macOS SDK path from Xcode
+MACOS_SDK_PATH=$(xcrun --sdk macosx --show-sdk-path)
+echo "Using macOS SDK: $MACOS_SDK_PATH"
+
 cmake \
   -DSHERPA_ONNX_ENABLE_BINARY=OFF \
   -DSHERPA_ONNX_BUILD_C_API_EXAMPLES=OFF \
   -DCMAKE_OSX_ARCHITECTURES="arm64;x86_64" \
+  -DCMAKE_OSX_SYSROOT="$MACOS_SDK_PATH" \
   -DCMAKE_INSTALL_PREFIX=./install \
   -DCMAKE_BUILD_TYPE=Release \
   -DBUILD_SHARED_LIBS=OFF \
diff --git a/build-xcframework.sh b/build-xcframework.sh
index 762189ee..66d1999c 100755
--- a/build-xcframework.sh
+++ b/build-xcframework.sh
@@ -1,90 +1,255 @@
 #!/usr/bin/env bash
 #
-# Script to merge iOS and macOS xcframeworks into a unified xcframework
+# Script to build iOS and macOS libraries and merge them into a unified xcframework
 # that supports all Apple platforms (iOS devices, iOS simulators, and macOS)
 #
-# Usage: ./build-xcframework.sh
+# Usage: ./build-xcframework.sh [options]
 #
-# Prerequisites:
-#   - Run ./build-ios.sh first to generate iOS xcframework
-#   - Run ./build-swift-macos.sh first to generate macOS xcframework
+# Options:
+#   --skip-ios      Skip iOS build (use existing build-ios/sherpa-onnx.xcframework)
+#   --skip-macos    Skip macOS build (use existing build-swift-macos/sherpa-onnx.xcframework)
+#   --skip-build    Skip both builds (only merge existing xcframeworks)
+#   --clean         Clean all build directories before building
+#   -h, --help      Show this help message
 #
 # Output:
 #   - build-apple/sherpa-onnx.xcframework
 
 set -e
 
+# Color codes for output
+GREEN='\033[0;32m'
+YELLOW='\033[1;33m'
+RED='\033[0;31m'
+BLUE='\033[0;34m'
+NC='\033[0m' # No Color
+
+# Default options
+SKIP_IOS=false
+SKIP_MACOS=false
+CLEAN_BUILD=false
+
 log() {
   echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
 }
 
+log_step() {
+  echo -e "${BLUE}[STEP]${NC} $1"
+}
+
 error() {
-  echo "[ERROR] $1" >&2
+  echo -e "${RED}[ERROR]${NC} $1" >&2
   exit 1
 }
 
-# Color codes for output
-GREEN='\033[0;32m'
-YELLOW='\033[1;33m'
-RED='\033[0;31m'
-NC='\033[0m' # No Color
+warn() {
+  echo -e "${YELLOW}[WARN]${NC} $1"
+}
+
+success() {
+  echo -e "${GREEN}[OK]${NC} $1"
+}
+
+# Show help message
+show_help() {
+  echo "Usage: ./build-xcframework.sh [options]"
+  echo ""
+  echo "Build sherpa-onnx xcframework for all Apple platforms."
+  echo ""
+  echo "Options:"
+  echo "  --skip-ios      Skip iOS build (use existing build)"
+  echo "  --skip-macos    Skip macOS build (use existing build)"
+  echo "  --skip-build    Skip both builds (only merge existing xcframeworks)"
+  echo "  --clean         Clean all build directories before building"
+  echo "  -h, --help      Show this help message"
+  echo ""
+  echo "Output:"
+  echo "  build-apple/sherpa-onnx.xcframework"
+  echo ""
+  echo "Examples:"
+  echo "  ./build-xcframework.sh              # Full build"
+  echo "  ./build-xcframework.sh --clean      # Clean and rebuild everything"
+  echo "  ./build-xcframework.sh --skip-ios   # Only rebuild macOS"
+  echo "  ./build-xcframework.sh --skip-build # Only merge existing builds"
+}
+
+# Parse command line arguments
+parse_args() {
+  while [[ $# -gt 0 ]]; do
+    case $1 in
+      --skip-ios)
+        SKIP_IOS=true
+        shift
+        ;;
+      --skip-macos)
+        SKIP_MACOS=true
+        shift
+        ;;
+      --skip-build)
+        SKIP_IOS=true
+        SKIP_MACOS=true
+        shift
+        ;;
+      --clean)
+        CLEAN_BUILD=true
+        shift
+        ;;
+      -h|--help)
+        show_help
+        exit 0
+        ;;
+      *)
+        error "Unknown option: $1. Use --help for usage information."
+        ;;
+    esac
+  done
+}
+
+# Clean build directories
+clean_builds() {
+  log_step "Cleaning build directories..."
+
+  if [ -d "build-ios" ]; then
+    log "Removing build-ios..."
+    rm -rf build-ios
+  fi
+
+  if [ -d "build-swift-macos" ]; then
+    log "Removing build-swift-macos..."
+    rm -rf build-swift-macos
+  fi
+
+  if [ -d "build-apple" ]; then
+    log "Removing build-apple..."
+    rm -rf build-apple
+  fi
+
+  success "Build directories cleaned"
+}
+
+# Build iOS xcframework
+build_ios() {
+  if [ "$SKIP_IOS" = true ]; then
+    log_step "Skipping iOS build (--skip-ios specified)"
+    if [ ! -d "build-ios/sherpa-onnx.xcframework" ]; then
+      error "iOS xcframework not found at build-ios/sherpa-onnx.xcframework. Cannot skip iOS build."
+    fi
+    return 0
+  fi
+
+  log_step "Building iOS xcframework..."
+  echo ""
+
+  # Check if build script exists
+  if [ ! -f "./build-ios.sh" ]; then
+    error "build-ios.sh not found in current directory"
+  fi
+
+  # Run the iOS build
+  local start_time=$(date +%s)
+
+  if ./build-ios.sh; then
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    success "iOS build completed in ${duration}s"
+  else
+    error "iOS build failed. Check the output above for details."
+  fi
+
+  # Verify the output
+  if [ ! -d "build-ios/sherpa-onnx.xcframework" ]; then
+    error "iOS xcframework not found after build. Build may have failed silently."
+  fi
+}
+
+# Build macOS xcframework
+build_macos() {
+  if [ "$SKIP_MACOS" = true ]; then
+    log_step "Skipping macOS build (--skip-macos specified)"
+    if [ ! -d "build-swift-macos/sherpa-onnx.xcframework" ]; then
+      error "macOS xcframework not found at build-swift-macos/sherpa-onnx.xcframework. Cannot skip macOS build."
+    fi
+    return 0
+  fi
+
+  log_step "Building macOS xcframework..."
+  echo ""
+
+  # Check if build script exists
+  if [ ! -f "./build-swift-macos.sh" ]; then
+    error "build-swift-macos.sh not found in current directory"
+  fi
+
+  # Run the macOS build
+  local start_time=$(date +%s)
+
+  if ./build-swift-macos.sh; then
+    local end_time=$(date +%s)
+    local duration=$((end_time - start_time))
+    success "macOS build completed in ${duration}s"
+  else
+    error "macOS build failed. Check the output above for details."
+  fi
+
+  # Verify the output
+  if [ ! -d "build-swift-macos/sherpa-onnx.xcframework" ]; then
+    error "macOS xcframework not found after build. Build may have failed silently."
+  fi
+}
 
 # Check if required source xcframeworks exist
 check_prerequisites() {
-  log "Checking prerequisites..."
-  
+  log_step "Checking prerequisites..."
+
   if [ ! -d "build-ios/sherpa-onnx.xcframework" ]; then
-    error "iOS xcframework not found at build-ios/sherpa-onnx.xcframework. Please run ./build-ios.sh first."
+    error "iOS xcframework not found at build-ios/sherpa-onnx.xcframework"
   fi
-  
+
   if [ ! -d "build-swift-macos/sherpa-onnx.xcframework" ]; then
-    error "macOS xcframework not found at build-swift-macos/sherpa-onnx.xcframework. Please run ./build-swift-macos.sh first."
+    error "macOS xcframework not found at build-swift-macos/sherpa-onnx.xcframework"
   fi
-  
-  log " Prerequisites check passed"
+
+  success "Prerequisites check passed"
 }
 
 # Clean and prepare output directory
 prepare_output_dir() {
-  log "Preparing output directory..."
-  
+  log_step "Preparing output directory..."
+
   # Remove existing build-apple directory if it exists
   if [ -d "build-apple" ]; then
     log "Removing existing build-apple directory..."
     rm -rf build-apple
   fi
-  
+
   # Create fresh build-apple directory
   mkdir -p build-apple
-  log " Output directory prepared"
+  success "Output directory prepared"
 }
 
 # Extract library paths from xcframework
 get_library_paths() {
   local xcframework_path=$1
   local platform=$2
-  
+
   if [ "$platform" = "ios" ]; then
-    # For iOS device
     echo "$xcframework_path/ios-arm64"
   elif [ "$platform" = "ios-simulator" ]; then
-    # For iOS simulator
     echo "$xcframework_path/ios-arm64_x86_64-simulator"
   elif [ "$platform" = "macos" ]; then
-    # For macOS
     echo "$xcframework_path/macos-arm64_x86_64"
   fi
 }
 
 # Create the unified xcframework
 create_unified_xcframework() {
-  log "Creating unified xcframework..."
-  
+  log_step "Creating unified xcframework..."
+
   # Get library paths from each xcframework
   local ios_device_path=$(get_library_paths "build-ios/sherpa-onnx.xcframework" "ios")
   local ios_sim_path=$(get_library_paths "build-ios/sherpa-onnx.xcframework" "ios-simulator")
   local macos_path=$(get_library_paths "build-swift-macos/sherpa-onnx.xcframework" "macos")
-  
+
   # Verify paths exist
   if [ ! -d "$ios_device_path" ]; then
     error "iOS device library not found at $ios_device_path"
@@ -95,7 +260,7 @@ create_unified_xcframework() {
   if [ ! -d "$macos_path" ]; then
     error "macOS library not found at $macos_path"
   fi
-  
+
   # Resolve static libraries and headers for each slice
   local ios_device_lib="$ios_device_path/libsherpa-onnx.a"
   local ios_sim_lib="$ios_sim_path/libsherpa-onnx.a"
@@ -120,16 +285,14 @@ create_unified_xcframework() {
   xcodebuild_cmd="$xcodebuild_cmd -library $ios_device_lib -headers $ios_device_headers"
   xcodebuild_cmd="$xcodebuild_cmd -library $ios_sim_lib -headers $ios_sim_headers"
   xcodebuild_cmd="$xcodebuild_cmd -library $macos_lib -headers $macos_headers"
-  
+
   # Set output path
   xcodebuild_cmd="$xcodebuild_cmd -output build-apple/sherpa-onnx.xcframework"
-  
+
   # Execute the command
   log "Executing: $xcodebuild_cmd"
-  eval $xcodebuild_cmd
-  
-  if [ $? -eq 0 ]; then
-    log " Successfully created unified xcframework"
+  if eval $xcodebuild_cmd; then
+    success "Successfully created unified xcframework"
   else
     error "Failed to create unified xcframework"
   fi
@@ -137,102 +300,138 @@ create_unified_xcframework() {
 
 # Verify the created xcframework
 verify_xcframework() {
-  log "Verifying unified xcframework..."
-  
+  log_step "Verifying unified xcframework..."
+
   local xcframework_path="build-apple/sherpa-onnx.xcframework"
-  
+
   if [ ! -d "$xcframework_path" ]; then
     error "Unified xcframework not found at $xcframework_path"
   fi
-  
+
   # Check Info.plist exists
   if [ ! -f "$xcframework_path/Info.plist" ]; then
     error "Info.plist not found in xcframework"
   fi
-  
+
   # List available architectures
   log "Available architectures in unified xcframework:"
-  
+
   # Parse Info.plist to show supported platforms
   /usr/libexec/PlistBuddy -c "Print :AvailableLibraries" "$xcframework_path/Info.plist" 2>/dev/null | grep -A 2 "SupportedPlatform" | grep -v "^--$" || true
-  
+
   # Check each platform directory
   local platforms=("ios-arm64" "ios-arm64_x86_64-simulator" "macos-arm64_x86_64")
   for platform in "${platforms[@]}"; do
     if [ -d "$xcframework_path/$platform" ]; then
-      echo -e "${GREEN} Found platform: $platform${NC}"
-      
+      echo -e "${GREEN}  [OK] $platform${NC}"
+
       # Verify library structure
       local lib_path="$xcframework_path/$platform"
       if [ -f "$lib_path/libsherpa-onnx.a" ] || [ -f "$lib_path/sherpa-onnx.a" ]; then
-        echo "   - Static library exists"
         # Show library size
         local lib_file=$(find "$lib_path" -name "*.a" | head -n1)
         if [ -f "$lib_file" ]; then
           local size=$(ls -lh "$lib_file" | awk '{print $5}')
-          echo "   - Library size: $size"
+          echo "       Library: $size"
         fi
       fi
       if [ -d "$lib_path/Headers" ]; then
-        local header_count=$(find "$lib_path/Headers" -name "*.h" | wc -l)
-        echo "   - Headers exist ($header_count header files)"
+        local header_count=$(find "$lib_path/Headers" -name "*.h" | wc -l | tr -d ' ')
+        echo "       Headers: $header_count files"
       fi
     else
-      echo -e "${YELLOW}  Platform not found: $platform${NC}"
+      echo -e "${YELLOW}  [MISSING] $platform${NC}"
     fi
   done
-  
-  log " Verification complete"
+
+  success "Verification complete"
 }
 
 # Display usage information
 show_usage_info() {
   echo ""
   echo -e "${GREEN}========================================${NC}"
-  echo -e "${GREEN} Unified XCFramework Created Successfully!${NC}"
+  echo -e "${GREEN}  Build Complete!${NC}"
   echo -e "${GREEN}========================================${NC}"
   echo ""
-  echo " Location: build-apple/sherpa-onnx.xcframework"
+  echo "Output: build-apple/sherpa-onnx.xcframework"
   echo ""
-  echo " Supported Platforms:"
-  echo "    iOS (arm64) - Physical devices"
-  echo "    iOS Simulator (arm64, x86_64)"
-  echo "    macOS (arm64, x86_64)"
+  echo "Supported Platforms:"
+  echo "  - iOS (arm64) - Physical devices"
+  echo "  - iOS Simulator (arm64, x86_64)"
+  echo "  - macOS (arm64, x86_64)"
   echo ""
-  echo " Usage in Xcode:"
-  echo "   1. Drag and drop build-apple/sherpa-onnx.xcframework into your Xcode project"
-  echo "   2. Make sure to select 'Copy items if needed'"
-  echo "   3. Add to your target's 'Frameworks, Libraries, and Embedded Content'"
-  echo "   4. Set to 'Embed & Sign'"
+  echo "Usage in Xcode:"
+  echo "  1. Drag build-apple/sherpa-onnx.xcframework into your project"
+  echo "  2. Select 'Copy items if needed'"
+  echo "  3. Add to 'Frameworks, Libraries, and Embedded Content'"
   echo ""
-  echo " Swift Package Manager:"
-  echo "   .binaryTarget("
-  echo "       name: \"sherpa-onnx\","
-  echo "       path: \"build-apple/sherpa-onnx.xcframework\""
-  echo "   )"
+  echo "Swift Package Manager:"
+  echo "  .binaryTarget("
+  echo "      name: \"sherpa-onnx\","
+  echo "      path: \"build-apple/sherpa-onnx.xcframework\""
+  echo "  )"
   echo ""
 }
 
 # Main execution
 main() {
+  local total_start_time=$(date +%s)
+
   echo -e "${GREEN}========================================${NC}"
-  echo -e "${GREEN}Sherpa-ONNX XCFramework Merger${NC}"
+  echo -e "${GREEN}  Sherpa-ONNX XCFramework Builder${NC}"
   echo -e "${GREEN}========================================${NC}"
   echo ""
-  
-  # Step 1: Check prerequisites
+
+  # Parse command line arguments
+  parse_args "$@"
+
+  # Show build configuration
+  log "Build configuration:"
+  echo "  - iOS build:   $([ "$SKIP_IOS" = true ] && echo "SKIP" || echo "BUILD")"
+  echo "  - macOS build: $([ "$SKIP_MACOS" = true ] && echo "SKIP" || echo "BUILD")"
+  echo "  - Clean build: $([ "$CLEAN_BUILD" = true ] && echo "YES" || echo "NO")"
+  echo ""
+
+  # Step 0: Clean if requested
+  if [ "$CLEAN_BUILD" = true ]; then
+    clean_builds
+    echo ""
+  fi
+
+  # Step 1: Build iOS
+  build_ios
+  echo ""
+
+  # Step 2: Build macOS
+  build_macos
+  echo ""
+
+  # Step 3: Check prerequisites (verify both builds exist)
   check_prerequisites
-  
-  # Step 2: Prepare output directory
+  echo ""
+
+  # Step 4: Prepare output directory
   prepare_output_dir
-  
-  # Step 3: Create unified xcframework
+  echo ""
+
+  # Step 5: Create unified xcframework
   create_unified_xcframework
-  
-  # Step 4: Verify the result
+  echo ""
+
+  # Step 6: Verify the result
   verify_xcframework
-  
-  # Step 5: Show usage information
+
+  # Calculate total time
+  local total_end_time=$(date +%s)
+  local total_duration=$((total_end_time - total_start_time))
+  local minutes=$((total_duration / 60))
+  local seconds=$((total_duration % 60))
+
+  echo ""
+  log "Total build time: ${minutes}m ${seconds}s"
+
+  # Step 7: Show usage information
   show_usage_info
 }
 
diff --git a/commit.log b/commit.log
new file mode 100644
index 00000000..4071d2dc
--- /dev/null
+++ b/commit.log
@@ -0,0 +1,51091 @@
+commit 0d080f556627f3ad6e3270eba049d1bcb34f6dc7
+Merge: b990ddb6 acd44bf9
+Author: Wang Qi <wangqi@users.noreply.github.com>
+Date:   Sat Dec 27 10:46:04 2025 -0500
+
+    Merge branch 'k2-fsa:master' into master
+
+commit acd44bf9ab0ee83c2189c93b442038c0b4e66786
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Fri Dec 26 15:20:23 2025 +0800
+
+    Fix creating a view of an Ort::Value tensor. (#2939)
+
+diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
+index a33252de..2abfc25a 100644
+--- a/sherpa-onnx/csrc/onnx-utils.cc
++++ b/sherpa-onnx/csrc/onnx-utils.cc
+@@ -189,8 +189,8 @@ Ort::Value View(Ort::Value *v) {
+   auto type_and_shape = v->GetTensorTypeAndShapeInfo();
+   std::vector<int64_t> shape = type_and_shape.GetShape();
+ 
+-  auto memory_info =
+-      Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
++  auto memory_info = v->GetTensorMemoryInfo();
++
+   switch (type_and_shape.GetElementType()) {
+     case ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32:
+       return Ort::Value::CreateTensor(
+
+commit 1f5f9633cc71f0426cbcea591fba56ef8a646984
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 25 17:25:56 2025 +0800
+
+    Add C++ runtime and Python API for Google MedASR models (#2935)
+    
+    This pull request significantly enhances sherpa-onnx by integrating comprehensive support for Google MedASR CTC models. The changes encompass the foundational C++ model implementation, a streamlined Python API for ease of use, and specialized audio feature processing and text post-processing logic to ensure accurate medical speech recognition. This integration also includes updates to the build system and continuous integration tests, alongside a practical Python example, making MedASR models readily available and verifiable within the framework.
+
+diff --git a/.github/scripts/test-python.sh b/.github/scripts/test-python.sh
+index 49523aaf..ea8679a2 100755
+--- a/.github/scripts/test-python.sh
++++ b/.github/scripts/test-python.sh
+@@ -8,6 +8,17 @@ log() {
+   echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
+ }
+ 
++log "test Google MedASR"
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++ls -lh sherpa-onnx-medasr-ctc-en-int8-2025-12-25
++
++ls -lh sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs
++
++python3 ./python-api-examples/offline-medasr-ctc-decode-files.py
++rm -rf sherpa-onnx-medasr-ctc-en-int8-2025-12-25
++
+ log "test omnilingual ASR"
+ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+ tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+diff --git a/.github/workflows/export-medasr-ctc-to-onnx.yaml b/.github/workflows/export-medasr-ctc-to-onnx.yaml
+index 80aa921f..9bfafe25 100644
+--- a/.github/workflows/export-medasr-ctc-to-onnx.yaml
++++ b/.github/workflows/export-medasr-ctc-to-onnx.yaml
+@@ -3,7 +3,7 @@ name: export-medasr-ctc-to-onnx
+ on:
+   push:
+     branches:
+-      - export-medasr-onnx
++      - cpp-medasr-2
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -42,7 +42,9 @@ jobs:
+         run: |
+           cd scripts/medasr
+ 
+-          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/0.wav
++          for i in $(seq 0 5); do
++            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/$i.wav
++          done
+ 
+           curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/transcript.txt
+ 
+@@ -53,7 +55,9 @@ jobs:
+         run: |
+           cd scripts/medasr
+ 
+-          python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./0.wav
++          for i in $(seq 0 5); do
++            python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./$i.wav
++          done
+ 
+           cat transcript.txt
+ 
+@@ -62,7 +66,9 @@ jobs:
+         run: |
+           cd scripts/medasr
+ 
+-          python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./0.wav
++          for i in $(seq 0 5); do
++            python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./$i.wav
++          done
+ 
+           cat transcript.txt
+ 
+diff --git a/python-api-examples/offline-medasr-ctc-decode-files.py b/python-api-examples/offline-medasr-ctc-decode-files.py
+new file mode 100755
+index 00000000..d231eb85
+--- /dev/null
++++ b/python-api-examples/offline-medasr-ctc-decode-files.py
+@@ -0,0 +1,142 @@
++#!/usr/bin/env python3
++
++"""
++This file shows how to use a non-streaming Google MedASR CTC model from
++https://huggingface.co/google/medasr
++to decode files.
++
++Please download model files from
++https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++
++For instance,
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
++"""
++
++import time
++from pathlib import Path
++
++import librosa
++import numpy as np
++import sherpa_onnx
++
++
++def create_recognizer():
++    model = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx"
++    tokens = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt"
++    test_wav_0 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav"
++    test_wav_1 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/1.wav"
++    test_wav_2 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/2.wav"
++    test_wav_3 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/3.wav"
++    test_wav_4 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/4.wav"
++    test_wav_5 = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/5.wav"
++
++    for f in [
++        model,
++        tokens,
++        test_wav_0,
++        test_wav_1,
++        test_wav_2,
++        test_wav_3,
++        test_wav_4,
++        test_wav_5,
++    ]:
++        if not Path(f).is_file():
++            print(f"{f} does not exist")
++
++            raise ValueError(
++                """Please download model files from
++                https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++                """
++            )
++    return (
++        sherpa_onnx.OfflineRecognizer.from_medasr_ctc(
++            model=model,
++            tokens=tokens,
++            num_threads=2,
++        ),
++        test_wav_0,
++        test_wav_1,
++        test_wav_2,
++        test_wav_3,
++        test_wav_4,
++        test_wav_5,
++    )
++
++
++def load_audio(filename):
++    audio, sample_rate = librosa.load(filename, sr=16000)
++    assert sample_rate == 16000, sample_rate
++
++    return np.ascontiguousarray(audio)
++
++
++def decode_single_file(recognizer, filename):
++    samples = load_audio(filename)
++
++    start_time = time.time()
++
++    stream = recognizer.create_stream()
++    stream.accept_waveform(sample_rate=16000, waveform=samples)
++    recognizer.decode_stream(stream)
++
++    end_time = time.time()
++    elapsed_seconds = end_time - start_time
++    audio_duration = len(samples) / 16000
++    real_time_factor = elapsed_seconds / audio_duration
++
++    print("---")
++    print(filename)
++    print(stream.result)
++    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
++    print(f"Audio duration in seconds: {audio_duration:.3f}")
++    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
++    print()
++
++
++def decode_multiple_files(recognizer, filenames):
++    streams = []
++
++    start_time = time.time()
++
++    audio_duration = 0
++
++    for filename in filenames:
++        samples = load_audio(filename)
++        audio_duration += len(samples) / 16000
++
++        stream = recognizer.create_stream()
++        stream.accept_waveform(sample_rate=16000, waveform=samples)
++        streams.append(stream)
++
++    recognizer.decode_streams(streams)
++
++    end_time = time.time()
++    elapsed_seconds = end_time - start_time
++    real_time_factor = elapsed_seconds / audio_duration
++
++    for name, stream in zip(filenames, streams):
++        print("---")
++        print(name)
++        print(stream.result)
++        print()
++
++    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
++    print(f"Audio duration in seconds: {audio_duration:.3f}")
++    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
++    print()
++    print()
++
++
++def main():
++    recognizer, *filenames = create_recognizer()
++
++    decode_single_file(recognizer, filenames[0])
++    decode_single_file(recognizer, filenames[1])
++    decode_multiple_files(recognizer, filenames[2:])
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/medasr/export_onnx.py b/scripts/medasr/export_onnx.py
+index a0fce456..2f7bd7e7 100755
+--- a/scripts/medasr/export_onnx.py
++++ b/scripts/medasr/export_onnx.py
+@@ -107,6 +107,7 @@ def main():
+         "model_author": "google",
+         "maintainer": "k2-fsa",
+         "vocab_size": processor.tokenizer.vocab_size,
++        "subsampling_factor": 4,
+         "url": "https://github.com/Google-Health/medasr",
+         "license": "https://developers.google.com/health-ai-developer-foundations/terms",
+     }
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 5c3338fe..d21995b5 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -40,6 +40,8 @@ set(sources
+   offline-fire-red-asr-model.cc
+   offline-lm-config.cc
+   offline-lm.cc
++  offline-medasr-ctc-model-config.cc
++  offline-medasr-ctc-model.cc
+   offline-model-config.cc
+   offline-moonshine-greedy-search-decoder.cc
+   offline-moonshine-model-config.cc
+diff --git a/sherpa-onnx/csrc/offline-ctc-model.cc b/sherpa-onnx/csrc/offline-ctc-model.cc
+index 4becf1af..4cc5d407 100644
+--- a/sherpa-onnx/csrc/offline-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-ctc-model.cc
+@@ -21,6 +21,7 @@
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-dolphin-model.h"
++#include "sherpa-onnx/csrc/offline-medasr-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
+@@ -126,6 +127,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
+     return std::make_unique<OfflineTeleSpeechCtcModel>(config);
+   } else if (!config.omnilingual.model.empty()) {
+     return std::make_unique<OfflineOmnilingualAsrCtcModel>(config);
++  } else if (!config.medasr.model.empty()) {
++    return std::make_unique<OfflineMedAsrCtcModel>(config);
+   }
+ 
+   // TODO(fangjun): Refactor it. We don't need to use model_type here
+@@ -192,6 +195,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
+     return std::make_unique<OfflineTeleSpeechCtcModel>(mgr, config);
+   } else if (!config.omnilingual.model.empty()) {
+     return std::make_unique<OfflineOmnilingualAsrCtcModel>(mgr, config);
++  } else if (!config.medasr.model.empty()) {
++    return std::make_unique<OfflineMedAsrCtcModel>(mgr, config);
+   }
+ 
+   // TODO(fangjun): Refactor it. We don't need to use model_type here
+diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
+new file mode 100644
+index 00000000..1fdac49d
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
+@@ -0,0 +1,40 @@
++// sherpa-onnx/csrc/offline-medasr-ctc-model-config.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
++
++#include <sstream>
++#include <string>
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++void OfflineMedAsrCtcModelConfig::Register(ParseOptions *po) {
++  po->Register(
++      "medasr", &model,
++      "Path to model.onnx from MedASR. Please see "
++      "https://github.com/k2-fsa/sherpa-onnx/pull/2934 for available models");
++}
++
++bool OfflineMedAsrCtcModelConfig::Validate() const {
++  if (!FileExists(model)) {
++    SHERPA_ONNX_LOGE("MedASR model: '%s' does not exist", model.c_str());
++    return false;
++  }
++
++  return true;
++}
++
++std::string OfflineMedAsrCtcModelConfig::ToString() const {
++  std::ostringstream os;
++
++  os << "OfflineMedAsrCtcModelConfig(";
++  os << "model=\"" << model << "\")";
++
++  return os.str();
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
+new file mode 100644
+index 00000000..02629e1b
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
+@@ -0,0 +1,29 @@
++// sherpa-onnx/csrc/offline-medasr-ctc-model-config.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
++
++#include <string>
++
++#include "sherpa-onnx/csrc/parse-options.h"
++
++namespace sherpa_onnx {
++
++struct OfflineMedAsrCtcModelConfig {
++  std::string model;
++
++  OfflineMedAsrCtcModelConfig() = default;
++  explicit OfflineMedAsrCtcModelConfig(const std::string &model)
++      : model(model) {}
++
++  void Register(ParseOptions *po);
++  bool Validate() const;
++
++  std::string ToString() const;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model.cc b/sherpa-onnx/csrc/offline-medasr-ctc-model.cc
+new file mode 100644
+index 00000000..de1255cf
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-medasr-ctc-model.cc
+@@ -0,0 +1,198 @@
++// sherpa-onnx/csrc/offline-medasr-ctc-model.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/offline-medasr-ctc-model.h"
++
++#include <algorithm>
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/onnx-utils.h"
++#include "sherpa-onnx/csrc/session.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++
++namespace sherpa_onnx {
++
++namespace {
++
++std::vector<int64_t> GetMask(Ort::Value length) {
++  auto shape = length.GetTensorTypeAndShapeInfo().GetShape();
++  if (shape.size() != 1) {
++    SHERPA_ONNX_LOGE("Invalid length dim %zu", shape.size());
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  auto batch_size = shape[0];
++
++  const int64_t *p = length.GetTensorData<int64_t>();
++
++  int64_t max_len = *std::max_element(p, p + batch_size);
++
++  std::vector<int64_t> ans(batch_size * max_len, 0);
++
++  int64_t *p_mask = ans.data();
++
++  for (int32_t i = 0; i < batch_size; ++i) {
++    auto len = p[i];
++    std::fill(p_mask, p_mask + len, 1);
++
++    p_mask += max_len;
++  }
++
++  return ans;
++}
++
++}  // namespace
++
++class OfflineMedAsrCtcModel::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config)
++      : config_(config),
++        env_(ORT_LOGGING_LEVEL_ERROR),
++        sess_opts_(GetSessionOptions(config)),
++        allocator_{} {
++    auto buf = ReadFile(config_.medasr.model);
++    Init(buf.data(), buf.size());
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config)
++      : config_(config),
++        env_(ORT_LOGGING_LEVEL_ERROR),
++        sess_opts_(GetSessionOptions(config)),
++        allocator_{} {
++    auto buf = ReadFile(mgr, config_.medasr.model);
++    Init(buf.data(), buf.size());
++  }
++
++  std::vector<Ort::Value> Forward(Ort::Value features,
++                                  Ort::Value features_length) {
++    std::vector<int64_t> mask = GetMask(std::move(features_length));
++
++    auto memory_info =
++        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
++
++    std::vector<int64_t> shape =
++        features.GetTensorTypeAndShapeInfo().GetShape();
++    shape.resize(2);
++
++    Ort::Value mask_tensor = Ort::Value::CreateTensor<int64_t>(
++        memory_info, mask.data(), mask.size(), shape.data(), shape.size());
++
++    std::array<Ort::Value, 2> inputs = {std::move(features),
++                                        std::move(mask_tensor)};
++
++    return sess_->Run({}, input_names_ptr_.data(), inputs.data(), inputs.size(),
++                      output_names_ptr_.data(), output_names_ptr_.size());
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++
++  int32_t SubsamplingFactor() const { return subsampling_factor_; }
++
++  OrtAllocator *Allocator() { return allocator_; }
++
++ private:
++  void Init(void *model_data, size_t model_data_length) {
++    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
++                                           sess_opts_);
++
++    GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
++
++    GetOutputNames(sess_.get(), &output_names_, &output_names_ptr_);
++
++    // get meta data
++    Ort::ModelMetadata meta_data = sess_->GetModelMetadata();
++    if (config_.debug) {
++      std::ostringstream os;
++      PrintModelMetadata(os, meta_data);
++#if __OHOS__
++      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
++#endif
++    }
++
++    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
++
++    std::string model_type;
++    SHERPA_ONNX_READ_META_DATA_STR(model_type, "model_type");
++    if (model_type != "medasr_ctc") {
++      SHERPA_ONNX_LOGE("Expect model type medasr_ctc. Given: '%s'",
++                       model_type.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    SHERPA_ONNX_READ_META_DATA(vocab_size_, "vocab_size");
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(subsampling_factor_,
++                                            "subsampling_factor", 4);
++  }
++
++ private:
++  OfflineModelConfig config_;
++  Ort::Env env_;
++  Ort::SessionOptions sess_opts_;
++  Ort::AllocatorWithDefaultOptions allocator_;
++
++  std::unique_ptr<Ort::Session> sess_;
++
++  std::vector<std::string> input_names_;
++  std::vector<const char *> input_names_ptr_;
++
++  std::vector<std::string> output_names_;
++  std::vector<const char *> output_names_ptr_;
++
++  int32_t vocab_size_ = 0;
++  int32_t subsampling_factor_ = 0;
++};
++
++OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(Manager *mgr,
++                                             const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++OfflineMedAsrCtcModel::~OfflineMedAsrCtcModel() = default;
++
++std::vector<Ort::Value> OfflineMedAsrCtcModel::Forward(
++    Ort::Value features, Ort::Value features_length) {
++  return impl_->Forward(std::move(features), std::move(features_length));
++}
++
++int32_t OfflineMedAsrCtcModel::VocabSize() const { return impl_->VocabSize(); }
++
++int32_t OfflineMedAsrCtcModel::SubsamplingFactor() const {
++  return impl_->SubsamplingFactor();
++}
++
++OrtAllocator *OfflineMedAsrCtcModel::Allocator() const {
++  return impl_->Allocator();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineMedAsrCtcModel::OfflineMedAsrCtcModel(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-medasr-ctc-model.h b/sherpa-onnx/csrc/offline-medasr-ctc-model.h
+new file mode 100644
+index 00000000..76085eff
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-medasr-ctc-model.h
+@@ -0,0 +1,65 @@
++// sherpa-onnx/csrc/offline-medasr-ctc-model.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
++#include "onnxruntime_cxx_api.h"  // NOLINT
++#include "sherpa-onnx/csrc/offline-ctc-model.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++/** This class implements the CTC model from MedASR.
++ *
++ * See
++ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/export_onnx.py
++ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/test_onnx.py
++ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/medasr/run.sh
++ *
++ */
++class OfflineMedAsrCtcModel : public OfflineCtcModel {
++ public:
++  explicit OfflineMedAsrCtcModel(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineMedAsrCtcModel(Manager *mgr, const OfflineModelConfig &config);
++
++  ~OfflineMedAsrCtcModel() override;
++
++  /** Run the forward method of the model.
++   *
++   * @param features  A tensor of shape (N, T, C).
++   * @param features_length  A 1-D tensor of shape (N,) containing number of
++   *                         valid frames in `features` before padding.
++   *                         Its dtype is int64_t.
++   *
++   * @return Return a vector containing:
++   *  - log_probs: A 3-D tensor of shape (N, T', vocab_size).
++   *  - log_probs_length A 1-D tensor of shape (N,). Its dtype is int64_t
++   */
++  std::vector<Ort::Value> Forward(Ort::Value features,
++                                  Ort::Value features_length) override;
++
++  /** Return the vocabulary size of the model
++   */
++  int32_t VocabSize() const override;
++
++  int32_t SubsamplingFactor() const override;
++
++  /** Return an allocator for allocating memory
++   */
++  OrtAllocator *Allocator() const override;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_MEDASR_CTC_MODEL_H_
+diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
+index 49189f9f..cae0ea78 100644
+--- a/sherpa-onnx/csrc/offline-model-config.cc
++++ b/sherpa-onnx/csrc/offline-model-config.cc
+@@ -25,6 +25,7 @@ void OfflineModelConfig::Register(ParseOptions *po) {
+   dolphin.Register(po);
+   canary.Register(po);
+   omnilingual.Register(po);
++  medasr.Register(po);
+ 
+   po->Register("telespeech-ctc", &telespeech_ctc,
+                "Path to model.onnx for telespeech ctc");
+@@ -156,6 +157,10 @@ bool OfflineModelConfig::Validate() const {
+     return omnilingual.Validate();
+   }
+ 
++  if (!medasr.model.empty()) {
++    return medasr.Validate();
++  }
++
+   if (!telespeech_ctc.empty() && !FileExists(telespeech_ctc)) {
+     SHERPA_ONNX_LOGE("telespeech_ctc: '%s' does not exist",
+                      telespeech_ctc.c_str());
+@@ -186,6 +191,7 @@ std::string OfflineModelConfig::ToString() const {
+   os << "dolphin=" << dolphin.ToString() << ", ";
+   os << "canary=" << canary.ToString() << ", ";
+   os << "omnilingual=" << omnilingual.ToString() << ", ";
++  os << "medasr=" << medasr.ToString() << ", ";
+   os << "telespeech_ctc=\"" << telespeech_ctc << "\", ";
+   os << "tokens=\"" << tokens << "\", ";
+   os << "num_threads=" << num_threads << ", ";
+diff --git a/sherpa-onnx/csrc/offline-model-config.h b/sherpa-onnx/csrc/offline-model-config.h
+index 6ef84edc..dec5c95e 100644
+--- a/sherpa-onnx/csrc/offline-model-config.h
++++ b/sherpa-onnx/csrc/offline-model-config.h
+@@ -9,6 +9,7 @@
+ #include "sherpa-onnx/csrc/offline-canary-model-config.h"
+ #include "sherpa-onnx/csrc/offline-dolphin-model-config.h"
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
++#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
+ #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
+ #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
+@@ -36,6 +37,7 @@ struct OfflineModelConfig {
+   OfflineDolphinModelConfig dolphin;
+   OfflineCanaryModelConfig canary;
+   OfflineOmnilingualAsrCtcModelConfig omnilingual;
++  OfflineMedAsrCtcModelConfig medasr;
+   std::string telespeech_ctc;
+ 
+   std::string tokens;
+@@ -71,6 +73,7 @@ struct OfflineModelConfig {
+                      const OfflineDolphinModelConfig &dolphin,
+                      const OfflineCanaryModelConfig &canary,
+                      const OfflineOmnilingualAsrCtcModelConfig &omnilingual,
++                     const OfflineMedAsrCtcModelConfig &medasr,
+                      const std::string &telespeech_ctc,
+                      const std::string &tokens, int32_t num_threads, bool debug,
+                      const std::string &provider, const std::string &model_type,
+@@ -89,6 +92,7 @@ struct OfflineModelConfig {
+         dolphin(dolphin),
+         canary(canary),
+         omnilingual(omnilingual),
++        medasr(medasr),
+         telespeech_ctc(telespeech_ctc),
+         tokens(tokens),
+         num_threads(num_threads),
+diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+index 11f7b81d..93505043 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+@@ -38,6 +38,11 @@ OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+       // tdnn models from yesno have a SIL token, we should remove it.
+       continue;
+     }
++
++    if (sym_table.Contains("</s>") && src.tokens[i] == sym_table["</s>"]) {
++      // Skip </s> for Google MedASR
++      continue;
++    }
+     auto sym = sym_table[src.tokens[i]];
+     text.append(sym);
+ 
+@@ -58,6 +63,10 @@ OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+     text = sym_table.DecodeByteBpe(text);
+   }
+ 
++  if (!text.empty() && text.front() == ' ') {
++    text.erase(0, 1);
++  }
++
+   r.text = std::move(text);
+ 
+   float frame_shift_s = frame_shift_ms / 1000. * subsampling_factor;
+@@ -144,6 +153,17 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
+       config_.feat_config.normalize_samples = false;
+     }
+ 
++    if (!config_.model_config.medasr.model.empty()) {
++      config_.feat_config.low_freq = 125;
++      config_.feat_config.high_freq = 7500;
++      config_.feat_config.remove_dc_offset = false;
++      config_.feat_config.dither = 0;
++      config_.feat_config.preemph_coeff = 0;
++      config_.feat_config.window_type = "hanning";
++      config_.feat_config.feature_dim = 128;
++      config_.feat_config.snip_edges = true;
++    }
++
+     config_.feat_config.nemo_normalize_type =
+         model_->FeatureNormalizationMethod();
+ 
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 7aabf8d8..d894f955 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -217,6 +217,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       !config.model_config.tdnn.model.empty() ||
+       !config.model_config.wenet_ctc.model.empty() ||
+       !config.model_config.omnilingual.model.empty() ||
++      !config.model_config.medasr.model.empty() ||
+       !config.model_config.dolphin.model.empty()) {
+     return std::make_unique<OfflineRecognizerCtcImpl>(config);
+   }
+@@ -547,6 +548,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       !config.model_config.tdnn.model.empty() ||
+       !config.model_config.wenet_ctc.model.empty() ||
+       !config.model_config.omnilingual.model.empty() ||
++      !config.model_config.medasr.model.empty() ||
+       !config.model_config.dolphin.model.empty()) {
+     return std::make_unique<OfflineRecognizerCtcImpl>(mgr, config);
+   }
+diff --git a/sherpa-onnx/python/csrc/CMakeLists.txt b/sherpa-onnx/python/csrc/CMakeLists.txt
+index 0b55ae91..69b47c50 100644
+--- a/sherpa-onnx/python/csrc/CMakeLists.txt
++++ b/sherpa-onnx/python/csrc/CMakeLists.txt
+@@ -14,6 +14,7 @@ set(srcs
+   offline-dolphin-model-config.cc
+   offline-fire-red-asr-model-config.cc
+   offline-lm-config.cc
++  offline-medasr-ctc-model-config.cc
+   offline-model-config.cc
+   offline-moonshine-model-config.cc
+   offline-nemo-enc-dec-ctc-model-config.cc
+diff --git a/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
+new file mode 100644
+index 00000000..8d13a2ef
+--- /dev/null
++++ b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
+@@ -0,0 +1,22 @@
++// sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/offline-medasr-ctc-model-config.h"
++
++#include <string>
++#include <vector>
++
++#include "sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h"
++
++namespace sherpa_onnx {
++
++void PybindOfflineMedAsrCtcModelConfig(py::module *m) {
++  using PyClass = OfflineMedAsrCtcModelConfig;
++  py::class_<PyClass>(*m, "OfflineMedAsrCtcModelConfig")
++      .def(py::init<const std::string &>(), py::arg("model"))
++      .def_readwrite("model", &PyClass::model)
++      .def("__str__", &PyClass::ToString);
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
+new file mode 100644
+index 00000000..f09a78b3
+--- /dev/null
++++ b/sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
+@@ -0,0 +1,16 @@
++// sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h
++//
++// Copyright (c)  2023  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
++#define SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
++
++#include "sherpa-onnx/python/csrc/sherpa-onnx.h"
++
++namespace sherpa_onnx {
++
++void PybindOfflineMedAsrCtcModelConfig(py::module *m);
++
++}
++
++#endif  // SHERPA_ONNX_PYTHON_CSRC_OFFLINE_MEDASR_CTC_MODEL_CONFIG_H_
+diff --git a/sherpa-onnx/python/csrc/offline-model-config.cc b/sherpa-onnx/python/csrc/offline-model-config.cc
+index 6c1286b8..59fa03ab 100644
+--- a/sherpa-onnx/python/csrc/offline-model-config.cc
++++ b/sherpa-onnx/python/csrc/offline-model-config.cc
+@@ -11,6 +11,7 @@
+ #include "sherpa-onnx/python/csrc/offline-canary-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-dolphin-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-fire-red-asr-model-config.h"
++#include "sherpa-onnx/python/csrc/offline-medasr-ctc-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-moonshine-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-nemo-enc-dec-ctc-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
+@@ -38,6 +39,7 @@ void PybindOfflineModelConfig(py::module *m) {
+   PybindOfflineDolphinModelConfig(m);
+   PybindOfflineCanaryModelConfig(m);
+   PybindOfflineOmnilingualAsrCtcModelConfig(m);
++  PybindOfflineMedAsrCtcModelConfig(m);
+ 
+   using PyClass = OfflineModelConfig;
+   py::class_<PyClass>(*m, "OfflineModelConfig")
+@@ -54,9 +56,10 @@ void PybindOfflineModelConfig(py::module *m) {
+                     const OfflineDolphinModelConfig &,
+                     const OfflineCanaryModelConfig &,
+                     const OfflineOmnilingualAsrCtcModelConfig &,
+-                    const std::string &, const std::string &, int32_t, bool,
++                    const OfflineMedAsrCtcModelConfig &, const std::string &,
++                    const std::string &, int32_t, bool, const std::string &,
+                     const std::string &, const std::string &,
+-                    const std::string &, const std::string &>(),
++                    const std::string &>(),
+            py::arg("transducer") = OfflineTransducerModelConfig(),
+            py::arg("paraformer") = OfflineParaformerModelConfig(),
+            py::arg("nemo_ctc") = OfflineNemoEncDecCtcModelConfig(),
+@@ -70,6 +73,7 @@ void PybindOfflineModelConfig(py::module *m) {
+            py::arg("dolphin") = OfflineDolphinModelConfig(),
+            py::arg("canary") = OfflineCanaryModelConfig(),
+            py::arg("omnilingual") = OfflineOmnilingualAsrCtcModelConfig(),
++           py::arg("medasr") = OfflineMedAsrCtcModelConfig(),
+            py::arg("telespeech_ctc") = "", py::arg("tokens") = "",
+            py::arg("num_threads") = 1, py::arg("debug") = false,
+            py::arg("provider") = "cpu", py::arg("model_type") = "",
+@@ -87,6 +91,7 @@ void PybindOfflineModelConfig(py::module *m) {
+       .def_readwrite("dolphin", &PyClass::dolphin)
+       .def_readwrite("canary", &PyClass::canary)
+       .def_readwrite("omnilingual", &PyClass::omnilingual)
++      .def_readwrite("medasr", &PyClass::medasr)
+       .def_readwrite("telespeech_ctc", &PyClass::telespeech_ctc)
+       .def_readwrite("tokens", &PyClass::tokens)
+       .def_readwrite("num_threads", &PyClass::num_threads)
+diff --git a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
+index d8bce13a..8e8e73c6 100644
+--- a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
++++ b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
+@@ -1,4 +1,4 @@
+-// sherpa-onnx/python/csrc/offline-wenet-model-config.cc
++// sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.cc
+ //
+ // Copyright (c)  2023  Xiaomi Corporation
+ 
+diff --git a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
+index ea92c46f..b9df4f07 100644
+--- a/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
++++ b/sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
+@@ -1,4 +1,4 @@
+-// sherpa-onnx/python/csrc/offline-wenet-model-config.h
++// sherpa-onnx/python/csrc/offline-wenet-ctc-model-config.h
+ //
+ // Copyright (c)  2023  Xiaomi Corporation
+ 
+diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+index 1194c664..7149f87a 100644
+--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
++++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+@@ -8,6 +8,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
+     HomophoneReplacerConfig,
+     OfflineCanaryModelConfig,
+     OfflineOmnilingualAsrCtcModelConfig,
++    OfflineMedAsrCtcModelConfig,
+     OfflineCtcFstDecoderConfig,
+     OfflineDolphinModelConfig,
+     OfflineFireRedAsrModelConfig,
+@@ -536,6 +537,56 @@ class OfflineRecognizer(object):
+         self.config = recognizer_config
+         return self
+ 
++    @classmethod
++    def from_medasr_ctc(
++        cls,
++        model: str,
++        tokens: str,
++        num_threads: int = 1,
++        decoding_method: str = "greedy_search",
++        debug: bool = False,
++        provider: str = "cpu",
++    ):
++        """
++        Please refer to
++        `<https://k2-fsa.github.io/sherpa/onnx/medasr/index.html>`_
++        to download pre-trained models.
++
++        Args:
++          model:
++            Path to ``model.onnx``.
++          tokens:
++            Path to ``tokens.txt``. Each line in ``tokens.txt`` contains two
++            columns::
++
++                symbol integer_id
++
++          num_threads:
++            Number of threads for neural network computation.
++          decoding_method:
++            The only supported decoding method is greedy_search.
++          debug:
++            True to show debug messages.
++          provider:
++            onnxruntime execution providers. Valid values are: cpu, cuda, coreml.
++        """
++        self = cls.__new__(cls)
++        model_config = OfflineModelConfig(
++            medasr=OfflineMedAsrCtcModelConfig(model=model),
++            tokens=tokens,
++            num_threads=num_threads,
++            debug=debug,
++            provider=provider,
++        )
++
++        recognizer_config = OfflineRecognizerConfig(
++            model_config=model_config,
++            decoding_method=decoding_method,
++        )
++        self.recognizer = _Recognizer(recognizer_config)
++        self.config = recognizer_config
++        return self
++
+     @classmethod
+     def from_omnilingual_asr_ctc(
+         cls,
+
+commit ee3611d5017f1e8eb49e05627b42041efc3f9491
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 25 13:56:11 2025 +0800
+
+    Export Google MedASR to sherpa-onnx (#2934)
+    
+    This pull request integrates Google's MedASR models into the "sherpa-onnx" project by providing a complete workflow for exporting these specialized medical speech recognition models to ONNX format. It includes scripts for model conversion, dynamic quantization, and testing, enabling efficient deployment and inference of MedASR within the "sherpa-onnx" environment. The changes ensure proper licensing attribution and metadata embedding for the exported models.
+
+diff --git a/.github/workflows/export-medasr-ctc-to-onnx.yaml b/.github/workflows/export-medasr-ctc-to-onnx.yaml
+new file mode 100644
+index 00000000..80aa921f
+--- /dev/null
++++ b/.github/workflows/export-medasr-ctc-to-onnx.yaml
+@@ -0,0 +1,161 @@
++name: export-medasr-ctc-to-onnx
++
++on:
++  push:
++    branches:
++      - export-medasr-onnx
++  workflow_dispatch:
++
++concurrency:
++  group: export-medasr-ctc-to-onnx-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  export-medasr-ctc-to-onnx:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: export medasr ctc
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [macos-latest]
++        python-version: ["3.10"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python ${{ matrix.python-version }}
++        uses: actions/setup-python@v5
++        with:
++          python-version: ${{ matrix.python-version }}
++
++      - name: Run
++        shell: bash
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        run: |
++          cd scripts/medasr
++          ./run.sh
++
++      - name: Download test data
++        shell: bash
++        run: |
++          cd scripts/medasr
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/0.wav
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25/resolve/main/test_wavs/transcript.txt
++
++          ls -lh
++
++      - name: Test fp32
++        shell: bash
++        run: |
++          cd scripts/medasr
++
++          python3 test_onnx.py --model ./model.onnx --tokens ./tokens.txt --wav ./0.wav
++
++          cat transcript.txt
++
++      - name: Test int8
++        shell: bash
++        run: |
++          cd scripts/medasr
++
++          python3 test_onnx.py --model ./model.int8.onnx --tokens ./tokens.txt --wav ./0.wav
++
++          cat transcript.txt
++
++      - name: Collect fp32 files
++        shell: bash
++        run: |
++          cd scripts/medasr
++
++          d=sherpa-onnx-medasr-ctc-en-2025-12-25
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v model.onnx $d
++          cp -v README.md $d
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs/
++          cp -v transcript.txt $d/test_wavs/
++
++          tar cjvf $d.tar.bz2 $d
++
++          ls -lh $d
++          ls -lh *.tar.bz2
++
++          mv $d ../..
++          mv $d.tar.bz2 ../..
++
++      - name: Collect int8 files
++        shell: bash
++        run: |
++          cd scripts/medasr
++
++          d=sherpa-onnx-medasr-ctc-en-int8-2025-12-25
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v model.int8.onnx $d
++          cp -v README.md $d
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs/
++          cp -v transcript.txt $d/test_wavs/
++
++          tar cjvf $d.tar.bz2 $d
++
++          ls -lh $d
++          ls -lh *.tar.bz2
++
++          mv $d ../..
++          mv $d.tar.bz2 ../..
++
++      - name: Release
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models
++
++      - name: Publish to huggingface
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 5
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            names=(
++             sherpa-onnx-medasr-ctc-en-2025-12-25
++             sherpa-onnx-medasr-ctc-en-int8-2025-12-25
++            )
++            for d in ${names[@]}; do
++              if [ ! -d $d ]; then
++                echo "$d does not exist - skip it"
++                continue;
++              fi
++
++              export GIT_LFS_SKIP_SMUDGE=1
++              export GIT_CLONE_PROTECTION_ACTIVE=false
++              rm -rf huggingface
++              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
++              cp -av $d/* ./huggingface
++              cd huggingface
++              git lfs track "*.onnx"
++              git lfs track "*.wav"
++              git status
++              git add .
++              git status
++              git commit -m "add models"
++              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
++              cd ..
++            done
+diff --git a/scripts/medasr/README.md b/scripts/medasr/README.md
+new file mode 100644
+index 00000000..654fbd01
+--- /dev/null
++++ b/scripts/medasr/README.md
+@@ -0,0 +1,24 @@
++---
++license: other
++license_name: health-ai-developer-foundations
++license_link: https://developers.google.com/health-ai-developer-foundations/terms
++language:
++- en
++pipeline_tag: automatic-speech-recognition
++library_name: transformers
++tags:
++- medical-asr
++- radiology
++- medical
++---
++
++# Introduction
++
++This directory includes models sourced from:
++
++https://github.com/Google-Health/medasr
++
++All model files are governed by the Health AI Developer Foundations Terms of Use.
++For full licensing details, please refer to:
++
++https://developers.google.com/health-ai-developer-foundations/terms
+diff --git a/scripts/medasr/export_onnx.py b/scripts/medasr/export_onnx.py
+new file mode 100755
+index 00000000..a0fce456
+--- /dev/null
++++ b/scripts/medasr/export_onnx.py
+@@ -0,0 +1,128 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++"""
++Make sure you have set the environment variable
++
++    export HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
++
++where hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx is your Huggingface access token.
++"""
++
++from typing import Any, Dict
++
++import onnx
++import torch
++from onnxruntime.quantization import QuantType, quantize_dynamic
++from transformers import AutoModelForCTC, AutoProcessor
++
++
++def add_meta_data(filename: str, meta_data: Dict[str, Any]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    model.metadata_props.clear()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++class Wrapper(torch.nn.Module):
++    def __init__(self, m):
++        super().__init__()
++        self.m = m
++
++    def forward(self, x: torch.Tensor, mask: torch.Tensor):
++        """
++        Args:
++          x: (N, T, C), dtype float32
++          mask: (N, T), dtype int64. Valid positions are 1. Padding positions are 0.
++        Returns:
++          logits: (N, T/4, vocob_size), dtype float32
++          logits_len: (N,), dtype int64
++        """
++        o = self.m(x, mask.bool())
++        logits_len = self.m._get_subsampling_output_length(mask.sum(-1)).to(torch.int64)
++        return o.logits, logits_len
++
++
++def generate_tokens(tokenizer):
++    vocab = tokenizer.get_vocab()
++    id2token = {i: t for t, i in vocab.items()}
++
++    with open("tokens.txt", "w", encoding="utf-8") as f:
++        for i in range(tokenizer.vocab_size):
++            if i == tokenizer.pad_token_id:
++                f.write(f"<blk> {i}\n")
++            else:
++                f.write(f"{id2token[i]} {i}\n")
++    print("saved to tokens.txt")
++
++
++@torch.no_grad()
++def main():
++    model_id = "google/medasr"
++    processor = AutoProcessor.from_pretrained(model_id)
++
++    generate_tokens(processor.tokenizer)
++
++    model = AutoModelForCTC.from_pretrained(model_id)
++
++    w = Wrapper(model)
++    w.eval()
++
++    filename = "model.onnx"
++    x = torch.rand(1, 100, 128)
++    mask = torch.ones(1, x.shape[1], dtype=torch.int64)
++    torch.onnx.export(
++        w,
++        (x, mask),
++        filename,
++        input_names=["x", "mask"],
++        output_names=["logits", "logits_len"],
++        dynamic_axes={
++            "x": {0: "N", 1: "T"},
++            "mask": {0: "N", 1: "T"},
++            "logits": {0: "N", 1: "T_4"},
++            "logits_len": {0: "N"},
++        },
++        opset_version=14,
++        external_data=False,
++        dynamo=False,
++    )
++
++    meta_data = {
++        "model_type": "medasr_ctc",
++        "version": "20251225",
++        "model_author": "google",
++        "maintainer": "k2-fsa",
++        "vocab_size": processor.tokenizer.vocab_size,
++        "url": "https://github.com/Google-Health/medasr",
++        "license": "https://developers.google.com/health-ai-developer-foundations/terms",
++    }
++    add_meta_data(filename=filename, meta_data=meta_data)
++
++    filename_int8 = "model.int8.onnx"
++    quantize_dynamic(
++        model_input=filename,
++        model_output=filename_int8,
++        op_types_to_quantize=["MatMul"],
++        # Note that we have to use QUInt8 here.
++        #
++        # When QInt8 is used, C++ onnxruntime produces incorrect results
++        weight_type=QuantType.QUInt8,
++    )
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/medasr/run.sh b/scripts/medasr/run.sh
+new file mode 100755
+index 00000000..779a13c6
+--- /dev/null
++++ b/scripts/medasr/run.sh
+@@ -0,0 +1,21 @@
++#!/usr/bin/env bash
++
++if [ -z $HF_TOKEN ]; then
++  echo "Please first run export HF_TOKEN=your_huggingface_access_token."
++  exit 1
++fi
++
++pip install \
++  accelerate \
++  bitsandbytes \
++  git+https://github.com/huggingface/transformers.git@65dc261512cbdb1ee72b88ae5b222f2605aad8e5 \
++  onnx==1.17.0 \
++  onnxruntime==1.17.1 \
++  librosa \
++  onnxscript \
++  "numpy<2" \
++  kaldi-native-fbank
++
++./export_onnx.py
++
++ls -lh
+diff --git a/scripts/medasr/test_onnx.py b/scripts/medasr/test_onnx.py
+new file mode 100755
+index 00000000..3436403f
+--- /dev/null
++++ b/scripts/medasr/test_onnx.py
+@@ -0,0 +1,163 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import argparse
++import time
++
++import kaldi_native_fbank as knf
++import librosa
++import numpy as np
++import onnxruntime as ort
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--model",
++        type=str,
++        required=True,
++        help="Path to onnx model file",
++    )
++
++    parser.add_argument(
++        "--tokens",
++        type=str,
++        required=True,
++        help="Path to tokens.txt",
++    )
++
++    parser.add_argument(
++        "--wav",
++        type=str,
++        required=True,
++        help="Path to test wav",
++    )
++    return parser.parse_args()
++
++
++class OnnxModel:
++    def __init__(self, filename):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = 1
++        session_opts.intra_op_num_threads = 1
++
++        self.session_opts = session_opts
++
++        self.model = ort.InferenceSession(
++            filename,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++
++    def __call__(self, x, mask):
++        """
++        Args:
++          x: (N, T, C), float32
++          mask: (N, T), int64
++        Returns:
++          logits: (N, T/4, vocab_size), float32
++          logits_len: (N,) int64
++        """
++        logits, logits_len = self.model.run(
++            [
++                self.model.get_outputs()[0].name,
++                self.model.get_outputs()[1].name,
++            ],
++            {
++                self.model.get_inputs()[0].name: x,
++                self.model.get_inputs()[1].name: mask,
++            },
++        )
++
++        return logits, logits_len
++
++
++def load_tokens(tokens):
++    id2token = dict()
++    with open(tokens, encoding="utf-8") as f:
++        for line in f:
++            fields = line.split()
++            if len(fields) == 1:
++                id2token[int(fields[0])] = " "
++            else:
++                t, idx = fields
++                id2token[int(idx)] = t
++    return id2token
++
++
++def compute_feat(samples):
++    opts = knf.FbankOptions()
++    opts.frame_opts.dither = 0
++    opts.frame_opts.snip_edges = True
++    opts.frame_opts.window_type = "hanning"
++    opts.frame_opts.samp_freq = 16000
++    opts.frame_opts.preemph_coeff = 0
++    opts.frame_opts.remove_dc_offset = False
++    opts.mel_opts.num_bins = 128
++
++    online_fbank = knf.OnlineFbank(opts)
++    online_fbank.accept_waveform(16000, samples.tolist())
++    online_fbank.input_finished()
++
++    features = np.stack(
++        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
++    )
++    assert features.dtype == np.float32, features.dtype
++
++    features = np.ascontiguousarray(features)
++
++    return features
++
++
++def main():
++    args = get_args()
++    print(vars(args))
++
++    model = OnnxModel(args.model)
++
++    samples, sample_rate = librosa.load(args.wav, sr=16000)
++
++    start = time.time()
++
++    assert sample_rate == 16000, sample_rate
++    features = compute_feat(samples)
++    mask = np.ones(features.shape[0], dtype=np.int64)[None]
++    features = features[None]
++
++    logits, logits_len = model(features, mask)
++    idx = logits[0, : logits_len[0]].argmax(axis=-1)
++
++    end = time.time()
++    elapsed_seconds = end - start
++    audio_duration = samples.shape[0] / 16000
++    real_time_factor = elapsed_seconds / audio_duration
++
++    print("idx", idx)
++
++    unique_ids = []
++    prev = -1
++    for i in idx.tolist():
++        if i == prev:
++            continue
++        unique_ids.append(i)
++        prev = i
++    print("unique_ids", unique_ids)
++    blank_id = 0
++    ids = [i for i in unique_ids if i != blank_id]
++    print(ids)
++
++    id2token = load_tokens(args.tokens)
++
++    tokens = [id2token[i] for i in ids]
++    text = "".join(tokens)
++    print(text)
++    text = text.replace("", " ")
++    print(text)
++    print(f"RTF: {real_time_factor}")
++
++
++if __name__ == "__main__":
++    main()
+
+commit 6cb50059730ad2adb72d92572e820e2c92b89879
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 24 17:24:36 2025 +0800
+
+    Add Android demo for Paraformer ASR with Qualcomm NPU. (#2932)
+    
+    This pull request significantly advances the Android demo by integrating Paraformer ASR models with Qualcomm NPU acceleration. The core changes involve extending the model configuration to support QNN-specific parameters for Paraformer, introducing robust asset management utilities for handling multi-file QNN models, and updating the Android application's initialization logic to prepare these models for NPU inference. Additionally, the build scripts have been adjusted to reflect the new model additions and naming conventions, ensuring a comprehensive update for NPU-powered ASR on Android.
+
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+index 20d25bee..a9508adb 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+@@ -23,6 +23,23 @@ fun assetExists(assetManager: AssetManager, path: String): Boolean {
+     return files.contains(fileName)
+ }
+ 
++fun assetListExists(
++    assetManager: AssetManager,
++    paths: String
++): Boolean {
++    if (paths.isBlank()) return false
++
++    val pathList = paths.split(",")
++        .map { it.trim() }
++        .filter { it.isNotEmpty() }
++
++    if (pathList.isEmpty()) return false
++
++    return pathList.all { path ->
++        assetExists(assetManager, path)
++    }
++}
++
+ fun copyAssetToInternalStorage(path: String, context: Context): String {
+     val targetRoot = context.filesDir
+     val outFile = File(targetRoot, path)
+@@ -56,6 +73,23 @@ fun copyAssetToInternalStorage(path: String, context: Context): String {
+     return outFile.absolutePath
+ }
+ 
++fun copyAssetListToInternalStorage(
++    paths: String,
++    context: Context
++): String {
++    if (paths.isBlank()) return paths
++
++    val pathList = paths.split(",")
++        .map { it.trim() }
++        .filter { it.isNotEmpty() }
++
++    val copiedPaths = pathList.map { path ->
++        copyAssetToInternalStorage(path, context)
++    }
++
++    return copiedPaths.joinToString(",")
++}
++
+ 
+ object SimulateStreamingAsr {
+     private var _recognizer: OfflineRecognizer? = null
+@@ -125,7 +159,10 @@ object SimulateStreamingAsr {
+                 OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
+ 
+                 // for qnn, we need to copy *.so files from assets folder to sd card
+-                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty() && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()) {
++                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()
++                    && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()
++                    && config.modelConfig.paraformer.qnnConfig.backendLib.isEmpty()
++                ) {
+                     Log.e(TAG, "You should provide libQnnHtp.so for qnn")
+                     throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
+                 }
+@@ -166,6 +203,25 @@ object SimulateStreamingAsr {
+                             config.modelConfig.zipformerCtc.qnnConfig.contextBinary,
+                             context
+                         )
++                } else if (config.modelConfig.paraformer.model.isNotEmpty()
++                    || assetListExists(
++                        context.assets,
++                        config.modelConfig.paraformer.qnnConfig.contextBinary
++                    )
++                ) {
++                    if (config.modelConfig.paraformer.model.isNotEmpty()) {
++                        config.modelConfig.paraformer.model =
++                            copyAssetListToInternalStorage(
++                                config.modelConfig.paraformer.model,
++                                context
++                            )
++                    }
++
++                    config.modelConfig.paraformer.qnnConfig.contextBinary =
++                        copyAssetListToInternalStorage(
++                            config.modelConfig.paraformer.qnnConfig.contextBinary,
++                            context
++                        )
+                 }
+ 
+                 if (config.hr.lexicon.isNotEmpty()) {
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+index ae5e1bd8..a4163b0b 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+@@ -163,7 +163,7 @@ fun HomeScreen() {
+                                 offset += windowSize
+                                 if (!isSpeechStarted && SimulateStreamingAsr.vad.isSpeechDetected()) {
+                                     isSpeechStarted = true
+-                                    // offset 0.25s
++                                    // offset 0.4s
+                                     speechStartOffset = offset - 6400
+                                     if(speechStartOffset < 0) {
+                                         speechStartOffset = 0
+diff --git a/scripts/apk/generate-asr-2pass-apk-script.py b/scripts/apk/generate-asr-2pass-apk-script.py
+index aed2cb94..36bec2d9 100755
+--- a/scripts/apk/generate-asr-2pass-apk-script.py
++++ b/scripts/apk/generate-asr-2pass-apk-script.py
+@@ -114,7 +114,7 @@ def get_2nd_models():
+             """,
+         ),
+         Model(
+-            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
++            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
+             idx=15,
+             lang="zh_en_ko_ja_yue",
+             short_name="sense_voice_2024_07_17_int8",
+@@ -122,7 +122,6 @@ def get_2nd_models():
+             pushd $model_name
+ 
+             rm -rfv test_wavs
+-            rm -fv model.onnx
+             rm -fv *.py
+ 
+             ls -lh
+@@ -410,7 +409,7 @@ def get_models():
+     second_zh = [
+         "sherpa-onnx-paraformer-zh-2023-09-14",
+         "icefall-asr-zipformer-wenetspeech-20230615",
+-        "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
++        "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
+         "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09",
+         "sherpa-onnx-dolphin-base-ctc-multi-lang-int8-2025-04-02",
+         "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10",
+@@ -434,7 +433,7 @@ def get_models():
+         ),
+         (
+             "sherpa-onnx-streaming-zipformer-en-20M-2023-02-17",
+-            "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
++            "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
+         ),
+         (
+             "sherpa-onnx-streaming-zipformer-en-20M-2023-02-17",
+diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+index d70bff23..86f43faf 100755
+--- a/scripts/apk/generate-qnn-vad-asr-apk-script.py
++++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+@@ -399,6 +399,38 @@ def get_models():
+ 
+             ls -lh
+ 
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-5-seconds-paraformer-zh-2023-03-28-int8",
++            idx=9023,
++            lang="zh",
++            short_name="5-seconds-paraformer_zh_2023_03_28_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-5-seconds-paraformer-zh-2025-10-07-int8",
++            idx=9024,
++            lang="zh",
++            short_name="5-seconds-paraformer_zh_2025_10_07_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
+             popd
+             """,
+         ),
+diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
+index c84ef99c..e9c8caa8 100755
+--- a/scripts/apk/generate-vad-asr-apk-script.py
++++ b/scripts/apk/generate-vad-asr-apk-script.py
+@@ -95,7 +95,7 @@ def get_models():
+             """,
+         ),
+         Model(
+-            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17",
++            model_name="sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17",
+             idx=15,
+             lang="zh_en_ko_ja_yue",
+             lang2="",
+@@ -105,7 +105,6 @@ def get_models():
+             pushd $model_name
+ 
+             rm -rfv test_wavs
+-            rm -fv model.onnx
+             rm -fv *.py
+ 
+             ls -lh
+diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+index 3d12d8ec..a1e13fc3 100644
+--- a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
++++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+@@ -97,7 +97,9 @@ class OfflineParaformerModelQnn::Impl {
+ 
+   template <typename Manager>
+   Impl(Manager *mgr, const OfflineModelConfig &config) {
+-    SHERPA_ONNX_LOGE("TODO(fangjun): To be implemented");
++    SHERPA_ONNX_LOGE(
++        "Please copy all files from assets to SD card and set assetManager to "
++        "null");
+     SHERPA_ONNX_EXIT(-1);
+   }
+ 
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
+index 7e99533a..546c0e62 100644
+--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineParaformerModelConfig.java
+@@ -4,9 +4,11 @@ package com.k2fsa.sherpa.onnx;
+ 
+ public class OfflineParaformerModelConfig {
+     private final String model;
++    private final QnnConfig qnnConfig;
+ 
+     private OfflineParaformerModelConfig(Builder builder) {
+         this.model = builder.model;
++        this.qnnConfig = builder.qnnConfig;
+     }
+ 
+     public static Builder builder() {
+@@ -17,9 +19,13 @@ public class OfflineParaformerModelConfig {
+         return model;
+     }
+ 
++    public QnnConfig getQnnConfig() {
++        return qnnConfig;
++    }
+ 
+     public static class Builder {
+         private String model = "";
++        private QnnConfig qnnConfig = QnnConfig.builder().build();
+ 
+         public OfflineParaformerModelConfig build() {
+             return new OfflineParaformerModelConfig(this);
+@@ -29,5 +35,10 @@ public class OfflineParaformerModelConfig {
+             this.model = model;
+             return this;
+         }
++
++        public Builder setQnnConfig(QnnConfig qnnConfig) {
++            this.qnnConfig = qnnConfig;
++            return this;
++        }
+     }
+ }
+diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
+index 3995d40a..fa2f7dff 100644
+--- a/sherpa-onnx/jni/offline-recognizer.cc
++++ b/sherpa-onnx/jni/offline-recognizer.cc
+@@ -99,6 +99,22 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
+   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.paraformer.model, model,
+                               paraformer_config_cls, paraformer_config);
+ 
++  fid = env->GetFieldID(paraformer_config_cls, "qnnConfig",
++                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
++  jobject qnn_config = env->GetObjectField(paraformer_config, fid);
++  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.paraformer.qnn_config.backend_lib, backendLib,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.paraformer.qnn_config.context_binary, contextBinary,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.paraformer.qnn_config.system_lib,
++                              systemLib, qnn_config_cls, qnn_config);
++
+   fid = env->GetFieldID(model_config_cls, "whisper",
+                         "Lcom/k2fsa/sherpa/onnx/OfflineWhisperModelConfig;");
+   jobject whisper_config = env->GetObjectField(model_config, fid);
+@@ -168,8 +184,8 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
+ 
+   fid = env->GetFieldID(sense_voice_config_cls, "qnnConfig",
+                         "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
+-  jobject qnn_config = env->GetObjectField(sense_voice_config, fid);
+-  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
++  qnn_config = env->GetObjectField(sense_voice_config, fid);
++  qnn_config_cls = env->GetObjectClass(qnn_config);
+ 
+   SHERPA_ONNX_JNI_READ_STRING(
+       ans.model_config.sense_voice.qnn_config.backend_lib, backendLib,
+diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+index f27dfbc5..7b7958fa 100644
+--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
++++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+@@ -22,6 +22,7 @@ data class OfflineTransducerModelConfig(
+ 
+ data class OfflineParaformerModelConfig(
+     var model: String = "",
++    var qnnConfig: QnnConfig = QnnConfig(),
+ )
+ 
+ data class OfflineNemoEncDecCtcModelConfig(
+@@ -405,7 +406,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         15 -> {
+-            val modelDir = "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17"
++            val modelDir = "sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17"
+             return OfflineModelConfig(
+                 senseVoice = OfflineSenseVoiceModelConfig(
+                     model = "$modelDir/model.int8.onnx",
+@@ -722,7 +723,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         42 -> {
+-            val modelDir = "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10"
++            val modelDir =
++                "sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10"
+             return OfflineModelConfig(
+                 wenetCtc = OfflineWenetCtcModelConfig(
+                     model = "$modelDir/model.int8.onnx",
+@@ -753,19 +755,20 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9000 -> {
+-            val modelDir = "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+                     model = "$modelDir/libmodel.so",
+                     qnnConfig = QnnConfig(
+-                      // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
+-                      //
+-                      // model.bin is created in the first run and is used from the second run
+-                      // to speed up the initialization
+-                      backendLib = "libQnnHtp.so",
+-                      systemLib = "libQnnSystem.so",
+-                      contextBinary = "$modelDir/model.bin",
++                        // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
++                        //
++                        // model.bin is created in the first run and is used from the second run
++                        // to speed up the initialization
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
+                     ),
+                 ),
+                 tokens = "$modelDir/tokens.txt",
+@@ -774,7 +777,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9001 -> {
+-            val modelDir = "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -791,7 +795,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9002 -> {
+-            val modelDir = "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -808,7 +813,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9003 -> {
+-            val modelDir = "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -824,7 +830,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9004 -> {
+-            val modelDir = "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -840,7 +847,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9005 -> {
+-            val modelDir = "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -856,7 +864,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9006 -> {
+-            val modelDir = "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -872,7 +881,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9007 -> {
+-            val modelDir = "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -888,7 +898,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9008 -> {
+-            val modelDir = "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -904,7 +915,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9009 -> {
+-            val modelDir = "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -920,7 +932,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9010 -> {
+-            val modelDir = "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -936,7 +949,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9011 -> {
+-            val modelDir = "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -953,7 +967,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9012 -> {
+-            val modelDir = "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -970,7 +985,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9013 -> {
+-            val modelDir = "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -987,7 +1003,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9014 -> {
+-            val modelDir = "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1003,7 +1020,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9015 -> {
+-            val modelDir = "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1019,7 +1037,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9016 -> {
+-            val modelDir = "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1035,7 +1054,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9017 -> {
+-            val modelDir = "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1051,7 +1071,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9018 -> {
+-            val modelDir = "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1067,7 +1088,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9019 -> {
+-            val modelDir = "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1083,7 +1105,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9020 -> {
+-            val modelDir = "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1099,7 +1122,8 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9021 -> {
+-            val modelDir = "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            val modelDir =
++                "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 zipformerCtc = OfflineZipformerCtcModelConfig(
+@@ -1115,8 +1139,9 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+         }
+ 
+         9022 -> {
+-            // for my Xiaomi 17 Pro
+-            val modelDir = "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
++            // for Xiaomi 17 Pro
++            val modelDir =
++                "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
+             return OfflineModelConfig(
+                 provider = "qnn",
+                 senseVoice = OfflineSenseVoiceModelConfig(
+@@ -1131,6 +1156,61 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                 debug = true,
+             )
+         }
++
++        9023 -> {
++            val modelDir = "sherpa-onnx-qnn-5-seconds-paraformer-zh-2023-03-28-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                paraformer = OfflineParaformerModelConfig(
++                    model = "$modelDir/libencoder.so,$modelDir/libpredictor.so,$modelDir/libdecoder.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        // The following three *.bin files are generated during the first run
++                        // and are used to replace the corresponding *.so files in later runs
++                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
++
++        9024 -> {
++            val modelDir = "sherpa-onnx-qnn-5-seconds-paraformer-zh-2025-10-07-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                paraformer = OfflineParaformerModelConfig(
++                    model = "$modelDir/libencoder.so,$modelDir/libpredictor.so,$modelDir/libdecoder.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        // The following three *.bin files are generated during the first run
++                        // and are used to replace the corresponding *.so files in later runs
++                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
++
++        9025 -> {
++            // for Xiaomi 17 Pro
++            val modelDir = "sherpa-onnx-qnn-SM8850-binary-5-seconds-paraformer-zh-2023-03-28-int8"
++            return OfflineModelConfig(
++                provider = "qnn",
++                paraformer = OfflineParaformerModelConfig(
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/encoder.bin,$modelDir/predictor.bin,$modelDir/decoder.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
+     }
+     return null
+ }
+
+commit cdbf0a8efa66eb1aaf0d7c0abe4ce0f7026a0b00
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 24 16:20:56 2025 +0800
+
+    Add C++ runtime for Paraformer ASR models with Qualcomm NPU using QNN (#2931)
+    
+    This pull request significantly expands the capabilities of the sherpa-onnx project by integrating C++ runtime support for Paraformer ASR models on Qualcomm NPUs. This allows for accelerated and more power-efficient speech recognition on devices equipped with Qualcomm hardware, enhancing the project's reach and performance on mobile and embedded platforms.
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 053c652e..5c3338fe 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -235,6 +235,7 @@ endif()
+ if(SHERPA_ONNX_ENABLE_QNN)
+   list(APPEND sources
+     ./qnn/offline-sense-voice-model-qnn.cc
++    ./qnn/offline-paraformer-model-qnn.cc
+     ./qnn/offline-zipformer-ctc-model-qnn.cc
+     ./qnn/qnn-backend.cc
+     ./qnn/qnn-model.cc
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+index e7075bc4..cf981ac3 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
++++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+@@ -21,6 +21,11 @@ void OfflineParaformerModelConfig::Register(ParseOptions *po) {
+       "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
+       "If you use RK NPU, it is "
+       "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn");
++
++  std::string prefix = "paraformer";
++  ParseOptions p(prefix, po);
++
++  qnn_config.Register(&p);
+ }
+ 
+ bool OfflineParaformerModelConfig::Validate() const {
+@@ -46,6 +51,13 @@ bool OfflineParaformerModelConfig::Validate() const {
+       return false;
+     }
+ 
++    for (const auto &name : filenames) {
++      if (!FileExists(name)) {
++        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
++        return false;
++      }
++    }
++
+     return true;
+   }
+ 
+@@ -63,11 +75,82 @@ bool OfflineParaformerModelConfig::Validate() const {
+       return false;
+     }
+ 
++    for (const auto &name : filenames) {
++      if (!FileExists(name)) {
++        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
++        return false;
++      }
++    }
++
+     return true;
+   }
+ 
+-  SHERPA_ONNX_LOGE("Please pass *.onnx, *.om, or *.rknn models. Given '%s'",
+-                   model.c_str());
++  if (EndsWith(model, ".so")) {
++    std::vector<std::string> filenames;
++    SplitStringToVector(model, ",", false, &filenames);
++    if (filenames.size() != 3 || !EndsWith(filenames[0], "encoder.so") ||
++        !EndsWith(filenames[1], "predictor.so") ||
++        !EndsWith(filenames[2], "decoder.so")) {
++      SHERPA_ONNX_LOGE(
++          "For QNN, you should pass "
++          "/path/libencoder.so,/path/libpredictor.so,/path/libdecoder.so. "
++          "Given '%s'",
++          model.c_str());
++      return false;
++    }
++
++    for (const auto &name : filenames) {
++      if (!FileExists(name)) {
++        SHERPA_ONNX_LOGE("Paraformer model '%s' does not exist", name.c_str());
++        return false;
++      }
++    }
++
++    if (!qnn_config.Validate()) {
++      return false;
++    }
++
++    return true;
++  }
++
++  if (model.empty() && !qnn_config.context_binary.empty()) {
++    // we require that the context_binary exists
++    if (!FileExists(qnn_config.context_binary)) {
++      SHERPA_ONNX_LOGE(
++          "Model is empty, but you provide a context binary that does not "
++          "exist");
++      return false;
++    }
++
++    std::vector<std::string> filenames;
++    SplitStringToVector(model, ",", false, &filenames);
++    if (filenames.size() != 3) {
++      SHERPA_ONNX_LOGE(
++          "For Paraformer with QNN, you should pass "
++          "/path/encoder.bin,/path/predictor.bin,/path/decoder.bin"
++          "Given '%s'",
++          model.c_str());
++      return false;
++    }
++
++    for (const auto &name : filenames) {
++      if (!FileExists(name)) {
++        SHERPA_ONNX_LOGE("Paraformer context binary '%s' does not exist",
++                         name.c_str());
++        return false;
++      }
++    }
++
++    if (!qnn_config.Validate()) {
++      return false;
++    }
++
++    return true;
++  }
++
++  SHERPA_ONNX_LOGE(
++      "Please pass *.onnx, *.om, *.rknn, or *.so models. Given '%s'",
++      model.c_str());
+   return false;
+ }
+ 
+@@ -75,7 +158,13 @@ std::string OfflineParaformerModelConfig::ToString() const {
+   std::ostringstream os;
+ 
+   os << "OfflineParaformerModelConfig(";
+-  os << "model=\"" << model << "\")";
++  os << "model=\"" << model << "\"";
++
++  if (!qnn_config.backend_lib.empty()) {
++    os << ", qnn_config=" << qnn_config.ToString();
++  }
++
++  os << ")";
+ 
+   return os.str();
+ }
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.h b/sherpa-onnx/csrc/offline-paraformer-model-config.h
+index e4fa94d8..8c7f0567 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model-config.h
++++ b/sherpa-onnx/csrc/offline-paraformer-model-config.h
+@@ -7,6 +7,7 @@
+ #include <string>
+ 
+ #include "sherpa-onnx/csrc/parse-options.h"
++#include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -17,8 +18,14 @@ struct OfflineParaformerModelConfig {
+   // for rknn,
+   // model is
+   // "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn"
++  //
++  // for qnn with shared libs, model is
++  // model is
++  // "/path/to/libencoder.so,/path/to/libpredictor.so,/path/to/libdecoder.so"
+   std::string model;
+ 
++  QnnConfig qnn_config;
++
+   OfflineParaformerModelConfig() = default;
+   explicit OfflineParaformerModelConfig(const std::string &model)
+       : model(model) {}
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 38bf3e5b..7aabf8d8 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -58,6 +58,7 @@
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_QNN
++#include "sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h"
+ #include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
+ #include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
+ #endif
+@@ -180,10 +181,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+                !config.model_config.zipformer_ctc.qnn_config.context_binary
+                     .empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
++    } else if (!config.model_config.paraformer.model.empty() ||
++               !config.model_config.paraformer.qnn_config.context_binary
++                    .empty()) {
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelQnn>>(
++          config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models and Zipformer CTC models are currently "
+-          "supported by qnn for non-streaming ASR.");
++          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
++          "supported by QNN for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+     }
+@@ -504,10 +511,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+                     .empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
+                                                                     config);
++    } else if (!config.model_config.paraformer.model.empty() ||
++               !config.model_config.paraformer.qnn_config.context_binary
++                    .empty()) {
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelQnn>>(
++          mgr, config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models and Zipformer CTC models are currently "
+-          "supported by qnn for non-streaming ASR.");
++          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
++          "supported by QNN for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+     }
+diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
+index 7b97fbac..cfbab2ad 100644
+--- a/sherpa-onnx/csrc/offline-stream.cc
++++ b/sherpa-onnx/csrc/offline-stream.cc
+@@ -14,6 +14,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "Eigen/Core"
+ #include "kaldi-native-fbank/csrc/online-feature.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/math.h"
+diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+new file mode 100644
+index 00000000..3d12d8ec
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
+@@ -0,0 +1,654 @@
++// sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h"
++
++#include <algorithm>
++#include <array>
++#include <memory>
++#include <mutex>
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/math.h"
++#include "sherpa-onnx/csrc/qnn/macros.h"
++#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
++#include "sherpa-onnx/csrc/qnn/qnn-model.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++
++namespace sherpa_onnx {
++
++class OfflineParaformerModelQnn::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    std::vector<std::string> filenames;
++    SplitStringToVector(config_.paraformer.model, ",", true, &filenames);
++    if (!filenames.empty()) {
++      if (filenames.size() != 3) {
++        SHERPA_ONNX_LOGE("Invalid Paraformer QNN model '%s'",
++                         config_.paraformer.model.c_str());
++        SHERPA_ONNX_EXIT(-1);
++      }
++    }
++
++    std::vector<std::string> binary_filenames;
++    SplitStringToVector(config_.paraformer.qnn_config.context_binary, ",", true,
++                        &binary_filenames);
++    if (!binary_filenames.empty()) {
++      if (binary_filenames.size() != 3) {
++        SHERPA_ONNX_LOGE(
++            "There should be 3 files for Paraformer context binary. Actual: "
++            "%d. '%s'",
++            static_cast<int32_t>(binary_filenames.size()),
++            config_.paraformer.qnn_config.context_binary.c_str());
++        SHERPA_ONNX_EXIT(-1);
++      }
++    }
++
++    if (filenames.empty() && binary_filenames.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You need to provide either a model or a context binary for "
++          "Paraformer with QNN");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    bool ok = InitEncoder(filenames.empty() ? "" : filenames[0],
++                          binary_filenames.empty() ? "" : binary_filenames[0]);
++    if (!ok) {
++      SHERPA_ONNX_LOGE(
++          "Failed to init encoder with lib file '%s', context binary: '%s'",
++          filenames.empty() ? "" : filenames[0].c_str(),
++          binary_filenames.empty() ? "" : binary_filenames[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    ok = InitPredictor(filenames.empty() ? "" : filenames[1],
++                       binary_filenames.empty() ? "" : binary_filenames[1]);
++    if (!ok) {
++      SHERPA_ONNX_LOGE(
++          "Failed to init predictor with lib file '%s', context binary: '%s'",
++          filenames.empty() ? "" : filenames[1].c_str(),
++          binary_filenames.empty() ? "" : binary_filenames[1].c_str());
++      return;
++    }
++
++    ok = InitDecoder(filenames.empty() ? "" : filenames[2],
++                     binary_filenames.empty() ? "" : binary_filenames[2]);
++    if (!ok) {
++      SHERPA_ONNX_LOGE(
++          "Failed to init decoder with lib file '%s', context binary: '%s'",
++          filenames.empty() ? "" : filenames[2].c_str(),
++          binary_filenames.empty() ? "" : binary_filenames[2].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) {
++    SHERPA_ONNX_LOGE("TODO(fangjun): To be implemented");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  std::vector<float> Run(std::vector<float> features) {
++    std::lock_guard<std::mutex> lock(mutex_);
++
++    std::vector<float> encoder_out = RunEncoder(std::move(features));
++    std::vector<float> transposed_encoder_out =
++        Transpose(encoder_out.data(), encoder_out_dim1_, encoder_out_dim2_);
++
++    std::vector<float> alphas = RunPredictor(transposed_encoder_out);
++
++    std::vector<float> acoustic_embedding =
++        ComputeAcousticEmbedding(encoder_out, alphas, encoder_out_dim2_);
++
++    int32_t num_tokens = acoustic_embedding.size() / encoder_out_dim2_;
++
++    acoustic_embedding.resize(encoder_out.size());
++
++    std::vector<float> transposed_acoustic_embedding = Transpose(
++        acoustic_embedding.data(), encoder_out_dim1_, encoder_out_dim2_);
++
++    std::vector<float> decoder_out = RunDecoder(
++        transposed_encoder_out, transposed_acoustic_embedding, num_tokens);
++
++    decoder_out = Transpose(decoder_out.data(), vocab_size_, encoder_out_dim1_);
++    decoder_out.resize(num_tokens * vocab_size_);
++    return decoder_out;
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++
++ private:
++  std::vector<float> RunEncoder(std::vector<float> features) const {
++    features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
++
++    encoder_model_->SetInputTensorData("x", features.data(), features.size());
++    encoder_model_->Run();
++    return encoder_model_->GetOutputTensorData("encoder_out");
++  }
++
++  std::vector<float> RunPredictor(
++      const std::vector<float> &transposed_encoder_out) const {
++    predictor_model_->SetInputTensorData("encoder_out",
++                                         transposed_encoder_out.data(),
++                                         transposed_encoder_out.size());
++    predictor_model_->Run();
++    return predictor_model_->GetOutputTensorData("alphas");
++  }
++
++  std::vector<float> RunDecoder(
++      const std::vector<float> &transposed_encoder_out,
++      const std::vector<float> &transposed_acoustic_embedding,
++      int32_t num_tokens) const {
++    std::vector<int32_t> mask(encoder_out_dim1_, 1);
++    std::fill(mask.begin() + num_tokens, mask.end(), 0);
++
++    decoder_model_->SetInputTensorData("encoder_out",
++                                       transposed_encoder_out.data(),
++                                       transposed_encoder_out.size());
++
++    decoder_model_->SetInputTensorData("acoustic_embedding",
++                                       transposed_acoustic_embedding.data(),
++                                       transposed_acoustic_embedding.size());
++
++    decoder_model_->SetInputTensorData("mask", mask.data(), mask.size());
++
++    decoder_model_->Run();
++
++    return decoder_model_->GetOutputTensorData("decoder_out");
++  }
++
++  std::vector<float> ApplyLFR(std::vector<float> in) const {
++    int32_t lfr_window_size = 7;
++    int32_t lfr_window_shift = 6;
++    int32_t in_feat_dim = 80;
++
++    int32_t in_num_frames = in.size() / in_feat_dim;
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
++    int32_t out_num_frames =
++        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
++
++    if (out_num_frames > num_input_frames_) {
++      SHERPA_ONNX_LOGE(
++          "Number of input frames %d is too large. Truncate it to %d frames.",
++          out_num_frames, num_input_frames_);
++
++      SHERPA_ONNX_LOGE(
++          "Recognition result may be truncated/incomplete. Please select a "
++          "model accepting longer audios.");
++
++      out_num_frames = num_input_frames_;
++    }
++
++    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
++
++    std::vector<float> out(num_input_frames_ * out_feat_dim);
++
++    const float *p_in = in.data();
++    float *p_out = out.data();
++
++    for (int32_t i = 0; i != out_num_frames; ++i) {
++      std::copy(p_in, p_in + out_feat_dim, p_out);
++
++      p_out += out_feat_dim;
++      p_in += lfr_window_shift * in_feat_dim;
++    }
++
++    return out;
++  }
++
++  bool InitEncoder(const std::string &lib_filename,
++                   const std::string &context_binary) {
++    encoder_backend_ = std::make_unique<QnnBackend>(
++        config_.paraformer.qnn_config.backend_lib, config_.debug);
++
++    if (context_binary.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from encoder model lib '%s' since context binary is not "
++            "given.",
++            lib_filename.c_str());
++      }
++
++      InitEncoderFromModelLib(lib_filename);
++
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Skip generating encoder context binary since you don't provide a "
++            "path to save it");
++      }
++    } else if (!FileExists(context_binary)) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init encoder from model lib '%s' since context binary '%s' does "
++            "not exist",
++            lib_filename.c_str(), context_binary.c_str());
++      }
++
++      InitEncoderFromModelLib(lib_filename);
++
++      CreateContextBinary(encoder_model_.get(), context_binary);
++    } else {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("Init from encoder context binary '%s'",
++                         context_binary.c_str());
++      }
++      InitEncoderFromContextBinary(context_binary);
++    }
++
++    PostInitEncoder();
++
++    return true;
++  }
++
++  bool InitPredictor(const std::string &lib_filename,
++                     const std::string &context_binary) {
++    predictor_backend_ = std::make_unique<QnnBackend>(
++        config_.paraformer.qnn_config.backend_lib, config_.debug);
++
++    if (context_binary.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from predictor model lib '%s' since context binary is not "
++            "given.",
++            lib_filename.c_str());
++      }
++
++      InitPredictorFromModelLib(lib_filename);
++
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Skip generating predictor context binary since you don't provide "
++            "a path to save it");
++      }
++    } else if (!FileExists(context_binary)) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init predictor from model lib '%s' since context binary '%s' does "
++            "not exist",
++            lib_filename.c_str(), context_binary.c_str());
++      }
++
++      InitPredictorFromModelLib(lib_filename);
++      CreateContextBinary(predictor_model_.get(), context_binary);
++    } else {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("Init from predictor context binary '%s'",
++                         context_binary.c_str());
++      }
++      InitPredictorFromContextBinary(context_binary);
++    }
++
++    PostInitPredictor();
++
++    return true;
++  }
++
++  bool InitDecoder(const std::string &lib_filename,
++                   const std::string &context_binary) {
++    decoder_backend_ = std::make_unique<QnnBackend>(
++        config_.paraformer.qnn_config.backend_lib, config_.debug);
++
++    if (context_binary.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from decoder model lib since context binary is not given");
++      }
++
++      InitDecoderFromModelLib(lib_filename);
++
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Skip generating decoder context binary since you don't provide "
++            "a path to save it");
++      }
++    } else if (!FileExists(context_binary)) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init decoder from model lib since context binary '%s' does not "
++            "exist",
++            context_binary.c_str());
++      }
++
++      InitDecoderFromModelLib(lib_filename);
++      CreateContextBinary(decoder_model_.get(), context_binary);
++    } else {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("Init from decoder context binary '%s'",
++                         context_binary.c_str());
++      }
++      InitDecoderFromContextBinary(context_binary);
++    }
++
++    PostInitDecoder();
++
++    return true;
++  }
++
++  void InitEncoderFromModelLib(const std::string &lib_filename) {
++    encoder_backend_->InitContext();
++    encoder_model_ = std::make_unique<QnnModel>(
++        lib_filename, encoder_backend_.get(), config_.debug);
++  }
++
++  void InitPredictorFromModelLib(const std::string &lib_filename) {
++    predictor_backend_->InitContext();
++    predictor_model_ = std::make_unique<QnnModel>(
++        lib_filename, predictor_backend_.get(), config_.debug);
++  }
++
++  void InitDecoderFromModelLib(const std::string &lib_filename) {
++    decoder_backend_->InitContext();
++    decoder_model_ = std::make_unique<QnnModel>(
++        lib_filename, decoder_backend_.get(), config_.debug);
++  }
++
++  void CreateContextBinary(QnnModel *model, const std::string &context_binary) {
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
++    }
++
++    bool ok = model->SaveBinaryContext(context_binary);
++
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
++                       context_binary.c_str());
++    }
++
++    if (config_.debug && ok) {
++      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
++      SHERPA_ONNX_LOGE(
++          "It should be super fast the next time you init the system.");
++      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
++    }
++  }
++
++  void InitEncoderFromContextBinary(const std::string &context_binary) {
++    if (config_.paraformer.qnn_config.system_lib.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You should provide --paraformer.qnn-system-lib if you also provide "
++          "context binary");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    encoder_model_ = std::make_unique<QnnModel>(
++        context_binary, config_.paraformer.qnn_config.system_lib,
++        encoder_backend_.get(), BinaryContextTag{}, config_.debug);
++  }
++
++  void InitPredictorFromContextBinary(const std::string &context_binary) {
++    if (config_.paraformer.qnn_config.system_lib.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You should provide --paraformer.qnn-system-lib if you also provide "
++          "context binary");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    predictor_model_ = std::make_unique<QnnModel>(
++        context_binary, config_.paraformer.qnn_config.system_lib,
++        predictor_backend_.get(), BinaryContextTag{}, config_.debug);
++  }
++
++  void InitDecoderFromContextBinary(const std::string &context_binary) {
++    if (config_.paraformer.qnn_config.system_lib.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You should provide --paraformer.qnn-system-lib if you also provide "
++          "context binary");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    decoder_model_ = std::make_unique<QnnModel>(
++        context_binary, config_.paraformer.qnn_config.system_lib,
++        decoder_backend_.get(), BinaryContextTag{}, config_.debug);
++  }
++
++  void PostInitEncoder() { CheckEncoderModel(); }
++
++  void PostInitPredictor() { CheckPredictorModel(); }
++
++  void PostInitDecoder() { CheckDecoderModel(); }
++
++  void CheckEncoderModel() {
++    const auto &input_tensor_names = encoder_model_->InputTensorNames();
++    if (input_tensor_names.size() != 1) {
++      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
++                       static_cast<int32_t>(input_tensor_names.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[0] != "x") {
++      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
++                       input_tensor_names[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> x_shape =
++        encoder_model_->TensorShape(input_tensor_names[0]);
++    if (x_shape.size() != 3) {
++      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
++                       static_cast<int32_t>(x_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    num_input_frames_ = x_shape[1];
++    feat_dim_ = x_shape[2];
++
++    if (!encoder_model_->HasTensor("encoder_out")) {
++      SHERPA_ONNX_LOGE("Model does not have output node 'encoder_out'");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> encoder_out_shape =
++        encoder_model_->TensorShape("encoder_out");
++
++    encoder_out_dim1_ = encoder_out_shape[1];
++    encoder_out_dim2_ = encoder_out_shape[2];
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("num_input_frames: %d", num_input_frames_);
++      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
++      SHERPA_ONNX_LOGE("encoder_out_dim1: %d", encoder_out_dim1_);
++      SHERPA_ONNX_LOGE("encoder_out_dim2: %d", encoder_out_dim2_);
++    }
++  }
++
++  void CheckPredictorModel() {
++    const auto &input_tensor_names = predictor_model_->InputTensorNames();
++    if (input_tensor_names.size() != 1) {
++      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
++                       static_cast<int32_t>(input_tensor_names.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[0] != "encoder_out") {
++      SHERPA_ONNX_LOGE("The 1st input should be encoder_out, actual '%s'",
++                       input_tensor_names[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> x_shape =
++        predictor_model_->TensorShape(input_tensor_names[0]);
++    if (x_shape.size() != 3) {
++      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
++                       static_cast<int32_t>(x_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[1] != encoder_out_dim2_) {
++      SHERPA_ONNX_LOGE(
++          "The input dim 1 of the predictor should be %d, given: %d",
++          encoder_out_dim2_, x_shape[1]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[2] != encoder_out_dim1_) {
++      SHERPA_ONNX_LOGE(
++          "The input dim 2 of the predictor should be %d, given: %d",
++          encoder_out_dim1_, x_shape[2]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (!predictor_model_->HasTensor("alphas")) {
++      SHERPA_ONNX_LOGE("Model does not have output node 'alphas'");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> alphas_shape = predictor_model_->TensorShape("alphas");
++    if (alphas_shape.size() != 2) {
++      SHERPA_ONNX_LOGE("alphas should be 2-d, given: %d",
++                       static_cast<int32_t>(alphas_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (alphas_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("We support only batch size 1 for alphas. Given: %d",
++                       alphas_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (alphas_shape[1] != encoder_out_dim1_) {
++      SHERPA_ONNX_LOGE("Expected output dim %d for alphas. Given: %d",
++                       encoder_out_dim1_, alphas_shape[1]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  void CheckDecoderModel() {
++    const auto &input_tensor_names = decoder_model_->InputTensorNames();
++    if (input_tensor_names.size() != 3) {
++      SHERPA_ONNX_LOGE("Expect 3 input tensors. Actual %d",
++                       static_cast<int32_t>(input_tensor_names.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[0] != "encoder_out") {
++      SHERPA_ONNX_LOGE("The 1st input should be encoder_out, actual '%s'",
++                       input_tensor_names[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[1] != "acoustic_embedding") {
++      SHERPA_ONNX_LOGE(
++          "The 2nd input should be acoustic_embedding, actual '%s'",
++          input_tensor_names[1].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[2] != "mask") {
++      SHERPA_ONNX_LOGE("The 3rd input should be mask, actual '%s'",
++                       input_tensor_names[2].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (!decoder_model_->HasTensor("decoder_out")) {
++      SHERPA_ONNX_LOGE("Model does not have output node 'decoder_out'");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> decoder_out_shape =
++        decoder_model_->TensorShape("decoder_out");
++    if (decoder_out_shape.size() != 3) {
++      SHERPA_ONNX_LOGE("decoder_out should be 3-d, given: %d",
++                       static_cast<int32_t>(decoder_out_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (decoder_out_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("We support only batch size 1 for decoder. Given: %d",
++                       decoder_out_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (decoder_out_shape[2] != encoder_out_dim1_) {
++      SHERPA_ONNX_LOGE("Expected output dim %d for decoder_out. Given: %d",
++                       encoder_out_dim1_, decoder_out_shape[2]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    vocab_size_ = decoder_out_shape[1];
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
++    }
++  }
++
++ private:
++  std::mutex mutex_;
++  OfflineModelConfig config_;
++
++  std::unique_ptr<QnnBackend> encoder_backend_;
++  std::unique_ptr<QnnModel> encoder_model_;
++
++  std::unique_ptr<QnnBackend> predictor_backend_;
++  std::unique_ptr<QnnModel> predictor_model_;
++
++  std::unique_ptr<QnnBackend> decoder_backend_;
++  std::unique_ptr<QnnModel> decoder_model_;
++
++  int32_t num_input_frames_ = 0;
++  int32_t feat_dim_ = 0;
++
++  int32_t encoder_out_dim1_ = 0;
++  int32_t encoder_out_dim2_ = 0;
++  int32_t vocab_size_ = 0;
++};
++
++OfflineParaformerModelQnn::~OfflineParaformerModelQnn() = default;
++
++OfflineParaformerModelQnn::OfflineParaformerModelQnn(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineParaformerModelQnn::OfflineParaformerModelQnn(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++std::vector<float> OfflineParaformerModelQnn::Run(
++    std::vector<float> features) const {
++  return impl_->Run(std::move(features));
++}
++
++int32_t OfflineParaformerModelQnn::VocabSize() const {
++  return impl_->VocabSize();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineParaformerModelQnn::OfflineParaformerModelQnn(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineParaformerModelQnn::OfflineParaformerModelQnn(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
+new file mode 100644
+index 00000000..76ab0be0
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
+@@ -0,0 +1,40 @@
++// sherpa-onnx/csrc/qnn/offline-paraformer-model-qnn.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
++#define SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++class OfflineParaformerModelQnn {
++ public:
++  ~OfflineParaformerModelQnn();
++
++  explicit OfflineParaformerModelQnn(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineParaformerModelQnn(Manager *mgr, const OfflineModelConfig &config);
++
++  /**
++   * @param features A tensor of shape (num_frames, feature_dim)
++   *                 before applying LFR.
++   * @returns Return a tensor of shape (num_output_frames, vocab_size)
++   */
++  std::vector<float> Run(std::vector<float> features) const;
++
++  int32_t VocabSize() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_PARAFORMER_MODEL_QNN_H_
+diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+index b628dc20..8224c216 100644
+--- a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
++++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+@@ -7,7 +7,7 @@
+ #include <algorithm>
+ #include <array>
+ #include <memory>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <string>
+ #include <utility>
+ #include <vector>
+@@ -88,6 +88,9 @@ class OfflineSenseVoiceModelQnn::Impl {
+     std::lock_guard<std::mutex> lock(mutex_);
+ 
+     features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
+ 
+     int32_t num_frames = features.size() / feat_dim_;
+ 
+@@ -207,6 +210,11 @@ class OfflineSenseVoiceModelQnn::Impl {
+     int32_t in_feat_dim = 80;
+ 
+     int32_t in_num_frames = in.size() / in_feat_dim;
++
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
+     int32_t out_num_frames =
+         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+ 
+
+commit c08541ce36e914cae98f22abb92a3e2456c7573b
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 24 11:23:45 2025 +0800
+
+    Optimize computation with Eigen. (#2928)
+    
+    This pull request introduces the Eigen library to optimize various numerical computations within the project. It systematically replaces existing manual, loop-based implementations for operations such as calculating mean and inverse standard deviation, applying feature normalization, scaling, windowing, and log-mel transformations. The primary goal of this refactoring is to enhance the overall computational performance and improve the readability and maintainability of the codebase by utilizing Eigen's highly optimized matrix and array functionalities.
+
+diff --git a/sherpa-onnx/csrc/math-test.cc b/sherpa-onnx/csrc/math-test.cc
+index ca4a967b..1083144a 100644
+--- a/sherpa-onnx/csrc/math-test.cc
++++ b/sherpa-onnx/csrc/math-test.cc
+@@ -65,4 +65,76 @@ TEST(Scale, Case2InPlace) {
+   EXPECT_EQ(src, expected);
+ }
+ 
++/*
++
++import numpy as np
++
++def compute_mean_and_inv_std(p: np.ndarray):
++    mean = p.mean(axis=0)
++    var = np.maximum((p**2).mean(axis=0) - mean**2, 0.0)
++    std = np.sqrt(var)
++    inv_std = 1.0 / (std + 1e-5)
++    return mean.astype(np.float32), inv_std.astype(np.float32)
++
++def dump_cpp_vector(name: str, arr: np.ndarray):
++    flat = arr.flatten()
++    print(f"std::vector<float> {name} = {{")
++    line = ""
++    for i, v in enumerate(flat):
++        line += f"{v:.8f}f, "
++        if (i + 1) % 8 == 0:
++            print("  " + line)
++            line = ""
++    if line:
++        print("  " + line)
++    print("};\n")
++
++np.random.seed(42)
++num_rows, num_cols = 4, 6
++x = np.random.randn(num_rows, num_cols).astype(np.float32)
++
++mean, inv_std = compute_mean_and_inv_std(x)
++
++dump_cpp_vector("x", x)
++dump_cpp_vector("mean", mean)
++dump_cpp_vector("inv_std", inv_std)
++
++ */
++
++TEST(ComputeMeanAndInvStd, Case1) {
++  std::vector<float> x = {
++      0.49671414f,  -0.13826430f, 0.64768857f, 1.52302980f,  -0.23415338f,
++      -0.23413695f, 1.57921278f,  0.76743472f, -0.46947438f, 0.54256004f,
++      -0.46341768f, -0.46572974f, 0.24196227f, -1.91328025f, -1.72491789f,
++      -0.56228751f, -1.01283109f, 0.31424734f, -0.90802407f, -1.41230369f,
++      1.46564877f,  -0.22577630f, 0.06752820f, -1.42474818f,
++  };
++
++  std::vector<float> expected_mean = {
++      0.35246629f, -0.67410338f, -0.02026373f,
++      0.31938151f, -0.41071847f, -0.45259190f,
++  };
++
++  std::vector<float> expected_inv_std = {
++      1.13103926f, 0.94854516f, 0.83320111f,
++      1.24679470f, 2.52932906f, 1.59057319f,
++  };
++
++  std::vector<float> mean;
++  std::vector<float> inv_std;
++
++  int32_t num_rows = 4;
++  int32_t num_cols = 6;
++
++  ComputeMeanAndInvStd(x.data(), num_rows, num_cols, &mean, &inv_std);
++
++  ASSERT_EQ(mean.size(), num_cols);
++  ASSERT_EQ(inv_std.size(), num_cols);
++
++  for (int32_t i = 0; i < num_cols; ++i) {
++    EXPECT_NEAR(mean[i], expected_mean[i], 1e-6f) << "at index " << i;
++    EXPECT_NEAR(inv_std[i], expected_inv_std[i], 1e-6f) << "at index " << i;
++  }
++}
++
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
+index adfcc036..64b7f1ee 100644
+--- a/sherpa-onnx/csrc/math.cc
++++ b/sherpa-onnx/csrc/math.cc
+@@ -4,6 +4,9 @@
+ #include "sherpa-onnx/csrc/math.h"
+ 
+ #include <vector>
++
++#include "Eigen/Dense"
++
+ namespace sherpa_onnx {
+ 
+ void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
+@@ -72,4 +75,27 @@ std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols) {
+   return output;
+ }
+ 
++void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
++                          std::vector<float> *mean,
++                          std::vector<float> *inv_stddev) {
++  using RowMajorMat =
++      Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
++
++  Eigen::Map<const RowMajorMat> X(p, num_rows, num_cols);
++
++  Eigen::RowVectorXf mean_vec = X.colwise().mean();
++
++  Eigen::RowVectorXf mean_sq = X.array().square().colwise().mean();
++
++  Eigen::RowVectorXf var = mean_sq.array() - mean_vec.array().square();
++
++  Eigen::RowVectorXf stddev = var.array().max(0.0f).sqrt();
++
++  Eigen::RowVectorXf inv_std = (stddev.array() + 1e-5f).inverse();
++
++  mean->assign(mean_vec.data(), mean_vec.data() + num_cols);
++
++  inv_stddev->assign(inv_std.data(), inv_std.data() + num_cols);
++}
++
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
+index 69956768..ac6eb976 100644
+--- a/sherpa-onnx/csrc/math.h
++++ b/sherpa-onnx/csrc/math.h
+@@ -147,5 +147,19 @@ std::vector<float> ComputeAcousticEmbedding(
+ // Transpose a 2-D matrix in row-major
+ std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols);
+ 
++/* Compute mean and inverse stddev over rows.
++ *
++ * @param p  A pointer to a 2-d array of shape (num_rows, num_cols)
++ * @param num_rows Number of rows
++ * @param num_cols Number of columns
++ * @param mean On return, it contains p.mean(axis=0). You don't need to
++ *             pre-allocate space for it.
++ * @param inv_stddev On return, it contains 1/p.std(axis=0) You don't need to
++ *                   pre-allocate space for it.
++ */
++void ComputeMeanAndInvStd(const float *p, int32_t num_rows, int32_t num_cols,
++                          std::vector<float> *mean,
++                          std::vector<float> *inv_stddev);
++
+ }  // namespace sherpa_onnx
+ #endif  // SHERPA_ONNX_CSRC_MATH_H_
+diff --git a/sherpa-onnx/csrc/offline-dolphin-model.cc b/sherpa-onnx/csrc/offline-dolphin-model.cc
+index 843a0127..ef9501ec 100644
+--- a/sherpa-onnx/csrc/offline-dolphin-model.cc
++++ b/sherpa-onnx/csrc/offline-dolphin-model.cc
+@@ -19,6 +19,7 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+@@ -65,16 +66,15 @@ class OfflineDolphinModel::Impl {
+ 
+   void NormalizeFeatures(float *features, int32_t num_frames,
+                          int32_t feat_dim) const {
+-    auto p = features;
+-    const auto &mean = meta_data_.mean;
+-    const auto &invstd = meta_data_.inv_stddev;
+-
+-    for (int32_t f = 0; f < num_frames; ++f) {
+-      for (int32_t d = 0; d < feat_dim; ++d) {
+-        p[d] = (p[d] - mean[d]) * invstd[d];
+-      }
+-      p += feat_dim;
+-    }
++    using RowMajorMat =
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
++    Eigen::Map<RowMajorMat> x(features, num_frames, feat_dim);
++
++    Eigen::Map<const Eigen::RowVectorXf> mean(meta_data_.mean.data(), feat_dim);
++    Eigen::Map<const Eigen::RowVectorXf> inv_std(meta_data_.inv_stddev.data(),
++                                                 feat_dim);
++    x.array() =
++        (x.array().rowwise() - mean.array()).rowwise() * inv_std.array();
+   }
+ 
+   OrtAllocator *Allocator() { return allocator_; }
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+index d5bbc176..bce191b0 100644
+--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+@@ -20,6 +20,7 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+@@ -85,19 +86,14 @@ class OfflineOmnilingualAsrCtcModel::Impl {
+       return;
+     }
+ 
+-    double s = 0;
+-    double sq = 0;
+-    for (int32_t i = 0; i < feat_dim; ++i) {
+-      s += features[i];
+-      sq += features[i] * features[i];
+-    }
+-
+-    double mean = s / feat_dim;
+-    double inv_stddev = 1 / std::sqrt(sq / feat_dim - mean * mean + 1e-5);
++    // Map the single-row feature vector
++    Eigen::Map<Eigen::ArrayXf> x(features, feat_dim);
++    float mean = x.mean();
++    float var = (x.square().mean() - mean * mean);
++    var = std::max(var, 0.0f);
++    float inv_stddev = 1.0f / std::sqrt(var + 1e-5f);
+ 
+-    for (int32_t i = 0; i < feat_dim; ++i) {
+-      features[i] = (features[i] - mean) * inv_stddev;
+-    }
++    x = (x - mean) * inv_stddev;
+   }
+ 
+  private:
+diff --git a/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h b/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
+index bda51066..34fc65c6 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h
+@@ -12,6 +12,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-decoder.h"
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.h"
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-model.h"
+@@ -131,20 +132,19 @@ class OfflineRecognizerFireRedAsrImpl : public OfflineRecognizerImpl {
+ 
+   void ApplyCMVN(std::vector<float> *v) const {
+     const auto &meta_data = model_->GetModelMetadata();
+-    const auto &mean = meta_data.mean;
+-    const auto &inv_stddev = meta_data.inv_stddev;
+-    int32_t feat_dim = static_cast<int32_t>(mean.size());
++    const auto &mean_vec = meta_data.mean;
++    const auto &inv_stddev_vec = meta_data.inv_stddev;
++    int32_t feat_dim = static_cast<int32_t>(mean_vec.size());
+     int32_t num_frames = static_cast<int32_t>(v->size()) / feat_dim;
+-
+-    float *p = v->data();
+-
+-    for (int32_t i = 0; i != num_frames; ++i) {
+-      for (int32_t k = 0; k != feat_dim; ++k) {
+-        p[k] = (p[k] - mean[k]) * inv_stddev[k];
+-      }
+-
+-      p += feat_dim;
+-    }
++    Eigen::Map<
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
++        mat(v->data(), num_frames, feat_dim);
++    Eigen::Map<const Eigen::RowVectorXf> mean(mean_vec.data(), feat_dim);
++    Eigen::Map<const Eigen::RowVectorXf> inv_std(inv_stddev_vec.data(),
++                                                 feat_dim);
++
++    mat.array() =
++        (mat.array().rowwise() - mean.array()).rowwise() * inv_std.array();
+   }
+ 
+  private:
+diff --git a/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h b/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
+index a6fe4cbd..de367686 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h
+@@ -11,6 +11,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-paraformer-decoder.h"
+ #include "sherpa-onnx/csrc/offline-paraformer-greedy-search-decoder.h"
+@@ -242,19 +243,18 @@ class OfflineRecognizerParaformerImpl : public OfflineRecognizerImpl {
+   void ApplyCMVN(std::vector<float> *v) const {
+     const std::vector<float> &neg_mean = model_->NegativeMean();
+     const std::vector<float> &inv_stddev = model_->InverseStdDev();
++    int32_t dim = static_cast<int32_t>(neg_mean.size());
++    int32_t num_frames = static_cast<int32_t>(v->size()) / dim;
+ 
+-    int32_t dim = neg_mean.size();
+-    int32_t num_frames = v->size() / dim;
++    Eigen::Map<
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
++        mat(v->data(), num_frames, dim);
+ 
+-    float *p = v->data();
++    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
++    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
+ 
+-    for (int32_t i = 0; i != num_frames; ++i) {
+-      for (int32_t k = 0; k != dim; ++k) {
+-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
+-      }
+-
+-      p += dim;
+-    }
++    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
++                  inv_stddev_vec.array();
+   }
+ 
+   OfflineRecognizerConfig config_;
+diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+index 02c06b44..056c79f0 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+@@ -11,6 +11,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-ctc-greedy-search-decoder.h"
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+@@ -402,22 +403,18 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+ 
+   void ApplyCMVN(std::vector<float> *v) const {
+     const auto &meta_data = model_->GetModelMetadata();
+-
+     const std::vector<float> &neg_mean = meta_data.neg_mean;
+     const std::vector<float> &inv_stddev = meta_data.inv_stddev;
+-
+-    int32_t dim = neg_mean.size();
+-    int32_t num_frames = v->size() / dim;
+-
+-    float *p = v->data();
+-
+-    for (int32_t i = 0; i != num_frames; ++i) {
+-      for (int32_t k = 0; k != dim; ++k) {
+-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
+-      }
+-
+-      p += dim;
+-    }
++    int32_t dim = static_cast<int32_t>(neg_mean.size());
++    int32_t num_frames = static_cast<int32_t>(v->size()) / dim;
++    Eigen::Map<
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
++        mat(v->data(), num_frames, dim);
++    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
++
++    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
++    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
++                  inv_stddev_vec.array();
+   }
+ 
+   SymbolTable symbol_table_;
+diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
+index 265c8dbe..7b97fbac 100644
+--- a/sherpa-onnx/csrc/offline-stream.cc
++++ b/sherpa-onnx/csrc/offline-stream.cc
+@@ -16,51 +16,12 @@
+ 
+ #include "kaldi-native-fbank/csrc/online-feature.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/math.h"
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+ #include "sherpa-onnx/csrc/resample.h"
+ 
+ namespace sherpa_onnx {
+ 
+-/* Compute mean and inverse stddev over rows.
+- *
+- * @param p  A pointer to a 2-d array of shape (num_rows, num_cols)
+- * @param num_rows Number of rows
+- * @param num_cols Number of columns
+- * @param mean On return, it contains p.mean(axis=0)
+- * @param inv_stddev On return, it contains 1/p.std(axis=0)
+- */
+-static void ComputeMeanAndInvStd(const float *p, int32_t num_rows,
+-                                 int32_t num_cols, std::vector<float> *mean,
+-                                 std::vector<float> *inv_stddev) {
+-  std::vector<float> sum(num_cols);
+-  std::vector<float> sum_sq(num_cols);
+-
+-  for (int32_t i = 0; i != num_rows; ++i) {
+-    for (int32_t c = 0; c != num_cols; ++c) {
+-      auto t = p[c];
+-      sum[c] += t;
+-      sum_sq[c] += t * t;
+-    }
+-    p += num_cols;
+-  }
+-
+-  mean->resize(num_cols);
+-  inv_stddev->resize(num_cols);
+-
+-  for (int32_t i = 0; i != num_cols; ++i) {
+-    auto t = sum[i] / num_rows;
+-    (*mean)[i] = t;
+-
+-    float stddev = std::sqrt(sum_sq[i] / num_rows - t * t);
+-
+-    if (stddev != stddev) {
+-      stddev = 0;
+-    }
+-
+-    (*inv_stddev)[i] = 1.0f / (stddev + 1e-5f);
+-  }
+-}
+-
+ class OfflineStream::Impl {
+  public:
+   explicit Impl(const FeatureExtractorConfig &config,
+@@ -305,17 +266,20 @@ class OfflineStream::Impl {
+ 
+   static void NemoNormalizePerFeature(float *p, int32_t num_frames,
+                                       int32_t feature_dim) {
+-    std::vector<float> mean;
+-    std::vector<float> inv_stddev;
++    using RowMajorMat =
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
+ 
+-    ComputeMeanAndInvStd(p, num_frames, feature_dim, &mean, &inv_stddev);
++    Eigen::Map<RowMajorMat> x(p, num_frames, feature_dim);
+ 
+-    for (int32_t n = 0; n != num_frames; ++n) {
+-      for (int32_t i = 0; i != feature_dim; ++i) {
+-        p[i] = (p[i] - mean[i]) * inv_stddev[i];
+-      }
+-      p += feature_dim;
+-    }
++    Eigen::RowVectorXf mean = x.colwise().mean();
++    Eigen::RowVectorXf var =
++        (x.array().square().colwise().mean() - mean.array().square())
++            .max(0.0f);  // avoid negative due to FP error
++
++    Eigen::RowVectorXf inv_std = (var.array().sqrt() + 1e-5f).inverse();
++
++    x.array() =
++        (x.array().rowwise() - mean.array()).rowwise() * inv_std.array();
+   }
+ 
+  private:
+diff --git a/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h b/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
+index 2f4b3c47..30ef4c91 100644
+--- a/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
++++ b/sherpa-onnx/csrc/online-recognizer-paraformer-impl.h
+@@ -11,6 +11,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/online-lm.h"
+@@ -412,19 +413,18 @@ class OnlineRecognizerParaformerImpl : public OnlineRecognizerImpl {
+   void ApplyCMVN(std::vector<float> *v) const {
+     const std::vector<float> &neg_mean = model_.NegativeMean();
+     const std::vector<float> &inv_stddev = model_.InverseStdDev();
++    int dim = static_cast<int>(neg_mean.size());
++    int num_frames = static_cast<int>(v->size()) / dim;
+ 
+-    int32_t dim = neg_mean.size();
+-    int32_t num_frames = v->size() / dim;
++    Eigen::Map<
++        Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
++        mat(v->data(), num_frames, dim);
+ 
+-    float *p = v->data();
++    Eigen::Map<const Eigen::RowVectorXf> neg_mean_vec(neg_mean.data(), dim);
++    Eigen::Map<const Eigen::RowVectorXf> inv_stddev_vec(inv_stddev.data(), dim);
+ 
+-    for (int32_t i = 0; i != num_frames; ++i) {
+-      for (int32_t k = 0; k != dim; ++k) {
+-        p[k] = (p[k] + neg_mean[k]) * inv_stddev[k];
+-      }
+-
+-      p += dim;
+-    }
++    mat.array() = (mat.array().rowwise() + neg_mean_vec.array()).rowwise() *
++                  inv_stddev_vec.array();
+   }
+ 
+   void PositionalEncoding(std::vector<float> *v, int32_t t_offset) const {
+diff --git a/sherpa-onnx/csrc/ten-vad-model.cc b/sherpa-onnx/csrc/ten-vad-model.cc
+index 69a64237..f86bc2f5 100644
+--- a/sherpa-onnx/csrc/ten-vad-model.cc
++++ b/sherpa-onnx/csrc/ten-vad-model.cc
+@@ -21,6 +21,7 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
++#include "Eigen/Dense"
+ #include "kaldi-native-fbank/csrc/mel-computations.h"
+ #include "kaldi-native-fbank/csrc/rfft.h"
+ #include "sherpa-onnx/csrc/file-utils.h"
+@@ -314,9 +315,10 @@ class TenVadModel::Impl {
+   }
+ 
+   static void Scale(const float *samples, int32_t n, float *out) {
+-    for (int32_t i = 0; i != n; ++i) {
+-      out[i] = samples[i] * 32768;
+-    }
++    Eigen::Map<const Eigen::ArrayXf> input(samples, n);
++    Eigen::Map<Eigen::ArrayXf> output(out, n);
++    constexpr float kScale = 32768.0f;
++    output = input * kScale;
+   }
+ 
+   void Preemphasis(const float *samples, int32_t n, float *out) {
+@@ -333,9 +335,10 @@ class TenVadModel::Impl {
+ 
+   static void ApplyWindow(const float *samples, const float *window, int32_t n,
+                           float *out) {
+-    for (int32_t i = 0; i != n; ++i) {
+-      out[i] = samples[i] * window[i];
+-    }
++    Eigen::Map<const Eigen::ArrayXf> samp_vec(samples, n);
++    Eigen::Map<const Eigen::ArrayXf> win_vec(window, n);
++    Eigen::Map<Eigen::ArrayXf> out_vec(out, n);
++    out_vec = samp_vec * win_vec;
+   }
+ 
+   static void ComputePowerSpectrum(const float *fft_bins, int32_t n,
+@@ -351,16 +354,21 @@ class TenVadModel::Impl {
+   }
+ 
+   static void LogMel(const float *in, int32_t n, float *out) {
+-    for (int32_t i = 0; i != n; ++i) {
+-      // 20.79441541679836 is log(32768*32768)
+-      out[i] = logf(in[i] + 1e-10f) - 20.79441541679836f;
+-    }
++    Eigen::Map<const Eigen::ArrayXf> input(in, n);
++    Eigen::Map<Eigen::ArrayXf> output(out, n);
++    // 20.79441541679836 is log(32768*32768)
++    constexpr float kLogScale = 20.79441541679836f;
++    output = (input + 1e-10f).log() - kLogScale;
+   }
+ 
+   void ApplyNormalization(const float *in, float *out) const {
+-    for (int32_t i = 0; i != static_cast<int32_t>(mean_.size()); ++i) {
+-      out[i] = (in[i] - mean_[i]) * inv_stddev_[i];
+-    }
++    int32_t dim = static_cast<int32_t>(mean_.size());
++
++    Eigen::Map<const Eigen::ArrayXf> input(in, dim);
++    Eigen::Map<Eigen::ArrayXf> output(out, dim);
++    Eigen::Map<const Eigen::ArrayXf> mean_vec(mean_.data(), dim);
++    Eigen::Map<const Eigen::ArrayXf> inv_stddev_vec(inv_stddev_.data(), dim);
++    output = (input - mean_vec) * inv_stddev_vec;
+   }
+ 
+   void ComputeFeatures(const float *samples, int32_t n) {
+
+commit 27a0bf4b9c082c519695ac166144151cadb5df40
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 24 09:48:15 2025 +0800
+
+    Add Transpose for a 2-D matrix. (#2926)
+
+diff --git a/scripts/paraformer/rknn/export_decoder_onnx.py b/scripts/paraformer/rknn/export_decoder_onnx.py
+index 98ff95f5..cec25683 100755
+--- a/scripts/paraformer/rknn/export_decoder_onnx.py
++++ b/scripts/paraformer/rknn/export_decoder_onnx.py
+@@ -26,7 +26,7 @@ def get_args():
+         "--float-mask",
+         type=int,
+         default=1,
+-        help="1 to use float master. 0 to use int32 mask",
++        help="1 to use float mask. 0 to use int32 mask",
+     )
+ 
+     parser.add_argument(
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index e28e9a30..053c652e 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -770,6 +770,7 @@ if(SHERPA_ONNX_ENABLE_TESTS)
+     cat-test.cc
+     circular-buffer-test.cc
+     context-graph-test.cc
++    math-test.cc
+     packed-sequence-test.cc
+     pad-sequence-test.cc
+     regex-lang-test.cc
+@@ -784,7 +785,6 @@ if(SHERPA_ONNX_ENABLE_TESTS)
+   )
+   if(SHERPA_ONNX_ENABLE_TTS)
+     list(APPEND sherpa_onnx_test_srcs
+-      offline-tts-zipvoice-frontend-test.cc
+       piper-phonemize-test.cc
+     )
+   endif()
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 77d47f39..fc372afd 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -40,8 +40,6 @@
+ namespace sherpa_onnx {
+ 
+ namespace {
+-// code in this anonymous namespace is written by ChatGPT
+-//
+ // Please see https://github.com/k2-fsa/sherpa-onnx/pull/2853
+ // for why we need to do the replacement
+ static const std::vector<std::pair<std::string, std::string>> kReplacements = {
+diff --git a/sherpa-onnx/csrc/math-test.cc b/sherpa-onnx/csrc/math-test.cc
+new file mode 100644
+index 00000000..ca4a967b
+--- /dev/null
++++ b/sherpa-onnx/csrc/math-test.cc
+@@ -0,0 +1,68 @@
++// sherpa-onnx/csrc/math-test.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/math.h"
++
++#include <vector>
++
++#include "gtest/gtest.h"
++
++namespace sherpa_onnx {
++
++TEST(Transpose, Case1) {
++  // 0 1 2
++  // 3 4 5
++  std::vector<float> in = {0, 1, 2, 3, 4, 5};
++  std::vector<float> out = Transpose(in.data(), 2, 3);
++
++  // 0 3
++  // 1 4
++  // 2 5
++  std::vector<float> expected_out = {0, 3, 1, 4, 2, 5};
++  EXPECT_EQ(out, expected_out);
++}
++
++TEST(Transpose, Case2) {
++  // 0 1
++  // 2 3
++  // 4 5
++  std::vector<float> in = {0, 1, 2, 3, 4, 5};
++  std::vector<float> out = Transpose(in.data(), 3, 2);
++
++  // 0 2 4
++  // 1 3 5
++  std::vector<float> expected_out = {0, 2, 4, 1, 3, 5};
++  EXPECT_EQ(out, expected_out);
++}
++
++TEST(ScaleAdd, Case1) {
++  std::vector<float> src = {1, 2, 3};
++  float scale = 10;
++  std::vector<float> in_out = {5, 6, 0};
++  ScaleAdd(src.data(), scale, src.size(), in_out.data());
++
++  std::vector<float> expected = {10 + 5, 20 + 6, 30 + 0};
++  EXPECT_EQ(in_out, expected);
++}
++
++TEST(Scale, Case1) {
++  std::vector<float> src = {1, 2, 3};
++  float scale = 10;
++  std::vector<float> in_out = {5, 6, 0};
++  Scale(src.data(), scale, src.size(), in_out.data());
++
++  std::vector<float> expected = {10, 20, 30};
++  EXPECT_EQ(in_out, expected);
++}
++
++TEST(Scale, Case2InPlace) {
++  std::vector<float> src = {1, 2, 3};
++  float scale = 10;
++  Scale(src.data(), scale, src.size(), src.data());
++
++  std::vector<float> expected = {10, 20, 30};
++  EXPECT_EQ(src, expected);
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
+index d3628191..adfcc036 100644
+--- a/sherpa-onnx/csrc/math.cc
++++ b/sherpa-onnx/csrc/math.cc
+@@ -6,16 +6,18 @@
+ #include <vector>
+ namespace sherpa_onnx {
+ 
+-static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
+-  for (int32_t i = 0; i < n; ++i) {
+-    in_out[i] += scale * src[i];
+-  }
++void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
++  Eigen::Map<const Eigen::ArrayXf> src_vec(src, n);
++  Eigen::Map<Eigen::ArrayXf> inout_vec(in_out, n);
++
++  inout_vec += scale * src_vec;
+ }
+ 
+-static void Scale(const float *src, float scale, int32_t n, float *out) {
+-  for (int32_t i = 0; i < n; ++i) {
+-    out[i] = scale * src[i];
+-  }
++void Scale(const float *src, float scale, int32_t n, float *out) {
++  Eigen::Map<const Eigen::ArrayXf> src_vec(src, n);
++  Eigen::Map<Eigen::ArrayXf> out_vec(out, n);
++
++  out_vec = scale * src_vec;
+ }
+ 
+ // this if for Paraformer
+@@ -54,4 +56,20 @@ std::vector<float> ComputeAcousticEmbedding(
+   return ans;
+ }
+ 
++std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols) {
++  std::vector<float> output(cols * rows);
++
++  Eigen::Map<const Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
++                                 Eigen::RowMajor>>
++      in(input, rows, cols);
++
++  Eigen::Map<
++      Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>
++      out(output.data(), cols, rows);
++
++  out.noalias() = in.transpose();
++
++  return output;
++}
++
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
+index 1edc065d..69956768 100644
+--- a/sherpa-onnx/csrc/math.h
++++ b/sherpa-onnx/csrc/math.h
+@@ -13,6 +13,8 @@
+ #include <numeric>
+ #include <vector>
+ 
++#include "Eigen/Dense"
++
+ namespace sherpa_onnx {
+ 
+ // logf(FLT_EPSILON)
+@@ -131,10 +133,19 @@ std::vector<int32_t> TopkIndex(const std::vector<std::vector<T>> &vec,
+   return TopkIndex(flatten.data(), flatten.size(), topk);
+ }
+ 
++// in_out[i] += src[i] * scale
++void ScaleAdd(const float *src, float scale, int32_t n, float *in_out);
++
++// out[i] = src[i] * scale
++void Scale(const float *src, float scale, int32_t n, float *out);
++
+ // For Paraformer
+ std::vector<float> ComputeAcousticEmbedding(
+     const std::vector<float> &encoder_out, const std::vector<float> &alphas,
+     int32_t encoder_dim);
+ 
++// Transpose a 2-D matrix in row-major
++std::vector<float> Transpose(const float *input, int32_t rows, int32_t cols);
++
+ }  // namespace sherpa_onnx
+ #endif  // SHERPA_ONNX_CSRC_MATH_H_
+diff --git a/sherpa-onnx/csrc/normal-data-generator.cc b/sherpa-onnx/csrc/normal-data-generator.cc
+index b62bede3..6c31863c 100644
+--- a/sherpa-onnx/csrc/normal-data-generator.cc
++++ b/sherpa-onnx/csrc/normal-data-generator.cc
+@@ -2,8 +2,6 @@
+ //
+ // Copyright      2025  Xiaomi Corporation
+ 
+-// Written by ChatGPT
+-
+ #include "sherpa-onnx/csrc/normal-data-generator.h"
+ 
+ #include <random>
+diff --git a/sherpa-onnx/csrc/normal-data-generator.h b/sherpa-onnx/csrc/normal-data-generator.h
+index e250f2b6..17601b4f 100644
+--- a/sherpa-onnx/csrc/normal-data-generator.h
++++ b/sherpa-onnx/csrc/normal-data-generator.h
+@@ -2,7 +2,6 @@
+ //
+ // Copyright      2025  Xiaomi Corporation
+ 
+-// Written by ChatGPT
+ #ifndef SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
+ #define SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
+ 
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+index a315d75b..1064c145 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+@@ -16,6 +16,7 @@
+ #include "kaldi-native-fbank/csrc/stft.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
++#include "sherpa-onnx/csrc/math.h"
+ #include "sherpa-onnx/csrc/offline-tts-frontend.h"
+ #include "sherpa-onnx/csrc/offline-tts-impl.h"
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
+@@ -282,17 +283,14 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+     int64_t C = mel_shape[2];
+ 
+     const float *mel_data = mel.GetTensorData<float>();
+-    std::vector<float> mel_permuted(C * T);
+ 
+     float inv_feat_scale = 1 / feat_scale;
+ 
+-    for (int64_t c = 0; c < C; ++c) {
+-      for (int64_t t = 0; t < T; ++t) {
+-        int64_t src_idx = t * C + c;  // src: [T, C] (row major)
+-        int64_t dst_idx = c * T + t;  // dst: [C, T] (row major)
+-        mel_permuted[dst_idx] = mel_data[src_idx] * inv_feat_scale;
+-      }
+-    }
++    // mel_permuted is (C, T)
++    std::vector<float> mel_permuted = Transpose(mel_data, T, C);
++
++    Scale(mel_permuted.data(), inv_feat_scale, mel_permuted.size(),
++          mel_permuted.data());
+ 
+     std::array<int64_t, 3> new_shape = {1, C, T};
+     Ort::Value mel_new = Ort::Value::CreateTensor<float>(
+
+commit f98a85ea6e1feb6075c0f85e9d602741e1af29ff
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 24 09:15:46 2025 +0800
+
+    Export Paraformer ASR models to QNN (#2925)
+
+diff --git a/.github/scripts/export-qnn/generate_paraformer.py b/.github/scripts/export-qnn/generate_paraformer.py
+new file mode 100755
+index 00000000..9797eb5e
+--- /dev/null
++++ b/.github/scripts/export-qnn/generate_paraformer.py
+@@ -0,0 +1,47 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import json
++
++from device_info import soc_info_dict
++from dataclasses import asdict, dataclass
++import itertools
++
++
++@dataclass
++class Config:
++    soc: str  # SM8850
++    soc_id: int  # 87
++    arch: str  # v81
++    input_in_seconds: str
++    framework: str
++
++
++def main():
++
++    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++    framework_list = ["FunASR", "WSChuan-ASR"]
++
++    configs = []
++
++    for name, soc in soc_info_dict.items():
++        for num_seconds, framework in itertools.product(
++            input_in_seconds, framework_list
++        ):
++            configs.append(
++                Config(
++                    soc=name,
++                    soc_id=soc.model.value,
++                    arch=soc.info.arch.name,
++                    input_in_seconds=num_seconds,
++                    framework=framework,
++                )
++            )
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/workflows/export-paraformer-to-qnn.yaml b/.github/workflows/export-paraformer-to-qnn.yaml
+new file mode 100644
+index 00000000..211dd96b
+--- /dev/null
++++ b/.github/workflows/export-paraformer-to-qnn.yaml
+@@ -0,0 +1,457 @@
++name: export-paraformer-to-qnn
++
++on:
++  push:
++    branches:
++      - export-paraformer-qnn-2
++  workflow_dispatch:
++
++concurrency:
++  group: export-paraformer-to-qnn-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-qnn/generate_paraformer.py
++          MATRIX=$(python3 .github/scripts/export-qnn/generate_paraformer.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
++  export-paraformer-to-qnn:
++    needs: generate_build_matrix
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
++    runs-on: ubuntu-22.04
++    strategy:
++      fail-fast: false
++      matrix:
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python 3.10
++        uses: actions/setup-python@v5
++        with:
++          python-version: "3.10"
++
++      - name: Display NDK HOME
++        shell: bash
++        run: |
++          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
++          ls -lh ${ANDROID_NDK_LATEST_HOME}
++
++      - name: Create directories
++        shell: bash
++        run: |
++          mkdir so binary
++
++      - name: Create Python virtual environment
++        shell: bash
++        run: |
++          python3 -m venv py310
++          which python3
++          source py310/bin/activate
++          which python3
++
++      - name: Show ndk-build help
++        shell: bash
++        run: |
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          ndk-build --help
++
++      - name: Download toolkit
++        shell: bash
++        run: |
++          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.40.0.251030.zip
++          ls -lh v2.40.0.251030.zip
++
++      - name: Unzip toolkit
++        shell: bash
++        run: |
++          unzip v2.40.0.251030.zip
++
++      - name: Show
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---ls -lh qairt---"
++
++          ls -lh qairt
++
++          echo "---"
++
++      - name: Install linux dependencies
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---"
++
++          ls -lh qairt
++
++          cd qairt/2.40.0.251030/bin
++          source envsetup.sh
++
++          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          cd qairt/2.40.0.251030/bin
++          source envsetup.sh
++
++          python3 -m pip install \
++            mock \
++            numpy \
++            opencv-python \
++            optuna \
++            packaging \
++            pandas \
++            paramiko \
++            pathlib2 \
++            pillow \
++            plotly \
++            protobuf \
++            psutil \
++            pydantic \
++            pytest \
++            pyyaml \
++            rich \
++            scikit-optimize \
++            scipy \
++            six \
++            tabulate \
++            typing-extensions \
++            xlsxwriter
++
++          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
++
++          which python3
++
++      - name: Install onnx dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++          python3 -m pip install --upgrade \
++            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
++            kaldi_native_fbank \
++            pip \
++            "numpy<2" \
++            onnx==1.17.0 \
++            onnxruntime==1.17.1 \
++            soundfile \
++            librosa \
++            onnxsim \
++            sentencepiece \
++            pyyaml
++
++          which python3
++
++      - name: Show qnn-onnx-converter help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.40.0.251030/bin
++          source envsetup.sh
++          popd
++
++          qnn-onnx-converter --help
++
++      - name: Show qnn-model-lib-generator help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.40.0.251030/bin
++          source envsetup.sh
++          popd
++
++          qnn-model-lib-generator --help
++
++      - name: Show qnn-net-run help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.40.0.251030/bin
++          source envsetup.sh
++          popd
++
++          qnn-net-run --help
++
++      - name: Run Paraformer from FunAsr
++        if: matrix.framework == 'FunASR'
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.40.0.251030/bin
++          source envsetup.sh
++          popd
++
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++
++          export t=${{ matrix.input_in_seconds }}
++          export soc=${{ matrix.soc }}
++
++          dir=$PWD
++
++          cd scripts/paraformer/qnn
++
++          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/am.mvn
++          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/config.yaml
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/tokens.txt
++
++          curl -SL -O https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/resolve/master/model.pt
++          mv model.pt model_state_dict.pt
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/0.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/1.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/2.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/3-sichuan.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/4-tianjin.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/5-henan.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/6-zh-en.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/test_wavs/8k.wav
++
++          rm -f README.md || true
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-03-28/resolve/main/README.md
++
++
++          ./convert_decoder.sh
++
++          ./convert_predictor.sh
++
++          ./convert_encoder.sh
++
++          ls -lh model_libs/*/lib*.so
++
++          ls -lh binary
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "collect results"
++
++          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-paraformer-zh-2023-03-28-int8
++
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v README.md $d
++          cp -v binary/encoder.bin $d/
++          cp -v binary/predictor.bin $d/
++          cp -v binary/decoder.bin $d/
++
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++
++          mv *.tar.bz2 ../../../binary/
++
++
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++
++              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2023-03-28-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2023-03-28-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v README.md $d
++
++            cp -v model_libs/$p/libencoder*.so $d/libencoder.so
++            cp -v model_libs/$p/libpredictor*.so $d/libpredictor.so
++            cp -v model_libs/$p/libdecoder*.so $d/libdecoder.so
++
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../../../so/
++
++
++      - name: Run Paraformer from WSChuan-ASR
++        if: matrix.framework == 'WSChuan-ASR'
++        shell: bash
++        run: |
++          dir=$PWD
++          source py310/bin/activate
++
++          pushd qairt/2.40.0.251030/bin
++          source envsetup.sh
++          popd
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++          export t=${{ matrix.input_in_seconds }}
++          export soc=${{ matrix.soc }}
++
++          cd scripts/paraformer/qnn
++
++          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/am.mvn
++          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/config.yaml
++          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/tokens.json
++          curl -SL -O https://hf-mirror.com/csukuangfj/WSChuan-ASR/resolve/main/Paraformer-large-Chuan/model_state_dict.pt
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/tokens.txt
++
++
++          for i in $(seq 1 16); do
++            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/test_wavs/$i.wav
++          done
++
++          rm -f README.md || true
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-int8-2025-10-07/resolve/main/README.md
++
++          ./convert_decoder.sh
++
++          ./convert_predictor.sh
++
++          ./convert_encoder.sh
++
++          ls -lh model_libs/*/lib*.so
++
++          ls -lh binary
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "collect results"
++
++          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-paraformer-zh-2025-10-07-int8
++
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v README.md $d
++          cp -v binary/encoder.bin $d/
++          cp -v binary/predictor.bin $d/
++          cp -v binary/decoder.bin $d/
++
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++
++          mv *.tar.bz2 ../../../binary/
++
++
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++
++              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2025-10-07-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-paraformer-zh-2025-10-07-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v README.md $d
++
++            cp -v model_libs/$p/libencoder*.so $d/libencoder.so
++            cp -v model_libs/$p/libpredictor*.so $d/libpredictor.so
++            cp -v model_libs/$p/libdecoder*.so $d/libdecoder.so
++
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../../../so/
++
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: ${{ matrix.framework }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
++          path: ./scripts/paraformer/qnn/my-config*/*.json
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./binary/*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn-binary
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
++          tag: asr-models-qnn
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./binary/*.tar.bz2
++          overwrite: true
++          tag: asr-models-qnn-binary
+diff --git a/scripts/paraformer/qnn/.gitignore b/scripts/paraformer/qnn/.gitignore
+new file mode 100644
+index 00000000..8a80e2e8
+--- /dev/null
++++ b/scripts/paraformer/qnn/.gitignore
+@@ -0,0 +1,2 @@
++*.raw
++*-list.txt
+diff --git a/scripts/paraformer/qnn/convert_decoder.sh b/scripts/paraformer/qnn/convert_decoder.sh
+new file mode 100755
+index 00000000..e16fe97e
+--- /dev/null
++++ b/scripts/paraformer/qnn/convert_decoder.sh
+@@ -0,0 +1,84 @@
++#!/usr/bin/env bash
++
++if [ -z $t ]; then
++  echo "Please run export t=num_input_seconds"
++  exit 1
++fi
++
++if [ -z $soc ]; then
++  echo "Please run export soc=SM8850, etc."
++  exit 1
++fi
++
++if [ -z $QNN_SDK_ROOT ]; then
++  echo "Please run setup QNN first"
++  exit 1
++fi
++
++echo "Export to onnx with num_seconds $t"
++
++python3 ./export_decoder_onnx.py --input-len-in-seconds $t --opset-version 17 --float-mask 0
++
++ls -lh decoder-*.onnx
++
++python3 ../../pyannote/segmentation/show-onnx.py --filename ./decoder-$t-seconds.onnx
++
++echo "Generate test data"
++
++python3 ./generate_decoder_data.py --input-len-in-seconds $t
++
++ls -lh decoder-*
++
++echo "---"
++cat ./decoder-input-list.txt
++echo "---"
++
++echo "Convert onnx to qnn"
++
++
++qnn-onnx-converter \
++  --input_network ./decoder-$t-seconds.onnx \
++  --output_path ./decoder-$t-seconds-quantized \
++  --input_list ./decoder-input-list.txt \
++  --use_native_input_files  \
++  --input_dtype encoder_out float32 \
++  --input_dtype acoustic_embedding float32 \
++  --input_dtype mask int32 \
++  --act_bitwidth 16 \
++  --bias_bitwidth 32
++
++  # Note(fangjun): It throws an error if we specify the layout for decoder inputs.
++  # --input_layout encoder_out NTF
++
++ls -lh
++
++mv -v decoder-$t-seconds-quantized decoder-$t-seconds-quantized.cpp
++
++python3 ../../qnn/generate_config.py \
++    --soc $soc \
++    --graph-name "decoder_${t}_seconds_quantized" \
++    --output-dir ./my-config-3 \
++    --qnn-sdk-root $QNN_SDK_ROOT
++
++ls -lh my-config-3
++
++head -n100 ./my-config-3/*.json
++
++python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++    -c "decoder-$t-seconds-quantized.cpp" \
++    -b "decoder-$t-seconds-quantized.bin" \
++    -o model_libs
++    # -t x86_64-linux-clang \
++
++ls -lh model_libs/x86_64-linux-clang/
++
++$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++  --model ./model_libs/x86_64-linux-clang/libdecoder-${t}-seconds-quantized.so \
++  --output_dir ./binary \
++  --binary_file decoder \
++  --config_file ./my-config-3/htp_backend_extensions.json
++
++ls -lh binary
++
++echo "Finish exporting decoder"
+diff --git a/scripts/paraformer/qnn/convert_encoder.sh b/scripts/paraformer/qnn/convert_encoder.sh
+new file mode 100755
+index 00000000..ca24217e
+--- /dev/null
++++ b/scripts/paraformer/qnn/convert_encoder.sh
+@@ -0,0 +1,81 @@
++#!/usr/bin/env bash
++
++if [ -z $t ]; then
++  echo "Please run export t=num_input_seconds"
++  exit 1
++fi
++
++if [ -z $soc ]; then
++  echo "Please run export soc=SM8850, etc."
++  exit 1
++fi
++
++if [ -z $QNN_SDK_ROOT ]; then
++  echo "Please run setup QNN first"
++  exit 1
++fi
++
++echo "Export to onnx with num_seconds $t"
++
++python3 ./export_encoder_onnx.py --input-len-in-seconds $t --opset-version 17
++
++ls -lh encoder-*.onnx
++
++python3 ../../pyannote/segmentation/show-onnx.py --filename ./encoder-$t-seconds.onnx
++
++echo "Generate test data"
++
++python3 ./generate_encoder_data.py --input-len-in-seconds $t
++
++ls -lh encoder-*
++
++echo "---"
++cat ./encoder-input-list.txt
++echo "---"
++
++echo "Convert onnx to qnn"
++
++
++qnn-onnx-converter \
++  --input_network ./encoder-$t-seconds.onnx \
++  --output_path ./encoder-$t-seconds-quantized \
++  --out_node encoder_out \
++  --input_list ./encoder-input-list.txt \
++  --use_native_input_files  \
++  --input_dtype x float32 \
++  --act_bitwidth 16 \
++  --bias_bitwidth 32 \
++  --input_layout x NTF
++
++ls -lh
++
++mv -v encoder-$t-seconds-quantized encoder-$t-seconds-quantized.cpp
++
++python3 ../../qnn/generate_config.py \
++    --soc $soc \
++    --graph-name "encoder_${t}_seconds_quantized" \
++    --output-dir ./my-config \
++    --qnn-sdk-root $QNN_SDK_ROOT
++
++ls -lh my-config
++
++head -n100 ./my-config/*.json
++
++python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++    -c "encoder-$t-seconds-quantized.cpp" \
++    -b "encoder-$t-seconds-quantized.bin" \
++    -o model_libs
++    # -t x86_64-linux-clang \
++
++ls -lh model_libs/x86_64-linux-clang/
++
++$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++  --model ./model_libs/x86_64-linux-clang/libencoder-${t}-seconds-quantized.so \
++  --output_dir ./binary \
++  --binary_file encoder \
++  --config_file ./my-config/htp_backend_extensions.json
++
++ls -lh binary
++
++echo "Finish exporting encoder"
+diff --git a/scripts/paraformer/qnn/convert_predictor.sh b/scripts/paraformer/qnn/convert_predictor.sh
+new file mode 100755
+index 00000000..e419bfda
+--- /dev/null
++++ b/scripts/paraformer/qnn/convert_predictor.sh
+@@ -0,0 +1,82 @@
++#!/usr/bin/env bash
++
++if [ -z $t ]; then
++  echo "Please run export t=num_input_seconds"
++  exit 1
++fi
++
++if [ -z $soc ]; then
++  echo "Please run export soc=SM8850, etc."
++  exit 1
++fi
++
++if [ -z $QNN_SDK_ROOT ]; then
++  echo "Please run setup QNN first"
++  exit 1
++fi
++
++echo "Export to onnx with num_seconds $t"
++
++python3 ./export_predictor_onnx.py --input-len-in-seconds $t --opset-version 17
++
++ls -lh predictor-*.onnx
++
++python3 ../../pyannote/segmentation/show-onnx.py --filename ./predictor-$t-seconds.onnx
++
++echo "Generate test data"
++
++python3 ./generate_predictor_data.py --input-len-in-seconds $t
++
++ls -lh predictor-*
++
++echo "---"
++cat ./predictor-input-list.txt
++echo "---"
++
++echo "Convert onnx to qnn"
++
++
++qnn-onnx-converter \
++  --input_network ./predictor-$t-seconds.onnx \
++  --output_path ./predictor-$t-seconds-quantized \
++  --input_list ./predictor-input-list.txt \
++  --use_native_input_files  \
++  --input_dtype encoder_out float32 \
++  --act_bitwidth 16 \
++  --bias_bitwidth 32
++
++  # Note(fangjun): It throws an error if we specify the layout for predictor input.
++  # --input_layout encoder_out NTF
++
++ls -lh
++
++mv -v predictor-$t-seconds-quantized predictor-$t-seconds-quantized.cpp
++
++python3 ../../qnn/generate_config.py \
++    --soc $soc \
++    --graph-name "predictor_${t}_seconds_quantized" \
++    --output-dir ./my-config-2 \
++    --qnn-sdk-root $QNN_SDK_ROOT
++
++ls -lh my-config-2
++
++head -n100 ./my-config-2/*.json
++
++python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++    -c "predictor-$t-seconds-quantized.cpp" \
++    -b "predictor-$t-seconds-quantized.bin" \
++    -o model_libs
++    # -t x86_64-linux-clang \
++
++ls -lh model_libs/x86_64-linux-clang/
++
++$QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++  --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++  --model ./model_libs/x86_64-linux-clang/libpredictor-${t}-seconds-quantized.so \
++  --output_dir ./binary \
++  --binary_file predictor \
++  --config_file ./my-config-2/htp_backend_extensions.json
++
++ls -lh binary
++
++echo "Finish exporting predictor"
+diff --git a/scripts/paraformer/qnn/export_decoder_onnx.py b/scripts/paraformer/qnn/export_decoder_onnx.py
+new file mode 120000
+index 00000000..7e2cce9f
+--- /dev/null
++++ b/scripts/paraformer/qnn/export_decoder_onnx.py
+@@ -0,0 +1 @@
++../rknn/export_decoder_onnx.py
+\ No newline at end of file
+diff --git a/scripts/paraformer/qnn/export_encoder_onnx.py b/scripts/paraformer/qnn/export_encoder_onnx.py
+new file mode 120000
+index 00000000..80cce3c0
+--- /dev/null
++++ b/scripts/paraformer/qnn/export_encoder_onnx.py
+@@ -0,0 +1 @@
++../rknn/export_encoder_onnx.py
+\ No newline at end of file
+diff --git a/scripts/paraformer/qnn/export_predictor_onnx.py b/scripts/paraformer/qnn/export_predictor_onnx.py
+new file mode 120000
+index 00000000..114602cc
+--- /dev/null
++++ b/scripts/paraformer/qnn/export_predictor_onnx.py
+@@ -0,0 +1 @@
++../rknn/export_predictor_onnx.py
+\ No newline at end of file
+diff --git a/scripts/paraformer/qnn/generate_decoder_data.py b/scripts/paraformer/qnn/generate_decoder_data.py
+new file mode 100755
+index 00000000..0b480862
+--- /dev/null
++++ b/scripts/paraformer/qnn/generate_decoder_data.py
+@@ -0,0 +1,99 @@
++#!/usr/bin/env python3
++# Copyright (c)  2025  Xiaomi Corporation
++
++import glob
++from pathlib import Path
++
++import numpy as np
++import torch
++
++from export_encoder_onnx import get_args, get_num_input_frames, load_model
++from export_predictor_onnx import modified_predictor_forward
++from test_onnx import compute_feat, get_acoustic_embedding
++from torch_model import CifPredictorV2
++
++CifPredictorV2.forward = modified_predictor_forward
++
++
++def pad(features, max_len):
++    if features.shape[0] > max_len:
++        return features[:max_len]
++    elif features.shape[0] < max_len:
++        features = np.pad(
++            features,
++            ((0, max_len - features.shape[0]), (0, 0)),
++            mode="constant",
++            constant_values=0,
++        )
++    return features
++
++
++@torch.no_grad()
++def main():
++    args = get_args()
++    print(vars(args))
++
++    input_len_in_seconds = int(args.input_len_in_seconds)
++    num_input_frames = get_num_input_frames(input_len_in_seconds)
++
++    wav_files = glob.glob("*.wav")
++
++    model = load_model()
++
++    name_list = []
++    for w in wav_files:
++        f = compute_feat(w)
++        print(w, f.shape)
++        f = pad(f, num_input_frames)
++        f = f[None]
++        print(f.shape)
++
++        f = torch.from_numpy(f)
++
++        encoder_out = model.encoder(f)
++        alpha = model.predictor(encoder_out)
++
++        acoustic_embedding = get_acoustic_embedding(
++            alpha[0].numpy(), encoder_out[0].numpy()
++        )
++        acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
++        num_tokens = acoustic_embedding.shape[1]
++
++        acoustic_embedding = torch.nn.functional.pad(
++            acoustic_embedding,
++            (0, 0, 0, encoder_out.shape[1] - num_tokens),
++            "constant",
++            0,
++        )
++
++        mask = torch.zeros(1, encoder_out.shape[1], dtype=torch.int32)
++
++        mask[0, :num_tokens] = 1
++
++        # NOTE(Fangjun): We have to transpose the data since QNN expects
++        # (N, C, T) for the decoder model
++        # Not sure why it has such a requirement.
++
++        encoder_out = encoder_out.permute(0, 2, 1).clone().numpy()
++        acoustic_embedding = acoustic_embedding.permute(0, 2, 1).clone().numpy()
++
++        print("inputs: ", encoder_out.shape, acoustic_embedding.shape, mask.shape)
++
++        name = Path(w).stem
++
++        first = f"decoder-input-{name}-0.raw"
++        second = f"decoder-input-{name}-1.raw"
++        third = f"decoder-input-{name}-2.raw"
++        encoder_out.tofile(first)
++        acoustic_embedding.tofile(second)
++        mask.numpy().tofile(third)
++
++        name_list.append((first, second, third))
++
++    with open("decoder-input-list.txt", "w") as f:
++        for first, second, third in name_list:
++            f.write(f"{first} {second} {third}\n")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/paraformer/qnn/generate_encoder_data.py b/scripts/paraformer/qnn/generate_encoder_data.py
+new file mode 100755
+index 00000000..fd87ddc8
+--- /dev/null
++++ b/scripts/paraformer/qnn/generate_encoder_data.py
+@@ -0,0 +1,53 @@
++#!/usr/bin/env python3
++# Copyright (c)  2025  Xiaomi Corporation
++
++import glob
++from pathlib import Path
++
++import numpy as np
++
++from export_encoder_onnx import get_args, get_num_input_frames
++from test_onnx import compute_feat
++
++
++def pad(features, max_len):
++    if features.shape[0] > max_len:
++        return features[:max_len]
++    elif features.shape[0] < max_len:
++        features = np.pad(
++            features,
++            ((0, max_len - features.shape[0]), (0, 0)),
++            mode="constant",
++            constant_values=0,
++        )
++    return features
++
++
++def main():
++    args = get_args()
++    print(vars(args))
++
++    input_len_in_seconds = int(args.input_len_in_seconds)
++    num_input_frames = get_num_input_frames(input_len_in_seconds)
++
++    wav_files = glob.glob("*.wav")
++    features_name = []
++    for w in wav_files:
++        f = compute_feat(w)
++        print(w, f.shape)
++        f = pad(f, num_input_frames)
++        print(f.shape)
++        print()
++        name = Path(w).stem
++
++        s = f"encoder-input-{name}.raw"
++        f.tofile(s)
++        features_name.append(s)
++
++    with open("encoder-input-list.txt", "w") as f:
++        for line in features_name:
++            f.write(f"{line}\n")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/paraformer/qnn/generate_predictor_data.py b/scripts/paraformer/qnn/generate_predictor_data.py
+new file mode 100755
+index 00000000..7ed09c93
+--- /dev/null
++++ b/scripts/paraformer/qnn/generate_predictor_data.py
+@@ -0,0 +1,76 @@
++#!/usr/bin/env python3
++# Copyright (c)  2025  Xiaomi Corporation
++
++import glob
++from pathlib import Path
++
++import numpy as np
++import torch
++
++from export_encoder_onnx import get_args, get_num_input_frames, load_model
++from export_predictor_onnx import modified_predictor_forward
++from test_onnx import compute_feat
++from torch_model import CifPredictorV2
++
++CifPredictorV2.forward = modified_predictor_forward
++
++
++def pad(features, max_len):
++    if features.shape[0] > max_len:
++        return features[:max_len]
++    elif features.shape[0] < max_len:
++        features = np.pad(
++            features,
++            ((0, max_len - features.shape[0]), (0, 0)),
++            mode="constant",
++            constant_values=0,
++        )
++    return features
++
++
++@torch.no_grad()
++def main():
++    args = get_args()
++    print(vars(args))
++
++    input_len_in_seconds = int(args.input_len_in_seconds)
++    num_input_frames = get_num_input_frames(input_len_in_seconds)
++
++    wav_files = glob.glob("*.wav")
++
++    model = load_model()
++
++    name_list = []
++    for w in wav_files:
++        f = compute_feat(w)
++        print(w, f.shape)
++        f = pad(f, num_input_frames)
++        f = f[None]
++        print(f.shape)
++
++        f = torch.from_numpy(f)
++
++        encoder_out = model.encoder(f)
++
++        # NOTE(Fangjun): We have to transpose the data since QNN expects
++        # (N, C, T) for the predictor model
++        # Not sure why it has such a requirement.
++
++        encoder_out = encoder_out.transpose(1, 2).clone().numpy()
++
++        print("encoder_out", encoder_out.shape)
++
++        name = Path(w).stem
++
++        s = f"predictor-input-{name}.raw"
++        encoder_out.tofile(s)
++        name_list.append(s)
++        print(encoder_out.shape)
++
++    with open("predictor-input-list.txt", "w") as f:
++        for line in name_list:
++            f.write(f"{line}\n")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/paraformer/qnn/test_onnx.py b/scripts/paraformer/qnn/test_onnx.py
+new file mode 120000
+index 00000000..dfb3c92a
+--- /dev/null
++++ b/scripts/paraformer/qnn/test_onnx.py
+@@ -0,0 +1 @@
++../rknn/test_onnx.py
+\ No newline at end of file
+diff --git a/scripts/paraformer/qnn/test_qnn.py b/scripts/paraformer/qnn/test_qnn.py
+new file mode 100755
+index 00000000..e914d0d2
+--- /dev/null
++++ b/scripts/paraformer/qnn/test_qnn.py
+@@ -0,0 +1,122 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import numpy as np
++import torch
++
++from export_encoder_onnx import load_model
++from export_predictor_onnx import modified_predictor_forward
++from test_onnx import get_acoustic_embedding
++from torch_model import CifPredictorV2
++
++CifPredictorV2.forward = modified_predictor_forward
++
++
++def load_tokens():
++    id2token = dict()
++    with open("./tokens.txt") as f:
++        for line in f:
++            fields = line.strip().split()
++            id2token[int(fields[1])] = fields[0]
++    return id2token
++
++
++@torch.no_grad()
++def main():
++    model = load_model()
++    encoder_params = sum(p.numel() for p in model.encoder.parameters())
++    predictor_params = sum(p.numel() for p in model.predictor.parameters())
++    decoder_params = sum(p.numel() for p in model.decoder.parameters())
++    print("encoder params (M)", encoder_params / 1024 / 1024)
++    print("predictor params (M)", predictor_params / 1024 / 1024)
++    print("decoder params (M)", decoder_params / 1024 / 1024)
++
++    features = np.fromfile("./encoder-input-zh.raw", dtype=np.float32).reshape(
++        (1, -1, 560)
++    )
++    features = torch.from_numpy(features)
++    encoder_out = model.encoder(features)
++    encoder_out.permute(0, 2, 1).numpy().tofile("predictor-in.raw")
++
++    alpha = model.predictor(encoder_out)
++
++    acoustic_embedding = get_acoustic_embedding(
++        alpha[0].numpy(), encoder_out[0].numpy()
++    )
++    acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
++
++    num_tokens = acoustic_embedding.shape[1]
++
++    acoustic_embedding = torch.nn.functional.pad(
++        acoustic_embedding,
++        (0, 0, 0, encoder_out.shape[1] - num_tokens),
++        "constant",
++        0,
++    )
++
++    mask = torch.zeros(1, encoder_out.shape[1], dtype=torch.float32)
++
++    mask[0, :num_tokens] = 1
++    logits = model.decoder(encoder_out, acoustic_embedding, mask)
++    print("encoder_out", encoder_out.shape)
++    print("acoustic_embedding", acoustic_embedding.shape)
++    print("mask", mask.shape)
++
++    encoder_out.permute(0, 2, 1).numpy().tofile("encoder_out.raw")
++    acoustic_embedding.permute(0, 2, 1).numpy().tofile("acoustic_embedding.raw")
++    mask.to(torch.int32).numpy().tofile("mask.raw")
++
++    yseq = logits[0, :num_tokens].argmax(axis=-1).tolist()
++    print(yseq, "-->", len(yseq))
++
++    id2token = load_tokens()
++    text = [id2token[i] for i in yseq]
++    print(text)
++
++    if False:
++        qnn_encoder_out = np.fromfile("./encoder_out.raw", dtype=np.float32).reshape(
++            1, -1, 512
++        )
++
++        qnn_encoder_out = torch.from_numpy(qnn_encoder_out)
++
++        qnn_alpha = np.fromfile("./alphas.raw", dtype=np.float32).reshape(1, -1)
++        qnn_alpha = torch.from_numpy(qnn_alpha)
++
++        acoustic_embedding = get_acoustic_embedding(
++            qnn_alpha[0].numpy(), qnn_encoder_out[0].numpy()
++        )
++        acoustic_embedding = torch.from_numpy(acoustic_embedding[None])
++
++        num_tokens = acoustic_embedding.shape[1]
++
++        acoustic_embedding = torch.nn.functional.pad(
++            acoustic_embedding,
++            (0, 0, 0, qnn_encoder_out.shape[1] - num_tokens),
++            "constant",
++            0,
++        )
++
++        mask = torch.zeros(1, qnn_encoder_out.shape[1], dtype=torch.float32)
++
++        mask[0, :num_tokens] = 1
++
++        logits = model.decoder(qnn_encoder_out, acoustic_embedding, mask)
++    else:
++        logits = np.fromfile("./decoder_out.raw", dtype=np.float32).reshape(
++            1,
++            -1,
++            encoder_out.shape[1],
++        )
++        logits = torch.from_numpy(logits)
++        logits = logits.permute(0, 2, 1)
++
++    yseq = logits[0, :num_tokens].argmax(axis=-1).tolist()
++    print(yseq, "-->", len(yseq))
++    text = [id2token[i] for i in yseq]
++    print(text)
++
++
++if __name__ == "__main__":
++    torch.manual_seed(20251013)
++    main()
+diff --git a/scripts/paraformer/qnn/torch_model.py b/scripts/paraformer/qnn/torch_model.py
+new file mode 120000
+index 00000000..2ed3ee10
+--- /dev/null
++++ b/scripts/paraformer/qnn/torch_model.py
+@@ -0,0 +1 @@
++../rknn/torch_model.py
+\ No newline at end of file
+diff --git a/scripts/paraformer/rknn/export_decoder_onnx.py b/scripts/paraformer/rknn/export_decoder_onnx.py
+index 5d93fa26..98ff95f5 100755
+--- a/scripts/paraformer/rknn/export_decoder_onnx.py
++++ b/scripts/paraformer/rknn/export_decoder_onnx.py
+@@ -3,7 +3,38 @@
+ 
+ import torch
+ 
+-from export_encoder_onnx import load_model, get_args, get_num_input_frames
++from export_encoder_onnx import load_model, get_num_input_frames
++
++import argparse
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--input-len-in-seconds",
++        type=int,
++        required=True,
++        help="""RKNN/QNN does not support dynamic shape, so we need to hard-code
++        how long the model can process.
++        """,
++    )
++
++    parser.add_argument(
++        "--float-mask",
++        type=int,
++        default=1,
++        help="1 to use float master. 0 to use int32 mask",
++    )
++
++    parser.add_argument(
++        "--opset-version",
++        type=int,
++        default=14,
++    )
++    return parser.parse_args()
+ 
+ 
+ @torch.no_grad()
+@@ -18,12 +49,15 @@ def main():
+ 
+     encoder_out = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
+     acoustic_embedding = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
+-    mask = torch.ones([num_input_frames], dtype=torch.float32)
++    if args.float_mask == 1:
++        mask = torch.ones([num_input_frames], dtype=torch.float32)
++    else:
++        mask = torch.ones([num_input_frames], dtype=torch.int32)
+ 
+     d = model.decoder(encoder_out, acoustic_embedding)
+     print("d", d.shape)
+ 
+-    opset_version = 14
++    opset_version = args.opset_version
+     filename = f"decoder-{input_len_in_seconds}-seconds.onnx"
+     torch.onnx.export(
+         model.decoder,
+diff --git a/scripts/paraformer/rknn/export_encoder_onnx.py b/scripts/paraformer/rknn/export_encoder_onnx.py
+index e6ba32da..e3ab3386 100755
+--- a/scripts/paraformer/rknn/export_encoder_onnx.py
++++ b/scripts/paraformer/rknn/export_encoder_onnx.py
+@@ -25,6 +25,12 @@ def get_args():
+         how long the model can process.
+         """,
+     )
++
++    parser.add_argument(
++        "--opset-version",
++        type=int,
++        default=14,
++    )
+     return parser.parse_args()
+ 
+ 
+@@ -155,7 +161,7 @@ def main():
+     x = torch.randn(1, num_input_frames, 560, dtype=torch.float32)
+     pos_emb = torch.rand(1, x.shape[1], 560, dtype=torch.float32)
+ 
+-    opset_version = 14
++    opset_version = args.opset_version
+     filename = f"encoder-{input_len_in_seconds}-seconds.onnx"
+     torch.onnx.export(
+         model.encoder,
+diff --git a/scripts/paraformer/rknn/export_predictor_onnx.py b/scripts/paraformer/rknn/export_predictor_onnx.py
+index 45af85ef..1a2e4aaf 100755
+--- a/scripts/paraformer/rknn/export_predictor_onnx.py
++++ b/scripts/paraformer/rknn/export_predictor_onnx.py
+@@ -6,25 +6,26 @@ import torch
+ from export_encoder_onnx import load_model, get_args, get_num_input_frames
+ from torch_model import CifPredictorV2
+ 
+-if __name__ == "__main__":
+ 
+-    def modified_predictor_forward(self: CifPredictorV2, hidden: torch.Tensor):
+-        h = hidden
+-        context = h.transpose(1, 2)
+-        queries = self.pad(context)
+-        output = torch.relu(self.cif_conv1d(queries))
+-        output = output.transpose(1, 2)
++def modified_predictor_forward(self: CifPredictorV2, hidden: torch.Tensor):
++    h = hidden
++    context = h.transpose(1, 2)
++    queries = self.pad(context)
++    output = torch.relu(self.cif_conv1d(queries))
++    output = output.transpose(1, 2)
++
++    output = self.cif_output(output)
++    alphas = torch.sigmoid(output)
++    alphas = torch.nn.functional.relu(
++        alphas * self.smooth_factor - self.noise_threshold
++    )
+ 
+-        output = self.cif_output(output)
+-        alphas = torch.sigmoid(output)
+-        alphas = torch.nn.functional.relu(
+-            alphas * self.smooth_factor - self.noise_threshold
+-        )
++    alphas = alphas.squeeze(-1)
+ 
+-        alphas = alphas.squeeze(-1)
++    return alphas
+ 
+-        return alphas
+ 
++if __name__ == "__main__":
+     CifPredictorV2.forward = modified_predictor_forward
+ 
+ 
+@@ -40,7 +41,7 @@ def main():
+ 
+     x = torch.randn(1, num_input_frames, 512, dtype=torch.float32)
+ 
+-    opset_version = 14
++    opset_version = args.opset_version
+     filename = f"predictor-{input_len_in_seconds}-seconds.onnx"
+     torch.onnx.export(
+         model.predictor,
+diff --git a/scripts/paraformer/rknn/test_onnx.py b/scripts/paraformer/rknn/test_onnx.py
+index b8f5bf70..aa73dc81 100755
+--- a/scripts/paraformer/rknn/test_onnx.py
++++ b/scripts/paraformer/rknn/test_onnx.py
+@@ -61,7 +61,7 @@ def compute_feat(filename):
+     )
+     assert features.data.contiguous is True
+     assert features.dtype == np.float32, features.dtype
+-    print("features sum", features.sum(), features.shape)
++    #  print("features sum", features.sum(), features.shape)
+ 
+     window_size = 7  # lfr_m
+     window_shift = 6  # lfr_n
+diff --git a/scripts/sense-voice/rknn/test_nano_torch.py b/scripts/sense-voice/rknn/test_nano_torch.py
+index 9616ba68..7e150742 100755
+--- a/scripts/sense-voice/rknn/test_nano_torch.py
++++ b/scripts/sense-voice/rknn/test_nano_torch.py
+@@ -47,6 +47,8 @@ def load_torch_model():
+ @torch.no_grad()
+ def main():
+     model = load_torch_model()
++    num_params = sum(p.numel() for p in model.parameters())
++    print("num_params (M)", num_params, num_params / 1000000)
+ 
+     samples, sample_rate = test_onnx.load_audio("./zh.wav")
+     assert sample_rate == 16000, sample_rate
+
+commit 2d189c4f1904ae977059e3e612da03904d9a59a0
+Author: Wei Kang <wkang.pku@gmail.com>
+Date:   Tue Dec 23 11:47:44 2025 +0800
+
+    [KWS] Add phone+ppinyin tokenization with lexicon support (for zh-en model) (#2922)
+
+diff --git a/scripts/text2token.py b/scripts/text2token.py
+index 71026d98..73f87e28 100755
+--- a/scripts/text2token.py
++++ b/scripts/text2token.py
+@@ -87,10 +87,19 @@ def get_args():
+         "--tokens-type",
+         type=str,
+         required=True,
+-        choices=["cjkchar", "bpe", "cjkchar+bpe", "fpinyin", "ppinyin"],
+-        help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin or ppinyin.
++        choices=[
++            "cjkchar",
++            "bpe",
++            "cjkchar+bpe",
++            "fpinyin",
++            "ppinyin",
++            "phone+ppinyin",
++        ],
++        help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin
++        ppinyin or phone+ppinyin.
+         fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
+         ppinyin means partial pinyin, it splits pinyin into initial and final,
++        phone means English phonemes in CMU dictionary format.
+         """,
+     )
+ 
+@@ -100,6 +109,12 @@ def get_args():
+         help="The path to bpe.model. Only required when tokens-type is bpe or cjkchar+bpe.",
+     )
+ 
++    parser.add_argument(
++        "--lexicon",
++        type=str,
++        help="The path to lexicon.txt. Only required when tokens-type is phone+ppinyin.",
++    )
++
+     parser.add_argument(
+         "--output",
+         type=str,
+@@ -134,6 +149,7 @@ def main():
+         tokens=args.tokens,
+         tokens_type=args.tokens_type,
+         bpe_model=args.bpe_model,
++        lexicon=args.lexicon,
+     )
+     with open(args.output, "w", encoding="utf8") as f:
+         for i, txt in enumerate(encoded_texts):
+diff --git a/sherpa-onnx/python/sherpa_onnx/cli.py b/sherpa-onnx/python/sherpa_onnx/cli.py
+index 0527e20a..edd8eb9e 100644
+--- a/sherpa-onnx/python/sherpa_onnx/cli.py
++++ b/sherpa-onnx/python/sherpa_onnx/cli.py
+@@ -1,12 +1,13 @@
+ # Copyright (c)  2023  Xiaomi Corporation
+ 
+ import logging
++
+ try:
+     import click
+ except ImportError:
+-    print('Please run')
+-    print('  pip install click')
+-    print('before you continue')
++    print("Please run")
++    print("  pip install click")
++    print("before you continue")
+     raise
+ 
+ from pathlib import Path
+@@ -36,12 +37,21 @@ def cli():
+ @click.option(
+     "--tokens-type",
+     type=click.Choice(
+-        ["cjkchar", "bpe", "cjkchar+bpe", "fpinyin", "ppinyin"], case_sensitive=True
++        [
++            "cjkchar",
++            "bpe",
++            "cjkchar+bpe",
++            "fpinyin",
++            "ppinyin",
++            "phone+ppinyin",
++        ],
++        case_sensitive=True,
+     ),
+     required=True,
+-    help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin or ppinyin.
++    help="""The type of modeling units, should be cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin or phone+ppinyin.
+     fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
+     ppinyin means partial pinyin, it splits pinyin into initial and final,
++    phone means English phonemes in CMU dictionary format.
+     """,
+ )
+ @click.option(
+@@ -49,8 +59,18 @@ def cli():
+     type=str,
+     help="The path to bpe.model. Only required when tokens-type is bpe or cjkchar+bpe.",
+ )
++@click.option(
++    "--lexicon",
++    type=str,
++    help="The path to lexicon.txt. Only required when tokens-type is phone+ppinyin.",
++)
+ def encode_text(
+-    input: Path, output: Path, tokens: Path, tokens_type: str, bpe_model: Path
++    input: Path,
++    output: Path,
++    tokens: Path,
++    tokens_type: str,
++    bpe_model: Path,
++    lexicon: Path,
+ ):
+     """
+     Encode the texts given by the INPUT to tokens and write the results to the OUTPUT.
+@@ -101,7 +121,11 @@ def encode_text(
+             extra_info.append(extra)
+ 
+     encoded_texts = text2token(
+-        texts, tokens=tokens, tokens_type=tokens_type, bpe_model=bpe_model
++        texts,
++        tokens=tokens,
++        tokens_type=tokens_type,
++        bpe_model=bpe_model,
++        lexicon=lexicon,
+     )
+     with open(output, "w", encoding="utf8") as f:
+         for i, txt in enumerate(encoded_texts):
+diff --git a/sherpa-onnx/python/sherpa_onnx/utils.py b/sherpa-onnx/python/sherpa_onnx/utils.py
+index fd36f2c0..de4b4b7c 100644
+--- a/sherpa-onnx/python/sherpa_onnx/utils.py
++++ b/sherpa-onnx/python/sherpa_onnx/utils.py
+@@ -10,6 +10,7 @@ def text2token(
+     tokens: str,
+     tokens_type: str = "cjkchar",
+     bpe_model: Optional[str] = None,
++    lexicon: Optional[str] = None,
+     output_ids: bool = False,
+ ) -> List[List[Union[str, int]]]:
+     """
+@@ -21,12 +22,15 @@ def text2token(
+       tokens:
+         The path of the tokens.txt.
+       tokens_type:
+-        The valid values are cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin.
++        The valid values are cjkchar, bpe, cjkchar+bpe, fpinyin, ppinyin, phone+ppinyin.
+         fpinyin means full pinyin, each cjkchar has a pinyin(with tone).
+         ppinyin means partial pinyin, it splits pinyin into initial and final,
++        phone means English phonemes in CMU dictionary format.
+       bpe_model:
+         The path of the bpe model. Only required when tokens_type is bpe or
+         cjkchar+bpe.
++      lexicon:
++        The path of the lexicon.txt. Only required when tokens_type is phone+ppinyin.
+       output_ids:
+         True to output token ids otherwise tokens.
+     Returns:
+@@ -64,34 +68,75 @@ def text2token(
+         sp = spm.SentencePieceProcessor()
+         sp.load(bpe_model)
+ 
++    phone_table = {}
++    if tokens_type == "phone+ppinyin":
++        assert (
++            lexicon and Path(lexicon).is_file()
++        ), f"File not exists, {lexicon}"
++        with open(lexicon, "r", encoding="utf-8") as f:
++            for line in f:
++                toks = line.strip().split()
++                assert len(toks) >= 2, len(toks)
++                word = toks[0]
++                phones = toks[1:]
++                phone_table[word] = phones
++
+     texts_list: List[List[str]] = []
+ 
++    def to_pinyin(txt: str, out_type: str) -> List[str]:
++        assert out_type in ["ppinyin", "fpinyin"], f"given {out_type}"
++        py = [x[0] for x in pinyin(txt)]
++        if "ppinyin" == out_type:
++            res = []
++            for x in py:
++                initial = to_initials(x, strict=False)
++                final = to_finals_tone(x, strict=False)
++                if initial == "" and final == "":
++                    res.append(x)
++                else:
++                    if initial:
++                        res.append(initial)
++                    if final:
++                        res.append(final)
++            return res
++        else:
++            return py
++
+     if tokens_type == "cjkchar":
+         texts_list = [list("".join(text.split())) for text in texts]
+     elif tokens_type == "bpe":
+         texts_list = sp.encode(texts, out_type=str)
+-    elif "pinyin" in tokens_type:
++    elif tokens_type == "ppinyin" or tokens_type == "fpinyin":
+         for txt in texts:
+-            py = [x[0] for x in pinyin(txt)]
+-            if "ppinyin" == tokens_type:
+-                res = []
+-                for x in py:
+-                    initial = to_initials(x, strict=False)
+-                    final = to_finals_tone(x, strict=False)
+-                    if initial == "" and final == "":
+-                        res.append(x)
++            texts_list.append(to_pinyin(txt, tokens_type))
++    elif tokens_type == "phone+ppinyin":
++        # CJK(China Japan Korea) unicode range is [U+4E00, U+9FFF], ref:
++        # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
++        pattern = re.compile(r"^[\u4e00-\u9fff]+$")
++        for text in texts:
++            words = text.strip().split()
++            text_list = []
++            skip_text = False
++            for w in words:
++                if w in phone_table:
++                    text_list += phone_table[w]
++                else:
++                    if pattern.fullmatch(w) is None:
++                        print(
++                            f"Word {w} not in lexicon and it is not a CJK character, "
++                            f"skipping text: {text}."
++                        )
++                        skip_text = True
++                        break
+                     else:
+-                        if initial != "":
+-                            res.append(initial)
+-                        if final != "":
+-                            res.append(final)
+-                texts_list.append(res)
+-            else:
+-                texts_list.append(py)
++                        text_list += to_pinyin(w, "ppinyin")
++            if not skip_text:
++                texts_list.append(text_list)
+     else:
+         assert (
+             tokens_type == "cjkchar+bpe"
+-        ), f"Supported tokens_type are cjkchar, bpe, cjkchar+bpe, given {tokens_type}"
++        ), f"Supported tokens_type are cjkchar, bpe, cjkchar+bpe, ppinyin, fpinyin, phone+ppinyin given {tokens_type}"
++
+         # CJK(China Japan Korea) unicode range is [U+4E00, U+9FFF], ref:
+         # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
+         pattern = re.compile(r"([\u4e00-\u9fff])")
+diff --git a/sherpa-onnx/python/tests/test_text2token.py b/sherpa-onnx/python/tests/test_text2token.py
+index 7bc065b6..c8924d5a 100755
+--- a/sherpa-onnx/python/tests/test_text2token.py
++++ b/sherpa-onnx/python/tests/test_text2token.py
+@@ -116,6 +116,93 @@ class TestText2Token(unittest.TestCase):
+             [685, 736, 275, 178, 179, 921, 736],
+         ], encoded_ids
+ 
++    def test_phone_ppinyin(self):
++        tokens = f"{d}/text2token/tokens_phone_ppinyin.txt"
++        lexicon = f"{d}/text2token/en.phone"
++
++        if not Path(tokens).is_file() or not Path(lexicon).is_file():
++            print(
++                f"No test data found, skipping test_phone_ppinyin().\n"
++                f"You can download the test data by: \n"
++                f"git clone https://github.com/pkufool/sherpa-test-data.git /tmp/sherpa-test-data"
++            )
++            return
++
++        texts = [" GOES TOGETHER", " GOES WITH "]
++        encoded_texts = sherpa_onnx.text2token(
++            texts,
++            tokens=tokens,
++            tokens_type="phone+ppinyin",
++            lexicon=lexicon,
++        )
++        assert encoded_texts == [
++            [
++                "sh",
++                "",
++                "j",
++                "i",
++                "r",
++                "n",
++                "m",
++                "n",
++                "G",
++                "OW1",
++                "Z",
++                "T",
++                "AH0",
++                "G",
++                "EH1",
++                "DH",
++                "ER0",
++            ],
++            [
++                "zh",
++                "ng",
++                "g",
++                "u",
++                "G",
++                "OW1",
++                "Z",
++                "W",
++                "IH1",
++                "DH",
++                "m",
++                "i",
++                "g",
++                "u",
++            ],
++        ], encoded_texts
++
++        encoded_ids = sherpa_onnx.text2token(
++            texts,
++            tokens=tokens,
++            tokens_type="phone+ppinyin",
++            lexicon=lexicon,
++            output_ids=True,
++        )
++        assert encoded_ids == [
++            [
++                139,
++                203,
++                127,
++                107,
++                137,
++                200,
++                130,
++                207,
++                35,
++                50,
++                70,
++                59,
++                9,
++                35,
++                26,
++                24,
++                28,
++            ],
++            [182, 241, 87, 163, 35, 50, 70, 68, 38, 24, 130, 231, 87, 163],
++        ], encoded_ids
++
+ 
+ if __name__ == "__main__":
+     unittest.main()
+
+commit 16e399fe63587564c4dcb4d75dab2f745a9dbf4f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Dec 22 18:42:50 2025 +0800
+
+    Epoxrt more zipformer ctc models to qnn (#2921)
+
+diff --git a/.github/scripts/export-qnn/generate_zipformer.py b/.github/scripts/export-qnn/generate_zipformer.py
+new file mode 100755
+index 00000000..09ae8702
+--- /dev/null
++++ b/.github/scripts/export-qnn/generate_zipformer.py
+@@ -0,0 +1,52 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import json
++
++from device_info import soc_info_dict
++from dataclasses import asdict, dataclass
++import itertools
++
++
++@dataclass
++class Config:
++    soc: str  # SM8850
++    soc_id: int  # 87
++    arch: str  # v81
++    input_in_seconds: str
++    model_name: str
++
++
++def main():
++
++    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++    model_name_list = ["20250703", "20251222"]
++
++    configs = []
++
++    for name, soc in soc_info_dict.items():
++        for num_seconds, model_name in itertools.product(
++            input_in_seconds, model_name_list
++        ):
++            if model_name == "20251222":
++                if num_seconds not in ["5"]:
++                    # TODO(fangjun): We only upload model-5-seconds.onnx right now
++                    continue
++
++            configs.append(
++                Config(
++                    soc=name,
++                    soc_id=soc.model.value,
++                    arch=soc.info.arch.name,
++                    input_in_seconds=num_seconds,
++                    model_name=model_name,
++                )
++            )
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
+index 4b411384..f54b9618 100644
+--- a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
++++ b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
+@@ -3,7 +3,7 @@ name: export-zipformer-ctc-to-qnn-20250703
+ on:
+   push:
+     branches:
+-      - qnn-zipformer-ctc-models
++      - zipformer-qnn-model-2
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -11,24 +11,50 @@ concurrency:
+   cancel-in-progress: true
+ 
+ jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-qnn/generate_zipformer.py
++          MATRIX=$(python3 .github/scripts/export-qnn/generate_zipformer.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
+   export-zipformer-ctc-to-qnn-20250703:
++    needs: generate_build_matrix
+     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+-    name: ${{ matrix.input_in_seconds }}
+-    runs-on: ${{ matrix.os }}
++    name: ${{ matrix.model_name }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
++    runs-on: ubuntu-22.04
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [ubuntu-22.04]
+-        python-version: ["3.10"]
+-        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+ 
+     steps:
+       - uses: actions/checkout@v4
+ 
+-      - name: Setup Python ${{ matrix.python-version }}
++      - name: Setup Python 3.10
+         uses: actions/setup-python@v5
+         with:
+-          python-version: ${{ matrix.python-version }}
++          python-version: "3.10"
++
++      - name: Create directories
++        shell: bash
++        run: |
++          mkdir so binary
+ 
+       - name: Display NDK HOME
+         shell: bash
+@@ -175,6 +201,7 @@ jobs:
+           qnn-net-run --help
+ 
+       - name: Run ${{ matrix.input_in_seconds }}
++        if: matrix.model_name == '20250703'
+         shell: bash
+         run: |
+           source py310/bin/activate
+@@ -185,6 +212,7 @@ jobs:
+ 
+           export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+           export LDFLAGS="-Wl,-z,max-page-size=16384"
++          dir=$PWD
+ 
+           mkdir tmp
+ 
+@@ -244,8 +272,43 @@ jobs:
+ 
+           readelf -lW model_libs/*/lib*.so
+ 
++          echo "Generate context binary"
++
++          $dir/scripts/qnn/generate_config.py  \
++            --soc ${{ matrix.soc }} \
++            --graph-name "model_${t}_seconds_quantized" \
++            --output-dir ./my-config \
++            --qnn-sdk-root $QNN_SDK_ROOT
++
++          ls -lh my-config
++
++          head -n 1000 my-config/*.json
++
++          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
++            --output_dir ./binary \
++            --binary_file model \
++            --config_file ./my-config/htp_backend_extensions.json
++
++          ls -lh binary/
++
+           echo "collect results"
+ 
++          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-zipformer-ctc-zh-2025-07-03-int8
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++          cp -v binary/model.bin $d/
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++          echo "num_frames=$num_frames" > $d/info.txt
++
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++          mv *.tar.bz2 ../binary/
++
+           for p in x86_64-linux-clang aarch64-android; do
+             if [[ $p == x86_64-linux-clang ]]; then
+               d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-linux-x64
+@@ -275,22 +338,181 @@ jobs:
+           echo "----show---"
+           ls -lh *.tar.bz2
+ 
+-          mv *.tar.bz2 ../
++          mv *.tar.bz2 ../so
++
++      - name: Run ${{ matrix.input_in_seconds }}
++        if: matrix.model_name == '20251222'
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++          dir=$PWD
++
++          mkdir tmp
++
++          cd tmp
++
++          t=${{ matrix.input_in_seconds }}
++          num_frames=$(($t*100))
++
++          echo "num_frames: $num_frames"
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
++          chmod +x generate_test_data.py
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
++
++          ./generate_test_data.py --num-frames $num_frames --wav 0.wav
++          ./generate_test_data.py --num-frames $num_frames --wav 1.wav
++          ./generate_test_data.py --num-frames $num_frames --wav 8k.wav
++
++          echo -e "0.raw\n1.raw\n8k.raw" > input_list.txt
++
++          curl -SL -O https://huggingface.co/csukuangfj/2025-12-22/resolve/main/zipformer-ctc-models/model-$t-seconds.onnx
++
++          python3 ../scripts/pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
++
++
++          echo "export to qnn"
++          echo "----------$t----------"
++
++          qnn-onnx-converter \
++            --input_network model-$t-seconds.onnx \
++            --output_path ./model-$t-seconds-quantized \
++            --out_node log_probs \
++            --input_list ./input_list.txt \
++            --use_native_input_files  \
++            --input_dtype x float32 \
++            --act_bitwidth 16 \
++            --bias_bitwidth 32 \
++            --input_layout x NTF
++
++          ls -lh
++          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
++          echo "----"
++          ls -lh
++
++          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++            -c "model-$t-seconds-quantized.cpp" \
++            -b "model-$t-seconds-quantized.bin" \
++            -o model_libs > /dev/null 2>&1
++
++          ls -lh model_libs/*/
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "Generate context binary"
++
++          $dir/scripts/qnn/generate_config.py  \
++            --soc ${{ matrix.soc }} \
++            --graph-name "model_${t}_seconds_quantized" \
++            --output-dir ./my-config \
++            --qnn-sdk-root $QNN_SDK_ROOT
++
++          ls -lh my-config
++
++          head -n 1000 my-config/*.json
++
++          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
++            --output_dir ./binary \
++            --binary_file model \
++            --config_file ./my-config/htp_backend_extensions.json
++
++          ls -lh binary/
++
++          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-zipformer-ctc-zh-2025-12-22-int8
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++          cp -v binary/model.bin $d/
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++          echo "num_frames=$num_frames" > $d/info.txt
++
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++          mv *.tar.bz2 ../binary/
++
++          echo "collect results"
++
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-12-22-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-12-22-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v model_libs/$p/lib*.so $d/libmodel.so
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++
++            echo "num_frames=$num_frames" > $d/info.txt
++            echo "target=$p" >> $d/info.txt
++
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../so
+ 
+       - uses: actions/upload-artifact@v4
+         with:
+-          name: ${{ matrix.input_in_seconds }}-seconds
++          name: ${{ matrix.model_name }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
+           path: ./tmp/*.json
+ 
++      - name: Release
++        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn
++
+       - name: Release
+         if: github.repository_owner == 'csukuangfj'
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+-          file: ./*.tar.bz2
++          file: ./binary/*.tar.bz2
+           overwrite: true
+           repo_name: k2-fsa/sherpa-onnx
+           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn-binary
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
+           tag: asr-models-qnn
+ 
+       - name: Release
+@@ -298,6 +520,6 @@ jobs:
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+-          file: ./*.tar.bz2
++          file: ./binary/*.tar.bz2
+           overwrite: true
+-          tag: asr-models-qnn
++          tag: asr-models-qnn-binary
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+index 8a09d884..20d25bee 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+@@ -147,9 +147,19 @@ object SimulateStreamingAsr {
+                             config.modelConfig.senseVoice.qnnConfig.contextBinary,
+                             context
+                         )
+-                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
+-                    config.modelConfig.zipformerCtc.model =
+-                        copyAssetToInternalStorage(config.modelConfig.zipformerCtc.model, context)
++                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty() ||
++                    assetExists(
++                        context.assets,
++                        path = config.modelConfig.zipformerCtc.qnnConfig.contextBinary
++                    )
++                ) {
++                    if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
++                        config.modelConfig.zipformerCtc.model =
++                            copyAssetToInternalStorage(
++                                config.modelConfig.zipformerCtc.model,
++                                context
++                            )
++                    }
+ 
+                     config.modelConfig.zipformerCtc.qnnConfig.contextBinary =
+                         copyAssetToInternalStorage(
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index a58b34ef..38bf3e5b 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -176,7 +176,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       return std::make_unique<
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+           config);
+-    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++    } else if (!config.model_config.zipformer_ctc.model.empty() ||
++               !config.model_config.zipformer_ctc.qnn_config.context_binary
++                    .empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
+     } else {
+       SHERPA_ONNX_LOGE(
+@@ -497,7 +499,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       return std::make_unique<
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+           mgr, config);
+-    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++    } else if (!config.model_config.zipformer_ctc.model.empty() ||
++               !config.model_config.zipformer_ctc.qnn_config.context_binary
++                    .empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
+                                                                     config);
+     } else {
+diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+index 0aaaacbb..33e7880a 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+@@ -22,13 +22,31 @@ void OfflineZipformerCtcModelConfig::Register(ParseOptions *po) {
+ }
+ 
+ bool OfflineZipformerCtcModelConfig::Validate() const {
+-  if (!FileExists(model)) {
+-    SHERPA_ONNX_LOGE("zipformer CTC model file '%s' does not exist",
+-                     model.c_str());
+-    return false;
++  if (qnn_config.context_binary.empty()) {
++    if (model.empty()) {
++      SHERPA_ONNX_LOGE("Please provide a Zipformer CTC model");
++      return false;
++    }
++
++    if (!FileExists(model)) {
++      SHERPA_ONNX_LOGE("Zipformer CTC model '%s' does not exist",
++                       model.c_str());
++      return false;
++    }
+   }
+ 
+-  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
++  if (model.empty() && !qnn_config.context_binary.empty()) {
++    // we require that the context_binary exists
++    if (!FileExists(qnn_config.context_binary)) {
++      SHERPA_ONNX_LOGE(
++          "Model is empty, but you provide a context binary that does not "
++          "exist");
++      return false;
++    }
++  }
++
++  if (EndsWith(model, ".so") || EndsWith(model, ".bin") ||
++      (model.empty() && !qnn_config.context_binary.empty())) {
+     return qnn_config.Validate();
+   }
+ 
+
+commit 94a040e396d5b612de1b54f173434e3489c357b4
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 18 17:43:25 2025 +0800
+
+    Refactor ZipVoice C++ code (#2911)
+    
+    This pull request introduces a significant refactoring of the ZipVoice C++ implementation, aiming to improve code structure, performance, and maintainability. The changes include the addition of a NormalDataGenerator for better random number generation, splitting the OfflineTtsZipvoiceModel into more modular encoder and decoder components, and optimizing the mel spectrogram computation.
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index d03dd75d..e28e9a30 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -26,6 +26,7 @@ set(sources
+   keyword-spotter.cc
+   lodr-fst.cc
+   math.cc
++  normal-data-generator.cc
+   offline-canary-model-config.cc
+   offline-canary-model.cc
+   offline-ctc-fst-decoder-config.cc
+diff --git a/sherpa-onnx/csrc/normal-data-generator.cc b/sherpa-onnx/csrc/normal-data-generator.cc
+new file mode 100644
+index 00000000..b62bede3
+--- /dev/null
++++ b/sherpa-onnx/csrc/normal-data-generator.cc
+@@ -0,0 +1,46 @@
++// sherpa-onnx/csrc/normal-data-generator.cc
++//
++// Copyright      2025  Xiaomi Corporation
++
++// Written by ChatGPT
++
++#include "sherpa-onnx/csrc/normal-data-generator.h"
++
++#include <random>
++#include <thread>
++
++namespace sherpa_onnx {
++
++// Helper type hidden in translation unit
++struct RNGHolder {
++  std::mt19937 rng;
++  std::normal_distribution<float> dist;
++
++  RNGHolder()
++      : rng([] {
++          std::random_device rd;
++          std::seed_seq seq{rd(),
++                            static_cast<unsigned>(std::hash<std::thread::id>{}(
++                                std::this_thread::get_id()))};
++          return std::mt19937(seq);
++        }()),
++        dist() {}
++};
++
++NormalDataGenerator::NormalDataGenerator(float mean /* = 0.0f */,
++                                         float stddev /* = 1.0f */)
++    : mean_(mean), stddev_(stddev) {}
++
++void NormalDataGenerator::Fill(float *data, std::size_t size) const {
++  // One RNGHolder per thread
++  static thread_local RNGHolder holder;
++
++  holder.dist.param(
++      std::normal_distribution<float>::param_type(mean_, stddev_));
++
++  for (std::size_t i = 0; i < size; ++i) {
++    data[i] = holder.dist(holder.rng);
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/normal-data-generator.h b/sherpa-onnx/csrc/normal-data-generator.h
+new file mode 100644
+index 00000000..e250f2b6
+--- /dev/null
++++ b/sherpa-onnx/csrc/normal-data-generator.h
+@@ -0,0 +1,26 @@
++// sherpa-onnx/csrc/normal-data-generator.h
++//
++// Copyright      2025  Xiaomi Corporation
++
++// Written by ChatGPT
++#ifndef SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
++#define SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
++
++#include <cstddef>
++
++namespace sherpa_onnx {
++
++class NormalDataGenerator {
++ public:
++  explicit NormalDataGenerator(float mean = 0.0f, float stddev = 1.0f);
++
++  // Fill pre-allocated memory
++  void Fill(float *data, std::size_t size) const;
++
++ private:
++  float mean_;
++  float stddev_;
++};
++
++}  // namespace sherpa_onnx
++#endif  // SHERPA_ONNX_CSRC_NORMAL_DATA_GENERATOR_H_
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+index 8e9861f5..a315d75b 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+@@ -33,6 +33,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+         model_(std::make_unique<OfflineTtsZipvoiceModel>(config.model)),
+         vocoder_(Vocoder::Create(config.model)) {
+     InitFrontend();
++
++    PostInit();
+   }
+ 
+   template <typename Manager>
+@@ -41,6 +43,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+         model_(std::make_unique<OfflineTtsZipvoiceModel>(mgr, config.model)),
+         vocoder_(Vocoder::Create(mgr, config.model)) {
+     InitFrontend(mgr);
++
++    PostInit();
+   }
+ 
+   int32_t SampleRate() const override {
+@@ -99,6 +103,33 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+   }
+ 
+  private:
++  void PostInit() { InitMelBanks(); }
++
++  void InitMelBanks() {
++    const auto &meta = model_->GetMetaData();
++    int32_t sample_rate = meta.sample_rate;
++    int32_t n_fft = meta.n_fft;
++    int32_t hop_length = meta.hop_length;
++    int32_t win_length = meta.window_length;
++    int32_t num_mels = meta.num_mels;
++
++    knf::FrameExtractionOptions frame_opts;
++    frame_opts.samp_freq = sample_rate;
++    frame_opts.frame_length_ms = win_length * 1000 / sample_rate;
++    frame_opts.frame_shift_ms = hop_length * 1000 / sample_rate;
++    frame_opts.window_type = "hanning";
++
++    knf::MelBanksOptions mel_opts;
++    mel_opts.num_bins = num_mels;
++    mel_opts.low_freq = 0;
++    mel_opts.high_freq = sample_rate / 2;
++    mel_opts.is_librosa = true;
++    mel_opts.use_slaney_mel_scale = false;
++    mel_opts.norm = "";
++
++    mel_banks_ = std::make_unique<knf::MelBanks>(mel_opts, frame_opts, 1.0f);
++  }
++
+   template <typename Manager>
+   void InitFrontend(Manager *mgr) {
+     frontend_ = std::make_unique<MatchaTtsLexicon>(
+@@ -112,9 +143,9 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+         config_.model.zipvoice.data_dir, config_.model.debug, true);
+   }
+ 
+-  std::vector<int32_t> ComputeMelSpectrogram(
+-      const std::vector<float> &_samples, int32_t sample_rate,
+-      std::vector<float> *prompt_features) const {
++  void ComputeMelSpectrogram(const std::vector<float> &_samples,
++                             int32_t sample_rate, float feat_scale,
++                             std::vector<float> *prompt_features) const {
+     const auto &meta = model_->GetMetaData();
+     if (sample_rate != meta.sample_rate) {
+       SHERPA_ONNX_LOGE(
+@@ -131,19 +162,18 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+           sample_rate, meta.sample_rate, lowpass_cutoff, lowpass_filter_width);
+       std::vector<float> samples;
+       resampler->Resample(_samples.data(), _samples.size(), true, &samples);
+-      return ComputeMelSpectrogram(samples, prompt_features);
+-    } else {
+-      // Use the original samples if the sample rate matches
+-      return ComputeMelSpectrogram(_samples, prompt_features);
++      ComputeMelSpectrogram(samples, feat_scale, prompt_features);
++      return;
+     }
++
++    ComputeMelSpectrogram(_samples, feat_scale, prompt_features);
+   }
+ 
+-  std::vector<int32_t> ComputeMelSpectrogram(
+-      const std::vector<float> &samples,
+-      std::vector<float> *prompt_features) const {
++  void ComputeMelSpectrogram(const std::vector<float> &samples,
++                             float feat_scale,
++                             std::vector<float> *prompt_features) const {
+     const auto &meta = model_->GetMetaData();
+ 
+-    int32_t sample_rate = meta.sample_rate;
+     int32_t n_fft = meta.n_fft;
+     int32_t hop_length = meta.hop_length;
+     int32_t win_length = meta.window_length;
+@@ -161,46 +191,23 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+     int32_t num_frames = stft_result.num_frames;
+     int32_t fft_bins = n_fft / 2 + 1;
+ 
+-    knf::FrameExtractionOptions frame_opts;
+-    frame_opts.samp_freq = sample_rate;
+-    frame_opts.frame_length_ms = win_length * 1000 / sample_rate;
+-    frame_opts.frame_shift_ms = hop_length * 1000 / sample_rate;
+-    frame_opts.window_type = "hanning";
+-
+-    knf::MelBanksOptions mel_opts;
+-    mel_opts.num_bins = num_mels;
+-    mel_opts.low_freq = 0;
+-    mel_opts.high_freq = sample_rate / 2;
+-    mel_opts.is_librosa = true;
+-    mel_opts.use_slaney_mel_scale = false;
+-    mel_opts.norm = "";
+-
+-    knf::MelBanks mel_banks(mel_opts, frame_opts, 1.0f);
++    prompt_features->resize(num_frames * num_mels);
++    float *p = prompt_features->data();
+ 
+-    prompt_features->clear();
+-    prompt_features->reserve(num_frames * num_mels);
++    std::vector<float> magnitude_spectrum(fft_bins);
+ 
+-    for (int32_t i = 0; i < num_frames; ++i) {
+-      std::vector<float> magnitude_spectrum(fft_bins);
++    for (int32_t i = 0; i < num_frames; ++i, p += num_mels) {
+       for (int32_t k = 0; k < fft_bins; ++k) {
+         float real = stft_result.real[i * fft_bins + k];
+         float imag = stft_result.imag[i * fft_bins + k];
+         magnitude_spectrum[k] = std::sqrt(real * real + imag * imag);
+       }
+-      std::vector<float> mel_features(num_mels, 0.0f);
+-      mel_banks.Compute(magnitude_spectrum.data(), mel_features.data());
+-      for (auto &v : mel_features) {
+-        v = std::log(v + 1e-10f);
++
++      mel_banks_->Compute(magnitude_spectrum.data(), p);
++
++      for (int32_t j = 0; j < num_mels; ++j) {
++        p[j] = std::log(p[j] + 1e-10f) * feat_scale;
+       }
+-      // Instead of push_back a vector, push elements individually
+-      prompt_features->insert(prompt_features->end(), mel_features.begin(),
+-                              mel_features.end());
+-    }
+-    if (num_frames == 0) {
+-      SHERPA_ONNX_LOGE("No frames extracted from the prompt audio");
+-      return {0, 0};
+-    } else {
+-      return {num_frames, num_mels};
+     }
+   }
+ 
+@@ -214,12 +221,14 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+ 
+     std::array<int64_t, 2> tokens_shape = {1,
+                                            static_cast<int64_t>(tokens.size())};
++
+     Ort::Value tokens_tensor = Ort::Value::CreateTensor(
+         memory_info, const_cast<int64_t *>(tokens.data()), tokens.size(),
+         tokens_shape.data(), tokens_shape.size());
+ 
+     std::array<int64_t, 2> prompt_tokens_shape = {
+         1, static_cast<int64_t>(prompt_tokens.size())};
++
+     Ort::Value prompt_tokens_tensor = Ort::Value::CreateTensor(
+         memory_info, const_cast<int64_t *>(prompt_tokens.data()),
+         prompt_tokens.size(), prompt_tokens_shape.data(),
+@@ -230,7 +239,7 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+ 
+     // Scale prompt_samples
+     std::vector<float> prompt_samples_scaled = prompt_samples;
+-    float prompt_rms = 0.0f;
++    double prompt_rms = 0.0;
+     double sum_sq = 0.0;
+     // Compute RMS of prompt_samples
+     for (float s : prompt_samples_scaled) {
+@@ -238,23 +247,24 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+     }
+     prompt_rms = std::sqrt(sum_sq / prompt_samples_scaled.size());
+     if (prompt_rms < target_rms && prompt_rms > 0.0f) {
+-      float scale = target_rms / static_cast<float>(prompt_rms);
++      float scale = target_rms / prompt_rms;
+       for (auto &s : prompt_samples_scaled) {
+         s *= scale;
+       }
+     }
+ 
+     std::vector<float> prompt_features;
+-    auto res_shape = ComputeMelSpectrogram(prompt_samples_scaled, sample_rate,
+-                                           &prompt_features);
+ 
+-    int32_t num_frames = res_shape[0];
+-    int32_t mel_dim = res_shape[1];
++    int32_t mel_dim = model_->GetMetaData().num_mels;
+ 
+-    if (feat_scale != 1.0f) {
+-      for (auto &item : prompt_features) {
+-        item *= feat_scale;
+-      }
++    ComputeMelSpectrogram(prompt_samples_scaled, sample_rate, feat_scale,
++                          &prompt_features);
++
++    int32_t num_frames = prompt_features.size() / mel_dim;
++
++    if (num_frames == 0) {
++      SHERPA_ONNX_LOGE("No frames extracted from the prompt audio");
++      return {};
+     }
+ 
+     std::array<int64_t, 3> shape = {1, num_frames, mel_dim};
+@@ -268,16 +278,19 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+ 
+     // Assume mel_shape = {1, T, C}
+     std::vector<int64_t> mel_shape = mel.GetTensorTypeAndShapeInfo().GetShape();
+-    int64_t T = mel_shape[1], C = mel_shape[2];
++    int64_t T = mel_shape[1];
++    int64_t C = mel_shape[2];
+ 
+-    float *mel_data = mel.GetTensorMutableData<float>();
++    const float *mel_data = mel.GetTensorData<float>();
+     std::vector<float> mel_permuted(C * T);
+ 
++    float inv_feat_scale = 1 / feat_scale;
++
+     for (int64_t c = 0; c < C; ++c) {
+       for (int64_t t = 0; t < T; ++t) {
+         int64_t src_idx = t * C + c;  // src: [T, C] (row major)
+         int64_t dst_idx = c * T + t;  // dst: [C, T] (row major)
+-        mel_permuted[dst_idx] = mel_data[src_idx] / feat_scale;
++        mel_permuted[dst_idx] = mel_data[src_idx] * inv_feat_scale;
+       }
+     }
+ 
+@@ -304,6 +317,8 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+   std::unique_ptr<OfflineTtsZipvoiceModel> model_;
+   std::unique_ptr<Vocoder> vocoder_;
+   std::unique_ptr<OfflineTtsFrontend> frontend_;
++
++  std::unique_ptr<knf::MelBanks> mel_banks_;
+ };
+ 
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
+index dd512caa..56131c3b 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h
+@@ -22,7 +22,6 @@ struct OfflineTtsZipvoiceModelMetaData {
+   int32_t window_length = 1024;
+   int32_t num_mels = 100;
+   int32_t use_espeak = 1;
+-  int32_t use_pinyin = 1;
+ };
+ 
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+index cdd8bc86..2890d982 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+@@ -24,6 +24,7 @@
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/normal-data-generator.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+ #include "sherpa-onnx/csrc/session.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+@@ -37,9 +38,11 @@ class OfflineTtsZipvoiceModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    auto text_buf = ReadFile(config.zipvoice.encoder);
+-    auto fm_buf = ReadFile(config.zipvoice.decoder);
+-    Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
++    auto buf = ReadFile(config.zipvoice.encoder);
++    InitEncoder(buf.data(), buf.size());
++
++    buf = ReadFile(config.zipvoice.decoder);
++    InitDecoder(buf.data(), buf.size());
+   }
+ 
+   template <typename Manager>
+@@ -48,9 +51,11 @@ class OfflineTtsZipvoiceModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    auto text_buf = ReadFile(mgr, config.zipvoice.encoder);
+-    auto fm_buf = ReadFile(mgr, config.zipvoice.decoder);
+-    Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
++    auto buf = ReadFile(mgr, config.zipvoice.encoder);
++    InitEncoder(buf.data(), buf.size());
++
++    buf = ReadFile(mgr, config.zipvoice.decoder);
++    InitDecoder(buf.data(), buf.size());
+   }
+ 
+   const OfflineTtsZipvoiceModelMetaData &GetMetaData() const {
+@@ -59,44 +64,19 @@ class OfflineTtsZipvoiceModel::Impl {
+ 
+   Ort::Value Run(Ort::Value tokens, Ort::Value prompt_tokens,
+                  Ort::Value prompt_features, float speed, int32_t num_steps) {
+-    auto memory_info =
+-        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+-
+     std::vector<int64_t> tokens_shape =
+         tokens.GetTensorTypeAndShapeInfo().GetShape();
++
+     int64_t batch_size = tokens_shape[0];
+-    if (batch_size != 1) {
+-      SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
+-                       static_cast<int32_t>(batch_size));
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+ 
+     std::vector<int64_t> prompt_feat_shape =
+         prompt_features.GetTensorTypeAndShapeInfo().GetShape();
+ 
+     int64_t prompt_feat_len = prompt_feat_shape[1];
+-    int64_t prompt_feat_len_shape = 1;
+-    Ort::Value prompt_feat_len_tensor = Ort::Value::CreateTensor<int64_t>(
+-        memory_info, &prompt_feat_len, 1, &prompt_feat_len_shape, 1);
+-
+-    int64_t speed_shape = 1;
+-    Ort::Value speed_tensor = Ort::Value::CreateTensor<float>(
+-        memory_info, &speed, 1, &speed_shape, 1);
+-
+-    std::vector<Ort::Value> text_inputs;
+-    text_inputs.reserve(4);
+-    text_inputs.push_back(std::move(tokens));
+-    text_inputs.push_back(std::move(prompt_tokens));
+-    text_inputs.push_back(std::move(prompt_feat_len_tensor));
+-    text_inputs.push_back(std::move(speed_tensor));
+ 
+-    // forward encoder
+-    auto text_out =
+-        text_sess_->Run({}, text_input_names_ptr_.data(), text_inputs.data(),
+-                        text_inputs.size(), text_output_names_ptr_.data(),
+-                        text_output_names_ptr_.size());
+-
+-    Ort::Value &text_condition = text_out[0];
++    Ort::Value text_condition =
++        RunEncoder(std::move(tokens), std::move(prompt_tokens),
++                   View(&prompt_features), speed);
+ 
+     std::vector<int64_t> text_cond_shape =
+         text_condition.GetTensorTypeAndShapeInfo().GetShape();
+@@ -105,25 +85,25 @@ class OfflineTtsZipvoiceModel::Impl {
+     int64_t feat_dim = meta_data_.feat_dim;
+ 
+     std::vector<float> x_data(batch_size * num_frames * feat_dim);
+-    std::random_device rd;
+-    std::default_random_engine rng(rd());
+-    std::normal_distribution<float> norm(0, 1);
+-    for (auto &v : x_data) {
+-      v = norm(rng);
+-    }
++
++    normal_gen_.Fill(x_data.data(), x_data.size());
++
++    auto memory_info =
++        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+ 
+     std::vector<int64_t> x_shape = {batch_size, num_frames, feat_dim};
+     Ort::Value x = Ort::Value::CreateTensor<float>(
+         memory_info, x_data.data(), x_data.size(), x_shape.data(),
+         x_shape.size());
+ 
+-    std::vector<float> speech_cond_data(batch_size * num_frames * feat_dim,
+-                                        0.0f);
++    std::vector<float> speech_cond_data(batch_size * num_frames * feat_dim);
+     const float *src = prompt_features.GetTensorData<float>();
+     float *dst = speech_cond_data.data();
+-    std::memcpy(dst, src,
+-                batch_size * prompt_feat_len * feat_dim * sizeof(float));
++    std::copy(src, src + batch_size * prompt_feat_len * feat_dim, dst);
++    prompt_features = Ort::Value{nullptr};
++
+     std::vector<int64_t> speech_cond_shape = {batch_size, num_frames, feat_dim};
++
+     Ort::Value speech_condition = Ort::Value::CreateTensor<float>(
+         memory_info, speech_cond_data.data(), speech_cond_data.size(),
+         speech_cond_shape.data(), speech_cond_shape.size());
+@@ -141,125 +121,125 @@ class OfflineTtsZipvoiceModel::Impl {
+     Ort::Value guidance_scale_tensor = Ort::Value::CreateTensor<float>(
+         memory_info, &guidance_scale, 1, &guidance_scale_shape, 1);
+ 
+-    std::vector<Ort::Value> fm_inputs;
+-    fm_inputs.reserve(5);
+-    // fm_inputs[0] is t tensor, will set in for loop
+-    fm_inputs.emplace_back(nullptr);
+-    fm_inputs.push_back(std::move(x));
+-    fm_inputs.push_back(std::move(text_condition));
+-    fm_inputs.push_back(std::move(speech_condition));
+-    fm_inputs.push_back(std::move(guidance_scale_tensor));
++    float *x_ptr = x.GetTensorMutableData<float>();
++
++    int64_t N = batch_size * num_frames * feat_dim;
+ 
+     for (int32_t step = 0; step < num_steps; ++step) {
+-      float t_val = timesteps[step];
+-      int64_t t_shape = 1;
+-      Ort::Value t_tensor =
+-          Ort::Value::CreateTensor<float>(memory_info, &t_val, 1, &t_shape, 1);
+-      fm_inputs[0] = std::move(t_tensor);
+-      auto fm_out = fm_sess_->Run(
+-          {}, fm_input_names_ptr_.data(), fm_inputs.data(), fm_inputs.size(),
+-          fm_output_names_ptr_.data(), fm_output_names_ptr_.size());
+-      Ort::Value &v = fm_out[0];
++      float t = timesteps[step];
++
++      Ort::Value v =
++          RunDecoder(t, View(&x), View(&text_condition),
++                     View(&speech_condition), View(&guidance_scale_tensor));
+ 
+       float delta_t = timesteps[step + 1] - timesteps[step];
+-      float *x_ptr = fm_inputs[1].GetTensorMutableData<float>();
++
+       const float *v_ptr = v.GetTensorData<float>();
+-      int64_t N = batch_size * num_frames * feat_dim;
+       for (int64_t i = 0; i < N; ++i) {
+         x_ptr[i] += v_ptr[i] * delta_t;
+       }
+     }
+ 
+-    int64_t keep_frames = num_frames - prompt_feat_len;
+-    std::vector<float> out_data(batch_size * keep_frames * feat_dim);
+-    x = std::move(fm_inputs[1]);
+-    const float *x_ptr = x.GetTensorData<float>();
+-    for (int64_t b = 0; b < batch_size; ++b) {
+-      std::memcpy(out_data.data() + b * keep_frames * feat_dim,
+-                  x_ptr + (b * num_frames + prompt_feat_len) * feat_dim,
+-                  keep_frames * feat_dim * sizeof(float));
+-    }
+-    std::vector<int64_t> out_shape = {batch_size, keep_frames, feat_dim};
++    int64_t kept_frames = num_frames - prompt_feat_len;
++
++    std::vector<int64_t> out_shape = {batch_size, kept_frames, feat_dim};
+ 
+     Ort::Value ans = Ort::Value::CreateTensor<float>(
+         allocator_, out_shape.data(), out_shape.size());
+ 
+-    std::copy(out_data.begin(), out_data.end(),
+-              ans.GetTensorMutableData<float>());
++    float *p_out = ans.GetTensorMutableData<float>();
++
++    for (int64_t b = 0; b < batch_size; ++b) {
++      auto begin = x_ptr + (b * num_frames + prompt_feat_len) * feat_dim;
++      auto end = begin + kept_frames * feat_dim;
++      std::copy(begin, end, p_out);
++      p_out += kept_frames * feat_dim;
++    }
+ 
+     return ans;
+   }
+ 
+  private:
+-  void Init(void *encoder_data, size_t encoder_data_length, void *fm_model_data,
+-            size_t fm_model_data_length) {
+-    // Init encoder model
+-    text_sess_ = std::make_unique<Ort::Session>(
++  void InitEncoder(void *encoder_data, size_t encoder_data_length) {
++    encoder_sess_ = std::make_unique<Ort::Session>(
+         env_, encoder_data, encoder_data_length, sess_opts_);
+-    GetInputNames(text_sess_.get(), &text_input_names_, &text_input_names_ptr_);
+-    GetOutputNames(text_sess_.get(), &text_output_names_,
+-                   &text_output_names_ptr_);
+-
+-    // Init flow-matching model
+-    fm_sess_ = std::make_unique<Ort::Session>(env_, fm_model_data,
+-                                              fm_model_data_length, sess_opts_);
+-    GetInputNames(fm_sess_.get(), &fm_input_names_, &fm_input_names_ptr_);
+-    GetOutputNames(fm_sess_.get(), &fm_output_names_, &fm_output_names_ptr_);
++    GetInputNames(encoder_sess_.get(), &encoder_input_names_,
++                  &encoder_names_ptr_);
++    GetOutputNames(encoder_sess_.get(), &encoder_output_names_,
++                   &encoder_output_names_ptr_);
+ 
+     Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
+-
+-    Ort::ModelMetadata meta_data = text_sess_->GetModelMetadata();
++    Ort::ModelMetadata meta_data = encoder_sess_->GetModelMetadata();
+     SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.use_espeak, "use_espeak",
+                                             1);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.use_pinyin, "use_pinyin",
+-                                            1);
+-
+-    meta_data = fm_sess_->GetModelMetadata();
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.version, "version", 1);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.feat_dim, "feat_dim",
+-                                            100);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.sample_rate,
+-                                            "sample_rate", 24000);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.n_fft, "n_fft", 1024);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.hop_length, "hop_length",
+-                                            256);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.window_length,
+-                                            "window_length", 1024);
+-    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.num_mels, "num_mels",
+-                                            100);
+ 
+     if (config_.debug) {
+       std::ostringstream os;
+ 
+       os << "---encoder---\n";
+-      Ort::ModelMetadata text_meta_data = text_sess_->GetModelMetadata();
++      Ort::ModelMetadata text_meta_data = encoder_sess_->GetModelMetadata();
+       PrintModelMetadata(os, text_meta_data);
+ 
+       os << "----------input names----------\n";
+       int32_t i = 0;
+-      for (const auto &s : text_input_names_) {
++      for (const auto &s : encoder_input_names_) {
+         os << i << " " << s << "\n";
+         ++i;
+       }
+       os << "----------output names----------\n";
+       i = 0;
+-      for (const auto &s : text_output_names_) {
++      for (const auto &s : encoder_output_names_) {
+         os << i << " " << s << "\n";
+         ++i;
+       }
+ 
++#if __OHOS__
++      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
++#endif
++    }
++  }
++
++  void InitDecoder(void *decoder_data, size_t decoder_data_length) {
++    decoder_sess_ = std::make_unique<Ort::Session>(
++        env_, decoder_data, decoder_data_length, sess_opts_);
++    GetInputNames(decoder_sess_.get(), &decoder_input_names_,
++                  &decoder_input_names_ptr_);
++    GetOutputNames(decoder_sess_.get(), &decoder_output_names_,
++                   &decoder_output_names_ptr_);
++
++    Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
++    auto meta_data = decoder_sess_->GetModelMetadata();
++
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.version, "version", 1);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.feat_dim, "feat_dim",
++                                            100);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.sample_rate,
++                                            "sample_rate", 24000);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.n_fft, "n_fft", 1024);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.hop_length, "hop_length",
++                                            256);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.window_length,
++                                            "window_length", 1024);
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.num_mels, "num_mels",
++                                            100);
++
++    if (config_.debug) {
++      std::ostringstream os;
++
+       os << "---decoder---\n";
+       PrintModelMetadata(os, meta_data);
+ 
+       os << "----------input names----------\n";
+-      i = 0;
+-      for (const auto &s : fm_input_names_) {
++      int32_t i = 0;
++      for (const auto &s : decoder_input_names_) {
+         os << i << " " << s << "\n";
+         ++i;
+       }
+       os << "----------output names----------\n";
+       i = 0;
+-      for (const auto &s : fm_output_names_) {
++      for (const auto &s : decoder_output_names_) {
+         os << i << " " << s << "\n";
+         ++i;
+       }
+@@ -272,28 +252,97 @@ class OfflineTtsZipvoiceModel::Impl {
+     }
+   }
+ 
++  Ort::Value RunEncoder(Ort::Value tokens, Ort::Value prompt_tokens,
++                        Ort::Value prompt_features, float speed) {
++    auto memory_info =
++        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
++
++    std::vector<int64_t> tokens_shape =
++        tokens.GetTensorTypeAndShapeInfo().GetShape();
++
++    int64_t batch_size = tokens_shape[0];
++    if (batch_size != 1) {
++      SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
++                       static_cast<int32_t>(batch_size));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int64_t> prompt_feat_shape =
++        prompt_features.GetTensorTypeAndShapeInfo().GetShape();
++
++    int64_t prompt_feat_len = prompt_feat_shape[1];
++    int64_t prompt_feat_len_shape = 1;
++    Ort::Value prompt_feat_len_tensor = Ort::Value::CreateTensor<int64_t>(
++        memory_info, &prompt_feat_len, 1, &prompt_feat_len_shape, 1);
++
++    int64_t speed_shape = 1;
++    Ort::Value speed_tensor = Ort::Value::CreateTensor<float>(
++        memory_info, &speed, 1, &speed_shape, 1);
++
++    std::vector<Ort::Value> encoder_inputs;
++    encoder_inputs.reserve(4);
++    encoder_inputs.push_back(std::move(tokens));
++    encoder_inputs.push_back(std::move(prompt_tokens));
++    encoder_inputs.push_back(std::move(prompt_feat_len_tensor));
++    encoder_inputs.push_back(std::move(speed_tensor));
++
++    auto encoder_out = encoder_sess_->Run(
++        {}, encoder_names_ptr_.data(), encoder_inputs.data(),
++        encoder_inputs.size(), encoder_output_names_ptr_.data(),
++        encoder_output_names_ptr_.size());
++
++    return std::move(encoder_out[0]);
++  }
++
++  Ort::Value RunDecoder(float t, Ort::Value x, Ort::Value text_condition,
++                        Ort::Value speech_condition,
++                        Ort::Value guidance_scale_tensor) {
++    auto memory_info =
++        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
++
++    int64_t t_shape = 1;
++    Ort::Value t_tensor =
++        Ort::Value::CreateTensor<float>(memory_info, &t, 1, &t_shape, 1);
++
++    std::vector<Ort::Value> decoder_inputs;
++    decoder_inputs.reserve(5);
++    decoder_inputs.emplace_back(std::move(t_tensor));
++    decoder_inputs.push_back(std::move(x));
++    decoder_inputs.push_back(std::move(text_condition));
++    decoder_inputs.push_back(std::move(speech_condition));
++    decoder_inputs.push_back(std::move(guidance_scale_tensor));
++
++    auto decoder_out = decoder_sess_->Run(
++        {}, decoder_input_names_ptr_.data(), decoder_inputs.data(),
++        decoder_inputs.size(), decoder_output_names_ptr_.data(),
++        decoder_output_names_ptr_.size());
++
++    return std::move(decoder_out[0]);
++  }
++
+  private:
+   OfflineTtsModelConfig config_;
+   Ort::Env env_;
+   Ort::SessionOptions sess_opts_;
+   Ort::AllocatorWithDefaultOptions allocator_;
+ 
+-  std::unique_ptr<Ort::Session> text_sess_;
+-  std::unique_ptr<Ort::Session> fm_sess_;
++  std::unique_ptr<Ort::Session> encoder_sess_;
++  std::unique_ptr<Ort::Session> decoder_sess_;
+ 
+-  std::vector<std::string> text_input_names_;
+-  std::vector<const char *> text_input_names_ptr_;
++  std::vector<std::string> encoder_input_names_;
++  std::vector<const char *> encoder_names_ptr_;
+ 
+-  std::vector<std::string> text_output_names_;
+-  std::vector<const char *> text_output_names_ptr_;
++  std::vector<std::string> encoder_output_names_;
++  std::vector<const char *> encoder_output_names_ptr_;
+ 
+-  std::vector<std::string> fm_input_names_;
+-  std::vector<const char *> fm_input_names_ptr_;
++  std::vector<std::string> decoder_input_names_;
++  std::vector<const char *> decoder_input_names_ptr_;
+ 
+-  std::vector<std::string> fm_output_names_;
+-  std::vector<const char *> fm_output_names_ptr_;
++  std::vector<std::string> decoder_output_names_;
++  std::vector<const char *> decoder_output_names_ptr_;
+ 
+   OfflineTtsZipvoiceModelMetaData meta_data_;
++  NormalDataGenerator normal_gen_;
+ };
+ 
+ OfflineTtsZipvoiceModel::OfflineTtsZipvoiceModel(
+
+commit 241cb9b3b1674d82d38f9395837dc8468a115cc4
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 18 12:46:52 2025 +0800
+
+    Fix publishing NPM packages (#2909)
+
+diff --git a/.github/workflows/build-wheels-macos-universal2.yaml b/.github/workflows/build-wheels-macos-universal2.yaml
+index 4980189b..070e5862 100644
+--- a/.github/workflows/build-wheels-macos-universal2.yaml
++++ b/.github/workflows/build-wheels-macos-universal2.yaml
+@@ -149,7 +149,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [macos-latest, macos-13]
++        os: [macos-latest, macos-15-intel]
+ 
+     steps:
+       - uses: actions/checkout@v4
+diff --git a/.github/workflows/build-wheels-macos-x64.yaml b/.github/workflows/build-wheels-macos-x64.yaml
+index 71600412..7b0e84ef 100644
+--- a/.github/workflows/build-wheels-macos-x64.yaml
++++ b/.github/workflows/build-wheels-macos-x64.yaml
+@@ -150,7 +150,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [macos-13]
++        os: [macos-15-intel]
+ 
+     steps:
+       - uses: actions/checkout@v4
+diff --git a/.github/workflows/jar.yaml b/.github/workflows/jar.yaml
+index 0039da08..23725abc 100644
+--- a/.github/workflows/jar.yaml
++++ b/.github/workflows/jar.yaml
+@@ -32,7 +32,7 @@ jobs:
+           - os: macos-latest
+             arch: "arm64"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             arch: "x64"
+ 
+           - os: windows-2022
+@@ -108,7 +108,7 @@ jobs:
+           rm -rf $src*
+ 
+       - name: Download libs ${{ matrix.os }} ${{ matrix.arch }}
+-        if: ${{ matrix.os == 'macos-13' && matrix.arch == 'x64' }}
++        if: ${{ matrix.os == 'macos-15-intel' && matrix.arch == 'x64' }}
+         shell: bash
+         run: |
+           SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+@@ -179,7 +179,7 @@ jobs:
+             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-linux-x64-$SHERPA_ONNX_VERSION.jar
+           elif [[ $os == "macos-latest" && $arch == "arm64" ]]; then
+             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-osx-aarch64-$SHERPA_ONNX_VERSION.jar
+-          elif [[ $os == "macos-13" && $arch == "x64" ]]; then
++          elif [[ $os == "macos-15-intel" && $arch == "x64" ]]; then
+             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-osx-x64-$SHERPA_ONNX_VERSION.jar
+           elif [[ $os == "windows-2022" && $arch == "x64" ]]; then
+             mv -v sherpa-onnx-native.jar sherpa-onnx-native-lib-win-x64-$SHERPA_ONNX_VERSION.jar
+@@ -234,7 +234,7 @@ jobs:
+             native_jar=sherpa-onnx-native-lib-linux-x64-$SHERPA_ONNX_VERSION.jar
+           elif [[ $os == "macos-latest" && $arch == "arm64" ]]; then
+             native_jar=sherpa-onnx-native-lib-osx-aarch64-$SHERPA_ONNX_VERSION.jar
+-          elif [[ $os == "macos-13" && $arch == "x64" ]]; then
++          elif [[ $os == "macos-15-intel" && $arch == "x64" ]]; then
+             native_jar=sherpa-onnx-native-lib-osx-x64-$SHERPA_ONNX_VERSION.jar
+           elif [[ $os == "windows-2022" && $arch == "x64" ]]; then
+             native_jar=sherpa-onnx-native-lib-win-x64-$SHERPA_ONNX_VERSION.jar
+diff --git a/.github/workflows/jni.yaml b/.github/workflows/jni.yaml
+index 8369006d..d61ea118 100644
+--- a/.github/workflows/jni.yaml
++++ b/.github/workflows/jni.yaml
+@@ -26,7 +26,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [ubuntu-latest, macos-latest, macos-13]
++        os: [ubuntu-latest, macos-latest, macos-15-intel]
+ 
+     steps:
+       - uses: actions/checkout@v4
+diff --git a/.github/workflows/lazarus.yaml b/.github/workflows/lazarus.yaml
+index 693b54cd..384080b2 100644
+--- a/.github/workflows/lazarus.yaml
++++ b/.github/workflows/lazarus.yaml
+@@ -30,7 +30,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [ubuntu-22.04, macos-latest, macos-13, windows-2022]
++        os: [ubuntu-22.04, macos-latest, macos-15-intel, windows-2022]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -128,7 +128,7 @@ jobs:
+         run: |
+           cd lazarus-examples/generate_subtitles
+           os=${{ matrix.os }}
+-          if [[ $os == macos-13 ]]; then
++          if [[ $os == macos-15-intel ]]; then
+             lazbuild --verbose --build-mode=Release --widgetset=cocoa ./generate_subtitles.lpi
+           elif [[ $os == macos-latest ]]; then
+             lazbuild --verbose --build-mode=Release --widgetset=cocoa --cpu=aarch64 ./generate_subtitles.lpi
+@@ -191,7 +191,7 @@ jobs:
+           ls -lh windows-x64
+ 
+       - name: Collect generating subtitles (macos)
+-        if: matrix.os == 'macos-13' || matrix.os == 'macos-latest'
++        if: matrix.os == 'macos-15-intel' || matrix.os == 'macos-latest'
+         shell: bash
+         run: |
+           SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+@@ -228,7 +228,7 @@ jobs:
+           path: /tmp/macos-arm64
+ 
+       - uses: actions/upload-artifact@v4
+-        if: matrix.os == 'macos-13'
++        if: matrix.os == 'macos-15-intel'
+         with:
+           name: macos-x64
+           path: /tmp/macos-x64
+diff --git a/.github/workflows/npm-addon-linux-aarch64.yaml b/.github/workflows/npm-addon-linux-aarch64.yaml
+index 2be4902c..840dab0f 100644
+--- a/.github/workflows/npm-addon-linux-aarch64.yaml
++++ b/.github/workflows/npm-addon-linux-aarch64.yaml
+@@ -54,6 +54,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Show .npmrc
+@@ -132,12 +133,8 @@ jobs:
+ 
+               ls -lh ./sherpa-onnx-node
+ 
+-              export NODE_AUTH_TOKEN=${{ secrets.NPM_TOKEN }}
+-
+               cd ./sherpa-onnx-node
+               cp -v /shared/.npmrc ./
+-              npm install
+-              npm ci
++              # https://docs.npmjs.com/trusted-publishers
+               ls -lh
+-              # see https://docs.npmjs.com/generating-provenance-statements
+-              npm publish --provenance --access public
++              npm publish --access public
+diff --git a/.github/workflows/npm-addon-linux-x64.yaml b/.github/workflows/npm-addon-linux-x64.yaml
+index 9a63108e..ed30b5d1 100644
+--- a/.github/workflows/npm-addon-linux-x64.yaml
++++ b/.github/workflows/npm-addon-linux-x64.yaml
+@@ -41,6 +41,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Display node version
+@@ -123,11 +124,7 @@ jobs:
+ 
+       - name: Publish
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
+         run: |
+           cd ./sherpa-onnx-node
+-          npm install
+-          npm ci
+-          # see https://docs.npmjs.com/generating-provenance-statements
+-          npm publish --provenance --access public
++          # https://docs.npmjs.com/trusted-publishers
++          npm publish --access public
+diff --git a/.github/workflows/npm-addon-macos.yaml b/.github/workflows/npm-addon-macos.yaml
+index 4623be69..30d852c3 100644
+--- a/.github/workflows/npm-addon-macos.yaml
++++ b/.github/workflows/npm-addon-macos.yaml
+@@ -20,7 +20,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [macos-13, macos-14]
++        os: [macos-15-intel, macos-14]
+         python-version: ["3.8"]
+ 
+     steps:
+@@ -46,6 +46,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Display node version
+@@ -117,11 +118,7 @@ jobs:
+ 
+       - name: Publish
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
+         run: |
+           cd ./sherpa-onnx-node
+-          npm install
+-          npm ci
+-          # see https://docs.npmjs.com/generating-provenance-statements
+-          npm publish --provenance --access public
++          # https://docs.npmjs.com/trusted-publishers
++          npm publish --access public
+diff --git a/.github/workflows/npm-addon-win-x64.yaml b/.github/workflows/npm-addon-win-x64.yaml
+index 8655c5af..2b2b847c 100644
+--- a/.github/workflows/npm-addon-win-x64.yaml
++++ b/.github/workflows/npm-addon-win-x64.yaml
+@@ -41,6 +41,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Display node version
+@@ -118,11 +119,7 @@ jobs:
+ 
+       - name: Publish
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
+         run: |
+           cd ./sherpa-onnx-node
+-          npm install
+-          npm ci
+-          # see https://docs.npmjs.com/generating-provenance-statements
+-          npm publish --provenance --access public
++          # https://docs.npmjs.com/trusted-publishers
++          npm publish --access public
+diff --git a/.github/workflows/npm-addon-win-x86.yaml b/.github/workflows/npm-addon-win-x86.yaml
+index 85dc95a8..e6426c7f 100644
+--- a/.github/workflows/npm-addon-win-x86.yaml
++++ b/.github/workflows/npm-addon-win-x86.yaml
+@@ -43,7 +43,7 @@ jobs:
+         with:
+           registry-url: 'https://registry.npmjs.org'
+           architecture: 'x86'
+-          node-version: 16
++          node-version: '16'
+ 
+       - name: Display node version
+         shell: bash
+@@ -158,6 +158,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Display node version
+@@ -179,10 +180,7 @@ jobs:
+ 
+       - name: Publish
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN2 }}
+-          NPM_TOKEN: ${{ secrets.NPM_TOKEN2 }}
+-          token: ${{ secrets.NPM_TOKEN2 }}
+         run: |
+           cd /tmp/files/sherpa-onnx-node
++          # https://docs.npmjs.com/trusted-publishers
+           npm publish --access public
+diff --git a/.github/workflows/npm-addon.yaml b/.github/workflows/npm-addon.yaml
+index 3a14d5e2..56513842 100644
+--- a/.github/workflows/npm-addon.yaml
++++ b/.github/workflows/npm-addon.yaml
+@@ -42,6 +42,7 @@ jobs:
+       - uses: actions/setup-node@v4
+         with:
+           registry-url: 'https://registry.npmjs.org'
++          node-version: '24'
+ 
+       - name: Display node version
+         shell: bash
+@@ -86,11 +87,7 @@ jobs:
+ 
+       - name: Publish
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
+         run: |
+           cd ./sherpa-onnx-node
+-          npm install
+-          npm ci
+-          # see https://docs.npmjs.com/generating-provenance-statements
+-          npm publish --provenance --access public
++          # https://docs.npmjs.com/trusted-publishers
++          npm publish --access public
+diff --git a/.github/workflows/npm.yaml b/.github/workflows/npm.yaml
+index b72c894f..490b281b 100644
+--- a/.github/workflows/npm.yaml
++++ b/.github/workflows/npm.yaml
+@@ -54,6 +54,7 @@ jobs:
+ 
+       - uses: actions/setup-node@v4
+         with:
++          node-version: '24'
+           registry-url: 'https://registry.npmjs.org'
+ 
+       - name: Display node version
+@@ -99,14 +100,10 @@ jobs:
+ 
+       - name: Build nodejs package
+         shell: bash
+-        env:
+-          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
+         run: |
+           cd scripts/nodejs
+ 
+           git diff
+ 
+-          npm install
+-          npm ci
+-          # see https://docs.npmjs.com/generating-provenance-statements
++          # https://docs.npmjs.com/trusted-publishers
+           npm publish --provenance --access public
+diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
+index 6f4c55e4..905af440 100644
+--- a/.github/workflows/pascal.yaml
++++ b/.github/workflows/pascal.yaml
+@@ -27,7 +27,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [ubuntu-latest, macos-latest, macos-13, windows-2022, ubuntu-22.04-arm]
++        os: [ubuntu-latest, macos-latest, macos-15-intel, windows-2022, ubuntu-22.04-arm]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -53,7 +53,7 @@ jobs:
+           sudo apt-get install -q -y fpc
+ 
+       - name: Install Free pascal compiler (macos)
+-        if: matrix.os == 'macos-latest' || matrix.os == 'macos-13'
++        if: matrix.os == 'macos-latest' || matrix.os == 'macos-15-intel'
+         shell: bash
+         run: |
+           brew install fpc
+diff --git a/.github/workflows/run-python-test-macos.yaml b/.github/workflows/run-python-test-macos.yaml
+index 2de0c6db..749f1d7e 100644
+--- a/.github/workflows/run-python-test-macos.yaml
++++ b/.github/workflows/run-python-test-macos.yaml
+@@ -31,10 +31,10 @@ jobs:
+         # macos-14 is for arm64
+         # macos-14-large is for x64
+         include:
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.8"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.9"
+           - os: macos-14
+             python-version: "3.10"
+diff --git a/.github/workflows/swift.yaml b/.github/workflows/swift.yaml
+index 55d13bd8..73c81103 100644
+--- a/.github/workflows/swift.yaml
++++ b/.github/workflows/swift.yaml
+@@ -39,7 +39,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [macos-latest, macos-13]
++        os: [macos-latest, macos-15-intel]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -72,7 +72,7 @@ jobs:
+           ./build-swift-macos.sh
+ 
+       - name: Copy files
+-        if: matrix.os == 'macos-13' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        if: matrix.os == 'macos-15-intel' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+         shell: bash
+         run: |
+           SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+@@ -88,7 +88,7 @@ jobs:
+           tar cjvf ${dst}.tar.bz2 $dst
+ 
+       - name: Release pre-compiled binaries and libs for macOS
+-        if: matrix.os == 'macos-13' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        if: matrix.os == 'macos-15-intel' && (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+diff --git a/.github/workflows/test-build-wheel.yaml b/.github/workflows/test-build-wheel.yaml
+index 35d2f907..d164148e 100644
+--- a/.github/workflows/test-build-wheel.yaml
++++ b/.github/workflows/test-build-wheel.yaml
+@@ -52,14 +52,14 @@ jobs:
+           - os: ubuntu-24.04-arm
+             python-version: "3.13"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.8"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.9"
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.10"
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.11"
+ 
+           - os: macos-latest
+diff --git a/.github/workflows/test-go-package.yaml b/.github/workflows/test-go-package.yaml
+index b6657569..05c11845 100644
+--- a/.github/workflows/test-go-package.yaml
++++ b/.github/workflows/test-go-package.yaml
+@@ -28,7 +28,7 @@ jobs:
+             arch: amd64
+           - os: ubuntu-22.04-arm
+             arch: arm64
+-          - os: macos-13
++          - os: macos-15-intel
+             arch: amd64
+           - os: macos-14
+             arch: arm64
+diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
+index e362418e..53faad2d 100644
+--- a/.github/workflows/test-go.yaml
++++ b/.github/workflows/test-go.yaml
+@@ -24,7 +24,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [macos-latest, macos-13, ubuntu-latest, windows-2022, ubuntu-22.04-arm]
++        os: [macos-latest, macos-15-intel, ubuntu-latest, windows-2022, ubuntu-22.04-arm]
+ 
+     steps:
+       - uses: actions/checkout@v4
+diff --git a/.github/workflows/test-pip-install.yaml b/.github/workflows/test-pip-install.yaml
+index a8c71ed3..fc904b1a 100644
+--- a/.github/workflows/test-pip-install.yaml
++++ b/.github/workflows/test-pip-install.yaml
+@@ -56,14 +56,14 @@ jobs:
+           - os: ubuntu-24.04-arm
+             python-version: "3.13"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.8"
+ 
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.9"
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.10"
+-          - os: macos-13
++          - os: macos-15-intel
+             python-version: "3.11"
+ 
+           - os: macos-14
+
+commit 5ce3d6d93a5f4fe11657bf11a6bf3a022eeef22f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 17 20:36:53 2025 +0800
+
+    Release v1.12.20 (#2907)
+
+diff --git a/CHANGELOG.md b/CHANGELOG.md
+index 439b0db9..21d55bf8 100644
+--- a/CHANGELOG.md
++++ b/CHANGELOG.md
+@@ -1,3 +1,25 @@
++## 1.12.20
++
++* Refactor axcl examples. (#2867)
++* Update README to include Axera NPU (#2870)
++* Add CI for Axera NPU (#2872)
++* Refactor sense voice impl (#2873)
++* Refactor Paraformer Impl (#2874)
++* Remove unused lock file (#2875)
++* Load QNN context binary for faster startup (#2877)
++* Export models to Ascend 910B4 (#2878)
++* Optimize streaming output results when VAD does not detect human voice for a long time (#2876)
++* Build APKs for MatchaTTS Chinese+English (#2882)
++* Publish WASM spaces for MatchaTTS Chinese+English model (#2885)
++* Add script for testing zipvoice onnx models (#2887)
++* upload zipvoice onnx models (#2890)
++* Remove cppinyin from zipvoice (#2892)
++* Fix building errors (#2893)
++* Use a shorter name for Zipvoice models. (#2894)
++* Export GigaAM v3 to sherpa-onnx (#2901)
++* Fix typos in URL (#2905)
++* Support Fun-ASR-Nano-2512 (#2906)
++
+ ## 1.12.19
+ 
+ * Fix building without TTS for C API (#2838)
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 57cb8e7f..1c34ad66 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -14,7 +14,7 @@ project(sherpa-onnx)
+ # Remember to update
+ # ./CHANGELOG.md
+ # ./new-release.sh
+-set(SHERPA_ONNX_VERSION "1.12.19")
++set(SHERPA_ONNX_VERSION "1.12.20")
+ 
+ # Disable warning about
+ #
+diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
+index 2ac752f2..df6b6c48 100644
+--- a/android/SherpaOnnx/app/build.gradle
++++ b/android/SherpaOnnx/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
+index 2ac752f2..df6b6c48 100644
+--- a/android/SherpaOnnx2Pass/app/build.gradle
++++ b/android/SherpaOnnx2Pass/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
+index d7942a39..e3d6bc56 100644
+--- a/android/SherpaOnnxAar/README.md
++++ b/android/SherpaOnnxAar/README.md
+@@ -4,8 +4,8 @@
+ git clone https://github.com/k2-fsa/sherpa-onnx
+ cd sherpa-onnx
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-v1.12.19-android.tar.bz2
+-tar xvf sherpa-onnx-v1.12.19-android.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-v1.12.20-android.tar.bz2
++tar xvf sherpa-onnx-v1.12.20-android.tar.bz2
+ 
+ cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
+ cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
+@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
+ 
+ ./gradlew :sherpa_onnx:assembleRelease
+ ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
+-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.19.aar
++cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.20.aar
+ ```
+diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+index dafdd504..1af7f56d 100644
+--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+index 29f894c1..e0de0cd4 100644
+--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
+         minSdk = 26
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
+index c0b95fdf..da437a02 100644
+--- a/android/SherpaOnnxJavaDemo/app/build.gradle
++++ b/android/SherpaOnnxJavaDemo/app/build.gradle
+@@ -9,8 +9,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 28
+         targetSdk 34
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+@@ -34,5 +34,5 @@ dependencies {
+     implementation 'pub.devrel:easypermissions:3.0.0'
+     implementation 'androidx.core:core-ktx:1.7.0'
+     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
+-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.19'
++    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.20'
+ }
+diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
+index 2ac752f2..df6b6c48 100644
+--- a/android/SherpaOnnxKws/app/build.gradle
++++ b/android/SherpaOnnxKws/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+index 571a805f..bef7ef4a 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+index ac309729..e9d0f91a 100644
+--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
+         minSdk = 28
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+@@ -58,7 +58,7 @@ dependencies {
+     implementation(libs.compose.foundation)
+     implementation(libs.activity.compose)
+     implementation(libs.core.splashscreen)
+-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.19")
++    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.20")
+     androidTestImplementation(platform(libs.compose.bom))
+     androidTestImplementation(libs.ui.test.junit4)
+     debugImplementation(libs.ui.tooling)
+diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+index fe528faa..a627d6e7 100644
+--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+index 12e210ee..5e84ec0e 100644
+--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+index fc0c03e1..0aad0cf2 100644
+--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.slid"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
+index 0b6ba6c0..221b89ea 100644
+--- a/android/SherpaOnnxTts/app/build.gradle
++++ b/android/SherpaOnnxTts/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+index 6a8978b9..9a5e8f26 100644
+--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
++++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251205
+-        versionName = "1.12.19"
++        versionCode = 20251217
++        versionName = "1.12.20"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
+index 66bf1f41..7d3afbcb 100644
+--- a/android/SherpaOnnxVad/app/build.gradle
++++ b/android/SherpaOnnxVad/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
+index 66bf1f41..7d3afbcb 100644
+--- a/android/SherpaOnnxVadAsr/app/build.gradle
++++ b/android/SherpaOnnxVadAsr/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
+index 3a29c0b1..1066d5ff 100644
+--- a/android/SherpaOnnxWebSocket/app/build.gradle
++++ b/android/SherpaOnnxWebSocket/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251205
+-        versionName "1.12.19"
++        versionCode 20251217
++        versionName "1.12.20"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/build-ios-shared.sh b/build-ios-shared.sh
+index df5c6037..064723a8 100755
+--- a/build-ios-shared.sh
++++ b/build-ios-shared.sh
+@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
+ 	<key>CFBundlePackageType</key>
+ 	<string>FMWK</string>
+ 	<key>CFBundleShortVersionString</key>
+-	<string>1.12.19</string>
++	<string>1.12.20</string>
+ 	<key>CFBundleSupportedPlatforms</key>
+ 	<array>
+ 		<string>iPhoneOS</string>
+diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
+index b77879a4..a45be760 100644
+--- a/dart-api-examples/add-punctuations/pubspec.yaml
++++ b/dart-api-examples/add-punctuations/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
+index 396260c2..8b2f37a7 100644
+--- a/dart-api-examples/audio-tagging/pubspec.yaml
++++ b/dart-api-examples/audio-tagging/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
+index 04eeb417..ce9b8725 100644
+--- a/dart-api-examples/keyword-spotter/pubspec.yaml
++++ b/dart-api-examples/keyword-spotter/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
+index fcae6b01..ff195c35 100644
+--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
+index 8a1fadb3..3090ff9e 100644
+--- a/dart-api-examples/speaker-diarization/pubspec.yaml
++++ b/dart-api-examples/speaker-diarization/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
+index ede5c6e4..95bb5c6d 100644
+--- a/dart-api-examples/speaker-identification/pubspec.yaml
++++ b/dart-api-examples/speaker-identification/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+index f42abe91..f2d5ab5e 100644
+--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
++++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
+index 0564cda2..a46585b6 100644
+--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
++++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
+index 377dd7bb..1884ef3b 100644
+--- a/dart-api-examples/streaming-asr/pubspec.yaml
++++ b/dart-api-examples/streaming-asr/pubspec.yaml
+@@ -11,7 +11,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
+index cb072fd3..a9c47b68 100644
+--- a/dart-api-examples/tts/pubspec.yaml
++++ b/dart-api-examples/tts/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+index d581ebee..7b6e0112 100644
+--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
+index 403fd353..c73ea2ae 100644
+--- a/dart-api-examples/vad/pubspec.yaml
++++ b/dart-api-examples/vad/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+index 70fbb929..ef743d3f 100644
+--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
++++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.19
++version: 1.12.20
+ 
+ topics:
+   - speech-recognition
+@@ -31,7 +31,7 @@ dependencies:
+   record: 6.0.0
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
+index e3527e8a..7c3cd89b 100644
+--- a/flutter-examples/streaming_asr/pubspec.yaml
++++ b/flutter-examples/streaming_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.19
++version: 1.12.20
+ 
+ topics:
+   - speech-recognition
+@@ -30,7 +30,7 @@ dependencies:
+   record: ^6.1.2
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
+index d98a541b..2956a40c 100644
+--- a/flutter-examples/tts/pubspec.yaml
++++ b/flutter-examples/tts/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none' # Remove this line if you wish to publish to pub.dev
+ 
+-version: 1.12.19
++version: 1.12.20
+ 
+ environment:
+   sdk: ">=2.17.0 <4.0.0"
+@@ -18,7 +18,7 @@ dependencies:
+   cupertino_icons: ^1.0.6
+   path_provider: ^2.1.3
+   path: ^1.9.0
+-  sherpa_onnx: ^1.12.19
++  sherpa_onnx: ^1.12.20
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   url_launcher: 6.2.6
+diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
+index b65ccfeb..ac02b6aa 100644
+--- a/flutter/sherpa_onnx/pubspec.yaml
++++ b/flutter/sherpa_onnx/pubspec.yaml
+@@ -17,7 +17,7 @@ topics:
+   - voice-activity-detection
+ 
+ # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+-version: 1.12.19
++version: 1.12.20
+ 
+ homepage: https://github.com/k2-fsa/sherpa-onnx
+ 
+@@ -30,23 +30,23 @@ dependencies:
+   flutter:
+     sdk: flutter
+ 
+-  sherpa_onnx_android: ^1.12.19
++  sherpa_onnx_android: ^1.12.20
+   # sherpa_onnx_android:
+   #   path: ../sherpa_onnx_android
+ 
+-  sherpa_onnx_macos: ^1.12.19
++  sherpa_onnx_macos: ^1.12.20
+   # sherpa_onnx_macos:
+   #   path: ../sherpa_onnx_macos
+ 
+-  sherpa_onnx_linux: ^1.12.19
++  sherpa_onnx_linux: ^1.12.20
+   # sherpa_onnx_linux:
+   #   path: ../sherpa_onnx_linux
+ 
+-  sherpa_onnx_windows: ^1.12.19
++  sherpa_onnx_windows: ^1.12.20
+   # sherpa_onnx_windows:
+   #   path: ../sherpa_onnx_windows
+ 
+-  sherpa_onnx_ios: ^1.12.19
++  sherpa_onnx_ios: ^1.12.20
+   # sherpa_onnx_ios:
+   #   path: ../sherpa_onnx_ios
+ 
+diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+index ffd952bf..a821c033 100644
+--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
++++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+@@ -7,7 +7,7 @@
+ # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_ios'
+-  s.version          = '1.12.19'
++  s.version          = '1.12.20'
+   s.summary          = 'A new Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ A new Flutter FFI plugin project.
+diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+index ec1ea5fd..ecefd905 100644
+--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
++++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+@@ -4,7 +4,7 @@
+ #
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_macos'
+-  s.version          = '1.12.19'
++  s.version          = '1.12.20'
+   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ sherpa-onnx Flutter FFI plugin project.
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+index 2a5e5dbf..7c18da38 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+@@ -1,7 +1,7 @@
+ /**
+  * Use these variables when you tailor your ArkTS code. They must be of the const type.
+  */
+-export const HAR_VERSION = '1.12.19';
++export const HAR_VERSION = '1.12.20';
+ export const BUILD_MODE_NAME = 'debug';
+ export const DEBUG = true;
+ export const TARGET_NAME = 'default';
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+index dde223ef..01087a2d 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
+ 
+ ```
+   "dependencies": {
+-    "sherpa_onnx": "1.12.19",
++    "sherpa_onnx": "1.12.20",
+   },
+ ```
+ 
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+index f3e43a76..fe2c2ae5 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+@@ -1,6 +1,6 @@
+ {
+   "name": "sherpa_onnx",
+-  "version": "1.12.19",
++  "version": "1.12.20",
+   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
+   "main": "Index.ets",
+   "author": "The next-gen Kaldi team",
+diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+index c703cba9..02da1ac7 100644
+--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.19"
++    "sherpa_onnx": "1.12.20"
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+index 21ccbcef..82551df9 100644
+--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.19",
++    "sherpa_onnx": "1.12.20",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+index 21ccbcef..82551df9 100644
+--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.19",
++    "sherpa_onnx": "1.12.20",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+index 21ccbcef..82551df9 100644
+--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.19",
++    "sherpa_onnx": "1.12.20",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+index af0510b9..672af848 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
++++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+@@ -1,6 +1,6 @@
+ # Introduction
+ 
+-Please download ./sherpa_onnx-v1.12.19.har
++Please download ./sherpa_onnx-v1.12.20.har
+ from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
+ 
+ Hint: For users who have no access to huggingface, please use
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+index c24b1baf..2f78fe76 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+@@ -7,7 +7,7 @@
+   "license": "",
+   "dependencies": {
+     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
+-    "sherpa_onnx": "1.12.19",
++    "sherpa_onnx": "1.12.20",
+   }
+ }
+ 
+diff --git a/jitpack.yml b/jitpack.yml
+index 5591d3ac..43a41983 100644
+--- a/jitpack.yml
++++ b/jitpack.yml
+@@ -2,8 +2,8 @@ jdk:
+   - openjdk17
+ 
+ before_install:
+-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-1.12.19.aar
++  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-1.12.20.aar
+ 
+ install:
+-  - FILE="-Dfile=sherpa-onnx-1.12.19.aar"
+-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.19 -Dpackaging=aar -DgeneratePom=true
++  - FILE="-Dfile=sherpa-onnx-1.12.20.aar"
++  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.20 -Dpackaging=aar -DgeneratePom=true
+diff --git a/mfc-examples/README.md b/mfc-examples/README.md
+index 970c9cc3..d9480032 100644
+--- a/mfc-examples/README.md
++++ b/mfc-examples/README.md
+@@ -5,9 +5,9 @@ for speech recognition.
+ 
+ |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
+ |---------|--------------------|-------------------|------------|
+-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x86-v1.12.19.exe)| Non-streaming speech recognition|
+-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x86-v1.12.19.exe)| Streaming speech recognition|
+-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x86-v1.12.19.exe)| Non-streaming text to speech|
++|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-asr-x86-v1.12.20.exe)| Non-streaming speech recognition|
++|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-streaming-asr-x86-v1.12.20.exe)| Streaming speech recognition|
++|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x64-v1.12.20.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.20/sherpa-onnx-non-streaming-tts-x86-v1.12.20.exe)| Non-streaming text to speech|
+ 
+ Caution: You need to use Windows and install Visual Studio 2022 in order to
+ compile it.
+diff --git a/new-release.sh b/new-release.sh
+index 4d2d2280..1588a030 100755
+--- a/new-release.sh
++++ b/new-release.sh
+@@ -2,11 +2,11 @@
+ 
+ set -ex
+ 
+-old_version_code=20251127
+-new_version_code=20251205
++old_version_code=20251205
++new_version_code=20251217
+ 
+-old_version="1\.12\.18"
+-new_version="1\.12\.19"
++old_version="1\.12\.19"
++new_version="1\.12\.20"
+ 
+ replace_str="s/$old_version/$new_version/g"
+ 
+diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
+index d24f1105..429c86dd 100644
+--- a/nodejs-addon-examples/package.json
++++ b/nodejs-addon-examples/package.json
+@@ -1,5 +1,5 @@
+ {
+   "dependencies": {
+-    "sherpa-onnx-node": "^1.12.19"
++    "sherpa-onnx-node": "^1.12.20"
+   }
+ }
+diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
+index af3538c6..710a5447 100644
+--- a/nodejs-examples/package.json
++++ b/nodejs-examples/package.json
+@@ -2,7 +2,7 @@
+   "dependencies": {
+     "mic": "^2.1.2",
+     "naudiodon2": "^2.4.0",
+-    "sherpa-onnx": "^1.12.19",
++    "sherpa-onnx": "^1.12.20",
+     "wav": "^1.0.2"
+   }
+ }
+diff --git a/pom.xml b/pom.xml
+index 0d84b015..dfbb89f2 100644
+--- a/pom.xml
++++ b/pom.xml
+@@ -4,7 +4,7 @@
+     <modelVersion>4.0.0</modelVersion>
+     <groupId>com.k2fsa.sherpa.onnx</groupId>
+     <artifactId>sherpa-onnx-android</artifactId>
+-    <version>1.12.19</version>
++    <version>1.12.20</version>
+     <url>https://github.com/k2-fsa/sherpa-onnx</url>
+     <packaging>pom</packaging>
+     <description>First Android Library</description>
+diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
+index e9a561da..29b11fb0 100644
+--- a/scripts/wheel/sherpa-onnx-bin/setup.py
++++ b/scripts/wheel/sherpa-onnx-bin/setup.py
+@@ -13,7 +13,7 @@ print("bin_files", bin_files)
+ 
+ setup(
+     name="sherpa-onnx-bin",
+-    version="1.12.19",
++    version="1.12.20",
+     description="Binary executables for sherpa-onnx",
+     author="The sherpa-onnx development team",
+     url="https://github.com/k2-fsa/sherpa-onnx",
+@@ -23,7 +23,7 @@ setup(
+     packages=[],
+     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
+     install_requires=[
+-        "sherpa-onnx-core==1.12.19",
++        "sherpa-onnx-core==1.12.20",
+     ],
+     classifiers=[
+         "Programming Language :: Python :: 3",
+diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
+index d053d293..8ac69af9 100644
+--- a/scripts/wheel/sherpa-onnx-core/setup.py
++++ b/scripts/wheel/sherpa-onnx-core/setup.py
+@@ -23,7 +23,7 @@ def get_binaries():
+ 
+ setup(
+     name="sherpa-onnx-core",
+-    version="1.12.19",
++    version="1.12.20",
+     description="Core shared libraries for sherpa-onnx",
+     packages=["sherpa_onnx"],
+     include_package_data=True,
+diff --git a/setup.py b/setup.py
+index 63991827..3eeb89a7 100644
+--- a/setup.py
++++ b/setup.py
+@@ -109,7 +109,7 @@ setuptools.setup(
+         ],
+     },
+     license="Apache licensed, as found in the LICENSE file",
+-    install_requires=["sherpa-onnx-core==1.12.19"] if need_split_package() else None,
++    install_requires=["sherpa-onnx-core==1.12.20"] if need_split_package() else None,
+ )
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
+diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
+index 315d3d2e..2d8a55a9 100644
+--- a/sherpa-onnx/csrc/version.cc
++++ b/sherpa-onnx/csrc/version.cc
+@@ -7,17 +7,17 @@
+ namespace sherpa_onnx {
+ 
+ const char *GetGitDate() {
+-  static const char *date = "Fri Dec 5 11:45:41 2025";
++  static const char *date = "Wed Dec 17 19:57:55 2025";
+   return date;
+ }
+ 
+ const char *GetGitSha1() {
+-  static const char *sha1 = "e6a6599f";
++  static const char *sha1 = "3290e1ce";
+   return sha1;
+ }
+ 
+ const char *GetVersionStr() {
+-  static const char *version = "1.12.19";
++  static const char *version = "1.12.20";
+   return version;
+ }
+ 
+
+commit 3290e1ce5660a384cc4d41353546b1343bc16467
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 17 19:57:55 2025 +0800
+
+    Support Fun-ASR-Nano-2512 (#2906)
+    
+    This pull request significantly expands the capabilities of sherpa-onnx by integrating the Fun-ASR-Nano-2512 model. The changes encompass the entire pipeline, from model conversion and quantization using new Python scripts to specialized C++ inference logic that correctly interprets the Nano model's unique output structure and metadata. This ensures seamless support for this new, highly optimized ASR model, which boasts features like far-field high-noise recognition and multi-dialect/multi-language support.
+
+diff --git a/.github/workflows/export-sense-voice-to-onnx.yaml b/.github/workflows/export-sense-voice-to-onnx.yaml
+index 1c3e9172..f094f780 100644
+--- a/.github/workflows/export-sense-voice-to-onnx.yaml
++++ b/.github/workflows/export-sense-voice-to-onnx.yaml
+@@ -26,10 +26,31 @@ jobs:
+         with:
+           python-version: ${{ matrix.python-version }}
+ 
++      - name: Install dependencies
++        shell: bash
++        run: |
++          sudo apt-get install -y -qq sox libsox-fmt-mp3
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          pip install \
++            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
++            onnx==1.17.0 \
++            onnxruntime==1.17.1 \
++            soundfile \
++            kaldi-native-fbank \
++            librosa
++
++          pip install  "numpy<2"
++
+       - name: Download test_wavs
+         shell: bash
+         run: |
+           sudo apt-get install -y -qq sox libsox-fmt-mp3
++
++          cd scripts/sense-voice
++
+           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/zh.mp3
+           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/en.mp3
+           curl -SL -O https://huggingface.co/FunAudioLLM/SenseVoiceSmall/resolve/main/example/ja.mp3
+@@ -44,13 +65,90 @@ jobs:
+           sox ko.mp3 -r 16k ko.wav
+           sox yue.mp3 -r 16k yue.wav
+ 
++
++      - name: Run
++        shell: bash
++        run: |
++          cd scripts/sense-voice
++          curl -SL -O https://huggingface.co/csukuangfj/funasr-nano-with-ctc/resolve/main/model.pt
++          curl -SL -O https://huggingface.co/csukuangfj/funasr-nano-with-ctc/resolve/main/tokens.txt
++          ls -lh
++          ./export_onnx_nano.py
++
++          ls -lh
++
++          d=sherpa-onnx-sense-voice-funasr-nano-2025-12-17
++          d2=sherpa-onnx-sense-voice-funasr-nano-int8-2025-12-17
++          mkdir -p $d $d2
++
++          cp README-nano.md $d/README.md
++          cp README-nano.md $d2/README.md
++
++          mv model.onnx $d/
++          mv model.int8.onnx $d2/
++
++          for m in $d $d2; do
++            mkdir -p $m/test_wavs
++            cp -v *.wav $m/test_wavs
++            cp -v tokens.txt $m/
++
++            ls -lh $m
++
++            tar cjfv $m.tar.bz2 $m
++
++            ls -lh $m.tar.bz2
++            mv $m.tar.bz2 ../../
++            mv $m ../../
++          done
++
++      - name: Publish v3 to huggingface
++        if: true
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 5
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            names=(
++              sherpa-onnx-sense-voice-funasr-nano-2025-12-17
++              sherpa-onnx-sense-voice-funasr-nano-int8-2025-12-17
++            )
++            for d in ${names[@]}; do
++              if [ ! -d $d ]; then
++                echo "$d does not exist - skip it"
++                continue;
++              fi
++
++              export GIT_LFS_SKIP_SMUDGE=1
++              export GIT_CLONE_PROTECTION_ACTIVE=false
++              rm -rf huggingface
++              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
++              cp -av $d/* ./huggingface
++              cd huggingface
++              git lfs track "*.onnx"
++              git lfs track "*.wav"
++              git status
++              git add .
++              git status
++              git commit -m "add models"
++              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
++              cd ..
++            done
++
+       - name: Run
+         shell: bash
++        if: false
+         run: |
+           cd scripts/sense-voice
+           ./run.sh
+ 
+       - name: Publish to huggingface
++        if: false
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+         uses: nick-fields/retry@v3
+diff --git a/scripts/sense-voice/README-nano.md b/scripts/sense-voice/README-nano.md
+new file mode 100644
+index 00000000..15aba991
+--- /dev/null
++++ b/scripts/sense-voice/README-nano.md
+@@ -0,0 +1,33 @@
++# Introduction
++
++This directory contains models converted from
++https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512
++
++## Core Features
++
++> From  https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512
++
++    - Far-field High-noise Recognition: Deeply optimized for far-distance sound pickup and high-noise scenarios (such as conference rooms, in-vehicle environments, industrial sites, etc.), improving recognition accuracy to 93%.
++
++    - Chinese Dialects and Regional Accents:
++
++        - Supports 7 major dialects: Wu, Cantonese, Min, Hakka, Gan, Xiang, Jin
++        - Covers 26 regional accents: including Henan, Shaanxi, Hubei, Sichuan, Chongqing, Yunnan, Guizhou, Guangdong, Guangxi and more than 20 other regions
++
++    - Multi-language Free Speech: Supports recognition of 31 languages, with focused optimization on East and Southeast Asian languages, supporting free language switching and mixed recognition.
++    - Music Background Lyric Recognition: Enhanced speech recognition performance under music background interference, supporting accurate recognition of lyric content in songs.
++
++
++
++## 
++
++> From https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512/blob/main/README_zh.md
++
++    -   **93%**
++    - 
++
++        -  7 
++        -  26  20 
++
++    -   31 
++    -  
+diff --git a/scripts/sense-voice/adaptor.py b/scripts/sense-voice/adaptor.py
+new file mode 120000
+index 00000000..998f6034
+--- /dev/null
++++ b/scripts/sense-voice/adaptor.py
+@@ -0,0 +1 @@
++rknn/adaptor.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/export-onnx.py b/scripts/sense-voice/export-onnx.py
+index 0153cebd..831a6577 100755
+--- a/scripts/sense-voice/export-onnx.py
++++ b/scripts/sense-voice/export-onnx.py
+@@ -7,6 +7,8 @@ https://hf-mirror.com/yuekai/model_repo_sense_voice_small/blob/main/export_onnx.
+ as a reference while writing this file.
+ 
+ Thanks to https://github.com/yuekaizhang for making the file public.
++
++You should install FunASR before you run this file.
+ """
+ 
+ import os
+@@ -120,7 +122,9 @@ def display_params(params):
+ 
+ @torch.no_grad()
+ def main():
+-    model, params = SenseVoiceSmall.from_pretrained(model="iic/SenseVoiceSmall", device="cpu")
++    model, params = SenseVoiceSmall.from_pretrained(
++        model="iic/SenseVoiceSmall", device="cpu"
++    )
+     model.eval()
+ 
+     display_params(params)
+diff --git a/scripts/sense-voice/export_onnx_nano.py b/scripts/sense-voice/export_onnx_nano.py
+new file mode 100755
+index 00000000..96350f50
+--- /dev/null
++++ b/scripts/sense-voice/export_onnx_nano.py
+@@ -0,0 +1,112 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import argparse
++import os
++from typing import Any, Dict
++
++import onnx
++import torch
++from onnxruntime.quantization import QuantType, quantize_dynamic
++
++from test_nano_torch import load_tokens, load_torch_model
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--opset-version",
++        type=int,
++        default=13,
++    )
++    return parser.parse_args()
++
++
++def add_meta_data(filename: str, meta_data: Dict[str, Any]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++@torch.no_grad()
++def main():
++    args = get_args()
++    print(vars(args))
++    id2tokens = load_tokens()
++
++    vocab_size = len(id2tokens)
++    blank_id = vocab_size - 1
++
++    print("loading model")
++
++    model = load_torch_model()
++    model.eval()
++
++    x = torch.randn(1, 30, 560, dtype=torch.float32)
++
++    opset_version = args.opset_version
++    filename = "model.onnx"
++    torch.onnx.export(
++        model,
++        x,
++        filename,
++        opset_version=opset_version,
++        input_names=["x"],
++        output_names=["logits"],
++        dynamic_axes={
++            "x": {1: "T"},
++        },
++    )
++
++    model_author = "FunAudioLLM"
++    comment = os.environ.get("comment", "FunAudioLLM/Fun-ASR-Nano-2512")
++    url = "https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512"
++
++    meta_data = {
++        "lfr_window_size": 7,
++        "lfr_window_shift": 6,
++        "normalize_samples": 0,  # input should be in the range [-32768, 32767]
++        "model_type": "sense_voice_ctc",
++        "version": "1",
++        "model_author": model_author,
++        "maintainer": "k2-fsa",
++        "vocab_size": vocab_size,
++        "blank_id": blank_id,
++        "comment": comment,
++        "url": url,
++    }
++    add_meta_data(filename=filename, meta_data=meta_data)
++
++    filename_int8 = "model.int8.onnx"
++    quantize_dynamic(
++        model_input=filename,
++        model_output=filename_int8,
++        op_types_to_quantize=["MatMul"],
++        # Note that we have to use QUInt8 here.
++        #
++        # When QInt8 is used, C++ onnxruntime produces incorrect results
++        weight_type=QuantType.QUInt8,
++    )
++
++
++if __name__ == "__main__":
++    torch.manual_seed(20251217)
++    main()
+diff --git a/scripts/sense-voice/nano.py b/scripts/sense-voice/nano.py
+new file mode 120000
+index 00000000..be26565e
+--- /dev/null
++++ b/scripts/sense-voice/nano.py
+@@ -0,0 +1 @@
++rknn/nano.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/rknn/adaptor.py b/scripts/sense-voice/rknn/adaptor.py
+new file mode 100644
+index 00000000..5b84004d
+--- /dev/null
++++ b/scripts/sense-voice/rknn/adaptor.py
+@@ -0,0 +1,248 @@
++import torch
++from torch import nn
++
++import torch_model
++
++
++class MultiHeadedAttention(nn.Module):
++    """
++    This class is copied and modified from
++    https://github.com/modelscope/FunASR/blob/main/funasr/models/transformer/attention.py
++    """
++
++    def __init__(self, n_head, n_feat, dropout_rate):
++        super().__init__()
++        assert n_feat % n_head == 0
++
++        # We assume d_v always equals d_k
++        self.d_k = n_feat // n_head
++        self.h = n_head
++        self.linear_q = nn.Linear(n_feat, n_feat)
++        self.linear_k = nn.Linear(n_feat, n_feat)
++        self.linear_v = nn.Linear(n_feat, n_feat)
++        self.linear_out = nn.Linear(n_feat, n_feat)
++        self.attn = None
++        self.dropout = nn.Dropout(p=dropout_rate)
++
++    def forward_qkv(self, query, key, value):
++        """Transform query, key and value.
++
++        Args:
++            query (torch.Tensor): Query tensor (#batch, time1, size).
++            key (torch.Tensor): Key tensor (#batch, time2, size).
++            value (torch.Tensor): Value tensor (#batch, time2, size).
++
++        Returns:
++            torch.Tensor: Transformed query tensor (#batch, n_head, time1, d_k).
++            torch.Tensor: Transformed key tensor (#batch, n_head, time2, d_k).
++            torch.Tensor: Transformed value tensor (#batch, n_head, time2, d_k).
++
++        """
++        n_batch = query.size(0)
++        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
++        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
++        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
++        q = q.transpose(1, 2)  # (batch, head, time1, d_k)
++        k = k.transpose(1, 2)  # (batch, head, time2, d_k)
++        v = v.transpose(1, 2)  # (batch, head, time2, d_k)
++
++        return q, k, v
++
++    def forward_attention(self, value, scores, mask):
++        """Compute attention context vector.
++
++        Args:
++            value (torch.Tensor): Transformed value (#batch, n_head, time2, d_k).
++            scores (torch.Tensor): Attention score (#batch, n_head, time1, time2).
++            mask (torch.Tensor): Mask (#batch, 1, time2) or (#batch, time1, time2).
++
++        Returns:
++            torch.Tensor: Transformed value (#batch, time1, d_model)
++                weighted by the attention score (#batch, time1, time2).
++
++        """
++        n_batch = value.size(0)
++        if mask is not None:
++            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, *, time2)
++
++            min_value = -float(
++                "inf"
++            )  # min_value = float(np.finfo(torch.tensor(0, dtype=qk.dtype).numpy().dtype).min)
++            scores = scores.masked_fill(mask, min_value)
++            attn = torch.softmax(scores, dim=-1).masked_fill(
++                mask, 0.0
++            )  # (batch, head, time1, time2)
++        else:
++            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)
++
++        p_attn = self.dropout(attn)
++        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)
++        x = (
++            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
++        )  # (batch, time1, d_model)
++
++        return self.linear_out(x)  # (batch, time1, d_model)
++
++    def forward(self, query, key, value, mask):
++        """Compute scaled dot product attention.
++
++        Args:
++            query (torch.Tensor): Query tensor (#batch, time1, size).
++            key (torch.Tensor): Key tensor (#batch, time2, size).
++            value (torch.Tensor): Value tensor (#batch, time2, size).
++            mask (torch.Tensor): Mask tensor (#batch, 1, time2) or
++                (#batch, time1, time2).
++
++        Returns:
++            torch.Tensor: Output tensor (#batch, time1, d_model).
++
++        """
++        q, k, v = self.forward_qkv(query, key, value)
++        #  scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
++        scores = torch.matmul(q, k.transpose(-2, -1)) * self.d_k ** (-0.5)
++
++        return self.forward_attention(v, scores, mask)
++
++
++class EncoderLayer(nn.Module):
++    """
++    This class is copied and modified from
++    https://github.com/modelscope/FunASR/blob/main/funasr/models/transformer/encoder.py
++    """
++
++    def __init__(
++        self,
++        size,
++        self_attn,
++        feed_forward,
++        dropout_rate,
++        normalize_before=True,
++        concat_after=False,
++        stochastic_depth_rate=0.0,
++    ):
++        super().__init__()
++
++        self.self_attn = self_attn
++        self.feed_forward = feed_forward
++        self.norm1 = nn.LayerNorm(size, eps=1e-12)
++        self.norm2 = nn.LayerNorm(size, eps=1e-12)
++        self.dropout = nn.Dropout(dropout_rate)
++        self.size = size
++        self.normalize_before = normalize_before
++        self.concat_after = concat_after
++        if self.concat_after:
++            self.concat_linear = nn.Linear(size + size, size)
++        self.stochastic_depth_rate = stochastic_depth_rate
++
++    def forward(self, x, mask=None, cache=None):
++        """Compute encoded features.
++
++        Args:
++            x_input (torch.Tensor): Input tensor (#batch, time, size).
++            mask (torch.Tensor): Mask tensor for the input (#batch, time).
++            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).
++
++        Returns:
++            torch.Tensor: Output tensor (#batch, time, size).
++            torch.Tensor: Mask tensor (#batch, time).
++
++        """
++        skip_layer = False
++        # with stochastic depth, residual connection `x + f(x)` becomes
++        # `x <- x + 1 / (1 - p) * f(x)` at training time.
++        stoch_layer_coeff = 1.0
++
++        if skip_layer:
++            if cache is not None:
++                x = torch.cat([cache, x], dim=1)
++            return x, mask
++
++        residual = x
++        if self.normalize_before:
++            x = self.norm1(x)
++
++        if cache is None:
++            x_q = x
++        else:
++            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)
++            x_q = x[:, -1:, :]
++            residual = residual[:, -1:, :]
++            mask = None if mask is None else mask[:, -1:, :]
++
++        if self.concat_after:
++            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)
++            x = residual + stoch_layer_coeff * self.concat_linear(x_concat)
++        else:
++            x = residual + stoch_layer_coeff * self.dropout(
++                self.self_attn(x_q, x, x, mask)
++            )
++        if not self.normalize_before:
++            x = self.norm1(x)
++
++        residual = x
++        if self.normalize_before:
++            x = self.norm2(x)
++        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))
++        if not self.normalize_before:
++            x = self.norm2(x)
++
++        if cache is not None:
++            x = torch.cat([cache, x], dim=1)
++
++        return x, mask
++
++
++class Transformer(nn.Module):
++    # This class is copied and modified from
++    # https://github.com/modelscope/FunASR/blob/main/funasr/models/llm_asr/adaptor.py
++    def __init__(
++        self,
++        downsample_rate=1,
++        encoder_dim=512,
++        llm_dim=512,
++        ffn_dim: int = 2048,
++        n_layer: int = 5,
++        **kwargs
++    ):
++        super().__init__()
++        assert downsample_rate == 1, downsample_rate
++        self.k = downsample_rate
++        self.encoder_dim = encoder_dim
++        self.llm_dim = llm_dim
++        self.linear1 = nn.Linear(self.encoder_dim * self.k, ffn_dim)
++        self.relu = nn.ReLU()
++        self.linear2 = nn.Linear(ffn_dim, self.llm_dim)
++
++        self.blocks = None
++        if n_layer > 0:
++            self.blocks = nn.ModuleList(
++                [
++                    EncoderLayer(
++                        llm_dim,
++                        MultiHeadedAttention(
++                            kwargs.get("attention_heads", 8),
++                            llm_dim,
++                            kwargs.get("attention_dropout_rate", 0.0),
++                        ),
++                        torch_model.PositionwiseFeedForward(
++                            llm_dim,
++                            llm_dim // 4,
++                            kwargs.get("dropout_rate", 0.0),
++                        ),
++                        kwargs.get("dropout_rate", 0.0),
++                    )
++                    for i in range(n_layer)
++                ]
++            )
++
++    def forward(self, x):
++        x = self.linear1(x)
++        x = self.relu(x)
++        x = self.linear2(x)
++
++        masks = None
++
++        if self.blocks is not None:
++            for layer, block in enumerate(self.blocks):
++                x, masks = block(x, masks)
++        return x
+diff --git a/scripts/sense-voice/rknn/nano.py b/scripts/sense-voice/rknn/nano.py
+new file mode 100755
+index 00000000..aae5b96f
+--- /dev/null
++++ b/scripts/sense-voice/rknn/nano.py
+@@ -0,0 +1,31 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++from torch import nn
++
++import adaptor
++import torch_model
++
++
++class Nano(nn.Module):
++    def __init__(self, vocab_size: int = 60515):
++        super().__init__()
++        self.audio_encoder = torch_model.SenseVoiceEncoderSmall()
++        self.ctc_decoder = adaptor.Transformer()
++        # blank is 60514, i.e., the last token id
++        self.ctc = torch_model.CTC(
++            odim=vocab_size,
++            encoder_output_size=self.audio_encoder.output_size,
++        )
++
++    def forward(self, x):
++        """
++        Args:
++          x: (N, T, C)
++        Returns:
++          - logits: (N, T, vocab_size)
++        """
++        encoder_out = self.audio_encoder(x)
++        encoder_out = self.ctc_decoder(encoder_out)
++        logits = self.ctc.ctc_lo(encoder_out)
++        return logits
+diff --git a/scripts/sense-voice/rknn/test_nano_torch.py b/scripts/sense-voice/rknn/test_nano_torch.py
+new file mode 100755
+index 00000000..9616ba68
+--- /dev/null
++++ b/scripts/sense-voice/rknn/test_nano_torch.py
+@@ -0,0 +1,78 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import base64
++from pathlib import Path
++
++import torch
++
++import nano
++import test_onnx
++
++
++def load_tokens(filename: str = "./tokens.txt"):
++    id2token = dict()
++    with open(filename, encoding="utf-8") as f:
++        for line in f:
++            try:
++                f = line.strip().split()
++                if len(f) == 2:
++                    t, i = f
++                else:
++                    t = " "
++                    i = f[0]
++                id2token[int(i)] = t
++            except Exception as ex:
++                print(ex)
++                raise
++    return id2token
++
++
++def load_torch_model():
++    if not Path("./model.pt").is_file():
++        raise ValueError(
++            "Please download files from https://huggingface.co/csukuangfj/funasr-nano-with-ctc"
++        )
++    model = nano.Nano()
++
++    state_dict = torch.load("./model.pt", map_location="cpu")
++    model.load_state_dict(state_dict, strict=True)
++    model.eval()
++
++    del state_dict
++
++    return model
++
++
++@torch.no_grad()
++def main():
++    model = load_torch_model()
++
++    samples, sample_rate = test_onnx.load_audio("./zh.wav")
++    assert sample_rate == 16000, sample_rate
++
++    features = test_onnx.compute_feat(samples=samples, sample_rate=sample_rate)
++    x = torch.from_numpy(features)[None]
++    logits = model(x)
++
++    idx = logits.squeeze(0).argmax(dim=-1)
++    print(idx)
++    idx = torch.unique_consecutive(idx).tolist()
++    print(idx)
++
++    id2token = load_tokens("./tokens.txt")
++    blank_id = len(id2token) - 1
++
++    idx = [i for i in idx if i != blank_id]
++    print(idx)
++
++    s = b""
++    for i in idx:
++        s += base64.b64decode(id2token[i])
++
++    text = s.decode().strip()
++    print(text)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/sense-voice/rknn/test_onnx.py b/scripts/sense-voice/rknn/test_onnx.py
+index d4ac38bc..eda42584 100755
+--- a/scripts/sense-voice/rknn/test_onnx.py
++++ b/scripts/sense-voice/rknn/test_onnx.py
+@@ -1,12 +1,16 @@
+ #!/usr/bin/env python3
+ # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+ 
++"""
++Note: This is for testing the onnx models that would be later used to export
++to RKNN
++"""
++
+ import argparse
+ from typing import Tuple
+ 
+ import kaldi_native_fbank as knf
+ import numpy as np
+-import onnxruntime
+ import onnxruntime as ort
+ import soundfile as sf
+ import torch
+@@ -132,7 +136,7 @@ def load_tokens(filename):
+ def compute_feat(
+     samples,
+     sample_rate,
+-    max_len: int,
++    max_len: int = -1,
+     window_size: int = 7,  # lfr_m
+     window_shift: int = 6,  # lfr_n
+ ):
+@@ -162,17 +166,19 @@ def compute_feat(
+ 
+     print("features.shape", features.shape)
+ 
+-    if features.shape[0] > max_len:
+-        features = features[:max_len]
+-    elif features.shape[0] < max_len:
+-        features = np.pad(
+-            features,
+-            ((0, max_len - features.shape[0]), (0, 0)),
+-            mode="constant",
+-            constant_values=0,
+-        )
++    if max_len > 0:
++        if features.shape[0] > max_len:
++            features = features[:max_len]
++        elif features.shape[0] < max_len:
++            features = np.pad(
++                features,
++                ((0, max_len - features.shape[0]), (0, 0)),
++                mode="constant",
++                constant_values=0,
++            )
+ 
+     print("features.shape", features.shape)
++    features = np.ascontiguousarray(features)
+ 
+     return features
+ 
+diff --git a/scripts/sense-voice/rknn/torch_model.py b/scripts/sense-voice/rknn/torch_model.py
+index cdd3bd37..700fabfd 100644
+--- a/scripts/sense-voice/rknn/torch_model.py
++++ b/scripts/sense-voice/rknn/torch_model.py
+@@ -344,17 +344,6 @@ class LayerNorm(nn.LayerNorm):
+         return output.type_as(input)
+ 
+ 
+-def sequence_mask(lengths, maxlen=None, dtype=torch.float32, device=None):
+-    if maxlen is None:
+-        maxlen = lengths.max()
+-    row_vector = torch.arange(0, maxlen, 1).to(lengths.device)
+-    matrix = torch.unsqueeze(lengths, dim=-1)
+-    mask = row_vector < matrix
+-    mask = mask.detach()
+-
+-    return mask.type(dtype).to(device) if device is not None else mask.type(dtype)
+-
+-
+ class SenseVoiceEncoderSmall(nn.Module):
+     def __init__(self):
+         super().__init__()
+diff --git a/scripts/sense-voice/test_nano_torch.py b/scripts/sense-voice/test_nano_torch.py
+new file mode 120000
+index 00000000..14404d3c
+--- /dev/null
++++ b/scripts/sense-voice/test_nano_torch.py
+@@ -0,0 +1 @@
++rknn/test_nano_torch.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/test_onnx.py b/scripts/sense-voice/test_onnx.py
+new file mode 120000
+index 00000000..68a6a695
+--- /dev/null
++++ b/scripts/sense-voice/test_onnx.py
+@@ -0,0 +1 @@
++rknn/test_onnx.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/test_onnx_nano.py b/scripts/sense-voice/test_onnx_nano.py
+new file mode 100755
+index 00000000..7bb1fd5a
+--- /dev/null
++++ b/scripts/sense-voice/test_onnx_nano.py
+@@ -0,0 +1,146 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++"""
++=========./model.onnx==========
++NodeArg(name='x', type='tensor(float)', shape=[1, 'T', 560])
++-----
++NodeArg(name='logits', type='tensor(float)', shape=['Addlogits_dim_0', 'Addlogits_dim_1', 60515])
++
++=========./model.int8.onnx==========
++NodeArg(name='x', type='tensor(float)', shape=[1, 'T', 560])
++-----
++NodeArg(name='logits', type='tensor(float)', shape=['Addlogits_dim_0', 'Addlogits_dim_1', 60515])
++"""
++
++import argparse
++import base64
++from typing import Tuple
++
++from test_onnx import compute_feat, load_audio
++
++import onnxruntime as ort
++import librosa
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--model",
++        type=str,
++        required=True,
++        help="Path to model.onnx",
++    )
++
++    parser.add_argument(
++        "--tokens",
++        type=str,
++        required=True,
++        help="Path to tokens.txt",
++    )
++
++    parser.add_argument(
++        "--wave",
++        type=str,
++        required=True,
++        help="The input wave to be recognized",
++    )
++
++    return parser.parse_args()
++
++
++class OnnxModel:
++    def __init__(self, filename):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = 1
++        session_opts.intra_op_num_threads = 1
++
++        self.session_opts = session_opts
++
++        self.model = ort.InferenceSession(
++            filename,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++
++        meta = self.model.get_modelmeta().custom_metadata_map
++
++        self.window_size = int(meta["lfr_window_size"])  # lfr_m
++        self.window_shift = int(meta["lfr_window_shift"])  # lfr_n
++        self.blank_id = int(meta["blank_id"])
++
++    def __call__(self, x):
++        logits = self.model.run(
++            [
++                self.model.get_outputs()[0].name,
++            ],
++            {
++                self.model.get_inputs()[0].name: x,
++            },
++        )[0]
++
++        return logits
++
++
++def load_tokens(filename: str):
++    ans = dict()
++    i = 0
++    with open(filename, encoding="utf-8") as f:
++        for line in f:
++            ans[i] = line.strip().split()[0]
++            i += 1
++    return ans
++
++
++def main():
++    args = get_args()
++    print(vars(args))
++    samples, sample_rate = load_audio(args.wave)
++    if sample_rate != 16000:
++        samples = librosa.resample(samples, orig_sr=sample_rate, target_sr=16000)
++        sample_rate = 16000
++
++    model = OnnxModel(filename=args.model)
++
++    features = compute_feat(
++        samples=samples,
++        sample_rate=sample_rate,
++        window_size=model.window_size,
++        window_shift=model.window_shift,
++    )
++
++    logits = model(
++        x=features[None],
++    )
++
++    idx = logits[0].argmax(axis=-1)
++    print("initial ids", idx)
++    id2token = load_tokens(args.tokens)
++    blank_id = model.blank_id
++    print("blank_id", blank_id)
++
++    unique_ids = []
++    prev = -1
++    for i in idx:
++        if i == prev:
++            continue
++        unique_ids.append(i)
++        prev = i
++    print("unique_ids", unique_ids)
++
++    ids = [i for i in unique_ids if i != blank_id]
++
++    print("ids without blank", ids)
++    s = b""
++    for i in ids:
++        s += base64.b64decode(id2token[i])
++
++    text = s.decode().strip()
++    print(text)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/sense-voice/torch_model.py b/scripts/sense-voice/torch_model.py
+new file mode 120000
+index 00000000..5c7bba09
+--- /dev/null
++++ b/scripts/sense-voice/torch_model.py
+@@ -0,0 +1 @@
++rknn/torch_model.py
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+index b172d76c..02c06b44 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h
+@@ -24,14 +24,18 @@ namespace sherpa_onnx {
+ 
+ OfflineRecognitionResult ConvertSenseVoiceResult(
+     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor) {
++    int32_t frame_shift_ms, int32_t subsampling_factor,
++    bool is_funasr_nano = false) {
+   OfflineRecognitionResult r;
+   r.tokens.reserve(src.tokens.size());
+   r.timestamps.reserve(src.timestamps.size());
+ 
+   std::string text;
+ 
+-  for (int32_t i = 4; i < src.tokens.size(); ++i) {
++  // Funasr NanO does not support emotion, event, language, etc.
++  int32_t start = is_funasr_nano ? 0 : 4;
++
++  for (int32_t i = start; i < src.tokens.size(); ++i) {
+     auto sym = sym_table[src.tokens[i]];
+     text.append(sym);
+ 
+@@ -41,18 +45,20 @@ OfflineRecognitionResult ConvertSenseVoiceResult(
+ 
+   float frame_shift_s = frame_shift_ms / 1000. * subsampling_factor;
+ 
+-  for (int32_t i = 4; i < src.timestamps.size(); ++i) {
+-    float time = frame_shift_s * (src.timestamps[i] - 4);
++  for (int32_t i = start; i < src.timestamps.size(); ++i) {
++    float time = frame_shift_s * (src.timestamps[i] - start);
+     r.timestamps.push_back(time);
+   }
+ 
+   r.words = std::move(src.words);
+ 
+-  // parse lang, emotion and event from tokens.
+-  if (src.tokens.size() >= 3) {
+-    r.lang = sym_table[src.tokens[0]];
+-    r.emotion = sym_table[src.tokens[1]];
+-    r.event = sym_table[src.tokens[2]];
++  if (!is_funasr_nano) {
++    // parse lang, emotion and event from tokens.
++    if (src.tokens.size() >= 3) {
++      r.lang = sym_table[src.tokens[0]];
++      r.emotion = sym_table[src.tokens[1]];
++      r.event = sym_table[src.tokens[2]];
++    }
+   }
+ 
+   return r;
+@@ -75,7 +81,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+       SHERPA_ONNX_EXIT(-1);
+     }
+ 
+-    InitFeatConfig();
++    PostInit();
+   }
+ 
+   template <typename Manager>
+@@ -95,7 +101,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+       SHERPA_ONNX_EXIT(-1);
+     }
+ 
+-    InitFeatConfig();
++    PostInit();
+   }
+ 
+   std::unique_ptr<OfflineStream> CreateStream() const override {
+@@ -103,12 +109,21 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+   }
+ 
+   void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    if (meta_data.is_funasr_nano) {
++      for (int32_t i = 0; i < n; ++i) {
++        DecodeOneStreamFunAsrNano(ss[i]);
++      }
++
++      return;
++    }
++
+     if (n == 1) {
+       DecodeOneStream(ss[0]);
+       return;
+     }
+ 
+-    const auto &meta_data = model_->GetModelMetadata();
+     // 1. Apply LFR
+     // 2. Apply CMVN
+     //
+@@ -229,6 +244,47 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+   OfflineRecognizerConfig GetConfig() const override { return config_; }
+ 
+  private:
++  void DecodeOneStreamFunAsrNano(OfflineStream *s) const {
++    const auto &meta_data = model_->GetModelMetadata();
++    auto memory_info =
++        Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
++
++    int32_t feat_dim = config_.feat_config.feature_dim * meta_data.window_size;
++    std::vector<float> f = s->GetFrames();
++    f = ApplyLFR(f);
++
++    int32_t num_frames = f.size() / feat_dim;
++    std::array<int64_t, 3> shape = {1, num_frames, feat_dim};
++    Ort::Value x = Ort::Value::CreateTensor(memory_info, f.data(), f.size(),
++                                            shape.data(), shape.size());
++
++    Ort::Value logits{nullptr};
++    try {
++      logits = model_->Forward(std::move(x));
++    } catch (const Ort::Exception &ex) {
++      SHERPA_ONNX_LOGE("\n\nCaught exception:\n\n%s\n\nReturn an empty result",
++                       ex.what());
++      return;
++    }
++
++    int64_t new_num_frames = logits.GetTensorTypeAndShapeInfo().GetShape()[1];
++    int64_t num_frame_shape = 1;
++    Ort::Value logits_length = Ort::Value::CreateTensor(
++        memory_info, &new_num_frames, 1, &num_frame_shape, 1);
++
++    auto results =
++        decoder_->Decode(std::move(logits), std::move(logits_length));
++
++    int32_t frame_shift_ms = 10;
++    int32_t subsampling_factor = meta_data.window_shift;
++    auto r = ConvertSenseVoiceResult(results[0], symbol_table_, frame_shift_ms,
++                                     subsampling_factor, true);
++
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
+   void DecodeOneStream(OfflineStream *s) const {
+     const auto &meta_data = model_->GetModelMetadata();
+ 
+@@ -299,6 +355,15 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+     s->SetResult(r);
+   }
+ 
++  void PostInit() {
++    InitFeatConfig();
++
++    const auto &meta_data = model_->GetModelMetadata();
++    if (meta_data.is_funasr_nano) {
++      symbol_table_.ApplyBase64Decode();
++    }
++  }
++
+   void InitFeatConfig() {
+     const auto &meta_data = model_->GetModelMetadata();
+ 
+@@ -307,6 +372,7 @@ class OfflineRecognizerSenseVoiceImpl : public OfflineRecognizerImpl {
+     config_.feat_config.high_freq = 0;
+     config_.feat_config.snip_edges = true;
+   }
++
+   std::vector<float> ApplyLFR(const std::vector<float> &in) const {
+     const auto &meta_data = model_->GetModelMetadata();
+ 
+diff --git a/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+index bdaeb855..e5bfcb94 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+@@ -21,7 +21,8 @@ namespace sherpa_onnx {
+ // defined in ../offline-recognizer-sense-voice-impl.h
+ OfflineRecognitionResult ConvertSenseVoiceResult(
+     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor);
++    int32_t frame_shift_ms, int32_t subsampling_factor,
++    bool is_funasr_nano /*= false*/);
+ 
+ template <typename SenseVoiceModel>
+ class OfflineRecognizerSenseVoiceTplImpl : public OfflineRecognizerImpl {
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h b/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
+index f8c858ce..207f329b 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h
+@@ -45,6 +45,8 @@ struct OfflineSenseVoiceModelMetaData {
+ 
+   std::vector<float> neg_mean;    // not used in rk npu and ascend npu
+   std::vector<float> inv_stddev;  // not used in rk npu and ascend npu
++
++  bool is_funasr_nano = false;
+ };
+ 
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.cc b/sherpa-onnx/csrc/offline-sense-voice-model.cc
+index 588cdd37..94f0296e 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model.cc
+@@ -63,6 +63,12 @@ class OfflineSenseVoiceModel::Impl {
+     return std::move(ans[0]);
+   }
+ 
++  Ort::Value Forward(Ort::Value features) {
++    auto ans = sess_->Run({}, input_names_ptr_.data(), &features, 1,
++                          output_names_ptr_.data(), output_names_ptr_.size());
++    return std::move(ans[0]);
++  }
++
+   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+     return meta_data_;
+   }
+@@ -91,37 +97,47 @@ class OfflineSenseVoiceModel::Impl {
+     }
+ 
+     Ort::AllocatorWithDefaultOptions allocator;  // used in the macro below
++
++    std::string comment;
++    SHERPA_ONNX_READ_META_DATA_STR_ALLOW_EMPTY(comment, "comment");
++
++    meta_data_.is_funasr_nano = Contains(comment, "Nano");
++
+     SHERPA_ONNX_READ_META_DATA(meta_data_.vocab_size, "vocab_size");
++    SHERPA_ONNX_READ_META_DATA_WITH_DEFAULT(meta_data_.blank_id, "blank_id", 0);
++
+     SHERPA_ONNX_READ_META_DATA(meta_data_.window_size, "lfr_window_size");
+     SHERPA_ONNX_READ_META_DATA(meta_data_.window_shift, "lfr_window_shift");
+     SHERPA_ONNX_READ_META_DATA(meta_data_.normalize_samples,
+                                "normalize_samples");
+ 
+-    SHERPA_ONNX_READ_META_DATA(meta_data_.with_itn_id, "with_itn");
++    if (!meta_data_.is_funasr_nano) {
++      SHERPA_ONNX_READ_META_DATA(meta_data_.with_itn_id, "with_itn");
+ 
+-    SHERPA_ONNX_READ_META_DATA(meta_data_.without_itn_id, "without_itn");
++      SHERPA_ONNX_READ_META_DATA(meta_data_.without_itn_id, "without_itn");
+ 
+-    int32_t lang_auto = 0;
+-    int32_t lang_zh = 0;
+-    int32_t lang_en = 0;
+-    int32_t lang_ja = 0;
+-    int32_t lang_ko = 0;
+-    int32_t lang_yue = 0;
++      int32_t lang_auto = 0;
++      int32_t lang_zh = 0;
++      int32_t lang_en = 0;
++      int32_t lang_ja = 0;
++      int32_t lang_ko = 0;
++      int32_t lang_yue = 0;
+ 
+-    SHERPA_ONNX_READ_META_DATA(lang_auto, "lang_auto");
+-    SHERPA_ONNX_READ_META_DATA(lang_zh, "lang_zh");
+-    SHERPA_ONNX_READ_META_DATA(lang_en, "lang_en");
+-    SHERPA_ONNX_READ_META_DATA(lang_ja, "lang_ja");
+-    SHERPA_ONNX_READ_META_DATA(lang_ko, "lang_ko");
+-    SHERPA_ONNX_READ_META_DATA(lang_yue, "lang_yue");
++      SHERPA_ONNX_READ_META_DATA(lang_auto, "lang_auto");
++      SHERPA_ONNX_READ_META_DATA(lang_zh, "lang_zh");
++      SHERPA_ONNX_READ_META_DATA(lang_en, "lang_en");
++      SHERPA_ONNX_READ_META_DATA(lang_ja, "lang_ja");
++      SHERPA_ONNX_READ_META_DATA(lang_ko, "lang_ko");
++      SHERPA_ONNX_READ_META_DATA(lang_yue, "lang_yue");
+ 
+-    meta_data_.lang2id = {
+-        {"auto", lang_auto}, {"zh", lang_zh}, {"en", lang_en},
+-        {"ja", lang_ja},     {"ko", lang_ko}, {"yue", lang_yue},
+-    };
++      meta_data_.lang2id = {
++          {"auto", lang_auto}, {"zh", lang_zh}, {"en", lang_en},
++          {"ja", lang_ja},     {"ko", lang_ko}, {"yue", lang_yue},
++      };
+ 
+-    SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.neg_mean, "neg_mean");
+-    SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.inv_stddev, "inv_stddev");
++      SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.neg_mean, "neg_mean");
++      SHERPA_ONNX_READ_META_DATA_VEC_FLOAT(meta_data_.inv_stddev, "inv_stddev");
++    }
+   }
+ 
+  private:
+@@ -159,6 +175,10 @@ Ort::Value OfflineSenseVoiceModel::Forward(Ort::Value features,
+                         std::move(language), std::move(text_norm));
+ }
+ 
++Ort::Value OfflineSenseVoiceModel::Forward(Ort::Value features) const {
++  return impl_->Forward(std::move(features));
++}
++
+ const OfflineSenseVoiceModelMetaData &OfflineSenseVoiceModel::GetModelMetadata()
+     const {
+   return impl_->GetModelMetadata();
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.h b/sherpa-onnx/csrc/offline-sense-voice-model.h
+index e82680c5..87904657 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model.h
++++ b/sherpa-onnx/csrc/offline-sense-voice-model.h
+@@ -39,6 +39,13 @@ class OfflineSenseVoiceModel {
+   Ort::Value Forward(Ort::Value features, Ort::Value features_length,
+                      Ort::Value language, Ort::Value text_norm) const;
+ 
++  /** For FunASR-Nano
++   *
++   * @param features A tensor of shape (1, T, C) with dtype float32
++   * @return Return logits of shape (1, T, C) with dtype float32
++   */
++  Ort::Value Forward(Ort::Value features) const;
++
+   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
+ 
+   /** Return an allocator for allocating memory
+diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
+index eafd4958..92925cb5 100644
+--- a/sherpa-onnx/csrc/symbol-table.cc
++++ b/sherpa-onnx/csrc/symbol-table.cc
+@@ -234,7 +234,14 @@ std::ostream &operator<<(std::ostream &os, const SymbolTable &symbol_table) {
+ void SymbolTable::ApplyBase64Decode() {
+   sym2id_.clear();
+   for (auto &p : id2sym_) {
+-    p.second = Base64Decode(p.second);
++    if (p.second == " ") {
++      // for FunASR nano models, there is an empty string in the tokens.txt,
++      // which is converted to " " while reading it in sherpa-onnx. We convert
++      // it back to "" here
++      p.second = "";
++    } else {
++      p.second = Base64Decode(p.second);
++    }
+     sym2id_[p.second] = p.first;
+   }
+ }
+
+commit 5f0049bd6aae3bf90eb3d5a0d13df4427bee1306
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 17 14:24:01 2025 +0800
+
+    Fix typos in URL (#2905)
+
+diff --git a/scripts/nemo/GigaAM/run-ctc-v2.sh b/scripts/nemo/GigaAM/run-ctc-v2.sh
+index 4dc4e3f8..667bd908 100755
+--- a/scripts/nemo/GigaAM/run-ctc-v2.sh
++++ b/scripts/nemo/GigaAM/run-ctc-v2.sh
+@@ -17,7 +17,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+diff --git a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
+index a03a350c..c4af3c2c 100755
+--- a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
++++ b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
+@@ -17,7 +17,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+diff --git a/scripts/nemo/GigaAM/run-ctc-v3.sh b/scripts/nemo/GigaAM/run-ctc-v3.sh
+index 0273935a..be0a4da9 100755
+--- a/scripts/nemo/GigaAM/run-ctc-v3.sh
++++ b/scripts/nemo/GigaAM/run-ctc-v3.sh
+@@ -17,7 +17,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+diff --git a/scripts/nemo/GigaAM/run-rnnt-v2.sh b/scripts/nemo/GigaAM/run-rnnt-v2.sh
+index bc9fa82e..af833876 100755
+--- a/scripts/nemo/GigaAM/run-rnnt-v2.sh
++++ b/scripts/nemo/GigaAM/run-rnnt-v2.sh
+@@ -18,7 +18,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+diff --git a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
+index fd13ceb3..8972f956 100755
+--- a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
++++ b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
+@@ -18,7 +18,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+diff --git a/scripts/nemo/GigaAM/run-rnnt-v3.sh b/scripts/nemo/GigaAM/run-rnnt-v3.sh
+index 42dcbcb5..1c7f5e1f 100755
+--- a/scripts/nemo/GigaAM/run-rnnt-v3.sh
++++ b/scripts/nemo/GigaAM/run-rnnt-v3.sh
+@@ -18,7 +18,7 @@ function install_gigaam() {
+ 
+ function download_files() {
+   curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
+-  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++  curl -SL -O https://raw.githubusercontent.com/salute-developers/GigaAM/main/LICENSE
+ }
+ 
+ install_gigaam
+
+commit 79d7e135c42060a587d64382267de754c7b39a83
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Dec 16 13:49:14 2025 +0800
+
+    Export GigaAM v3 to sherpa-onnx (#2901)
+
+diff --git a/.github/workflows/export-nemo-giga-am-to-onnx.yaml b/.github/workflows/export-nemo-giga-am-to-onnx.yaml
+index 2636d73c..defae973 100644
+--- a/.github/workflows/export-nemo-giga-am-to-onnx.yaml
++++ b/.github/workflows/export-nemo-giga-am-to-onnx.yaml
+@@ -1,6 +1,9 @@
+ name: export-nemo-giga-am-to-onnx
+ 
+ on:
++  push:
++    branches:
++      - export-giga-am-v3
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -27,6 +30,7 @@ jobs:
+           python-version: ${{ matrix.python-version }}
+ 
+       - name: Run CTC
++        if: false
+         shell: bash
+         run: |
+           pushd scripts/nemo/GigaAM
+@@ -53,6 +57,7 @@ jobs:
+           tar cjvf ${d}.tar.bz2 $d
+ 
+       - name: Run Transducer
++        if: false
+         shell: bash
+         run: |
+           pushd scripts/nemo/GigaAM
+@@ -82,6 +87,7 @@ jobs:
+           tar cjvf ${d}.tar.bz2 $d
+ 
+       - name: Run CTC v2
++        if: false
+         shell: bash
+         run: |
+           pushd scripts/nemo/GigaAM
+@@ -107,6 +113,7 @@ jobs:
+           tar cjvf ${d}.tar.bz2 $d
+ 
+       - name: Run Transducer v2
++        if: false
+         shell: bash
+         run: |
+           pushd scripts/nemo/GigaAM
+@@ -134,6 +141,123 @@ jobs:
+ 
+           tar cjvf ${d}.tar.bz2 $d
+ 
++      - name: Run CTC v3
++        if: true
++        shell: bash
++        run: |
++          pushd scripts/nemo/GigaAM
++          ./run-ctc-v3.sh
++          popd
++
++          d=sherpa-onnx-nemo-ctc-giga-am-v3-russian-2025-12-16
++          mkdir $d
++          mkdir $d/test_wavs
++          ls -lh scripts/nemo/GigaAM/v3_ctc.onnx
++          rm scripts/nemo/GigaAM/v3_ctc.onnx
++          cp -v scripts/nemo/GigaAM/*.md $d/
++          mv -v scripts/nemo/GigaAM/*.int8.onnx $d/
++          cp -v scripts/nemo/GigaAM/LICENSE $d/
++          mv -v scripts/nemo/GigaAM/tokens.txt $d/
++          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
++          mv -v scripts/nemo/GigaAM/run-ctc-v3.sh $d/
++          mv -v scripts/nemo/GigaAM/*-ctc-v3.py $d/
++          cp -v scripts/nemo/GigaAM/test-onnx-ctc.py $d/
++
++          ls -lh scripts/nemo/GigaAM/
++
++          ls -lh $d
++
++          tar cjvf ${d}.tar.bz2 $d
++
++          ls -lh *.tar.bz2
++
++      - name: Run CTC v3 with punctuations
++        if: true
++        shell: bash
++        run: |
++          pushd scripts/nemo/GigaAM
++          ./run-ctc-v3-punct.sh
++          popd
++
++          d=sherpa-onnx-nemo-ctc-punct-giga-am-v3-russian-2025-12-16
++          mkdir $d
++          mkdir $d/test_wavs
++          rm scripts/nemo/GigaAM/v3_e2e_ctc.onnx
++          cp -v scripts/nemo/GigaAM/*.md $d/
++          mv -v scripts/nemo/GigaAM/*.int8.onnx $d/
++          cp -v scripts/nemo/GigaAM/LICENSE $d/
++          mv -v scripts/nemo/GigaAM/tokens.txt $d/
++          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
++          mv -v scripts/nemo/GigaAM/run-ctc-v3-punct.sh $d/
++          mv -v scripts/nemo/GigaAM/*-ctc-v3-punct.py $d/
++          cp -v scripts/nemo/GigaAM/test-onnx-ctc.py $d/
++
++          ls -lh scripts/nemo/GigaAM/
++
++          ls -lh $d
++
++          tar cjvf ${d}.tar.bz2 $d
++
++          ls -lh *.tar.bz2
++
++      - name: Run Transducer v3
++        if: false
++        shell: bash
++        run: |
++          pushd scripts/nemo/GigaAM
++          ./run-rnnt-v3.sh
++          popd
++
++          d=sherpa-onnx-nemo-transducer-giga-am-v3-russian-2025-12-16
++          mkdir $d
++          mkdir $d/test_wavs
++
++          mv -v scripts/nemo/GigaAM/encoder.int8.onnx $d/
++          mv -v scripts/nemo/GigaAM/decoder.onnx $d/
++          mv -v scripts/nemo/GigaAM/joiner.onnx $d/
++
++          cp -v scripts/nemo/GigaAM/*.md $d/
++          cp -v scripts/nemo/GigaAM/LICENSE $d/
++          mv -v scripts/nemo/GigaAM/tokens.txt $d/
++          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
++          mv -v scripts/nemo/GigaAM/run-rnnt-v3.sh $d/
++          cp -v scripts/nemo/GigaAM/test-onnx-rnnt.py $d/
++
++          ls -lh scripts/nemo/GigaAM/
++
++          ls -lh $d
++
++          tar cjvf ${d}.tar.bz2 $d
++
++      - name: Run Transducer v3 with punctuations
++        if: false
++        shell: bash
++        run: |
++          pushd scripts/nemo/GigaAM
++          ./run-rnnt-v3-punct.sh
++          popd
++
++          d=sherpa-onnx-nemo-transducer-punct-giga-am-v3-russian-2025-12-16
++          mkdir $d
++          mkdir $d/test_wavs
++
++          mv -v scripts/nemo/GigaAM/encoder.int8.onnx $d/
++          mv -v scripts/nemo/GigaAM/decoder.onnx $d/
++          mv -v scripts/nemo/GigaAM/joiner.onnx $d/
++
++          cp -v scripts/nemo/GigaAM/*.md $d/
++          cp -v scripts/nemo/GigaAM/LICENSE $d/
++          mv -v scripts/nemo/GigaAM/tokens.txt $d/
++          mv -v scripts/nemo/GigaAM/*.wav $d/test_wavs/
++          mv -v scripts/nemo/GigaAM/run-rnnt-v3-punct.sh $d/
++          cp -v scripts/nemo/GigaAM/test-onnx-rnnt.py $d/
++
++          ls -lh scripts/nemo/GigaAM/
++
++          ls -lh $d
++
++          tar cjvf ${d}.tar.bz2 $d
++
+       - name: Release
+         if: github.repository_owner == 'csukuangfj'
+         uses: svenstaro/upload-release-action@v2
+@@ -155,6 +279,7 @@ jobs:
+           tag: asr-models
+ 
+       - name: Publish to huggingface (CTC)
++        if: false
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+         uses: nick-fields/retry@v3
+@@ -182,6 +307,7 @@ jobs:
+             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+ 
+       - name: Publish to huggingface (Transducer)
++        if: false
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+         uses: nick-fields/retry@v3
+@@ -209,6 +335,7 @@ jobs:
+             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+ 
+       - name: Publish v2 to huggingface (CTC)
++        if: false
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+         uses: nick-fields/retry@v3
+@@ -236,6 +363,7 @@ jobs:
+             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
+ 
+       - name: Publish v2 to huggingface (Transducer)
++        if: false
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+         uses: nick-fields/retry@v3
+@@ -261,3 +389,44 @@ jobs:
+             git status
+             git commit -m "add models"
+             git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
++
++      - name: Publish v3 to huggingface
++        if: true
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 5
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            names=(
++             sherpa-onnx-nemo-ctc-giga-am-v3-russian-2025-12-16
++             sherpa-onnx-nemo-ctc-punct-giga-am-v3-russian-2025-12-16
++             sherpa-onnx-nemo-transducer-giga-am-v3-russian-2025-12-16
++             sherpa-onnx-nemo-transducer-punct-giga-am-v3-russian-2025-12-16
++            )
++            for d in ${names[@]}; do
++              if [ ! -d $d ]; then
++                echo "$d does not exist - skip it"
++                continue;
++              fi
++
++              export GIT_LFS_SKIP_SMUDGE=1
++              export GIT_CLONE_PROTECTION_ACTIVE=false
++              rm -rf huggingface
++              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
++              cp -av $d/* ./huggingface
++              cd huggingface
++              git lfs track "*.onnx"
++              git lfs track "*.wav"
++              git status
++              git add .
++              git status
++              git commit -m "add models"
++              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main
++              cd ..
++            done
+diff --git a/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py b/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py
+new file mode 100755
+index 00000000..b6f801de
+--- /dev/null
++++ b/scripts/nemo/GigaAM/export-onnx-ctc-v3-punct.py
+@@ -0,0 +1,88 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import gigaam
++import onnx
++import torch
++from onnxruntime.quantization import QuantType, quantize_dynamic
++
++"""
++==========Input==========
++NodeArg(name='features', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
++NodeArg(name='feature_lengths', type='tensor(int64)', shape=['batch_size'])
++==========Output==========
++NodeArg(name='log_probs', type='tensor(float)', shape=['batch_size', 'seq_len', 257])
++"""
++
++
++def add_meta_data(filename: str, meta_data: dict[str, str]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++"""
++{'model_class': 'ctc', 'sample_rate': 16000, 'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor',
++'sample_rate': 16000, 'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
++'mel_norm': None, 'center': False}, 'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64,
++'n_layers': 16, 'd_model': 768, 'subsampling': 'conv1d', 'subs_kernel_size': 5, 'subsampling_factor': 4,
++'ff_expansion_factor': 4, 'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
++'conv_kernel_size': 5, 'flash_attn': False, 'conv_norm_type': 'layer_norm'}, 'head': {'_target_':
++'gigaam.decoder.CTCHead', 'feat_in': 768, 'num_classes': 257}, 'decoding': {'_target_':
++'gigaam.decoding.CTCGreedyDecoding', 'vocabulary': None,
++'model_path': '/root/.cache/gigaam/v3_e2e_ctc_tokenizer.model'},
++'model_name': 'v3_e2e_ctc', 'hashes': {'model': 'c15fd0dbca70363a146016d197ee0e2a',
++'tokenizer': '2a9cd0c246db42d076e92abb31055deb'}}
++"""
++
++
++def main() -> None:
++    model_name = "v3_e2e_ctc"
++    model = gigaam.load_model(model_name)
++
++    # <blk> is the last token
++    sp = model.decoding.tokenizer.model
++    with open("./tokens.txt", "w", encoding="utf-8") as f:
++        for i in range(sp.vocab_size()):
++            f.write(f"{sp.id_to_piece(i)} {i}\n")
++
++        f.write(f"<blk> {i+1}\n")
++        print("Saved to tokens.txt")
++    model.to_onnx(".")
++    meta_data = {
++        "vocab_size": sp.vocab_size() + 1,
++        "normalize_type": "",
++        "subsampling_factor": 4,
++        "model_type": "EncDecCTCModel",
++        "version": "1",
++        "model_author": "https://github.com/salute-developers/GigaAM",
++        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
++        "language": "Russian",
++        "comment": "v3 with puncutations",
++        "is_giga_am": 1,
++    }
++    add_meta_data(f"./{model_name}.onnx", meta_data)
++    quantize_dynamic(
++        model_input=f"./{model_name}.onnx",
++        model_output="./model.int8.onnx",
++        weight_type=QuantType.QUInt8,
++    )
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/nemo/GigaAM/export-onnx-ctc-v3.py b/scripts/nemo/GigaAM/export-onnx-ctc-v3.py
+new file mode 100755
+index 00000000..227f30af
+--- /dev/null
++++ b/scripts/nemo/GigaAM/export-onnx-ctc-v3.py
+@@ -0,0 +1,87 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import gigaam
++import onnx
++import torch
++from onnxruntime.quantization import QuantType, quantize_dynamic
++
++"""
++NodeArg(name='features', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
++NodeArg(name='feature_lengths', type='tensor(int64)', shape=['batch_size'])
++-----
++NodeArg(name='log_probs', type='tensor(float)', shape=['batch_size', 'seq_len', 34])
++"""
++
++
++def add_meta_data(filename: str, meta_data: dict[str, str]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++"""
++{'model_class': 'ctc', 'sample_rate': 16000,
++'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000, 'features': 64,
++'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320, 'mel_norm': None, 'center': False},
++'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16, 'd_model': 768,
++'subsampling': 'conv1d', 'subs_kernel_size': 5, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
++'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16, 'conv_kernel_size': 5,
++'flash_attn': False, 'conv_norm_type': 'layer_norm'}, 'head': {'_target_': 'gigaam.decoder.CTCHead',
++'feat_in': 768, 'num_classes': 34}, 'decoding': {'_target_': 'gigaam.decoding.CTCGreedyDecoding',
++'vocabulary': [' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',
++'', '', '', '', '', '', '', '', '', '', '', '', '', '']}, 'model_name': 'v3_ctc',
++'hashes': {'model': '1bdc12052560591b7cdf35bef02619fa'}}
++"""
++
++
++def main() -> None:
++    model_name = "v3_ctc"
++    model = gigaam.load_model(model_name)
++
++    # use characters
++    # space is 0
++    # <blk> is the last token
++    with open("./tokens.txt", "w", encoding="utf-8") as f:
++        for i, s in enumerate(model.cfg["decoding"]["vocabulary"]):
++            f.write(f"{s} {i}\n")
++        f.write(f"<blk> {i+1}\n")
++        print("Saved to tokens.txt")
++    model.to_onnx(".")
++    meta_data = {
++        "vocab_size": len(model.cfg["decoding"]["vocabulary"]) + 1,
++        "normalize_type": "",
++        "subsampling_factor": 4,
++        "model_type": "EncDecCTCModel",
++        "version": "1",
++        "model_author": "https://github.com/salute-developers/GigaAM",
++        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
++        "language": "Russian",
++        "comment": "v3",
++        "is_giga_am": 1,
++    }
++    add_meta_data(f"./{model_name}.onnx", meta_data)
++    quantize_dynamic(
++        model_input=f"./{model_name}.onnx",
++        model_output="./model.int8.onnx",
++        weight_type=QuantType.QUInt8,
++    )
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py b/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py
+new file mode 100755
+index 00000000..b2791aae
+--- /dev/null
++++ b/scripts/nemo/GigaAM/export-onnx-rnnt-v3-punct.py
+@@ -0,0 +1,176 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++import os
++
++import gigaam
++import onnx
++import torch
++from gigaam.utils import onnx_converter
++from onnxruntime.quantization import QuantType, quantize_dynamic
++from torch import Tensor
++
++# encoder input length should be of int64
++# encder output length can be int64 or int32
++
++"""
++==========Input==========
++NodeArg(name='audio_signal', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
++NodeArg(name='length', type='tensor(int64)', shape=['batch_size'])
++==========Output==========
++NodeArg(name='encoded', type='tensor(float)', shape=['batch_size', 768, 'Transposeencoded_dim_2'])
++NodeArg(name='encoded_len', type='tensor(int32)', shape=['batch_size'])
++==========Input==========
++NodeArg(name='x', type='tensor(int32)', shape=[1, 1])
++NodeArg(name='unused_x_len.1', type='tensor(int32)', shape=[1])
++NodeArg(name='h.1', type='tensor(float)', shape=[1, 1, 320])
++NodeArg(name='c.1', type='tensor(float)', shape=[1, 1, 320])
++==========Output==========
++NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
++NodeArg(name='unused_x_len', type='tensor(int32)', shape=[1])
++NodeArg(name='h', type='tensor(float)', shape=[1, 1, 320])
++NodeArg(name='c', type='tensor(float)', shape=[1, 1, 320])
++==========Input==========
++NodeArg(name='enc', type='tensor(float)', shape=[1, 768, 1])
++NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
++==========Output==========
++NodeArg(name='joint', type='tensor(float)', shape=[1, 1, 1, 1025])
++"""
++
++
++def add_meta_data(filename: str, meta_data: dict[str, str]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++class EncoderWrapper(torch.nn.Module):
++    def __init__(self, m):
++        super().__init__()
++        self.m = m
++
++    def forward(self, audio_signal: Tensor, length: Tensor):
++        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/encoder.py#L499
++        out, out_len = self.m.encoder(audio_signal, length)
++
++        return out, out_len.to(torch.int64)
++
++    def to_onnx(self, dir_path: str = "."):
++        onnx_converter(
++            model_name=f"{self.m.cfg.model_name}_encoder",
++            out_dir=dir_path,
++            module=self.m.encoder,
++            dynamic_axes=self.m.encoder.dynamic_axes(),
++        )
++
++
++class DecoderWrapper(torch.nn.Module):
++    def __init__(self, m):
++        super().__init__()
++        self.m = m
++
++    def forward(self, x: Tensor, unused_x_len: Tensor, h: Tensor, c: Tensor):
++        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/decoder.py#L110C17-L110C54
++        emb = self.m.head.decoder.embed(x)
++        g, (h, c) = self.m.head.decoder.lstm(emb.transpose(0, 1), (h, c))
++        return g.permute(1, 2, 0), unused_x_len + 1, h, c
++
++    def to_onnx(self, dir_path: str = "."):
++        label, hidden_h, hidden_c = self.m.head.decoder.input_example()
++        label = label.to(torch.int32)
++        label_len = torch.zeros(1, dtype=torch.int32)
++
++        onnx_converter(
++            model_name=f"{self.m.cfg.model_name}_decoder",
++            out_dir=dir_path,
++            module=self,
++            dynamic_axes=self.m.encoder.dynamic_axes(),
++            inputs=(label, label_len, hidden_h, hidden_c),
++            input_names=["x", "unused_x_len.1", "h.1", "c.1"],
++            output_names=["dec", "unused_x_len", "h", "c"],
++        )
++
++
++"""
++{'model_class': 'rnnt', 'sample_rate': 16000,
++'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000,
++'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
++'mel_norm': None, 'center': False},
++'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16,
++'d_model': 768, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
++'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
++'conv_kernel_size': 5, 'flash_attn': False, 'subs_kernel_size': 5,
++'subsampling': 'conv1d', 'conv_norm_type': 'layer_norm'},
++'head': {'_target_': 'gigaam.decoder.RNNTHead',
++'decoder': {'pred_hidden': 320, 'pred_rnn_layers': 1, 'num_classes': 1025},
++'joint': {'enc_hidden': 768, 'pred_hidden': 320, 'joint_hidden': 320, 'num_classes': 1025}},
++'decoding': {'_target_': 'gigaam.decoding.RNNTGreedyDecoding',
++'vocabulary': None, 'model_path': '/root/.cache/gigaam/v3_e2e_rnnt_tokenizer.model'}, 'model_name': 'v3_e2e_rnnt', 'hashes': {'model': '72e2a9b5c7caad963b2bbfd2f298c252', 'tokenizer': '3b3bf8370e882885d79731592fc99f98'}}
++"""
++
++
++def main() -> None:
++    model_name = "v3_e2e_rnnt"
++    model = gigaam.load_model(model_name)
++
++    # <blk> is the last token
++    sp = model.decoding.tokenizer.model
++    with open("./tokens.txt", "w", encoding="utf-8") as f:
++        for i in range(sp.vocab_size()):
++            f.write(f"{sp.id_to_piece(i)} {i}\n")
++
++        f.write(f"<blk> {i+1}\n")
++        print("Saved to tokens.txt")
++
++    EncoderWrapper(model).to_onnx(".")
++    DecoderWrapper(model).to_onnx(".")
++
++    onnx_converter(
++        model_name=f"{model.cfg.model_name}_joint",
++        out_dir=".",
++        module=model.head.joint,
++    )
++    meta_data = {
++        # vocab_size does not include the blank
++        # we will increase vocab_size by 1 in the c++ code
++        "vocab_size": model.cfg["head"]["decoder"]["num_classes"] - 1,
++        "pred_rnn_layers": model.cfg["head"]["decoder"]["pred_rnn_layers"],
++        "pred_hidden": model.cfg["head"]["decoder"]["pred_hidden"],
++        "normalize_type": "",
++        "subsampling_factor": 4,
++        "model_type": "EncDecRNNTBPEModel",
++        "version": "3",
++        "model_author": "https://github.com/salute-developers/GigaAM",
++        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
++        "language": "Russian",
++        "comment": "v3",
++        "is_giga_am": 1,
++    }
++
++    add_meta_data(f"./{model_name}_encoder.onnx", meta_data)
++    quantize_dynamic(
++        model_input=f"./{model_name}_encoder.onnx",
++        model_output="./encoder.int8.onnx",
++        weight_type=QuantType.QUInt8,
++    )
++    os.rename(f"./{model_name}_decoder.onnx", "decoder.onnx")
++    os.rename(f"./{model_name}_joint.onnx", "joiner.onnx")
++    os.remove(f"./{model_name}_encoder.onnx")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py b/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py
+new file mode 100755
+index 00000000..72e604f5
+--- /dev/null
++++ b/scripts/nemo/GigaAM/export-onnx-rnnt-v3.py
+@@ -0,0 +1,178 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++import os
++
++import gigaam
++import onnx
++import torch
++from gigaam.utils import onnx_converter
++from onnxruntime.quantization import QuantType, quantize_dynamic
++from torch import Tensor
++
++# encoder input length should be of int64
++# encder output length can be int64 or int32
++
++"""
++==========Input==========
++NodeArg(name='audio_signal', type='tensor(float)', shape=['batch_size', 64, 'seq_len'])
++NodeArg(name='length', type='tensor(int64)', shape=['batch_size'])
++==========Output==========
++NodeArg(name='encoded', type='tensor(float)', shape=['batch_size', 768, 'Transposeencoded_dim_2'])
++NodeArg(name='encoded_len', type='tensor(int32)', shape=['batch_size'])
++==========Input==========
++NodeArg(name='x', type='tensor(int32)', shape=[1, 1])
++NodeArg(name='unused_x_len.1', type='tensor(int32)', shape=[1])
++NodeArg(name='h.1', type='tensor(float)', shape=[1, 1, 320])
++NodeArg(name='c.1', type='tensor(float)', shape=[1, 1, 320])
++==========Output==========
++NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
++NodeArg(name='unused_x_len', type='tensor(int32)', shape=[1])
++NodeArg(name='h', type='tensor(float)', shape=[1, 1, 320])
++NodeArg(name='c', type='tensor(float)', shape=[1, 1, 320])
++==========Input==========
++NodeArg(name='enc', type='tensor(float)', shape=[1, 768, 1])
++NodeArg(name='dec', type='tensor(float)', shape=[1, 320, 1])
++==========Output==========
++NodeArg(name='joint', type='tensor(float)', shape=[1, 1, 1, 34])
++"""
++
++
++def add_meta_data(filename: str, meta_data: dict[str, str]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++    onnx.save(model, filename)
++
++
++class EncoderWrapper(torch.nn.Module):
++    def __init__(self, m):
++        super().__init__()
++        self.m = m
++
++    def forward(self, audio_signal: Tensor, length: Tensor):
++        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/encoder.py#L499
++        out, out_len = self.m.encoder(audio_signal, length)
++
++        return out, out_len.to(torch.int64)
++
++    def to_onnx(self, dir_path: str = "."):
++        onnx_converter(
++            model_name=f"{self.m.cfg.model_name}_encoder",
++            out_dir=dir_path,
++            module=self.m.encoder,
++            dynamic_axes=self.m.encoder.dynamic_axes(),
++        )
++
++
++class DecoderWrapper(torch.nn.Module):
++    def __init__(self, m):
++        super().__init__()
++        self.m = m
++
++    def forward(self, x: Tensor, unused_x_len: Tensor, h: Tensor, c: Tensor):
++        # https://github.com/salute-developers/GigaAM/blob/main/gigaam/decoder.py#L110C17-L110C54
++        emb = self.m.head.decoder.embed(x)
++        g, (h, c) = self.m.head.decoder.lstm(emb.transpose(0, 1), (h, c))
++        return g.permute(1, 2, 0), unused_x_len + 1, h, c
++
++    def to_onnx(self, dir_path: str = "."):
++        label, hidden_h, hidden_c = self.m.head.decoder.input_example()
++        label = label.to(torch.int32)
++        label_len = torch.zeros(1, dtype=torch.int32)
++
++        onnx_converter(
++            model_name=f"{self.m.cfg.model_name}_decoder",
++            out_dir=dir_path,
++            module=self,
++            dynamic_axes=self.m.encoder.dynamic_axes(),
++            inputs=(label, label_len, hidden_h, hidden_c),
++            input_names=["x", "unused_x_len.1", "h.1", "c.1"],
++            output_names=["dec", "unused_x_len", "h", "c"],
++        )
++
++
++"""
++{'model_class': 'rnnt', 'sample_rate': 16000,
++'preprocessor': {'_target_': 'gigaam.preprocess.FeatureExtractor', 'sample_rate': 16000,
++'features': 64, 'win_length': 320, 'hop_length': 160, 'mel_scale': 'htk', 'n_fft': 320,
++'mel_norm': None, 'center': False},
++'encoder': {'_target_': 'gigaam.encoder.ConformerEncoder', 'feat_in': 64, 'n_layers': 16,
++'d_model': 768, 'subsampling_factor': 4, 'ff_expansion_factor': 4,
++'self_attention_model': 'rotary', 'pos_emb_max_len': 5000, 'n_heads': 16,
++'conv_kernel_size': 5, 'flash_attn': False, 'subs_kernel_size': 5,
++'subsampling': 'conv1d', 'conv_norm_type': 'layer_norm'},
++'head': {'_target_': 'gigaam.decoder.RNNTHead',
++'decoder': {'pred_hidden': 320, 'pred_rnn_layers': 1, 'num_classes': 34},
++'joint': {'enc_hidden': 768, 'pred_hidden': 320, 'joint_hidden': 320, 'num_classes': 34}},
++'decoding': {'_target_': 'gigaam.decoding.RNNTGreedyDecoding',
++'vocabulary': [' ', '', '', '', '', '', '', '', '', '', '', '', '', '', '',
++'', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']},
++'model_name': 'v3_rnnt', 'hashes': {'model': 'be62a7bc46de1311ec288d3bf8ee2818'}}
++"""
++
++
++def main() -> None:
++    model_name = "v3_rnnt"
++    model = gigaam.load_model(model_name)
++
++    # use characters
++    # space is 0
++    # <blk> is the last token
++    with open("./tokens.txt", "w", encoding="utf-8") as f:
++        for i, s in enumerate(model.cfg["decoding"]["vocabulary"]):
++            f.write(f"{s} {i}\n")
++        f.write(f"<blk> {i+1}\n")
++        print("Saved to tokens.txt")
++
++    EncoderWrapper(model).to_onnx(".")
++    DecoderWrapper(model).to_onnx(".")
++
++    onnx_converter(
++        model_name=f"{model.cfg.model_name}_joint",
++        out_dir=".",
++        module=model.head.joint,
++    )
++    meta_data = {
++        # vocab_size does not include the blank
++        # we will increase vocab_size by 1 in the c++ code
++        "vocab_size": model.cfg["head"]["decoder"]["num_classes"] - 1,
++        "pred_rnn_layers": model.cfg["head"]["decoder"]["pred_rnn_layers"],
++        "pred_hidden": model.cfg["head"]["decoder"]["pred_hidden"],
++        "normalize_type": "",
++        "subsampling_factor": 4,
++        "model_type": "EncDecRNNTBPEModel",
++        "version": "3",
++        "model_author": "https://github.com/salute-developers/GigaAM",
++        "license": "https://github.com/salute-developers/GigaAM/blob/main/LICENSE",
++        "language": "Russian",
++        "comment": "v3",
++        "is_giga_am": 1,
++    }
++
++    add_meta_data(f"./{model_name}_encoder.onnx", meta_data)
++    quantize_dynamic(
++        model_input=f"./{model_name}_encoder.onnx",
++        model_output="./encoder.int8.onnx",
++        weight_type=QuantType.QUInt8,
++    )
++    os.rename(f"./{model_name}_decoder.onnx", "decoder.onnx")
++    os.rename(f"./{model_name}_joint.onnx", "joiner.onnx")
++    os.remove(f"./{model_name}_encoder.onnx")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/nemo/GigaAM/run-ctc-v3-punct.sh b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
+new file mode 100755
+index 00000000..a03a350c
+--- /dev/null
++++ b/scripts/nemo/GigaAM/run-ctc-v3-punct.sh
+@@ -0,0 +1,28 @@
++#!/usr/bin/env bash
++
++set -ex
++
++function install_gigaam() {
++  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
++  python3 get-pip.py
++  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
++  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
++
++  BRANCH='main'
++  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
++
++  python3 -m pip install -qq kaldi-native-fbank
++  pip install numpy==1.26.4
++}
++
++function download_files() {
++  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
++  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++}
++
++install_gigaam
++download_files
++
++python3 ./export-onnx-ctc-v3-punct.py
++ls -lh
++python3 ./test-onnx-ctc.py
+diff --git a/scripts/nemo/GigaAM/run-ctc-v3.sh b/scripts/nemo/GigaAM/run-ctc-v3.sh
+new file mode 100755
+index 00000000..0273935a
+--- /dev/null
++++ b/scripts/nemo/GigaAM/run-ctc-v3.sh
+@@ -0,0 +1,28 @@
++#!/usr/bin/env bash
++
++set -ex
++
++function install_gigaam() {
++  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
++  python3 get-pip.py
++  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
++  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
++
++  BRANCH='main'
++  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
++
++  python3 -m pip install -qq kaldi-native-fbank
++  pip install numpy==1.26.4
++}
++
++function download_files() {
++  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
++  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++}
++
++install_gigaam
++download_files
++
++python3 ./export-onnx-ctc-v3.py
++ls -lh
++python3 ./test-onnx-ctc.py
+diff --git a/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
+new file mode 100755
+index 00000000..fd13ceb3
+--- /dev/null
++++ b/scripts/nemo/GigaAM/run-rnnt-v3-punct.sh
+@@ -0,0 +1,29 @@
++#!/usr/bin/env bash
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++set -ex
++
++function install_gigaam() {
++  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
++  python3 get-pip.py
++  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
++  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
++
++  BRANCH='main'
++  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
++
++  python3 -m pip install -qq kaldi-native-fbank
++  pip install numpy==1.26.4
++}
++
++function download_files() {
++  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
++  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++}
++
++install_gigaam
++download_files
++
++python3 ./export-onnx-rnnt-v3-punct.py
++ls -lh
++python3 ./test-onnx-rnnt.py
+diff --git a/scripts/nemo/GigaAM/run-rnnt-v3.sh b/scripts/nemo/GigaAM/run-rnnt-v3.sh
+new file mode 100755
+index 00000000..42dcbcb5
+--- /dev/null
++++ b/scripts/nemo/GigaAM/run-rnnt-v3.sh
+@@ -0,0 +1,29 @@
++#!/usr/bin/env bash
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++set -ex
++
++function install_gigaam() {
++  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
++  python3 get-pip.py
++  pip install torch==2.4.0 torchaudio==2.4.0 -f https://download.pytorch.org/whl/torch_stable.html
++  pip install -qq wget text-unidecode "matplotlib>=3.3.2" onnx onnxruntime==1.17.1 pybind11 Cython einops kaldi-native-fbank soundfile librosa
++
++  BRANCH='main'
++  python3 -m pip install git+https://github.com/salute-developers/GigaAM.git@$BRANCH#egg=gigaam
++
++  python3 -m pip install -qq kaldi-native-fbank
++  pip install numpy==1.26.4
++}
++
++function download_files() {
++  curl -SL -O https://huggingface.co/csukuangfj/tmp-files/resolve/main/GigaAM/example.wav
++  curl -SL -O https://github.com/salute-developers/GigaAM/blob/main/LICENSE
++}
++
++install_gigaam
++download_files
++
++python3 ./export-onnx-rnnt-v3.py
++ls -lh
++python3 ./test-onnx-rnnt.py
+
+commit a6a36e8cef858aa4f252f037a94463f20cc48d62
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 11 20:30:19 2025 +0800
+
+    Use a shorter name for Zipvoice models. (#2894)
+
+diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+index 2ad7c6b2..9009be24 100644
+--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
++++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+@@ -216,8 +216,8 @@ final class SherpaOnnxOfflineTtsKittenModelConfig extends Struct {
+ 
+ final class SherpaOnnxOfflineTtsZipVoiceModelConfig extends Struct {
+   external Pointer<Utf8> tokens;
+-  external Pointer<Utf8> textModel;
+-  external Pointer<Utf8> flowMatchingModel;
++  external Pointer<Utf8> encoder;
++  external Pointer<Utf8> decoder;
+   external Pointer<Utf8> vocoder;
+   external Pointer<Utf8> dataDir;
+   external Pointer<Utf8> lexicon;
+diff --git a/flutter/sherpa_onnx/lib/src/tts.dart b/flutter/sherpa_onnx/lib/src/tts.dart
+index dc1b1a8a..5e622ffd 100644
+--- a/flutter/sherpa_onnx/lib/src/tts.dart
++++ b/flutter/sherpa_onnx/lib/src/tts.dart
+@@ -36,14 +36,14 @@ class OfflineTtsVitsModelConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'model': model,
+-        'lexicon': lexicon,
+-        'tokens': tokens,
+-        'dataDir': dataDir,
+-        'noiseScale': noiseScale,
+-        'noiseScaleW': noiseScaleW,
+-        'lengthScale': lengthScale,
+-      };
++    'model': model,
++    'lexicon': lexicon,
++    'tokens': tokens,
++    'dataDir': dataDir,
++    'noiseScale': noiseScale,
++    'noiseScaleW': noiseScaleW,
++    'lengthScale': lengthScale,
++  };
+ 
+   final String model;
+   final String lexicon;
+@@ -85,14 +85,14 @@ class OfflineTtsMatchaModelConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'acousticModel': acousticModel,
+-        'vocoder': vocoder,
+-        'lexicon': lexicon,
+-        'tokens': tokens,
+-        'dataDir': dataDir,
+-        'noiseScale': noiseScale,
+-        'lengthScale': lengthScale,
+-      };
++    'acousticModel': acousticModel,
++    'vocoder': vocoder,
++    'lexicon': lexicon,
++    'tokens': tokens,
++    'dataDir': dataDir,
++    'noiseScale': noiseScale,
++    'lengthScale': lengthScale,
++  };
+ 
+   final String acousticModel;
+   final String vocoder;
+@@ -134,14 +134,14 @@ class OfflineTtsKokoroModelConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'model': model,
+-        'voices': voices,
+-        'tokens': tokens,
+-        'dataDir': dataDir,
+-        'lengthScale': lengthScale,
+-        'lexicon': lexicon,
+-        'lang': lang,
+-      };
++    'model': model,
++    'voices': voices,
++    'tokens': tokens,
++    'dataDir': dataDir,
++    'lengthScale': lengthScale,
++    'lexicon': lexicon,
++    'lang': lang,
++  };
+ 
+   final String model;
+   final String voices;
+@@ -178,12 +178,12 @@ class OfflineTtsKittenModelConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'model': model,
+-        'voices': voices,
+-        'tokens': tokens,
+-        'dataDir': dataDir,
+-        'lengthScale': lengthScale,
+-      };
++    'model': model,
++    'voices': voices,
++    'tokens': tokens,
++    'dataDir': dataDir,
++    'lengthScale': lengthScale,
++  };
+ 
+   final String model;
+   final String voices;
+@@ -195,8 +195,8 @@ class OfflineTtsKittenModelConfig {
+ class OfflineTtsZipVoiceModelConfig {
+   const OfflineTtsZipVoiceModelConfig({
+     this.tokens = '',
+-    this.textModel = '',
+-    this.flowMatchingModel = '',
++    this.encoder = '',
++    this.decoder = '',
+     this.vocoder = '',
+     this.dataDir = '',
+     this.lexicon = '',
+@@ -209,8 +209,8 @@ class OfflineTtsZipVoiceModelConfig {
+   factory OfflineTtsZipVoiceModelConfig.fromJson(Map<String, dynamic> json) {
+     return OfflineTtsZipVoiceModelConfig(
+       tokens: json['tokens'] as String? ?? '',
+-      textModel: json['textModel'] as String? ?? '',
+-      flowMatchingModel: json['flowMatchingModel'] as String? ?? '',
++      encoder: json['encoder'] as String? ?? '',
++      decoder: json['decoder'] as String? ?? '',
+       vocoder: json['vocoder'] as String? ?? '',
+       dataDir: json['dataDir'] as String? ?? '',
+       lexicon: json['lexicon'] as String? ?? '',
+@@ -223,25 +223,25 @@ class OfflineTtsZipVoiceModelConfig {
+ 
+   @override
+   String toString() {
+-    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
++    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, encoder: $encoder, decoder: $decoder, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'tokens': tokens,
+-        'textModel': textModel,
+-        'flowMatchingModel': flowMatchingModel,
+-        'vocoder': vocoder,
+-        'dataDir': dataDir,
+-        'lexicon': lexicon,
+-        'featScale': featScale,
+-        'tShift': tShift,
+-        'targetRms': targetRms,
+-        'guidanceScale': guidanceScale,
+-      };
++    'tokens': tokens,
++    'encoder': encoder,
++    'decoder': decoder,
++    'vocoder': vocoder,
++    'dataDir': dataDir,
++    'lexicon': lexicon,
++    'featScale': featScale,
++    'tShift': tShift,
++    'targetRms': targetRms,
++    'guidanceScale': guidanceScale,
++  };
+ 
+   final String tokens;
+-  final String textModel;
+-  final String flowMatchingModel;
++  final String encoder;
++  final String decoder;
+   final String vocoder;
+   final String dataDir;
+   final String lexicon;
+@@ -266,15 +266,20 @@ class OfflineTtsModelConfig {
+   factory OfflineTtsModelConfig.fromJson(Map<String, dynamic> json) {
+     return OfflineTtsModelConfig(
+       vits: OfflineTtsVitsModelConfig.fromJson(
+-          json['vits'] as Map<String, dynamic>? ?? const {}),
++        json['vits'] as Map<String, dynamic>? ?? const {},
++      ),
+       matcha: OfflineTtsMatchaModelConfig.fromJson(
+-          json['matcha'] as Map<String, dynamic>? ?? const {}),
++        json['matcha'] as Map<String, dynamic>? ?? const {},
++      ),
+       kokoro: OfflineTtsKokoroModelConfig.fromJson(
+-          json['kokoro'] as Map<String, dynamic>? ?? const {}),
++        json['kokoro'] as Map<String, dynamic>? ?? const {},
++      ),
+       kitten: OfflineTtsKittenModelConfig.fromJson(
+-          json['kitten'] as Map<String, dynamic>? ?? const {}),
++        json['kitten'] as Map<String, dynamic>? ?? const {},
++      ),
+       zipvoice: OfflineTtsZipVoiceModelConfig.fromJson(
+-          json['zipvoice'] as Map<String, dynamic>? ?? const {}),
++        json['zipvoice'] as Map<String, dynamic>? ?? const {},
++      ),
+       numThreads: json['numThreads'] as int? ?? 1,
+       debug: json['debug'] as bool? ?? true,
+       provider: json['provider'] as String? ?? 'cpu',
+@@ -287,15 +292,15 @@ class OfflineTtsModelConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'vits': vits.toJson(),
+-        'matcha': matcha.toJson(),
+-        'kokoro': kokoro.toJson(),
+-        'kitten': kitten.toJson(),
+-        'zipvoice': zipvoice.toJson(),
+-        'numThreads': numThreads,
+-        'debug': debug,
+-        'provider': provider,
+-      };
++    'vits': vits.toJson(),
++    'matcha': matcha.toJson(),
++    'kokoro': kokoro.toJson(),
++    'kitten': kitten.toJson(),
++    'zipvoice': zipvoice.toJson(),
++    'numThreads': numThreads,
++    'debug': debug,
++    'provider': provider,
++  };
+ 
+   final OfflineTtsVitsModelConfig vits;
+   final OfflineTtsMatchaModelConfig matcha;
+@@ -318,8 +323,9 @@ class OfflineTtsConfig {
+ 
+   factory OfflineTtsConfig.fromJson(Map<String, dynamic> json) {
+     return OfflineTtsConfig(
+-      model:
+-          OfflineTtsModelConfig.fromJson(json['model'] as Map<String, dynamic>),
++      model: OfflineTtsModelConfig.fromJson(
++        json['model'] as Map<String, dynamic>,
++      ),
+       ruleFsts: json['ruleFsts'] as String? ?? '',
+       maxNumSenetences: json['maxNumSenetences'] as int? ?? 1,
+       ruleFars: json['ruleFars'] as String? ?? '',
+@@ -333,12 +339,12 @@ class OfflineTtsConfig {
+   }
+ 
+   Map<String, dynamic> toJson() => {
+-        'model': model.toJson(),
+-        'ruleFsts': ruleFsts,
+-        'maxNumSenetences': maxNumSenetences,
+-        'ruleFars': ruleFars,
+-        'silenceScale': silenceScale,
+-      };
++    'model': model.toJson(),
++    'ruleFsts': ruleFsts,
++    'maxNumSenetences': maxNumSenetences,
++    'ruleFars': ruleFars,
++    'silenceScale': silenceScale,
++  };
+ 
+   final OfflineTtsModelConfig model;
+   final String ruleFsts;
+@@ -348,10 +354,7 @@ class OfflineTtsConfig {
+ }
+ 
+ class GeneratedAudio {
+-  GeneratedAudio({
+-    required this.samples,
+-    required this.sampleRate,
+-  });
++  GeneratedAudio({required this.samples, required this.sampleRate});
+ 
+   final Float32List samples;
+   final int sampleRate;
+@@ -378,8 +381,8 @@ class OfflineTts {
+     c.ref.model.vits.noiseScaleW = config.model.vits.noiseScaleW;
+     c.ref.model.vits.lengthScale = config.model.vits.lengthScale;
+ 
+-    c.ref.model.matcha.acousticModel =
+-        config.model.matcha.acousticModel.toNativeUtf8();
++    c.ref.model.matcha.acousticModel = config.model.matcha.acousticModel
++        .toNativeUtf8();
+     c.ref.model.matcha.vocoder = config.model.matcha.vocoder.toNativeUtf8();
+     c.ref.model.matcha.lexicon = config.model.matcha.lexicon.toNativeUtf8();
+     c.ref.model.matcha.tokens = config.model.matcha.tokens.toNativeUtf8();
+@@ -402,8 +405,8 @@ class OfflineTts {
+     c.ref.model.kitten.lengthScale = config.model.kitten.lengthScale;
+ 
+     c.ref.model.zipvoice.tokens = config.model.zipvoice.tokens.toNativeUtf8();
+-    c.ref.model.zipvoice.textModel = config.model.zipvoice.textModel.toNativeUtf8();
+-    c.ref.model.zipvoice.flowMatchingModel = config.model.zipvoice.flowMatchingModel.toNativeUtf8();
++    c.ref.model.zipvoice.encoder = config.model.zipvoice.encoder.toNativeUtf8();
++    c.ref.model.zipvoice.decoder = config.model.zipvoice.decoder.toNativeUtf8();
+     c.ref.model.zipvoice.vocoder = config.model.zipvoice.vocoder.toNativeUtf8();
+     c.ref.model.zipvoice.dataDir = config.model.zipvoice.dataDir.toNativeUtf8();
+     c.ref.model.zipvoice.lexicon = config.model.zipvoice.lexicon.toNativeUtf8();
+@@ -430,8 +433,8 @@ class OfflineTts {
+     calloc.free(c.ref.model.zipvoice.lexicon);
+     calloc.free(c.ref.model.zipvoice.dataDir);
+     calloc.free(c.ref.model.zipvoice.vocoder);
+-    calloc.free(c.ref.model.zipvoice.flowMatchingModel);
+-    calloc.free(c.ref.model.zipvoice.textModel);
++    calloc.free(c.ref.model.zipvoice.decoder);
++    calloc.free(c.ref.model.zipvoice.encoder);
+     calloc.free(c.ref.model.zipvoice.tokens);
+ 
+     calloc.free(c.ref.model.kitten.dataDir);
+@@ -470,12 +473,15 @@ class OfflineTts {
+     ptr = nullptr;
+   }
+ 
+-  GeneratedAudio generate(
+-      {required String text, int sid = 0, double speed = 1.0}) {
++  GeneratedAudio generate({
++    required String text,
++    int sid = 0,
++    double speed = 1.0,
++  }) {
+     final Pointer<Utf8> textPtr = text.toNativeUtf8();
+     final p =
+         SherpaOnnxBindings.offlineTtsGenerate?.call(ptr, textPtr, sid, speed) ??
+-            nullptr;
++        nullptr;
+     calloc.free(textPtr);
+ 
+     if (p == nullptr) {
+@@ -491,26 +497,35 @@ class OfflineTts {
+     return GeneratedAudio(samples: newSamples, sampleRate: sampleRate);
+   }
+ 
+-  GeneratedAudio generateWithCallback(
+-      {required String text,
+-      int sid = 0,
+-      double speed = 1.0,
+-      required int Function(Float32List samples) callback}) {
++  GeneratedAudio generateWithCallback({
++    required String text,
++    int sid = 0,
++    double speed = 1.0,
++    required int Function(Float32List samples) callback,
++  }) {
+     // see
+     // https://github.com/dart-lang/sdk/issues/54276#issuecomment-1846109285
+     // https://stackoverflow.com/questions/69537440/callbacks-in-dart-dartffi-only-supports-calling-static-dart-functions-from-nat
+     // https://github.com/dart-lang/sdk/blob/main/tests/ffi/isolate_local_function_callbacks_test.dart#L46
+     final wrapper =
+-        NativeCallable<SherpaOnnxGeneratedAudioCallbackNative>.isolateLocal(
+-            (Pointer<Float> samples, int n) {
+-      final s = samples.asTypedList(n);
+-      final newSamples = Float32List.fromList(s);
+-      return callback(newSamples);
+-    }, exceptionalReturn: 0);
++        NativeCallable<SherpaOnnxGeneratedAudioCallbackNative>.isolateLocal((
++          Pointer<Float> samples,
++          int n,
++        ) {
++          final s = samples.asTypedList(n);
++          final newSamples = Float32List.fromList(s);
++          return callback(newSamples);
++        }, exceptionalReturn: 0);
+ 
+     final Pointer<Utf8> textPtr = text.toNativeUtf8();
+-    final p = SherpaOnnxBindings.offlineTtsGenerateWithCallback
+-            ?.call(ptr, textPtr, sid, speed, wrapper.nativeFunction) ??
++    final p =
++        SherpaOnnxBindings.offlineTtsGenerateWithCallback?.call(
++          ptr,
++          textPtr,
++          sid,
++          speed,
++          wrapper.nativeFunction,
++        ) ??
+         nullptr;
+ 
+     calloc.free(textPtr);
+diff --git a/go-api-examples/non-streaming-tts/main.go b/go-api-examples/non-streaming-tts/main.go
+index 05e8a9e3..978ea6de 100644
+--- a/go-api-examples/non-streaming-tts/main.go
++++ b/go-api-examples/non-streaming-tts/main.go
+@@ -50,10 +50,10 @@ func main() {
+ 	flag.Float32Var(&config.Model.Kitten.LengthScale, "kitten-length-scale", 1.0, "length_scale for kitten. small -> faster; large -> slower")
+ 
+ 	flag.StringVar(&config.Model.Zipvoice.Tokens, "zipvoice-tokens", "", "Path to tokens.txt for ZipVoice")
+-	flag.StringVar(&config.Model.Zipvoice.TextModel, "zipvoice-text-model", "", "Path to ZipVoice text encoder model")
+-	flag.StringVar(&config.Model.Zipvoice.FlowMatchingModel, "zipvoice-flow-matching-model", "", "Path to ZipVoice flow-matching decoder")
++	flag.StringVar(&config.Model.Zipvoice.Encoder, "zipvoice-encoder", "", "Path to ZipVoice text encoder model")
++	flag.StringVar(&config.Model.Zipvoice.Decoder, "zipvoice-decoder", "", "Path to ZipVoice flow-matching decoder")
+ 	flag.StringVar(&config.Model.Zipvoice.DataDir, "zipvoice-data-dir", "", "Path to espeak-ng-data")
+-	flag.StringVar(&config.Model.Zipvoice.PinyinDict, "zipvoice-pinyin-dict", "", "Path to pinyin.raw (for zh)")
++	flag.StringVar(&config.Model.Zipvoice.Lexicon, "zipvoice-lexicon", "", "Path to lexicon.txt (for zh)")
+ 	flag.StringVar(&config.Model.Zipvoice.Vocoder, "zipvoice-vocoder", "", "Path to vocoder (e.g., vocos_24khz.onnx)")
+ 
+ 	flag.Float32Var(&config.Model.Zipvoice.FeatScale, "zipvoice-feat-scale", 0.1, "Feature scale for ZipVoice")
+diff --git a/go-api-examples/non-streaming-tts/run-zipvoice.sh b/go-api-examples/non-streaming-tts/run-zipvoice.sh
+index 43493e5e..4b0125d7 100755
+--- a/go-api-examples/non-streaming-tts/run-zipvoice.sh
++++ b/go-api-examples/non-streaming-tts/run-zipvoice.sh
+@@ -3,26 +3,30 @@
+ set -ex
+ 
+ # to download more models
+-if [ ! -f ./sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx ]; then
+-  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+-  tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+-  rm sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
++if [ ! -f ./sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++  tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++  rm sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++fi
++
++if [ ! -f vocos_24khz.onnx ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
+ fi
+ 
+ go mod tidy
+ go build
+ 
+ ./non-streaming-tts \
+-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
+-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
+-  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
+-  --zipvoice-pinyin-dict sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
+-  --zipvoice-tokens sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
+-  --zipvoice-vocoder sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
+-  --prompt-audio sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
++  --zipvoice-encoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
++  --zipvoice-decoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
++  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
++  --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
++  --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
++  --zipvoice-vocoder ./vocos_24khz.onnx \
++  --prompt-audio sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
+   --zipvoice-num-steps 4 \
+   --num-threads 4 \
+   --output-filename=./test-zipvoice.wav \
+-  --prompt-text "" \
+-  ""
++  --prompt-text ", . ." \
++  ", . . , ."
+ 
+diff --git a/python-api-examples/offline-zeroshot-tts.py b/python-api-examples/offline-zeroshot-tts.py
+index 746fbb3d..db2d7b3c 100755
+--- a/python-api-examples/offline-zeroshot-tts.py
++++ b/python-api-examples/offline-zeroshot-tts.py
+@@ -16,8 +16,8 @@ tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
+ wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
+ 
+ python3 ./python-api-examples/offline-zeroshot-tts.py \
+-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
+-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
++  --zipvoice-encoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
++  --zipvoice-decoder sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
+   --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
+   --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
+   --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
+@@ -49,17 +49,17 @@ def add_zipvoice_args(parser):
+     )
+ 
+     parser.add_argument(
+-        "--zipvoice-text-model",
++        "--zipvoice-encoder",
+         type=str,
+         default="",
+-        help="Path to zipvoice text model.",
++        help="Path to zipvoice text encoder model.",
+     )
+ 
+     parser.add_argument(
+-        "--zipvoice-flow-matching-model",
++        "--zipvoice-decoder",
+         type=str,
+         default="",
+-        help="Path to zipvoice flow matching model.",
++        help="Path to zipvoice flow matching decoder model.",
+     )
+ 
+     parser.add_argument(
+@@ -235,8 +235,8 @@ def main():
+         model=sherpa_onnx.OfflineTtsModelConfig(
+             zipvoice=sherpa_onnx.OfflineTtsZipvoiceModelConfig(
+                 tokens=args.zipvoice_tokens,
+-                text_model=args.zipvoice_text_model,
+-                flow_matching_model=args.zipvoice_flow_matching_model,
++                encoder=args.zipvoice_encoder,
++                decoder=args.zipvoice_decoder,
+                 data_dir=args.zipvoice_data_dir,
+                 lexicon=args.zipvoice_lexicon,
+                 vocoder=args.zipvoice_vocoder,
+diff --git a/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs b/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
+index 96d226ac..82ddeb5b 100644
+--- a/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
++++ b/scripts/dotnet/OfflineTtsZipVoiceModelConfig.cs
+@@ -10,11 +10,11 @@ namespace SherpaOnnx
+         public OfflineTtsZipVoiceModelConfig()
+         {
+             Tokens = "";
+-            TextModel = "";
+-            FlowMatchingModel = "";
++            Encoder = "";
++            Decoder = "";
+             Vocoder = "";
+             DataDir = "";
+-            PinyinDict = "";
++            Lexicon = "";
+ 
+             FeatScale = 0.1F;
+             Tshift = 0.5F;
+@@ -25,10 +25,10 @@ namespace SherpaOnnx
+         public string Tokens;
+ 
+         [MarshalAs(UnmanagedType.LPStr)]
+-        public string TextModel;
++        public string Encoder;
+ 
+         [MarshalAs(UnmanagedType.LPStr)]
+-        public string FlowMatchingModel;
++        public string Decoder;
+ 
+         [MarshalAs(UnmanagedType.LPStr)]
+         public string Vocoder;
+@@ -37,7 +37,7 @@ namespace SherpaOnnx
+         public string DataDir;
+ 
+         [MarshalAs(UnmanagedType.LPStr)]
+-        public string PinyinDict;
++        public string Lexicon;
+ 
+         public float FeatScale;
+         public float Tshift;
+diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
+index e91fa835..3e754f06 100644
+--- a/scripts/go/sherpa_onnx.go
++++ b/scripts/go/sherpa_onnx.go
+@@ -960,12 +960,12 @@ type OfflineTtsKittenModelConfig struct {
+ }
+ 
+ type OfflineTtsZipvoiceModelConfig struct {
+-	Tokens            string // Path to tokens.txt for ZipVoice
+-	TextModel         string // Path to text encoder (e.g. text_encoder.onnx)
+-	FlowMatchingModel string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
+-	DataDir           string // Path to espeak-ng-data
+-	Lexicon           string // Path to lexicon.txt (needed for zh)
+-	Vocoder           string // Path to vocoder (e.g. vocos_24khz.onnx)
++	Tokens  string // Path to tokens.txt for ZipVoice
++	Encoder string // Path to text encoder (e.g. encoder.onnx)
++	Decoder string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
++	DataDir string // Path to espeak-ng-data
++	Lexicon string // Path to lexicon.txt (needed for zh)
++	Vocoder string // Path to vocoder (e.g. vocos_24khz.onnx)
+ 
+ 	FeatScale     float32 // Feature scale
+ 	TShift        float32 // t-shift (<1 shifts to smaller t)
+@@ -1136,11 +1136,11 @@ func NewOfflineTts(config *OfflineTtsConfig) *OfflineTts {
+ 	c.model.zipvoice.tokens = C.CString(config.Model.Zipvoice.Tokens)
+ 	defer C.free(unsafe.Pointer(c.model.zipvoice.tokens))
+ 
+-	c.model.zipvoice.text_model = C.CString(config.Model.Zipvoice.TextModel)
+-	defer C.free(unsafe.Pointer(c.model.zipvoice.text_model))
++	c.model.zipvoice.encoder = C.CString(config.Model.Zipvoice.Encoder)
++	defer C.free(unsafe.Pointer(c.model.zipvoice.encoder))
+ 
+-	c.model.zipvoice.flow_matching_model = C.CString(config.Model.Zipvoice.FlowMatchingModel)
+-	defer C.free(unsafe.Pointer(c.model.zipvoice.flow_matching_model))
++	c.model.zipvoice.decoder = C.CString(config.Model.Zipvoice.Decoder)
++	defer C.free(unsafe.Pointer(c.model.zipvoice.decoder))
+ 
+ 	c.model.zipvoice.vocoder = C.CString(config.Model.Zipvoice.Vocoder)
+ 	defer C.free(unsafe.Pointer(c.model.zipvoice.vocoder))
+diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
+index bfb30355..c00286ac 100644
+--- a/sherpa-onnx/c-api/c-api.cc
++++ b/sherpa-onnx/c-api/c-api.cc
+@@ -1242,10 +1242,10 @@ static sherpa_onnx::OfflineTtsConfig GetOfflineTtsConfig(
+   // zipvoice
+   tts_config.model.zipvoice.tokens =
+       SHERPA_ONNX_OR(config->model.zipvoice.tokens, "");
+-  tts_config.model.zipvoice.text_model =
+-      SHERPA_ONNX_OR(config->model.zipvoice.text_model, "");
+-  tts_config.model.zipvoice.flow_matching_model =
+-      SHERPA_ONNX_OR(config->model.zipvoice.flow_matching_model, "");
++  tts_config.model.zipvoice.encoder =
++      SHERPA_ONNX_OR(config->model.zipvoice.encoder, "");
++  tts_config.model.zipvoice.decoder =
++      SHERPA_ONNX_OR(config->model.zipvoice.decoder, "");
+   tts_config.model.zipvoice.vocoder =
+       SHERPA_ONNX_OR(config->model.zipvoice.vocoder, "");
+   tts_config.model.zipvoice.data_dir =
+diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
+index 38d99c5a..f622c26d 100644
+--- a/sherpa-onnx/c-api/c-api.h
++++ b/sherpa-onnx/c-api/c-api.h
+@@ -1068,8 +1068,8 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsKittenModelConfig {
+ 
+ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsZipvoiceModelConfig {
+   const char *tokens;
+-  const char *text_model;
+-  const char *flow_matching_model;
++  const char *encoder;
++  const char *decoder;
+   const char *vocoder;
+   const char *data_dir;
+   const char *lexicon;
+diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
+index f1f2feec..25bb9f36 100644
+--- a/sherpa-onnx/c-api/cxx-api.cc
++++ b/sherpa-onnx/c-api/cxx-api.cc
+@@ -414,9 +414,8 @@ OfflineTts OfflineTts::Create(const OfflineTtsConfig &config) {
+   c.model.kitten.length_scale = config.model.kitten.length_scale;
+ 
+   c.model.zipvoice.tokens = config.model.zipvoice.tokens.c_str();
+-  c.model.zipvoice.text_model = config.model.zipvoice.text_model.c_str();
+-  c.model.zipvoice.flow_matching_model =
+-      config.model.zipvoice.flow_matching_model.c_str();
++  c.model.zipvoice.encoder = config.model.zipvoice.encoder.c_str();
++  c.model.zipvoice.decoder = config.model.zipvoice.decoder.c_str();
+   c.model.zipvoice.vocoder = config.model.zipvoice.vocoder.c_str();
+   c.model.zipvoice.data_dir = config.model.zipvoice.data_dir.c_str();
+   c.model.zipvoice.lexicon = config.model.zipvoice.lexicon.c_str();
+diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
+index f7144c50..7f46946c 100644
+--- a/sherpa-onnx/c-api/cxx-api.h
++++ b/sherpa-onnx/c-api/cxx-api.h
+@@ -428,8 +428,8 @@ struct OfflineTtsKittenModelConfig {
+ 
+ struct OfflineTtsZipvoiceModelConfig {
+   std::string tokens;
+-  std::string text_model;
+-  std::string flow_matching_model;
++  std::string encoder;
++  std::string decoder;
+   std::string vocoder;
+   std::string data_dir;
+   std::string lexicon;
+diff --git a/sherpa-onnx/csrc/offline-tts-impl.cc b/sherpa-onnx/csrc/offline-tts-impl.cc
+index 4ebf3b1d..2c94ae59 100644
+--- a/sherpa-onnx/csrc/offline-tts-impl.cc
++++ b/sherpa-onnx/csrc/offline-tts-impl.cc
+@@ -42,8 +42,8 @@ std::unique_ptr<OfflineTtsImpl> OfflineTtsImpl::Create(
+     return std::make_unique<OfflineTtsVitsImpl>(config);
+   } else if (!config.model.matcha.acoustic_model.empty()) {
+     return std::make_unique<OfflineTtsMatchaImpl>(config);
+-  } else if (!config.model.zipvoice.text_model.empty() &&
+-             !config.model.zipvoice.flow_matching_model.empty()) {
++  } else if (!config.model.zipvoice.encoder.empty() &&
++             !config.model.zipvoice.decoder.empty()) {
+     return std::make_unique<OfflineTtsZipvoiceImpl>(config);
+   } else if (!config.model.kokoro.model.empty()) {
+     return std::make_unique<OfflineTtsKokoroImpl>(config);
+@@ -63,8 +63,8 @@ std::unique_ptr<OfflineTtsImpl> OfflineTtsImpl::Create(
+     return std::make_unique<OfflineTtsVitsImpl>(mgr, config);
+   } else if (!config.model.matcha.acoustic_model.empty()) {
+     return std::make_unique<OfflineTtsMatchaImpl>(mgr, config);
+-  } else if (!config.model.zipvoice.text_model.empty() &&
+-             !config.model.zipvoice.flow_matching_model.empty()) {
++  } else if (!config.model.zipvoice.encoder.empty() &&
++             !config.model.zipvoice.decoder.empty()) {
+     return std::make_unique<OfflineTtsZipvoiceImpl>(mgr, config);
+   } else if (!config.model.kokoro.model.empty()) {
+     return std::make_unique<OfflineTtsKokoroImpl>(mgr, config);
+diff --git a/sherpa-onnx/csrc/offline-tts-model-config.cc b/sherpa-onnx/csrc/offline-tts-model-config.cc
+index df7bf06f..176560ad 100644
+--- a/sherpa-onnx/csrc/offline-tts-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-model-config.cc
+@@ -41,7 +41,7 @@ bool OfflineTtsModelConfig::Validate() const {
+     return matcha.Validate();
+   }
+ 
+-  if (!zipvoice.flow_matching_model.empty()) {
++  if (!zipvoice.decoder.empty()) {
+     return zipvoice.Validate();
+   }
+ 
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+index 70de8667..712590ff 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+@@ -18,10 +18,9 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
+   po->Register("zipvoice-data-dir", &data_dir,
+                "Path to the directory containing dict for espeak-ng.");
+   po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
+-  po->Register("zipvoice-text-model", &text_model,
+-               "Path to zipvoice text model");
+-  po->Register("zipvoice-flow-matching-model", &flow_matching_model,
+-               "Path to zipvoice flow-matching model, i.e., the decoder model");
++  po->Register("zipvoice-encoder", &encoder, "Path to zipvoice text model");
++  po->Register("zipvoice-decoder", &decoder,
++               "Path to zipvoice flow-matching decoder model");
+   po->Register("zipvoice-vocoder", &vocoder, "Path to zipvoice vocoder");
+   po->Register("zipvoice-feat-scale", &feat_scale,
+                "Feature scale for ZipVoice (default: 0.1)");
+@@ -46,23 +45,23 @@ bool OfflineTtsZipvoiceModelConfig::Validate() const {
+     return false;
+   }
+ 
+-  if (text_model.empty()) {
+-    SHERPA_ONNX_LOGE("Please provide --zipvoice-text-model");
++  if (encoder.empty()) {
++    SHERPA_ONNX_LOGE("Please provide --zipvoice-encoder");
+     return false;
+   }
+-  if (!FileExists(text_model)) {
+-    SHERPA_ONNX_LOGE("--zipvoice-text-model: '%s' does not exist",
+-                     text_model.c_str());
++  if (!FileExists(encoder)) {
++    SHERPA_ONNX_LOGE("--zipvoice-encoder: '%s' does not exist",
++                     encoder.c_str());
+     return false;
+   }
+ 
+-  if (flow_matching_model.empty()) {
+-    SHERPA_ONNX_LOGE("Please provide --zipvoice-flow-matching-model");
++  if (decoder.empty()) {
++    SHERPA_ONNX_LOGE("Please provide --zipvoice-decoder");
+     return false;
+   }
+-  if (!FileExists(flow_matching_model)) {
+-    SHERPA_ONNX_LOGE("--zipvoice-flow-matching-model: '%s' does not exist",
+-                     flow_matching_model.c_str());
++  if (!FileExists(decoder)) {
++    SHERPA_ONNX_LOGE("--zipvoice-decoder: '%s' does not exist",
++                     decoder.c_str());
+     return false;
+   }
+ 
+@@ -126,8 +125,8 @@ std::string OfflineTtsZipvoiceModelConfig::ToString() const {
+ 
+   os << "OfflineTtsZipvoiceModelConfig(";
+   os << "tokens=\"" << tokens << "\", ";
+-  os << "text_model=\"" << text_model << "\", ";
+-  os << "flow_matching_model=\"" << flow_matching_model << "\", ";
++  os << "encoder=\"" << encoder << "\", ";
++  os << "decoder=\"" << decoder << "\", ";
+   os << "vocoder=\"" << vocoder << "\", ";
+   os << "data_dir=\"" << data_dir << "\", ";
+   os << "lexicon=\"" << lexicon << "\", ";
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+index 702760d0..eb837ece 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+@@ -14,8 +14,8 @@ namespace sherpa_onnx {
+ 
+ struct OfflineTtsZipvoiceModelConfig {
+   std::string tokens;
+-  std::string text_model;
+-  std::string flow_matching_model;  // decoder
++  std::string encoder;
++  std::string decoder;
+   std::string vocoder;
+ 
+   std::string data_dir;
+@@ -29,14 +29,14 @@ struct OfflineTtsZipvoiceModelConfig {
+   OfflineTtsZipvoiceModelConfig() = default;
+ 
+   OfflineTtsZipvoiceModelConfig(
+-      const std::string &tokens, const std::string &text_model,
+-      const std::string &flow_matching_model, const std::string &vocoder,
++      const std::string &tokens, const std::string &encoder,
++      const std::string &decoder, const std::string &vocoder,
+       const std::string &data_dir, const std::string &lexicon,
+       float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
+       float guidance_scale = 1.0)
+       : tokens(tokens),
+-        text_model(text_model),
+-        flow_matching_model(flow_matching_model),
++        encoder(encoder),
++        decoder(decoder),
+         vocoder(vocoder),
+         data_dir(data_dir),
+         lexicon(lexicon),
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+index 8b4d321b..cdd8bc86 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+@@ -37,8 +37,8 @@ class OfflineTtsZipvoiceModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    auto text_buf = ReadFile(config.zipvoice.text_model);
+-    auto fm_buf = ReadFile(config.zipvoice.flow_matching_model);
++    auto text_buf = ReadFile(config.zipvoice.encoder);
++    auto fm_buf = ReadFile(config.zipvoice.decoder);
+     Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
+   }
+ 
+@@ -48,8 +48,8 @@ class OfflineTtsZipvoiceModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    auto text_buf = ReadFile(mgr, config.zipvoice.text_model);
+-    auto fm_buf = ReadFile(mgr, config.zipvoice.flow_matching_model);
++    auto text_buf = ReadFile(mgr, config.zipvoice.encoder);
++    auto fm_buf = ReadFile(mgr, config.zipvoice.decoder);
+     Init(text_buf.data(), text_buf.size(), fm_buf.data(), fm_buf.size());
+   }
+ 
+@@ -90,7 +90,7 @@ class OfflineTtsZipvoiceModel::Impl {
+     text_inputs.push_back(std::move(prompt_feat_len_tensor));
+     text_inputs.push_back(std::move(speed_tensor));
+ 
+-    // forward text-encoder
++    // forward encoder
+     auto text_out =
+         text_sess_->Run({}, text_input_names_ptr_.data(), text_inputs.data(),
+                         text_inputs.size(), text_output_names_ptr_.data(),
+@@ -191,11 +191,11 @@ class OfflineTtsZipvoiceModel::Impl {
+   }
+ 
+  private:
+-  void Init(void *text_model_data, size_t text_model_data_length,
+-            void *fm_model_data, size_t fm_model_data_length) {
+-    // Init text-encoder model
++  void Init(void *encoder_data, size_t encoder_data_length, void *fm_model_data,
++            size_t fm_model_data_length) {
++    // Init encoder model
+     text_sess_ = std::make_unique<Ort::Session>(
+-        env_, text_model_data, text_model_data_length, sess_opts_);
++        env_, encoder_data, encoder_data_length, sess_opts_);
+     GetInputNames(text_sess_.get(), &text_input_names_, &text_input_names_ptr_);
+     GetOutputNames(text_sess_.get(), &text_output_names_,
+                    &text_output_names_ptr_);
+@@ -231,7 +231,7 @@ class OfflineTtsZipvoiceModel::Impl {
+     if (config_.debug) {
+       std::ostringstream os;
+ 
+-      os << "---zipvoice text-encoder model---\n";
++      os << "---encoder---\n";
+       Ort::ModelMetadata text_meta_data = text_sess_->GetModelMetadata();
+       PrintModelMetadata(os, text_meta_data);
+ 
+@@ -248,7 +248,7 @@ class OfflineTtsZipvoiceModel::Impl {
+         ++i;
+       }
+ 
+-      os << "---zipvoice flow-matching model---\n";
++      os << "---decoder---\n";
+       PrintModelMetadata(os, meta_data);
+ 
+       os << "----------input names----------\n";
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+index ad2e3352..67ea1b19 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+@@ -25,21 +25,23 @@ Offline/Non-streaming zero-shot text-to-speech with sherpa-onnx
+ 
+ Usage example:
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+-tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
+ 
+ ./bin/sherpa-onnx-offline-zeroshot-tts \
+-  --zipvoice-flow-matching-model=sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
+-  --zipvoice-text-model=sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
+-  --zipvoice-data-dir=sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
+-  --zipvoice-pinyin-dict=sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
+-  --zipvoice-tokens=sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
+-  --zipvoice-vocoder=sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
+-  --prompt-audio=sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
++  --zipvoice-encoder=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
++  --zipvoice-decoder=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
++  --zipvoice-data-dir=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
++  --zipvoice-lexicon=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
++  --zipvoice-tokens=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
++  --zipvoice-vocoder=./vocos_24khz.onnx \
++  --prompt-audio=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
+   --num-steps=4 \
+   --num-threads=4 \
+-  --prompt-text="" \
+-  ""
++  --prompt-text=", . ." \
++  ", . . , ."
+ 
+ It will generate a file ./generated.wav as specified by --output-filename.
+ )usage";
+diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+index 5d86b3c9..257aa30e 100644
+--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
++++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+@@ -103,11 +103,11 @@ type
+ 
+   TSherpaOnnxOfflineTtsZipVoiceModelConfig = record
+     Tokens: AnsiString;
+-    TextModel: AnsiString;
+-    FlowMatchingModel: AnsiString;
++    Encoder: AnsiString;
++    Decoder: AnsiString;
+     Vocoder: AnsiString;
+     DataDir: AnsiString;
+-    PinyinDict: AnsiString;
++    Lexicon: AnsiString;
+     FeatScale: Single;
+     Tshift: Single;
+     TargetRms: Single;
+@@ -983,11 +983,11 @@ type
+ 
+   SherpaOnnxOfflineTtsZipVoiceModelConfig = record
+     Tokens: PAnsiChar;
+-    TextModel: PAnsiChar;
+-    FlowMatchingModel: PAnsiChar;
++    Encoder: PAnsiChar;
++    Decoder: PAnsiChar;
+     Vocoder: PAnsiChar;
+     DataDir: PAnsiChar;
+-    PinyinDict: PAnsiChar;
++    Lexicon: PAnsiChar;
+     FeatScale: cfloat;
+     Tshift: cfloat;
+     TargetRms: cfloat;
+@@ -2423,18 +2423,18 @@ function TSherpaOnnxOfflineTtsZipVoiceModelConfig.ToString: AnsiString;
+ begin
+   Result := Format('TSherpaOnnxOfflineTtsZipVoiceModelConfig(' +
+     'Tokens := %s, ' +
+-    'TextModel := %s, ' +
+-    'FlowMatchingModel := %s, ' +
++    'Encoder := %s, ' +
++    'Decoder := %s, ' +
+     'Vocoder := %s, ' +
+     'DataDir := %s, ' +
+-    'PinyinDict := %s, ' +
++    'Lexicon := %s, ' +
+     'FeatScale := %.2f, ' +
+     'Tshift := %.2f, ' +
+     'TargetRms := %.2f, ' +
+     'GuidanceScale := %.2f' +
+     ')',
+-    [Self.Tokens, Self.TextModel, Self.FlowMatchingModel, Self.Vocoder,
+-     Self.DataDir, Self.PinyinDict, Self.FeatScale, Self.Tshift,
++    [Self.Tokens, Self.Encoder, Self.Decoder, Self.Vocoder,
++     Self.DataDir, Self.Lexicon, Self.FeatScale, Self.Tshift,
+      Self.TargetRms, Self.GuidanceScale]);
+ end;
+ 
+@@ -2528,11 +2528,11 @@ begin
+   C.Model.Kitten.LengthScale := Config.Model.Kitten.LengthScale;
+ 
+   C.Model.ZipVoice.Tokens := PAnsiChar(Config.Model.ZipVoice.Tokens);
+-  C.Model.ZipVoice.TextModel := PAnsiChar(Config.Model.ZipVoice.TextModel);
+-  C.Model.ZipVoice.FlowMatchingModel := PAnsiChar(Config.Model.ZipVoice.FlowMatchingModel);
++  C.Model.ZipVoice.Encoder := PAnsiChar(Config.Model.ZipVoice.Encoder);
++  C.Model.ZipVoice.Decoder := PAnsiChar(Config.Model.ZipVoice.Decoder);
+   C.Model.ZipVoice.Vocoder := PAnsiChar(Config.Model.ZipVoice.Vocoder);
+   C.Model.ZipVoice.DataDir := PAnsiChar(Config.Model.ZipVoice.DataDir);
+-  C.Model.ZipVoice.PinyinDict := PAnsiChar(Config.Model.ZipVoice.PinyinDict);
++  C.Model.ZipVoice.Lexicon := PAnsiChar(Config.Model.ZipVoice.Lexicon);
+   C.Model.ZipVoice.FeatScale := Config.Model.ZipVoice.FeatScale;
+   C.Model.ZipVoice.Tshift := Config.Model.ZipVoice.Tshift;
+   C.Model.ZipVoice.TargetRms := Config.Model.ZipVoice.TargetRms;
+diff --git a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+index dc6e8fb2..55bb5ade 100644
+--- a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+@@ -19,14 +19,14 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
+                     const std::string &, const std::string &,
+                     const std::string &, const std::string &, float, float,
+                     float, float>(),
+-           py::arg("tokens"), py::arg("text_model"),
+-           py::arg("flow_matching_model"), py::arg("vocoder"),
+-           py::arg("data_dir") = "", py::arg("lexicon") = "",
+-           py::arg("feat_scale") = 0.1, py::arg("t_shift") = 0.5,
+-           py::arg("target_rms") = 0.1, py::arg("guidance_scale") = 1.0)
++           py::arg("tokens"), py::arg("encoder"), py::arg("decoder"),
++           py::arg("vocoder"), py::arg("data_dir") = "",
++           py::arg("lexicon") = "", py::arg("feat_scale") = 0.1,
++           py::arg("t_shift") = 0.5, py::arg("target_rms") = 0.1,
++           py::arg("guidance_scale") = 1.0)
+       .def_readwrite("tokens", &PyClass::tokens)
+-      .def_readwrite("text_model", &PyClass::text_model)
+-      .def_readwrite("flow_matching_model", &PyClass::flow_matching_model)
++      .def_readwrite("encoder", &PyClass::encoder)
++      .def_readwrite("decoder", &PyClass::decoder)
+       .def_readwrite("vocoder", &PyClass::vocoder)
+       .def_readwrite("data_dir", &PyClass::data_dir)
+       .def_readwrite("lexicon", &PyClass::lexicon)
+diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
+index ee40fbde..1b1ca137 100644
+--- a/swift-api-examples/SherpaOnnx.swift
++++ b/swift-api-examples/SherpaOnnx.swift
+@@ -929,8 +929,8 @@ func sherpaOnnxOfflineTtsKittenModelConfig(
+ 
+ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
+   tokens: String = "",
+-  textModel: String = "",
+-  flowMatchingModel: String = "",
++  encoder: String = "",
++  decoder: String = "",
+   vocoder: String = "",
+   dataDir: String = "",
+   lexicon: String = "",
+@@ -941,8 +941,8 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
+ ) -> SherpaOnnxOfflineTtsZipvoiceModelConfig {
+   return SherpaOnnxOfflineTtsZipvoiceModelConfig(
+     tokens: toCPointer(tokens),
+-    text_model: toCPointer(textModel),
+-    flow_matching_model: toCPointer(flowMatchingModel),
++    encoder: toCPointer(encoder),
++    decoder: toCPointer(decoder),
+     vocoder: toCPointer(vocoder),
+     data_dir: toCPointer(dataDir),
+     lexicon: toCPointer(lexicon),
+diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
+index 2c1b0c36..2234d1b6 100644
+--- a/wasm/tts/sherpa-onnx-tts.js
++++ b/wasm/tts/sherpa-onnx-tts.js
+@@ -264,15 +264,14 @@ function initSherpaOnnxOfflineTtsKittenModelConfig(config, Module) {
+ 
+ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+   const tokensLen = Module.lengthBytesUTF8(config.tokens || '') + 1;
+-  const textModelLen = Module.lengthBytesUTF8(config.textModel || '') + 1;
+-  const flowMatchingModelLen =
+-      Module.lengthBytesUTF8(config.flowMatchingModel || '') + 1;
++  const encoderLen = Module.lengthBytesUTF8(config.encoder || '') + 1;
++  const decoderLen = Module.lengthBytesUTF8(config.decoder || '') + 1;
+   const vocoderLen = Module.lengthBytesUTF8(config.vocoder || '') + 1;
+   const dataDirLen = Module.lengthBytesUTF8(config.dataDir || '') + 1;
+   const lexiconLen = Module.lengthBytesUTF8(config.lexicon || '') + 1;
+ 
+-  const n = tokensLen + textModelLen + flowMatchingModelLen + vocoderLen +
+-      dataDirLen + lexiconLen;
++  const n = tokensLen + encoderLen + decoderLen + vocoderLen + dataDirLen +
++      lexiconLen;
+ 
+   const buffer = Module._malloc(n);
+ 
+@@ -283,12 +282,11 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+   Module.stringToUTF8(config.tokens || '', buffer + offset, tokensLen);
+   offset += tokensLen;
+ 
+-  Module.stringToUTF8(config.textModel || '', buffer + offset, textModelLen);
+-  offset += textModelLen;
++  Module.stringToUTF8(config.encoder || '', buffer + offset, encoderLen);
++  offset += encoderLen;
+ 
+-  Module.stringToUTF8(
+-      config.flowMatchingModel || '', buffer + offset, flowMatchingModelLen);
+-  offset += flowMatchingModelLen;
++  Module.stringToUTF8(config.decoder || '', buffer + offset, decoderLen);
++  offset += decoderLen;
+ 
+   Module.stringToUTF8(config.vocoder || '', buffer + offset, vocoderLen);
+   offset += vocoderLen;
+@@ -304,10 +302,10 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+   offset += tokensLen;
+ 
+   Module.setValue(ptr + 4, buffer + offset, 'i8*');
+-  offset += textModelLen;
++  offset += encoderLen;
+ 
+   Module.setValue(ptr + 8, buffer + offset, 'i8*');
+-  offset += flowMatchingModelLen;
++  offset += decoderLen;
+ 
+   Module.setValue(ptr + 12, buffer + offset, 'i8*');
+   offset += vocoderLen;
+@@ -377,8 +375,8 @@ function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
+   if (!('offlineTtsZipVoiceModelConfig' in config)) {
+     config.offlineTtsZipVoiceModelConfig = {
+       tokens: '',
+-      textModel: '',
+-      flowMatchingModel: '',
++      encoder: '',
++      decoder: '',
+       vocoder: '',
+       dataDir: '',
+       lexicon: '',
+diff --git a/wasm/tts/sherpa-onnx-wasm-main-tts.cc b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+index 99deb46c..09e06d69 100644
+--- a/wasm/tts/sherpa-onnx-wasm-main-tts.cc
++++ b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+@@ -75,8 +75,8 @@ void MyPrint(SherpaOnnxOfflineTtsConfig *tts_config) {
+ 
+   fprintf(stdout, "----------zipvoice model config----------\n");
+   fprintf(stdout, "tokens: %s\n", zipvoice->tokens);
+-  fprintf(stdout, "text_model: %s\n", zipvoice->text_model);
+-  fprintf(stdout, "flow_matching_model: %s\n", zipvoice->flow_matching_model);
++  fprintf(stdout, "encoder: %s\n", zipvoice->encoder);
++  fprintf(stdout, "decoder: %s\n", zipvoice->decoder);
+   fprintf(stdout, "vocoder: %s\n", zipvoice->vocoder);
+   fprintf(stdout, "data_dir: %s\n", zipvoice->data_dir);
+   fprintf(stdout, "lexicon: %s\n", zipvoice->lexicon);
+
+commit 1e4ac57888414f1cfbf6ce5fd46f582a0b74e55a
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 11 19:15:53 2025 +0800
+
+    Fix building errors (#2893)
+    
+    This pull request addresses and resolves building errors by refactoring the OfflineTtsZipVoiceModelConfig across various language bindings. The core change involves renaming the pinyinDict parameter to lexicon to standardize terminology and improve clarity in text-to-speech configurations. Additionally, the Python example script for offline zero-shot TTS has been updated to align with these changes, including new model paths and example content, ensuring the examples remain functional and up-to-date.
+
+diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+index 8ce77797..2ad7c6b2 100644
+--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
++++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+@@ -220,7 +220,7 @@ final class SherpaOnnxOfflineTtsZipVoiceModelConfig extends Struct {
+   external Pointer<Utf8> flowMatchingModel;
+   external Pointer<Utf8> vocoder;
+   external Pointer<Utf8> dataDir;
+-  external Pointer<Utf8> pinyinDict;
++  external Pointer<Utf8> lexicon;
+ 
+   @Float()
+   external double featScale;
+diff --git a/flutter/sherpa_onnx/lib/src/tts.dart b/flutter/sherpa_onnx/lib/src/tts.dart
+index be3fce65..dc1b1a8a 100644
+--- a/flutter/sherpa_onnx/lib/src/tts.dart
++++ b/flutter/sherpa_onnx/lib/src/tts.dart
+@@ -199,7 +199,7 @@ class OfflineTtsZipVoiceModelConfig {
+     this.flowMatchingModel = '',
+     this.vocoder = '',
+     this.dataDir = '',
+-    this.pinyinDict = '',
++    this.lexicon = '',
+     this.featScale = 0.1,
+     this.tShift = 0.5,
+     this.targetRms = 0.1,
+@@ -213,7 +213,7 @@ class OfflineTtsZipVoiceModelConfig {
+       flowMatchingModel: json['flowMatchingModel'] as String? ?? '',
+       vocoder: json['vocoder'] as String? ?? '',
+       dataDir: json['dataDir'] as String? ?? '',
+-      pinyinDict: json['pinyinDict'] as String? ?? '',
++      lexicon: json['lexicon'] as String? ?? '',
+       featScale: (json['featScale'] as num?)?.toDouble() ?? 0.1,
+       tShift: (json['tShift'] as num?)?.toDouble() ?? 0.5,
+       targetRms: (json['targetRms'] as num?)?.toDouble() ?? 0.1,
+@@ -223,7 +223,7 @@ class OfflineTtsZipVoiceModelConfig {
+ 
+   @override
+   String toString() {
+-    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, pinyinDict: $pinyinDict, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
++    return 'OfflineTtsZipVoiceModelConfig(tokens: $tokens, textModel: $textModel, flowMatchingModel: $flowMatchingModel, vocoder: $vocoder, dataDir: $dataDir, lexicon: $lexicon, featScale: $featScale, tShift: $tShift, targetRms: $targetRms, guidanceScale: $guidanceScale)';
+   }
+ 
+   Map<String, dynamic> toJson() => {
+@@ -232,7 +232,7 @@ class OfflineTtsZipVoiceModelConfig {
+         'flowMatchingModel': flowMatchingModel,
+         'vocoder': vocoder,
+         'dataDir': dataDir,
+-        'pinyinDict': pinyinDict,
++        'lexicon': lexicon,
+         'featScale': featScale,
+         'tShift': tShift,
+         'targetRms': targetRms,
+@@ -244,7 +244,7 @@ class OfflineTtsZipVoiceModelConfig {
+   final String flowMatchingModel;
+   final String vocoder;
+   final String dataDir;
+-  final String pinyinDict;
++  final String lexicon;
+   final double featScale;
+   final double tShift;
+   final double targetRms;
+@@ -406,7 +406,7 @@ class OfflineTts {
+     c.ref.model.zipvoice.flowMatchingModel = config.model.zipvoice.flowMatchingModel.toNativeUtf8();
+     c.ref.model.zipvoice.vocoder = config.model.zipvoice.vocoder.toNativeUtf8();
+     c.ref.model.zipvoice.dataDir = config.model.zipvoice.dataDir.toNativeUtf8();
+-    c.ref.model.zipvoice.pinyinDict = config.model.zipvoice.pinyinDict.toNativeUtf8();
++    c.ref.model.zipvoice.lexicon = config.model.zipvoice.lexicon.toNativeUtf8();
+     c.ref.model.zipvoice.featScale = config.model.zipvoice.featScale;
+     c.ref.model.zipvoice.tShift = config.model.zipvoice.tShift;
+     c.ref.model.zipvoice.targetRms = config.model.zipvoice.targetRms;
+@@ -427,7 +427,7 @@ class OfflineTts {
+     calloc.free(c.ref.ruleFsts);
+     calloc.free(c.ref.model.provider);
+ 
+-    calloc.free(c.ref.model.zipvoice.pinyinDict);
++    calloc.free(c.ref.model.zipvoice.lexicon);
+     calloc.free(c.ref.model.zipvoice.dataDir);
+     calloc.free(c.ref.model.zipvoice.vocoder);
+     calloc.free(c.ref.model.zipvoice.flowMatchingModel);
+diff --git a/python-api-examples/offline-zeroshot-tts.py b/python-api-examples/offline-zeroshot-tts.py
+index 805f06fe..746fbb3d 100755
+--- a/python-api-examples/offline-zeroshot-tts.py
++++ b/python-api-examples/offline-zeroshot-tts.py
+@@ -10,21 +10,23 @@ Usage:
+ 
+ Example (zipvoice)
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
+-tar xf sherpa-onnx-zipvoice-distill-zh-en-emilia.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++tar xf sherpa-onnx-zipvoice-distill-int8-zh-en-emilia.tar.bz2
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos_24khz.onnx
+ 
+ python3 ./python-api-examples/offline-zeroshot-tts.py \
+-  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-zh-en-emilia/fm_decoder.onnx \
+-  --zipvoice-text-model sherpa-onnx-zipvoice-distill-zh-en-emilia/text_encoder.onnx \
+-  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-zh-en-emilia/espeak-ng-data \
+-  --zipvoice-pinyin-dict sherpa-onnx-zipvoice-distill-zh-en-emilia/pinyin.raw \
+-  --zipvoice-tokens sherpa-onnx-zipvoice-distill-zh-en-emilia/tokens.txt \
+-  --zipvoice-vocoder sherpa-onnx-zipvoice-distill-zh-en-emilia/vocos_24khz.onnx \
+-  --prompt-audio sherpa-onnx-zipvoice-distill-zh-en-emilia/prompt.wav \
++  --zipvoice-flow-matching-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/decoder.int8.onnx \
++  --zipvoice-text-model sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/encoder.int8.onnx \
++  --zipvoice-data-dir sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/espeak-ng-data \
++  --zipvoice-lexicon sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/lexicon.txt \
++  --zipvoice-tokens sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/tokens.txt \
++  --zipvoice-vocoder vocos_24khz.onnx \
++  --prompt-audio sherpa-onnx-zipvoice-distill-int8-zh-en-emilia/test_wavs/leijun-1.wav \
+   --zipvoice-num-steps 4 \
+   --num-threads 4 \
+-  --prompt-text "" \
+-  ""
++  --prompt-text ", . ." \
++  ", . . , ."
+ """
+ 
+ import argparse
+@@ -68,10 +70,10 @@ def add_zipvoice_args(parser):
+     )
+ 
+     parser.add_argument(
+-        "--zipvoice-pinyin-dict",
++        "--zipvoice-lexicon",
+         type=str,
+         default="",
+-        help="Path to the pinyin dictionary.",
++        help="Path to the lexicon.txt",
+     )
+ 
+     parser.add_argument(
+@@ -236,7 +238,7 @@ def main():
+                 text_model=args.zipvoice_text_model,
+                 flow_matching_model=args.zipvoice_flow_matching_model,
+                 data_dir=args.zipvoice_data_dir,
+-                pinyin_dict=args.zipvoice_pinyin_dict,
++                lexicon=args.zipvoice_lexicon,
+                 vocoder=args.zipvoice_vocoder,
+                 feat_scale=args.zipvoice_feat_scale,
+                 t_shift=args.zipvoice_t_shift,
+@@ -268,9 +270,7 @@ def main():
+     end = time.time()
+ 
+     if len(audio.samples) == 0:
+-        print(
+-            "Error in generating audios. Please read previous error messages."
+-        )
++        print("Error in generating audios. Please read previous error messages.")
+         return
+ 
+     elapsed_seconds = end - start
+@@ -287,9 +287,7 @@ def main():
+     print(f"The text is '{args.text}'")
+     print(f"Elapsed seconds: {elapsed_seconds:.3f}")
+     print(f"Audio duration in seconds: {audio_duration:.3f}")
+-    print(
+-        f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}"
+-    )
++    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
+ 
+ 
+ if __name__ == "__main__":
+diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
+index 721db857..e91fa835 100644
+--- a/scripts/go/sherpa_onnx.go
++++ b/scripts/go/sherpa_onnx.go
+@@ -484,7 +484,7 @@ type OfflineModelConfig struct {
+ 	ZipformerCtc OfflineZipformerCtcModelConfig
+ 	Canary       OfflineCanaryModelConfig
+ 	WenetCtc     OfflineWenetCtcModelConfig
+-	Omnilingual     OfflineOmnilingualAsrCtcModelConfig
++	Omnilingual  OfflineOmnilingualAsrCtcModelConfig
+ 	Tokens       string // Path to tokens.txt
+ 
+ 	// Number of threads to use for neural network computation
+@@ -964,7 +964,7 @@ type OfflineTtsZipvoiceModelConfig struct {
+ 	TextModel         string // Path to text encoder (e.g. text_encoder.onnx)
+ 	FlowMatchingModel string // Path to flow-matching decoder (e.g. fm_decoder.onnx)
+ 	DataDir           string // Path to espeak-ng-data
+-	PinyinDict        string // Path to pinyin.raw (needed for zh)
++	Lexicon           string // Path to lexicon.txt (needed for zh)
+ 	Vocoder           string // Path to vocoder (e.g. vocos_24khz.onnx)
+ 
+ 	FeatScale     float32 // Feature scale
+@@ -1148,8 +1148,8 @@ func NewOfflineTts(config *OfflineTtsConfig) *OfflineTts {
+ 	c.model.zipvoice.data_dir = C.CString(config.Model.Zipvoice.DataDir)
+ 	defer C.free(unsafe.Pointer(c.model.zipvoice.data_dir))
+ 
+-	c.model.zipvoice.pinyin_dict = C.CString(config.Model.Zipvoice.PinyinDict)
+-	defer C.free(unsafe.Pointer(c.model.zipvoice.pinyin_dict))
++	c.model.zipvoice.lexicon = C.CString(config.Model.Zipvoice.Lexicon)
++	defer C.free(unsafe.Pointer(c.model.zipvoice.lexicon))
+ 
+ 	c.model.zipvoice.feat_scale = C.float(config.Model.Zipvoice.FeatScale)
+ 	c.model.zipvoice.t_shift = C.float(config.Model.Zipvoice.TShift)
+diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
+index 99422a06..bfb30355 100644
+--- a/sherpa-onnx/c-api/c-api.cc
++++ b/sherpa-onnx/c-api/c-api.cc
+@@ -703,9 +703,11 @@ const SherpaOnnxOfflineRecognizerResult *SherpaOnnxGetOfflineStreamResult(
+       r->durations = nullptr;
+     }
+ 
+-    if (!result.ys_log_probs.empty() && result.ys_log_probs.size() == r->count) {
++    if (!result.ys_log_probs.empty() &&
++        result.ys_log_probs.size() == r->count) {
+       r->ys_log_probs = new float[r->count];
+-      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(), r->ys_log_probs);
++      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(),
++                r->ys_log_probs);
+     } else {
+       r->ys_log_probs = nullptr;
+     }
+@@ -1248,8 +1250,8 @@ static sherpa_onnx::OfflineTtsConfig GetOfflineTtsConfig(
+       SHERPA_ONNX_OR(config->model.zipvoice.vocoder, "");
+   tts_config.model.zipvoice.data_dir =
+       SHERPA_ONNX_OR(config->model.zipvoice.data_dir, "");
+-  tts_config.model.zipvoice.pinyin_dict =
+-      SHERPA_ONNX_OR(config->model.zipvoice.pinyin_dict, "");
++  tts_config.model.zipvoice.lexicon =
++      SHERPA_ONNX_OR(config->model.zipvoice.lexicon, "");
+   tts_config.model.zipvoice.feat_scale =
+       SHERPA_ONNX_OR(config->model.zipvoice.feat_scale, 0.1f);
+   tts_config.model.zipvoice.t_shift =
+diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
+index e3f7e016..38d99c5a 100644
+--- a/sherpa-onnx/c-api/c-api.h
++++ b/sherpa-onnx/c-api/c-api.h
+@@ -1072,7 +1072,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineTtsZipvoiceModelConfig {
+   const char *flow_matching_model;
+   const char *vocoder;
+   const char *data_dir;
+-  const char *pinyin_dict;
++  const char *lexicon;
+   float feat_scale;
+   float t_shift;
+   float target_rms;
+diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
+index a7e35c55..f1f2feec 100644
+--- a/sherpa-onnx/c-api/cxx-api.cc
++++ b/sherpa-onnx/c-api/cxx-api.cc
+@@ -419,7 +419,7 @@ OfflineTts OfflineTts::Create(const OfflineTtsConfig &config) {
+       config.model.zipvoice.flow_matching_model.c_str();
+   c.model.zipvoice.vocoder = config.model.zipvoice.vocoder.c_str();
+   c.model.zipvoice.data_dir = config.model.zipvoice.data_dir.c_str();
+-  c.model.zipvoice.pinyin_dict = config.model.zipvoice.pinyin_dict.c_str();
++  c.model.zipvoice.lexicon = config.model.zipvoice.lexicon.c_str();
+   c.model.zipvoice.feat_scale = config.model.zipvoice.feat_scale;
+   c.model.zipvoice.t_shift = config.model.zipvoice.t_shift;
+   c.model.zipvoice.target_rms = config.model.zipvoice.target_rms;
+diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
+index 77401e67..f7144c50 100644
+--- a/sherpa-onnx/c-api/cxx-api.h
++++ b/sherpa-onnx/c-api/cxx-api.h
+@@ -432,7 +432,7 @@ struct OfflineTtsZipvoiceModelConfig {
+   std::string flow_matching_model;
+   std::string vocoder;
+   std::string data_dir;
+-  std::string pinyin_dict;
++  std::string lexicon;
+ 
+   float feat_scale = 0.1;
+   float t_shift = 0.5;
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+index 1226adf8..70de8667 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+@@ -18,9 +18,6 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
+   po->Register("zipvoice-data-dir", &data_dir,
+                "Path to the directory containing dict for espeak-ng.");
+   po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
+-  po->Register("zipvoice-pinyin-dict", &pinyin_dict,
+-               "Path to the pinyin dictionary for cppinyin (i.e converting "
+-               "Chinese into phones).");
+   po->Register("zipvoice-text-model", &text_model,
+                "Path to zipvoice text model");
+   po->Register("zipvoice-flow-matching-model", &flow_matching_model,
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+index 5b5d0c24..702760d0 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+@@ -21,9 +21,6 @@ struct OfflineTtsZipvoiceModelConfig {
+   std::string data_dir;
+   std::string lexicon;
+ 
+-  // Used for converting Chinese characters to pinyin
+-  std::string pinyin_dict;
+-
+   float feat_scale = 0.1;
+   float t_shift = 0.5;
+   float target_rms = 0.1;
+@@ -35,15 +32,14 @@ struct OfflineTtsZipvoiceModelConfig {
+       const std::string &tokens, const std::string &text_model,
+       const std::string &flow_matching_model, const std::string &vocoder,
+       const std::string &data_dir, const std::string &lexicon,
+-      const std::string &pinyin_dict, float feat_scale = 0.1,
+-      float t_shift = 0.5, float target_rms = 0.1, float guidance_scale = 1.0)
++      float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
++      float guidance_scale = 1.0)
+       : tokens(tokens),
+         text_model(text_model),
+         flow_matching_model(flow_matching_model),
+         vocoder(vocoder),
+         data_dir(data_dir),
+         lexicon(lexicon),
+-        pinyin_dict(pinyin_dict),
+         feat_scale(feat_scale),
+         t_shift(t_shift),
+         target_rms(target_rms),
+diff --git a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+index 53451077..dc6e8fb2 100644
+--- a/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/python/csrc/offline-tts-zipvoice-model-config.cc
+@@ -21,7 +21,7 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
+                     float, float>(),
+            py::arg("tokens"), py::arg("text_model"),
+            py::arg("flow_matching_model"), py::arg("vocoder"),
+-           py::arg("data_dir") = "", py::arg("pinyin_dict") = "",
++           py::arg("data_dir") = "", py::arg("lexicon") = "",
+            py::arg("feat_scale") = 0.1, py::arg("t_shift") = 0.5,
+            py::arg("target_rms") = 0.1, py::arg("guidance_scale") = 1.0)
+       .def_readwrite("tokens", &PyClass::tokens)
+@@ -29,7 +29,7 @@ void PybindOfflineTtsZipvoiceModelConfig(py::module *m) {
+       .def_readwrite("flow_matching_model", &PyClass::flow_matching_model)
+       .def_readwrite("vocoder", &PyClass::vocoder)
+       .def_readwrite("data_dir", &PyClass::data_dir)
+-      .def_readwrite("pinyin_dict", &PyClass::pinyin_dict)
++      .def_readwrite("lexicon", &PyClass::lexicon)
+       .def_readwrite("feat_scale", &PyClass::feat_scale)
+       .def_readwrite("t_shift", &PyClass::t_shift)
+       .def_readwrite("target_rms", &PyClass::target_rms)
+diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
+index b18b8e1a..ee40fbde 100644
+--- a/swift-api-examples/SherpaOnnx.swift
++++ b/swift-api-examples/SherpaOnnx.swift
+@@ -933,7 +933,7 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
+   flowMatchingModel: String = "",
+   vocoder: String = "",
+   dataDir: String = "",
+-  pinyinDict: String = "",
++  lexicon: String = "",
+   featScale: Float = 0.1,
+   tShift: Float = 0.5,
+   targetRms: Float = 0.1,
+@@ -945,7 +945,7 @@ func sherpaOnnxOfflineTtsZipvoiceModelConfig(
+     flow_matching_model: toCPointer(flowMatchingModel),
+     vocoder: toCPointer(vocoder),
+     data_dir: toCPointer(dataDir),
+-    pinyin_dict: toCPointer(pinyinDict),
++    lexicon: toCPointer(lexicon),
+     feat_scale: featScale,
+     t_shift: tShift,
+     target_rms: targetRms,
+diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
+index e24bbc3f..2c1b0c36 100644
+--- a/wasm/tts/sherpa-onnx-tts.js
++++ b/wasm/tts/sherpa-onnx-tts.js
+@@ -269,10 +269,10 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+       Module.lengthBytesUTF8(config.flowMatchingModel || '') + 1;
+   const vocoderLen = Module.lengthBytesUTF8(config.vocoder || '') + 1;
+   const dataDirLen = Module.lengthBytesUTF8(config.dataDir || '') + 1;
+-  const pinyinDictLen = Module.lengthBytesUTF8(config.pinyinDict || '') + 1;
++  const lexiconLen = Module.lengthBytesUTF8(config.lexicon || '') + 1;
+ 
+   const n = tokensLen + textModelLen + flowMatchingModelLen + vocoderLen +
+-      dataDirLen + pinyinDictLen;
++      dataDirLen + lexiconLen;
+ 
+   const buffer = Module._malloc(n);
+ 
+@@ -296,8 +296,8 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+   Module.stringToUTF8(config.dataDir || '', buffer + offset, dataDirLen);
+   offset += dataDirLen;
+ 
+-  Module.stringToUTF8(config.pinyinDict || '', buffer + offset, pinyinDictLen);
+-  offset += pinyinDictLen;
++  Module.stringToUTF8(config.lexicon || '', buffer + offset, lexiconLen);
++  offset += lexiconLen;
+ 
+   offset = 0;
+   Module.setValue(ptr, buffer + offset, 'i8*');
+@@ -316,7 +316,7 @@ function initSherpaOnnxOfflineTtsZipVoiceModelConfig(config, Module) {
+   offset += dataDirLen;
+ 
+   Module.setValue(ptr + 20, buffer + offset, 'i8*');
+-  offset += pinyinDictLen;
++  offset += lexiconLen;
+ 
+   Module.setValue(ptr + 24, config.featScale || 0.1, 'float');
+   Module.setValue(ptr + 28, config.tShift || 0.5, 'float');
+@@ -381,7 +381,7 @@ function initSherpaOnnxOfflineTtsModelConfig(config, Module) {
+       flowMatchingModel: '',
+       vocoder: '',
+       dataDir: '',
+-      pinyinDict: '',
++      lexicon: '',
+       featScale: 0.1,
+       tShift: 0.5,
+       targetRMS: 0.1,
+diff --git a/wasm/tts/sherpa-onnx-wasm-main-tts.cc b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+index e6707135..99deb46c 100644
+--- a/wasm/tts/sherpa-onnx-wasm-main-tts.cc
++++ b/wasm/tts/sherpa-onnx-wasm-main-tts.cc
+@@ -79,7 +79,7 @@ void MyPrint(SherpaOnnxOfflineTtsConfig *tts_config) {
+   fprintf(stdout, "flow_matching_model: %s\n", zipvoice->flow_matching_model);
+   fprintf(stdout, "vocoder: %s\n", zipvoice->vocoder);
+   fprintf(stdout, "data_dir: %s\n", zipvoice->data_dir);
+-  fprintf(stdout, "pinyin_dict: %s\n", zipvoice->pinyin_dict);
++  fprintf(stdout, "lexicon: %s\n", zipvoice->lexicon);
+   fprintf(stdout, "feat scale: %.3f\n", zipvoice->feat_scale);
+   fprintf(stdout, "t_shift: %.3f\n", zipvoice->t_shift);
+   fprintf(stdout, "target_rms: %.3f\n", zipvoice->target_rms);
+
+commit 1b59efdbd23acbd61242d0d7cb0ff02bab6d3382
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 11 18:57:22 2025 +0800
+
+    Remove cppinyin from zipvoice (#2892)
+
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 84be55ea..57cb8e7f 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -496,8 +496,6 @@ if(SHERPA_ONNX_ENABLE_WEBSOCKET)
+ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_TTS)
+-  include(cppinyin)
+-
+   include(espeak-ng-for-piper)
+   set(ESPEAK_NG_DIR ${espeak_ng_SOURCE_DIR})
+   message(STATUS "ESPEAK_NG_DIR: ${ESPEAK_NG_DIR}")
+diff --git a/build-ios.sh b/build-ios.sh
+index c2bba819..6e23ea0c 100755
+--- a/build-ios.sh
++++ b/build-ios.sh
+@@ -127,7 +127,7 @@ cmake --build build/os64 --target install
+ echo "Generate xcframework"
+ 
+ mkdir -p "build/simulator/lib"
+-for f in libcppinyin_core.a libkaldi-native-fbank-core.a libkissfft-float.a libsherpa-onnx-c-api.a libsherpa-onnx-core.a \
++for f in libkaldi-native-fbank-core.a libkissfft-float.a libsherpa-onnx-c-api.a libsherpa-onnx-core.a \
+          libsherpa-onnx-fstfar.a libssentencepiece_core.a \
+          libsherpa-onnx-fst.a libsherpa-onnx-kaldifst-core.a libkaldi-decoder-core.a \
+          libucd.a libpiper_phonemize.a libespeak-ng.a; do
+@@ -139,7 +139,6 @@ done
+ # Merge archive first, because the following xcodebuild create xcframework
+ # cannot accept multi archive with the same architecture.
+ libtool -static -o build/simulator/libsherpa-onnx.a \
+-  build/simulator/lib/libcppinyin_core.a \
+   build/simulator/lib/libkaldi-native-fbank-core.a \
+   build/simulator/lib/libkissfft-float.a \
+   build/simulator/lib/libsherpa-onnx-c-api.a \
+@@ -154,7 +153,6 @@ libtool -static -o build/simulator/libsherpa-onnx.a \
+   build/simulator/lib/libssentencepiece_core.a
+ 
+ libtool -static -o build/os64/libsherpa-onnx.a \
+-  build/os64/lib/libcppinyin_core.a \
+   build/os64/lib/libkaldi-native-fbank-core.a \
+   build/os64/lib/libkissfft-float.a \
+   build/os64/lib/libsherpa-onnx-c-api.a \
+diff --git a/build-swift-macos.sh b/build-swift-macos.sh
+index 8fef5da7..1e1e8e9c 100755
+--- a/build-swift-macos.sh
++++ b/build-swift-macos.sh
+@@ -27,7 +27,6 @@ make install
+ rm -fv ./install/include/cargs.h
+ 
+ libtool -static -o ./install/lib/libsherpa-onnx.a \
+-  ./install/lib/libcppinyin_core.a \
+   ./install/lib/libsherpa-onnx-c-api.a \
+   ./install/lib/libsherpa-onnx-core.a \
+   ./install/lib/libkaldi-native-fbank-core.a \
+diff --git a/c-api-examples/Makefile b/c-api-examples/Makefile
+index 6480e985..cbaaf63c 100644
+--- a/c-api-examples/Makefile
++++ b/c-api-examples/Makefile
+@@ -4,7 +4,7 @@ CUR_DIR :=$(shell pwd)
+ CFLAGS := -I ../ -I ../build/_deps/cargs-src/include/
+ LDFLAGS := -L ../build/lib
+ LDFLAGS += -L ../build/_deps/onnxruntime-src/lib
+-LDFLAGS += -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lcargs -lonnxruntime -lcppinyin_core
++LDFLAGS += -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lcargs -lonnxruntime
+ LDFLAGS += -framework Foundation
+ LDFLAGS += -lc++
+ LDFLAGS += -Wl,-rpath,${CUR_DIR}/../build/lib
+diff --git a/cmake/cppinyin.cmake b/cmake/cppinyin.cmake
+deleted file mode 100644
+index 9e2c92b5..00000000
+--- a/cmake/cppinyin.cmake
++++ /dev/null
+@@ -1,82 +0,0 @@
+-function(download_cppinyin)
+-  include(FetchContent)
+-
+-  set(cppinyin_URL "https://github.com/pkufool/cppinyin/archive/refs/tags/v0.10.tar.gz")
+-  set(cppinyin_URL2 "https://gh-proxy.com/https://github.com/pkufool/cppinyin/archive/refs/tags/v0.10.tar.gz")
+-  set(cppinyin_HASH "SHA256=abe6584d7ee56829e8f4b5fbda3b50ecdf49a13be8e413a78d1b0d5d5c019982")
+-
+-  # If you don't have access to the Internet,
+-  # please pre-download cppinyin
+-  set(possible_file_locations
+-    $ENV{HOME}/Downloads/cppinyin-0.10.tar.gz
+-    ${CMAKE_SOURCE_DIR}/cppinyin-0.10.tar.gz
+-    ${CMAKE_BINARY_DIR}/cppinyin-0.10.tar.gz
+-    /tmp/cppinyin-0.10.tar.gz
+-    /star-fj/fangjun/download/github/cppinyin-0.10.tar.gz
+-  )
+-
+-  foreach(f IN LISTS possible_file_locations)
+-    if(EXISTS ${f})
+-      set(cppinyin_URL  "${f}")
+-      file(TO_CMAKE_PATH "${cppinyin_URL}" cppinyin_URL)
+-      message(STATUS "Found local downloaded cppinyin: ${cppinyin_URL}")
+-      set(cppinyin_URL2)
+-      break()
+-    endif()
+-  endforeach()
+-
+-  set(CPPINYIN_ENABLE_TESTS OFF CACHE BOOL "" FORCE)
+-  set(CPPINYIN_BUILD_PYTHON OFF CACHE BOOL "" FORCE)
+-
+-  FetchContent_Declare(cppinyin
+-    URL
+-      ${cppinyin_URL}
+-      ${cppinyin_URL2}
+-    URL_HASH
+-      ${cppinyin_HASH}
+-  )
+-
+-  FetchContent_GetProperties(cppinyin)
+-  if(NOT cppinyin_POPULATED)
+-    message(STATUS "Downloading cppinyin ${cppinyin_URL}")
+-    FetchContent_Populate(cppinyin)
+-
+-    file(REMOVE ${cppinyin_SOURCE_DIR}/CMakeLists.txt)
+-    configure_file(
+-        ${CMAKE_CURRENT_LIST_DIR}/cppinyin.patch
+-        ${cppinyin_SOURCE_DIR}/CMakeLists.txt
+-        COPYONLY
+-    )
+-  endif()
+-
+-  message(STATUS "cppinyin is downloaded to ${cppinyin_SOURCE_DIR}")
+-
+-  if(BUILD_SHARED_LIBS)
+-    set(_build_shared_libs_bak ${BUILD_SHARED_LIBS})
+-    set(BUILD_SHARED_LIBS OFF)
+-  endif()
+-
+-  add_subdirectory(${cppinyin_SOURCE_DIR} ${cppinyin_BINARY_DIR} EXCLUDE_FROM_ALL)
+-
+-  if(_build_shared_libs_bak)
+-    set_target_properties(cppinyin_core
+-      PROPERTIES
+-        POSITION_INDEPENDENT_CODE ON
+-        C_VISIBILITY_PRESET hidden
+-        CXX_VISIBILITY_PRESET hidden
+-    )
+-    set(BUILD_SHARED_LIBS ON)
+-  endif()
+-
+-  target_include_directories(cppinyin_core
+-    PUBLIC
+-      ${cppinyin_SOURCE_DIR}/
+-  )
+-
+-  if(NOT BUILD_SHARED_LIBS)
+-    install(TARGETS cppinyin_core DESTINATION lib)
+-  endif()
+-
+-endfunction()
+-
+-download_cppinyin()
+diff --git a/cmake/cppinyin.patch b/cmake/cppinyin.patch
+deleted file mode 100644
+index ec146874..00000000
+--- a/cmake/cppinyin.patch
++++ /dev/null
+@@ -1,81 +0,0 @@
+-cmake_minimum_required(VERSION 3.12 FATAL_ERROR)
+-
+-project(cppinyin)
+-
+-set(CPPINYIN_VERSION "0.10")
+-
+-set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/lib")
+-set(CMAKE_LIBRARY_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/lib")
+-set(CMAKE_RUNTIME_OUTPUT_DIRECTORY "${CMAKE_BINARY_DIR}/bin")
+-
+-set(CMAKE_SKIP_BUILD_RPATH FALSE)
+-set(BUILD_RPATH_USE_ORIGIN TRUE)
+-set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+-
+-if(NOT APPLE)
+-  set(CPPINYIN_RPATH_ORIGIN "$ORIGIN")
+-else()
+-  set(CPPINYIN_RPATH_ORIGIN "@loader_path")
+-endif()
+-
+-set(CMAKE_INSTALL_RPATH ${CPPINYIN_RPATH_ORIGIN})
+-set(CMAKE_BUILD_RPATH ${CPPINYIN_RPATH_ORIGIN})
+-
+-option(CPPINYIN_ENABLE_TESTS "Whether to build tests" OFF)
+-option(CPPINYIN_BUILD_PYTHON "Whether to build Python" OFF)
+-option(BUILD_SHARED_LIBS "Whether to build shared libraries" ON)
+-
+-if(NOT CMAKE_BUILD_TYPE)
+-  message(STATUS "No CMAKE_BUILD_TYPE given, default to Release")
+-  set(CMAKE_BUILD_TYPE Release)
+-endif()
+-
+-list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake/Modules)
+-list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)
+-
+-include(CheckCXXCompilerFlag)
+-if(NOT WIN32)
+-  check_cxx_compiler_flag("-std=c++14" CPPINYIN_COMPILER_SUPPORTS_CXX14)
+-else()
+-  # windows x86 or x86_64
+-  check_cxx_compiler_flag("/std:c++14" CPPINYIN_COMPILER_SUPPORTS_CXX14)
+-endif()
+-if(NOT CPPINYIN_COMPILER_SUPPORTS_CXX14)
+-  message(FATAL_ERROR "
+-    cppinyin requires a compiler supporting at least C++14.
+-    If you are using GCC, please upgrade it to at least version 7.0.
+-    If you are using Clang, please upgrade it to at least version 3.4.")
+-endif()
+-
+-if(NOT CMAKE_CXX_STANDARD)
+-  set(CMAKE_CXX_STANDARD 14 CACHE STRING "The C++ version to be used.")
+-endif()
+-set(CMAKE_CXX_EXTENSIONS OFF)
+-message(STATUS "C++ Standard version: ${CMAKE_CXX_STANDARD}")
+-
+-if(CPPINYIN_BUILD_PYTHON)
+-  include(pybind11)
+-endif()
+-
+-include_directories(${CMAKE_SOURCE_DIR})
+-
+-if(WIN32)
+-  # disable various warnings for MSVC
+-  # 4244: 'initializing': conversion from 'float' to 'int32_t',
+-  # 4267: 'argument': conversion from 'size_t' to 'uint32_t', possible loss of data
+-  set(disabled_warnings
+-      /wd4244
+-      /wd4267
+-  )
+-  message(STATUS "Disabled warnings: ${disabled_warnings}")
+-  foreach(w IN LISTS disabled_warnings)
+-    string(APPEND CMAKE_CXX_FLAGS " ${w} ")
+-  endforeach()
+-endif()
+-
+-if(CPPINYIN_ENABLE_TESTS)
+-  include(googletest)
+-  enable_testing()
+-endif()
+-
+-add_subdirectory(cppinyin)
+diff --git a/cmake/sherpa-onnx-static.pc.in b/cmake/sherpa-onnx-static.pc.in
+index 061905fc..42beb084 100644
+--- a/cmake/sherpa-onnx-static.pc.in
++++ b/cmake/sherpa-onnx-static.pc.in
+@@ -22,4 +22,4 @@ Cflags: -I"${includedir}"
+ # Note: -lcargs is required only for the following file
+ # https://github.com/k2-fsa/sherpa-onnx/blob/master/c-api-examples/decode-file-c-api.c
+ # We add it here so that users don't need to specify -lcargs when compiling decode-file-c-api.c
+-Libs: -L"${libdir}" -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lonnxruntime -lssentencepiece_core -lcppinyin_core -Wl,-rpath,${libdir} @SHERPA_ONNX_PKG_WITH_CARGS@ @SHERPA_ONNX_PKG_CONFIG_EXTRA_LIBS@
++Libs: -L"${libdir}" -lsherpa-onnx-c-api -lsherpa-onnx-core -lkaldi-decoder-core -lsherpa-onnx-kaldifst-core -lsherpa-onnx-fstfar -lsherpa-onnx-fst -lkaldi-native-fbank-core -lkissfft-float -lpiper_phonemize -lespeak-ng -lucd -lonnxruntime -lssentencepiece_core -Wl,-rpath,${libdir} @SHERPA_ONNX_PKG_WITH_CARGS@ @SHERPA_ONNX_PKG_CONFIG_EXTRA_LIBS@
+diff --git a/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props b/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
+index cc0aa014..9b35d475 100644
+--- a/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
++++ b/mfc-examples/NonStreamingSpeechRecognition/sherpa-onnx-deps.props
+@@ -15,7 +15,6 @@
+         sherpa-onnx-fst.lib;
+         kaldi-native-fbank-core.lib;
+         kissfft-float.lib;
+-        cppinyin_core.lib;
+         onnxruntime.lib;
+         piper_phonemize.lib;
+         espeak-ng.lib;
+diff --git a/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props b/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
+index cc0aa014..9b35d475 100644
+--- a/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
++++ b/mfc-examples/NonStreamingTextToSpeech/sherpa-onnx-deps.props
+@@ -15,7 +15,6 @@
+         sherpa-onnx-fst.lib;
+         kaldi-native-fbank-core.lib;
+         kissfft-float.lib;
+-        cppinyin_core.lib;
+         onnxruntime.lib;
+         piper_phonemize.lib;
+         espeak-ng.lib;
+diff --git a/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props b/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
+index cc0aa014..9b35d475 100644
+--- a/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
++++ b/mfc-examples/StreamingSpeechRecognition/sherpa-onnx-deps.props
+@@ -15,7 +15,6 @@
+         sherpa-onnx-fst.lib;
+         kaldi-native-fbank-core.lib;
+         kissfft-float.lib;
+-        cppinyin_core.lib;
+         onnxruntime.lib;
+         piper_phonemize.lib;
+         espeak-ng.lib;
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 2a7e7433..d03dd75d 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -261,7 +261,6 @@ if(SHERPA_ONNX_ENABLE_TTS)
+     offline-tts-model-config.cc
+     offline-tts-vits-model-config.cc
+     offline-tts-vits-model.cc
+-    offline-tts-zipvoice-frontend.cc
+     offline-tts-zipvoice-model-config.cc
+     offline-tts-zipvoice-model.cc
+     offline-tts.cc
+@@ -417,7 +416,6 @@ target_link_libraries(sherpa-onnx-core fstfar fst)
+ 
+ if(SHERPA_ONNX_ENABLE_TTS)
+   target_link_libraries(sherpa-onnx-core
+-    cppinyin_core
+     piper_phonemize)
+ endif()
+ 
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 873918d0..77d47f39 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -123,8 +123,12 @@ std::vector<std::string> SplitTokensUTF8(const std::string &s) {
+ }
+ 
+ std::vector<std::string> ProcessPhonemes(
+-    const std::vector<std::vector<char32_t>> &phonemes) {
++    const std::vector<std::vector<char32_t>> &phonemes, bool skip_replacement) {
+   auto tokens = ConvertPhonemesToUTF8(phonemes);
++  if (skip_replacement) {
++    return tokens;
++  }
++
+   std::string joined = Join(tokens);
+   std::string replaced = ApplyReplacements(joined);
+   return SplitTokensUTF8(replaced);
+@@ -139,8 +143,8 @@ void CallPhonemizeEspeak(const std::string &text,
+ class MatchaTtsLexicon::Impl {
+  public:
+   Impl(const std::string &lexicon, const std::string &tokens,
+-       const std::string &data_dir, bool debug)
+-      : debug_(debug) {
++       const std::string &data_dir, bool debug, bool skip_replacement)
++      : debug_(debug), skip_replacement_(skip_replacement) {
+     if (lexicon.empty()) {
+       SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
+       SHERPA_ONNX_EXIT(-1);
+@@ -163,8 +167,8 @@ class MatchaTtsLexicon::Impl {
+ 
+   template <typename Manager>
+   Impl(Manager *mgr, const std::string &lexicon, const std::string &tokens,
+-       const std::string &data_dir, bool debug)
+-      : debug_(debug) {
++       const std::string &data_dir, bool debug, bool skip_replacement)
++      : debug_(debug), skip_replacement_(skip_replacement) {
+     if (lexicon.empty()) {
+       SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
+       SHERPA_ONNX_EXIT(-1);
+@@ -360,7 +364,7 @@ class MatchaTtsLexicon::Impl {
+         std::vector<std::vector<piper::Phoneme>> phonemes;
+         CallPhonemizeEspeak(w, config, &phonemes);
+ 
+-        auto pp = ProcessPhonemes(phonemes);
++        auto pp = ProcessPhonemes(phonemes, skip_replacement_);
+ 
+         for (const auto &p : pp) {
+           if (token2id_.count(p)) {
+@@ -477,20 +481,25 @@ class MatchaTtsLexicon::Impl {
+   std::unordered_map<int32_t, std::string> id2token_;
+ 
+   bool debug_ = false;
++  bool skip_replacement_ = false;
+ };  // namespace sherpa_onnx
+ 
+ MatchaTtsLexicon::~MatchaTtsLexicon() = default;
+ 
+ MatchaTtsLexicon::MatchaTtsLexicon(const std::string &lexicon,
+                                    const std::string &tokens,
+-                                   const std::string &data_dir, bool debug)
+-    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug)) {}
++                                   const std::string &data_dir, bool debug,
++                                   bool skip_replacement)
++    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug,
++                                   skip_replacement)) {}
+ 
+ template <typename Manager>
+ MatchaTtsLexicon::MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
+                                    const std::string &tokens,
+-                                   const std::string &data_dir, bool debug)
+-    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug)) {}
++                                   const std::string &data_dir, bool debug,
++                                   bool skip_replacement)
++    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug,
++                                   skip_replacement)) {}
+ 
+ std::vector<TokenIDs> MatchaTtsLexicon::ConvertTextToTokenIds(
+     const std::string &text, const std::string & /*unused_voice = ""*/) const {
+@@ -502,7 +511,7 @@ template MatchaTtsLexicon::MatchaTtsLexicon(AAssetManager *mgr,
+                                             const std::string &lexicon,
+                                             const std::string &tokens,
+                                             const std::string &data_dir,
+-                                            bool debug);
++                                            bool debug, bool skip_replacement);
+ #endif
+ 
+ #if __OHOS__
+@@ -510,7 +519,7 @@ template MatchaTtsLexicon::MatchaTtsLexicon(NativeResourceManager *mgr,
+                                             const std::string &lexicon,
+                                             const std::string &tokens,
+                                             const std::string &data_dir,
+-                                            bool debug);
++                                            bool debug, bool skip_replacement);
+ #endif
+ 
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.h b/sherpa-onnx/csrc/matcha-tts-lexicon.h
+index f9da31a6..f23df439 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.h
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.h
+@@ -20,12 +20,13 @@ class MatchaTtsLexicon : public OfflineTtsFrontend {
+   ~MatchaTtsLexicon() override;
+ 
+   MatchaTtsLexicon(const std::string &lexicon, const std::string &tokens,
+-                   const std::string &data_dir, bool debug);
++                   const std::string &data_dir, bool debug,
++                   bool skip_replacement);
+ 
+   template <typename Manager>
+   MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
+                    const std::string &tokens, const std::string &data_dir,
+-                   bool debug);
++                   bool debug, bool skip_replacement);
+ 
+   std::vector<TokenIDs> ConvertTextToTokenIds(
+       const std::string &text,
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+index 92d7efff..9440e25a 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+@@ -386,7 +386,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+     if (meta_data.is_zh_en) {
+       frontend_ = std::make_unique<MatchaTtsLexicon>(
+           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
+-          config_.model.matcha.data_dir, config_.model.debug);
++          config_.model.matcha.data_dir, config_.model.debug, false);
+     } else if (meta_data.jieba) {
+       frontend_ = std::make_unique<CharacterLexicon>(
+           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
+@@ -407,7 +407,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+     if (meta_data.is_zh_en) {
+       frontend_ = std::make_unique<MatchaTtsLexicon>(
+           config_.model.matcha.lexicon, config_.model.matcha.tokens,
+-          config_.model.matcha.data_dir, config_.model.debug);
++          config_.model.matcha.data_dir, config_.model.debug, false);
+     } else if (meta_data.jieba) {
+       frontend_ = std::make_unique<CharacterLexicon>(
+           config_.model.matcha.lexicon, config_.model.matcha.tokens,
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+deleted file mode 100644
+index 0cf582c7..00000000
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
++++ /dev/null
+@@ -1,81 +0,0 @@
+-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
+-
+-#include <string>
+-#include <vector>
+-
+-#include "espeak-ng/speak_lib.h"
+-#include "gtest/gtest.h"
+-#include "phoneme_ids.hpp"  // NOLINT
+-#include "phonemize.hpp"    // NOLINT
+-#include "sherpa-onnx/csrc/file-utils.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-
+-namespace sherpa_onnx {
+-
+-TEST(ZipVoiceFrontend, Case1) {
+-  std::string data_dir = "../zipvoice/espeak-ng-data";
+-  if (!FileExists(data_dir + "/en_dict")) {
+-    SHERPA_ONNX_LOGE("%s/en_dict does not exist. Skipping test",
+-                     data_dir.c_str());
+-    return;
+-  }
+-
+-  if (!FileExists(data_dir + "/phontab")) {
+-    SHERPA_ONNX_LOGE("%s/phontab does not exist. Skipping test",
+-                     data_dir.c_str());
+-    return;
+-  }
+-
+-  if (!FileExists(data_dir + "/phonindex")) {
+-    SHERPA_ONNX_LOGE("%s/phonindex does not exist. Skipping test",
+-                     data_dir.c_str());
+-    return;
+-  }
+-
+-  if (!FileExists(data_dir + "/phondata")) {
+-    SHERPA_ONNX_LOGE("%s/phondata does not exist. Skipping test",
+-                     data_dir.c_str());
+-    return;
+-  }
+-
+-  if (!FileExists(data_dir + "/intonations")) {
+-    SHERPA_ONNX_LOGE("%s/intonations does not exist. Skipping test",
+-                     data_dir.c_str());
+-    return;
+-  }
+-
+-  std::string pinyin_dict = data_dir + "/../pinyin.dict";
+-  if (!FileExists(pinyin_dict)) {
+-    SHERPA_ONNX_LOGE("%s does not exist. Skipping test", pinyin_dict.c_str());
+-    return;
+-  }
+-
+-  std::string tokens_file = data_dir + "/../tokens.txt";
+-  if (!FileExists(tokens_file)) {
+-    SHERPA_ONNX_LOGE("%s does not exist. Skipping test", tokens_file.c_str());
+-    return;
+-  }
+-
+-  auto frontend = OfflineTtsZipvoiceFrontend(
+-      tokens_file, data_dir, pinyin_dict,
+-      OfflineTtsZipvoiceModelMetaData{.use_espeak = true, .use_pinyin = true},
+-      true);
+-
+-  std::string text = "how are you doing?";
+-  std::vector<sherpa_onnx::TokenIDs> ans =
+-      frontend.ConvertTextToTokenIds(text, "en-us");
+-
+-  text = "";
+-  ans = frontend.ConvertTextToTokenIds(text, "en-us");
+-
+-  text =
+-      "<pin1><yin2> [S1]and hello "
+-      "world[S2]";
+-  ans = frontend.ConvertTextToTokenIds(text, "en-us");
+-}
+-
+-}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+deleted file mode 100644
+index 26ca18f4..00000000
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
++++ /dev/null
+@@ -1,386 +0,0 @@
+-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#include <algorithm>
+-#include <cctype>
+-#include <codecvt>
+-#include <fstream>
+-#include <locale>
+-#include <memory>
+-#include <regex>
+-#include <sstream>
+-#include <string>
+-#include <strstream>
+-#include <unordered_map>
+-#include <utility>
+-#include <vector>
+-
+-#if __ANDROID_API__ >= 9
+-#include "android/asset_manager.h"
+-#include "android/asset_manager_jni.h"
+-#endif
+-
+-#if __OHOS__
+-#include "rawfile/raw_file_manager.h"
+-#endif
+-
+-#include "cppinyin/csrc/cppinyin.h"
+-#include "espeak-ng/speak_lib.h"
+-#include "phoneme_ids.hpp"  // NOLINT
+-#include "phonemize.hpp"    // NOLINT
+-#include "sherpa-onnx/csrc/file-utils.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
+-#include "sherpa-onnx/csrc/text-utils.h"
+-
+-namespace sherpa_onnx {
+-
+-void CallPhonemizeEspeak(const std::string &text,
+-                         piper::eSpeakPhonemeConfig &config,  // NOLINT
+-                         std::vector<std::vector<piper::Phoneme>> *phonemes);
+-
+-static std::unordered_map<std::string, int32_t> ReadTokens(std::istream &is) {
+-  std::unordered_map<std::string, int32_t> token2id;
+-
+-  std::string line;
+-  std::string sym;
+-  int32_t id = 0;
+-  while (std::getline(is, line)) {
+-    std::istringstream iss(line);
+-    iss >> sym;
+-    if (iss.eof()) {
+-      id = atoi(sym.c_str());
+-      sym = " ";
+-    } else {
+-      iss >> id;
+-    }
+-    // eat the trailing \r\n on windows
+-    iss >> std::ws;
+-    if (!iss.eof()) {
+-      SHERPA_ONNX_LOGE("Error when reading tokens: %s", line.c_str());
+-      exit(-1);
+-    }
+-
+-    if (token2id.count(sym)) {
+-      SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
+-                       sym.c_str(), line.c_str(), token2id.at(sym));
+-      exit(-1);
+-    }
+-    token2id.insert({sym, id});
+-  }
+-  return token2id;
+-}
+-
+-static std::string MapPunctuations(
+-    const std::string &text,
+-    const std::unordered_map<std::string, std::string> &punct_map) {
+-  std::string result = text;
+-  for (const auto &kv : punct_map) {
+-    // Replace all occurrences of kv.first with kv.second
+-    size_t pos = 0;
+-    while ((pos = result.find(kv.first, pos)) != std::string::npos) {
+-      result.replace(pos, kv.first.length(), kv.second);
+-      pos += kv.second.length();
+-    }
+-  }
+-  return result;
+-}
+-
+-static void ProcessPinyin(
+-    const std::string &pinyin, const cppinyin::PinyinEncoder *pinyin_encoder,
+-    const std::unordered_map<std::string, int32_t> &token2id,
+-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
+-  auto initial = pinyin_encoder->ToInitial(pinyin);
+-  if (!initial.empty()) {
+-    // append '0' to fix the conflict with espeak token
+-    initial = initial + "0";
+-    if (token2id.count(initial)) {
+-      tokens_ids->push_back(token2id.at(initial));
+-      tokens->push_back(initial);
+-    } else {
+-      SHERPA_ONNX_LOGE("Skip unknown initial %s", initial.c_str());
+-    }
+-  }
+-  auto final_t = pinyin_encoder->ToFinal(pinyin);
+-  if (!final_t.empty()) {
+-    if (!std::isdigit(final_t.back())) {
+-      final_t = final_t + "5";  // use 5 for neutral tone
+-    }
+-    if (token2id.count(final_t)) {
+-      tokens_ids->push_back(token2id.at(final_t));
+-      tokens->push_back(final_t);
+-    } else {
+-      SHERPA_ONNX_LOGE("Skip unknown final %s", final_t.c_str());
+-    }
+-  }
+-}
+-
+-static void TokenizeZh(const std::string &words,
+-                       const cppinyin::PinyinEncoder *pinyin_encoder,
+-                       const std::unordered_map<std::string, int32_t> &token2id,
+-                       std::vector<int64_t> *token_ids,
+-                       std::vector<std::string> *tokens) {
+-  std::vector<std::string> pinyins;
+-  pinyin_encoder->Encode(words, &pinyins, "number" /*tone*/, false /*partial*/);
+-  for (const auto &pinyin : pinyins) {
+-    if (pinyin_encoder->ValidPinyin(pinyin, "number" /*tone*/)) {
+-      ProcessPinyin(pinyin, pinyin_encoder, token2id, token_ids, tokens);
+-    } else {
+-      auto wstext = ToWideString(pinyin);
+-      for (auto &wc : wstext) {
+-        auto c = ToString(std::wstring(1, wc));
+-        if (token2id.count(c)) {
+-          token_ids->push_back(token2id.at(c));
+-          tokens->push_back(c);
+-        } else {
+-          SHERPA_ONNX_LOGE("Skip unknown character %s", c.c_str());
+-        }
+-      }
+-    }
+-  }
+-}
+-
+-static void TokenizeEn(const std::string &words,
+-                       const std::unordered_map<std::string, int32_t> &token2id,
+-                       const std::string &voice,
+-                       std::vector<int64_t> *token_ids,
+-                       std::vector<std::string> *tokens) {
+-  piper::eSpeakPhonemeConfig config;
+-  // ./bin/espeak-ng-bin --path  ./install/share/espeak-ng-data/ --voices
+-  // to list available voices
+-  config.voice = voice;  // e.g., voice is en-us
+-
+-  std::vector<std::vector<piper::Phoneme>> phonemes;
+-
+-  CallPhonemizeEspeak(words, config, &phonemes);
+-
+-  for (const auto &p : phonemes) {
+-    for (const auto &ph : p) {
+-      auto token = Utf32ToUtf8(std::u32string(1, ph));
+-      if (token2id.count(token)) {
+-        token_ids->push_back(token2id.at(token));
+-        tokens->push_back(token);
+-      } else {
+-        SHERPA_ONNX_LOGE("Skip unknown phoneme %s", token.c_str());
+-      }
+-    }
+-  }
+-}
+-
+-static void TokenizeTag(
+-    const std::string &words,
+-    const std::unordered_map<std::string, int32_t> &token2id,
+-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
+-  // in zipvoice tags are all in upper case
+-  std::string tag = ToUpperAscii(words);
+-  if (token2id.count(tag)) {
+-    tokens_ids->push_back(token2id.at(tag));
+-    tokens->push_back(tag);
+-  } else {
+-    SHERPA_ONNX_LOGE("Skip unknown tag %s", tag.c_str());
+-  }
+-}
+-
+-static void TokenizePinyin(
+-    const std::string &words, const cppinyin::PinyinEncoder *pinyin_encoder,
+-    const std::unordered_map<std::string, int32_t> &token2id,
+-    std::vector<int64_t> *tokens_ids, std::vector<std::string> *tokens) {
+-  // words are in the form of <ha3>, <ha4>
+-  std::string pinyin = words.substr(1, words.size() - 2);
+-  if (!pinyin.empty()) {
+-    if (pinyin[pinyin.size() - 1] == '5') {
+-      pinyin = pinyin.substr(0, pinyin.size() - 1);  // remove the tone
+-    }
+-    if (pinyin_encoder->ValidPinyin(pinyin, "number" /*tone*/)) {
+-      ProcessPinyin(pinyin, pinyin_encoder, token2id, tokens_ids, tokens);
+-    } else {
+-      SHERPA_ONNX_LOGE("Invalid pinyin %s", pinyin.c_str());
+-    }
+-  }
+-}
+-
+-OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
+-    const std::string &tokens, const std::string &data_dir,
+-    const std::string &pinyin_dict,
+-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug /*= false*/)
+-    : debug_(debug), meta_data_(meta_data) {
+-  std::ifstream is(tokens);
+-  token2id_ = ReadTokens(is);
+-  if (meta_data_.use_pinyin) {
+-    pinyin_encoder_ = std::make_unique<cppinyin::PinyinEncoder>(pinyin_dict);
+-  } else {
+-    pinyin_encoder_ = nullptr;
+-  }
+-  if (meta_data_.use_espeak) {
+-    // We should copy the directory of espeak-ng-data from the asset to
+-    // some internal or external storage and then pass the directory to
+-    // data_dir.
+-    InitEspeak(data_dir);
+-  }
+-}
+-
+-template <typename Manager>
+-OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
+-    Manager *mgr, const std::string &tokens, const std::string &data_dir,
+-    const std::string &pinyin_dict,
+-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug)
+-    : debug_(debug), meta_data_(meta_data) {
+-  auto buf = ReadFile(mgr, tokens);
+-  std::istrstream is(buf.data(), buf.size());
+-  token2id_ = ReadTokens(is);
+-  if (meta_data_.use_pinyin) {
+-    auto buf = ReadFile(mgr, pinyin_dict);
+-    std::istringstream iss(std::string(buf.begin(), buf.end()));
+-    pinyin_encoder_ = std::make_unique<cppinyin::PinyinEncoder>(iss);
+-  } else {
+-    pinyin_encoder_ = nullptr;
+-  }
+-  if (meta_data_.use_espeak) {
+-    // We should copy the directory of espeak-ng-data from the asset to
+-    // some internal or external storage and then pass the directory to
+-    // data_dir.
+-    InitEspeak(data_dir);
+-  }
+-}
+-
+-std::vector<TokenIDs> OfflineTtsZipvoiceFrontend::ConvertTextToTokenIds(
+-    const std::string &_text, const std::string &voice) const {
+-  std::string text = _text;
+-  if (meta_data_.use_espeak) {
+-    text = ToLowerAscii(_text);
+-  }
+-
+-  text = MapPunctuations(text, punct_map_);
+-
+-  auto wstext = ToWideString(text);
+-
+-  std::vector<std::string> parts;
+-  // Match <...>, [...], or single character
+-  std::wregex part_pattern(LR"([<\[].*?[>\]]|.)");
+-  auto words_begin =
+-      std::wsregex_iterator(wstext.begin(), wstext.end(), part_pattern);
+-  auto words_end = std::wsregex_iterator();
+-  for (std::wsregex_iterator i = words_begin; i != words_end; ++i) {
+-    parts.push_back(ToString(i->str()));
+-  }
+-
+-  // types are en, zh, tag, pinyin, other
+-  // tag is [...]
+-  // pinyin is <...>
+-  // other is any other text that does not match the above, normally numbers and
+-  // punctuations
+-  std::vector<std::string> types;
+-  for (auto &word : parts) {
+-    if (word.size() == 1 && std::isalpha(word[0])) {
+-      // single character, e.g., 'a', 'b', 'c'
+-      types.push_back("en");
+-    } else if (word.size() > 1 && word[0] == '<' && word.back() == '>') {
+-      // e.g., <ha3>, <ha4>
+-      types.push_back("pinyin");
+-    } else if (word.size() > 1 && word[0] == '[' && word.back() == ']') {
+-      types.push_back("tag");
+-    } else if (ContainsCJK(word)) {  // word contains one CJK characters
+-      types.push_back("zh");
+-    } else {
+-      types.push_back("other");
+-    }
+-  }
+-
+-  std::vector<std::pair<std::string, std::string>> parts_with_types;
+-  std::ostringstream oss;
+-  std::string t_lang;
+-  oss.str("");
+-  std::ostringstream debug_oss;
+-  if (debug_) {
+-    debug_oss << "Text : " << _text << ", Parts with types: \n";
+-  }
+-  for (int32_t i = 0; i < types.size(); ++i) {
+-    if (i == 0) {
+-      oss << parts[i];
+-      t_lang = types[i];
+-    } else {
+-      if (t_lang == "other" && (types[i] != "tag" && types[i] != "pinyin")) {
+-        // combine into current type if the previous part is "other"
+-        // do not combine with "tag" or "pinyin"
+-        oss << parts[i];
+-        t_lang = types[i];
+-      } else {
+-        if ((t_lang == types[i] || types[i] == "other") && t_lang != "pinyin" &&
+-            t_lang != "tag") {
+-          // same language or other, continue
+-          // do not combine other into "pinyin" or "tag"
+-          oss << parts[i];
+-        } else {
+-          // different language, start a new sentence
+-          std::string part = oss.str();
+-          oss.str("");
+-          parts_with_types.emplace_back(part, t_lang);
+-          if (debug_) {
+-            debug_oss << "(" << part << ", " << t_lang << "),";
+-          }
+-          oss << parts[i];
+-          t_lang = types[i];
+-        }
+-      }
+-    }
+-  }
+-
+-  std::string part = oss.str();
+-  oss.str("");
+-  parts_with_types.emplace_back(part, t_lang);
+-  if (debug_) {
+-    debug_oss << "(" << part << ", " << t_lang << ")\n";
+-    SHERPA_ONNX_LOGE("%s", debug_oss.str().c_str());
+-    debug_oss.str("");
+-  }
+-
+-  std::vector<int64_t> token_ids;
+-  std::vector<std::string> tokens;  // for debugging
+-  for (const auto &pt : parts_with_types) {
+-    if (pt.second == "zh") {
+-      TokenizeZh(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
+-                 &tokens);
+-    } else if (pt.second == "en") {
+-      TokenizeEn(pt.first, token2id_, "en-us", &token_ids, &tokens);
+-    } else if (pt.second == "pinyin") {
+-      TokenizePinyin(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
+-                     &tokens);
+-    } else if (pt.second == "tag") {
+-      TokenizeTag(pt.first, token2id_, &token_ids, &tokens);
+-    } else {
+-      SHERPA_ONNX_LOGE("Unexpected type: %s", pt.second.c_str());
+-      exit(-1);
+-    }
+-  }
+-  if (debug_) {
+-    debug_oss << "Tokens and IDs: \n";
+-    for (int32_t i = 0; i < tokens.size(); i++) {
+-      debug_oss << "(" << tokens[i] << ", " << token_ids[i] << "),";
+-    }
+-    debug_oss << "\n";
+-    SHERPA_ONNX_LOGE("%s", debug_oss.str().c_str());
+-  }
+-
+-  std::vector<TokenIDs> ans;
+-  ans.push_back(TokenIDs(std::move(token_ids)));
+-  return ans;
+-}
+-
+-#if __ANDROID_API__ >= 9
+-template OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
+-    AAssetManager *mgr, const std::string &tokens, const std::string &data_dir,
+-    const std::string &pinyin_dict,
+-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug = false);
+-
+-#endif
+-
+-#if __OHOS__
+-template OfflineTtsZipvoiceFrontend::OfflineTtsZipvoiceFrontend(
+-    NativeResourceManager *mgr, const std::string &tokens,
+-    const std::string &data_dir, const std::string &pinyin_dict,
+-    const OfflineTtsZipvoiceModelMetaData &meta_data, bool debug = false);
+-
+-#endif
+-
+-}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
+deleted file mode 100644
+index 1e47103a..00000000
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
++++ /dev/null
+@@ -1,62 +0,0 @@
+-// sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#ifndef SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
+-#define SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
+-#include <cstdint>
+-#include <memory>
+-#include <string>
+-#include <unordered_map>
+-#include <vector>
+-
+-#include "cppinyin/csrc/cppinyin.h"
+-#include "sherpa-onnx/csrc/offline-tts-frontend.h"
+-#include "sherpa-onnx/csrc/offline-tts-zipvoice-model-meta-data.h"
+-
+-namespace sherpa_onnx {
+-
+-class OfflineTtsZipvoiceFrontend : public OfflineTtsFrontend {
+- public:
+-  OfflineTtsZipvoiceFrontend(const std::string &tokens,
+-                             const std::string &data_dir,
+-                             const std::string &pinyin_dict,
+-                             const OfflineTtsZipvoiceModelMetaData &meta_data,
+-                             bool debug = false);
+-
+-  template <typename Manager>
+-  OfflineTtsZipvoiceFrontend(Manager *mgr, const std::string &tokens,
+-                             const std::string &data_dir,
+-                             const std::string &pinyin_dict,
+-                             const OfflineTtsZipvoiceModelMetaData &meta_data,
+-                             bool debug = false);
+-
+-  /** Convert a string to token IDs.
+-   *
+-   * @param text The input text.
+-   *             Example 1: "This is the first sample sentence; this is the
+-   *             second one." Example 2: ""
+-   * @param voice Optional. It is for espeak-ng.
+-   *
+-   * @return Return a vector-of-vector of token IDs. Each subvector contains
+-   *         a sentence that can be processed independently.
+-   *         If a frontend does not support splitting the text into
+-   * sentences, the resulting vector contains only one subvector.
+-   */
+-  std::vector<TokenIDs> ConvertTextToTokenIds(
+-      const std::string &text, const std::string &voice = "") const override;
+-
+- private:
+-  bool debug_ = false;
+-  std::unordered_map<std::string, int32_t> token2id_;
+-  const std::unordered_map<std::string, std::string> punct_map_ = {
+-      {"", ","}, {"", "."}, {"", "!"},  {"", "?"},     {"", ";"},
+-      {"", ":"}, {"", ","}, {"", "'"},   {"", "\""},     {"", "\""},
+-      {"", "'"},  {"", ""},  {"", ""}, {"", ""}, {"...", ""}};
+-  OfflineTtsZipvoiceModelMetaData meta_data_;
+-  std::unique_ptr<cppinyin::PinyinEncoder> pinyin_encoder_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_OFFLINE_TTS_ZIPVOICE_FRONTEND_H_
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+index ff16bf60..8e9861f5 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-impl.h
+@@ -15,9 +15,9 @@
+ #include "kaldi-native-fbank/csrc/mel-computations.h"
+ #include "kaldi-native-fbank/csrc/stft.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+ #include "sherpa-onnx/csrc/offline-tts-frontend.h"
+ #include "sherpa-onnx/csrc/offline-tts-impl.h"
+-#include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-model.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+@@ -83,8 +83,16 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+     }
+ 
+     // we assume batch size is 1
+-    std::vector<int64_t> tokens = text_token_ids[0].tokens;
+-    std::vector<int64_t> prompt_tokens = prompt_token_ids[0].tokens;
++    std::vector<int64_t> tokens;
++    for (const auto &t : text_token_ids) {
++      tokens.insert(tokens.end(), t.tokens.begin(), t.tokens.end());
++    }
++
++    std::vector<int64_t> prompt_tokens;
++    for (const auto &t : prompt_token_ids) {
++      prompt_tokens.insert(prompt_tokens.end(), t.tokens.begin(),
++                           t.tokens.end());
++    }
+ 
+     return Process(tokens, prompt_tokens, prompt_samples, sample_rate, speed,
+                    num_steps);
+@@ -93,28 +101,15 @@ class OfflineTtsZipvoiceImpl : public OfflineTtsImpl {
+  private:
+   template <typename Manager>
+   void InitFrontend(Manager *mgr) {
+-    const auto &meta_data = model_->GetMetaData();
+-    frontend_ = std::make_unique<OfflineTtsZipvoiceFrontend>(
+-        mgr, config_.model.zipvoice.tokens, config_.model.zipvoice.data_dir,
+-        config_.model.zipvoice.pinyin_dict, meta_data, config_.model.debug);
++    frontend_ = std::make_unique<MatchaTtsLexicon>(
++        mgr, config_.model.zipvoice.lexicon, config_.model.zipvoice.tokens,
++        config_.model.zipvoice.data_dir, config_.model.debug, true);
+   }
+ 
+   void InitFrontend() {
+-    const auto &meta_data = model_->GetMetaData();
+-
+-    if (meta_data.use_pinyin && config_.model.zipvoice.pinyin_dict.empty()) {
+-      SHERPA_ONNX_LOGE(
+-          "Please provide --zipvoice-pinyin-dict for converting Chinese into "
+-          "pinyin.");
+-      exit(-1);
+-    }
+-    if (meta_data.use_espeak && config_.model.zipvoice.data_dir.empty()) {
+-      SHERPA_ONNX_LOGE("Please provide --zipvoice-data-dir for espeak-ng.");
+-      exit(-1);
+-    }
+-    frontend_ = std::make_unique<OfflineTtsZipvoiceFrontend>(
+-        config_.model.zipvoice.tokens, config_.model.zipvoice.data_dir,
+-        config_.model.zipvoice.pinyin_dict, meta_data, config_.model.debug);
++    frontend_ = std::make_unique<MatchaTtsLexicon>(
++        config_.model.zipvoice.lexicon, config_.model.zipvoice.tokens,
++        config_.model.zipvoice.data_dir, config_.model.debug, true);
+   }
+ 
+   std::vector<int32_t> ComputeMelSpectrogram(
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+index 453bd6f6..1226adf8 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+@@ -17,13 +17,14 @@ void OfflineTtsZipvoiceModelConfig::Register(ParseOptions *po) {
+                "Path to tokens.txt for ZipVoice models");
+   po->Register("zipvoice-data-dir", &data_dir,
+                "Path to the directory containing dict for espeak-ng.");
++  po->Register("zipvoice-lexicon", &lexicon, "Path to lexicon.txt for Chinese");
+   po->Register("zipvoice-pinyin-dict", &pinyin_dict,
+                "Path to the pinyin dictionary for cppinyin (i.e converting "
+                "Chinese into phones).");
+   po->Register("zipvoice-text-model", &text_model,
+                "Path to zipvoice text model");
+   po->Register("zipvoice-flow-matching-model", &flow_matching_model,
+-               "Path to zipvoice flow-matching model");
++               "Path to zipvoice flow-matching model, i.e., the decoder model");
+   po->Register("zipvoice-vocoder", &vocoder, "Path to zipvoice vocoder");
+   po->Register("zipvoice-feat-scale", &feat_scale,
+                "Feature scale for ZipVoice (default: 0.1)");
+@@ -96,12 +97,6 @@ bool OfflineTtsZipvoiceModelConfig::Validate() const {
+     }
+   }
+ 
+-  if (!pinyin_dict.empty() && !FileExists(pinyin_dict)) {
+-    SHERPA_ONNX_LOGE("--zipvoice-pinyin-dict: '%s' does not exist",
+-                     pinyin_dict.c_str());
+-    return false;
+-  }
+-
+   if (feat_scale <= 0) {
+     SHERPA_ONNX_LOGE("--zipvoice-feat-scale must be positive. Given: %f",
+                      feat_scale);
+@@ -138,7 +133,7 @@ std::string OfflineTtsZipvoiceModelConfig::ToString() const {
+   os << "flow_matching_model=\"" << flow_matching_model << "\", ";
+   os << "vocoder=\"" << vocoder << "\", ";
+   os << "data_dir=\"" << data_dir << "\", ";
+-  os << "pinyin_dict=\"" << pinyin_dict << "\", ";
++  os << "lexicon=\"" << lexicon << "\", ";
+   os << "feat_scale=" << feat_scale << ", ";
+   os << "t_shift=" << t_shift << ", ";
+   os << "target_rms=" << target_rms << ", ";
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+index ef43bf41..5b5d0c24 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h
+@@ -15,12 +15,11 @@ namespace sherpa_onnx {
+ struct OfflineTtsZipvoiceModelConfig {
+   std::string tokens;
+   std::string text_model;
+-  std::string flow_matching_model;
++  std::string flow_matching_model;  // decoder
+   std::string vocoder;
+ 
+-  // If data_dir is given, lexicon is ignored
+-  // data_dir is for piper-phonemize, which uses espeak-ng
+   std::string data_dir;
++  std::string lexicon;
+ 
+   // Used for converting Chinese characters to pinyin
+   std::string pinyin_dict;
+@@ -35,14 +34,15 @@ struct OfflineTtsZipvoiceModelConfig {
+   OfflineTtsZipvoiceModelConfig(
+       const std::string &tokens, const std::string &text_model,
+       const std::string &flow_matching_model, const std::string &vocoder,
+-      const std::string &data_dir, const std::string &pinyin_dict,
+-      float feat_scale = 0.1, float t_shift = 0.5, float target_rms = 0.1,
+-      float guidance_scale = 1.0)
++      const std::string &data_dir, const std::string &lexicon,
++      const std::string &pinyin_dict, float feat_scale = 0.1,
++      float t_shift = 0.5, float target_rms = 0.1, float guidance_scale = 1.0)
+       : tokens(tokens),
+         text_model(text_model),
+         flow_matching_model(flow_matching_model),
+         vocoder(vocoder),
+         data_dir(data_dir),
++        lexicon(lexicon),
+         pinyin_dict(pinyin_dict),
+         feat_scale(feat_scale),
+         t_shift(t_shift),
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+index 05f324c9..8b4d321b 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+@@ -68,7 +68,7 @@ class OfflineTtsZipvoiceModel::Impl {
+     if (batch_size != 1) {
+       SHERPA_ONNX_LOGE("Support only batch_size == 1. Given: %d",
+                        static_cast<int32_t>(batch_size));
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ 
+     std::vector<int64_t> prompt_feat_shape =
+@@ -108,7 +108,10 @@ class OfflineTtsZipvoiceModel::Impl {
+     std::random_device rd;
+     std::default_random_engine rng(rd());
+     std::normal_distribution<float> norm(0, 1);
+-    for (auto &v : x_data) v = norm(rng);
++    for (auto &v : x_data) {
++      v = norm(rng);
++    }
++
+     std::vector<int64_t> x_shape = {batch_size, num_frames, feat_dim};
+     Ort::Value x = Ort::Value::CreateTensor<float>(
+         memory_info, x_data.data(), x_data.size(), x_shape.data(),
+diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+index 2357724d..5d86b3c9 100644
+--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
++++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+@@ -698,7 +698,6 @@ const
+      {$linklib sherpa-onnx-kaldifst-core}
+      {$linklib sherpa-onnx-fstfar}
+      {$linklib sherpa-onnx-fst}
+-     {$linklib cppinyin_core}
+      {$linklib kissfft-float}
+      {$linklib kaldi-native-fbank-core}
+      {$linklib piper_phonemize}
+
+commit 42dcaf194109bc8e8e748118da3ac76e5731b00a
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 11 18:03:13 2025 +0800
+
+    upload zipvoice onnx models (#2890)
+
+diff --git a/.github/workflows/upload-models.yaml b/.github/workflows/upload-models.yaml
+index f5922704..a596c26b 100644
+--- a/.github/workflows/upload-models.yaml
++++ b/.github/workflows/upload-models.yaml
+@@ -4,7 +4,6 @@ on:
+   push:
+     branches:
+       - upload-models
+-      # - upload-more-models
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -31,6 +30,10 @@ jobs:
+           git config --global user.email "csukuangfj@gmail.com"
+           git config --global user.name "Fangjun Kuang"
+ 
++      - name: Setup tmate session
++        if: false
++        uses: mxschmitt/action-tmate@v3
++
+       - name: Streaming zipformer from Banafo/Kroko-ASR
+         if: false
+         shell: bash
+@@ -178,7 +181,7 @@ jobs:
+           ls -lh *.tar.bz2
+ 
+       - name: wenetspeech chuan paraformer
+-        if: true
++        if: false
+         shell: bash
+         run: |
+           git lfs install
+@@ -482,7 +485,14 @@ jobs:
+               popd
+             done
+ 
++      - uses: actions/upload-artifact@v4
++        if: false
++        with:
++          name: here
++          path: ./*.tar.bz2
++
+       - name: Release
++        if: false
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+diff --git a/.github/workflows/upload-zipvoice-models.yaml b/.github/workflows/upload-zipvoice-models.yaml
+new file mode 100644
+index 00000000..cc209c63
+--- /dev/null
++++ b/.github/workflows/upload-zipvoice-models.yaml
+@@ -0,0 +1,135 @@
++name: upload-zipvoice-models
++
++on:
++  push:
++    branches:
++      - upload-zipvoice-onnx-models
++  workflow_dispatch:
++
++concurrency:
++  group: upload-zipvoice-models-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  upload-zipvoice-models:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: upload zipvoice models
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        python-version: ["3.10"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: git config
++        shell: bash
++        run: |
++          git config --global user.email "csukuangfj@gmail.com"
++          git config --global user.name "Fangjun Kuang"
++
++      - name: Setup Python 3.10
++        uses: actions/setup-python@v5
++        with:
++          python-version: "3.10"
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          python3 -m pip install --upgrade pip pypinyin
++
++      - name: sherpa-onnx-zipvoice-distill-zh-en-emilia-int8
++        shell: bash
++        run: |
++          echo "Generate lexicon.txt"
++
++          python3 ./scripts/zipvoice/zh-en/generate_lexicon.py
++
++          d=sherpa-onnx-zipvoice-distill-int8-zh-en-emilia
++          mkdir $d
++
++          cp lexicon.txt $d
++
++          pushd $d
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/prompt.txt
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female-2.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/leijun-1.wav
++
++
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/fm_decoder_int8.onnx
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/text_encoder_int8.onnx
++
++          mv fm_decoder_int8.onnx decoder.int8.onnx
++          mv text_encoder_int8.onnx encoder.int8.onnx
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
++          tar xf espeak-ng-data.tar.bz2
++          rm espeak-ng-data.tar.bz2
++
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/tokens.txt
++          mkdir test_wavs
++          mv *.wav test_wavs
++
++          mv prompt.txt test_wavs
++
++          ls -lh
++          popd
++          tar cjfv $d.tar.bz2 $d
++          rm -rf $d
++          ls -lh $d.tar.bz2
++
++      - name: sherpa-onnx-zipvoice-distill-zh-en-emilia-fp32
++        shell: bash
++        run: |
++          echo "Generate lexicon.txt"
++
++          python3 ./scripts/zipvoice/zh-en/generate_lexicon.py
++
++          d=sherpa-onnx-zipvoice-distill-fp32-zh-en-emilia
++          mkdir $d
++
++          cp lexicon.txt $d
++
++          pushd $d
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/prompt.txt
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/news-female-2.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/leijun-1.wav
++
++
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/fm_decoder.onnx
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/text_encoder.onnx
++
++          mv fm_decoder.onnx decoder.onnx
++          mv text_encoder.onnx encoder.onnx
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
++          tar xf espeak-ng-data.tar.bz2
++          rm espeak-ng-data.tar.bz2
++
++          curl -SL -O https://huggingface.co/k2-fsa/ZipVoice/resolve/main/zipvoice_distill/tokens.txt
++          mkdir test_wavs
++          mv *.wav test_wavs
++
++          mv prompt.txt test_wavs
++
++          ls -lh
++          popd
++          tar cjfv $d.tar.bz2 $d
++          rm -rf $d
++          ls -lh $d.tar.bz2
++
++      - name: Release
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: tts-models
+
+commit 7f6ec017d3a336dc9e5eb7ed22adb0381bc640ba
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Dec 11 14:26:18 2025 +0800
+
+    Add script for testing zipvoice onnx models (#2887)
+
+diff --git a/scripts/zipvoice/zh-en/generate_lexicon.py b/scripts/zipvoice/zh-en/generate_lexicon.py
+new file mode 100755
+index 00000000..2f2211e0
+--- /dev/null
++++ b/scripts/zipvoice/zh-en/generate_lexicon.py
+@@ -0,0 +1,90 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++
++from pypinyin import Style, lazy_pinyin, load_phrases_dict, phrases_dict, pinyin_dict
++from pypinyin.contrib.tone_convert import to_finals_tone3, to_initials
++
++load_phrases_dict(
++    {
++        "": [["hang2"], ["zhang3"]],
++        "": [["yin2"], ["hang2"], ["hang2"], ["zhang3"]],
++    }
++)
++user_defined = {
++    "": ["wei1", "tiao2"],
++    "": ["zhe4", "ge4"],
++    "": ["fang1", "bian2", "de1"],
++}
++
++
++def get_initial_final(token):
++    if isinstance(token, list):
++        ans = ""
++        sep = ""
++        for t in token:
++            ans += sep + get_initial_final(t)
++            sep = " "
++        return ans
++
++    initial = to_initials(token, strict=False)
++
++    final = to_finals_tone3(
++        token,
++        strict=False,
++        neutral_tone_with_five=True,
++    )
++
++    ans = ""
++    if initial:
++        ans = initial + "0"
++
++    if final:
++        ans += f" {final}"
++
++    return ans
++
++
++def main():
++    filename = "lexicon.txt"
++
++    word_dict = pinyin_dict.pinyin_dict
++    phrases = phrases_dict.phrases_dict
++
++    with open(filename, "w", encoding="utf-8") as f:
++        for key in word_dict:
++            if not (0x4E00 <= key <= 0x9FFF):
++                continue
++
++            w = chr(key)
++            token = lazy_pinyin(
++                w,
++                style=Style.TONE3,
++                tone_sandhi=True,
++                neutral_tone_with_five=True,
++            )[0]
++
++            initial_final = get_initial_final(token)
++
++            f.write(f"{w} {initial_final}\n")
++
++        for key, value in user_defined.items():
++            initial_final = get_initial_final(value)
++            f.write(f"{key} {initial_final}\n")
++
++        for key in phrases:
++            if key in user_defined:
++                continue
++            token = lazy_pinyin(
++                key,
++                style=Style.TONE3,
++                tone_sandhi=True,
++                neutral_tone_with_five=True,
++            )
++            initial_final = get_initial_final(token)
++
++            f.write(f"{key} {initial_final}\n")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/zipvoice/zh-en/test_onnx.py b/scripts/zipvoice/zh-en/test_onnx.py
+new file mode 100755
+index 00000000..4e9fbc8c
+--- /dev/null
++++ b/scripts/zipvoice/zh-en/test_onnx.py
+@@ -0,0 +1,381 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++
++import kaldi_native_fbank as knf
++import numpy as np
++import onnxruntime as ort
++import soundfile as sf
++
++
++def compute_features(samples):
++    stft_config = knf.StftConfig(
++        n_fft=1024,
++        hop_length=256,
++        win_length=1024,
++        center=True,
++        window_type="hann",
++    )
++    knf_stft = knf.Stft(stft_config)
++    stft_result = knf_stft(samples.tolist())
++    real = np.array(stft_result.real, dtype=np.float32).reshape(
++        stft_result.num_frames, -1
++    )
++    imag = np.array(stft_result.imag, dtype=np.float32).reshape(
++        stft_result.num_frames, -1
++    )
++
++    mag = np.sqrt(real * real + imag * imag).astype(np.float32)
++
++    mel_opts = knf.MelBanksOptions()
++    mel_opts.num_bins = 100
++    mel_opts.low_freq = 0
++    mel_opts.high_freq = 24000 // 2
++    mel_opts.is_librosa = True
++    mel_opts.norm = ""
++    mel_opts.use_slaney_mel_scale = False
++
++    frame_opts = knf.FrameExtractionOptions()
++    frame_opts.samp_freq = 24000
++    #  frame_opts.frame_length_ms = 1024 * 1000 / 24000
++    #  frame_opts.frame_shift_ms = 256 * 1000 / 24000
++
++    mel_filters = knf.MelBanks(mel_opts, frame_opts)
++    mel_features = np.zeros((mag.shape[0], 100))
++    for i in range(mag.shape[0]):
++        mel_features[i] = mel_filters.compute(mag[i])
++    print("sum", np.sum(mel_features), np.mean(mel_features))
++
++    mel_features = np.log(mel_features + 1e-10)
++    return mel_features
++
++
++class OnnxModel:
++    def __init__(
++        self,
++        text_encoder_path: str,
++        fm_decoder_path: str,
++        num_thread: int = 1,
++    ):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = num_thread
++        session_opts.intra_op_num_threads = num_thread
++
++        self.session_opts = session_opts
++
++        self.init_text_encoder(text_encoder_path)
++        self.init_fm_decoder(fm_decoder_path)
++
++    def init_text_encoder(self, model_path: str):
++        self.text_encoder = ort.InferenceSession(
++            model_path,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++
++    def init_fm_decoder(self, model_path: str):
++        self.fm_decoder = ort.InferenceSession(
++            model_path,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++        meta = self.fm_decoder.get_modelmeta().custom_metadata_map
++        self.feat_dim = int(meta["feat_dim"])
++
++    def run_text_encoder(
++        self,
++        tokens: np.ndarray,
++        prompt_tokens: np.ndarray,
++        prompt_features_len: np.ndarray,
++        speed: np.ndarray,
++    ) -> np.ndarray:
++        out = self.text_encoder.run(
++            [
++                self.text_encoder.get_outputs()[0].name,
++            ],
++            {
++                self.text_encoder.get_inputs()[0].name: tokens,
++                self.text_encoder.get_inputs()[1].name: prompt_tokens,
++                self.text_encoder.get_inputs()[2].name: prompt_features_len,
++                self.text_encoder.get_inputs()[3].name: speed,
++            },
++        )
++        return out[0]
++
++    def run_fm_decoder(
++        self,
++        t: np.ndarray,
++        x: np.ndarray,
++        text_condition: np.ndarray,
++        speech_condition: np.ndarray,
++        guidance_scale: np.ndarray,
++    ) -> np.ndarray:
++        out = self.fm_decoder.run(
++            [
++                self.fm_decoder.get_outputs()[0].name,
++            ],
++            {
++                self.fm_decoder.get_inputs()[0].name: t,
++                self.fm_decoder.get_inputs()[1].name: x,
++                self.fm_decoder.get_inputs()[2].name: text_condition,
++                self.fm_decoder.get_inputs()[3].name: speech_condition,
++                self.fm_decoder.get_inputs()[4].name: guidance_scale,
++            },
++        )
++        return out[0]
++
++
++class OnnxVocosModel:
++    def __init__(
++        self,
++        filename: str,
++    ):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = 1
++        session_opts.intra_op_num_threads = 1
++
++        self.session_opts = session_opts
++        self.model = ort.InferenceSession(
++            filename,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++        print(f"vocos {self.model.get_modelmeta().custom_metadata_map}")
++
++        print("----------vocos----------")
++        for i in self.model.get_inputs():
++            print(i)
++
++        print("-----")
++
++        for i in self.model.get_outputs():
++            print(i)
++        print()
++
++    def __call__(self, x: np.ndarray):
++        """
++        Args:
++          x: (N, feat_dim, num_frames)
++        Returns:
++          mag: (N, n_fft/2+1, num_frames)
++          x: (N, n_fft/2+1, num_frames)
++          y: (N, n_fft/2+1, num_frames)
++
++        The complex spectrum is mag * (x + j*y)
++        """
++        assert x.ndim == 3, x.shape
++        assert x.shape[0] == 1, x.shape
++
++        mag, x, y = self.model.run(
++            [
++                self.model.get_outputs()[0].name,
++                self.model.get_outputs()[1].name,
++                self.model.get_outputs()[2].name,
++            ],
++            {
++                self.model.get_inputs()[0].name: x,
++            },
++        )
++
++        return mag, x, y
++
++
++def get_phones(text):
++    if text[-1] != ".":
++        text += "."
++
++    word2tokens = dict()
++    with open("./lexicon.txt", encoding="utf-8") as f:
++        for line in f:
++            fields = line.split()
++            word = fields[0]
++            tokens = fields[1:]
++            word2tokens[word] = tokens
++
++    token2id = dict()
++    with open("./tokens.txt", encoding="utf-8") as f:
++        for line in f:
++            fields = line.strip().split()
++            if len(fields) == 1:
++                token2id[" "] = int(fields[0])
++            else:
++                token2id[fields[0]] = int(fields[1])
++
++    tokens = []
++    for w in text:
++        if w in word2tokens:
++            tokens += word2tokens[w]
++        else:
++            tokens.append(w)
++    ids = []
++    for t in tokens:
++        if t in token2id:
++            ids.append(token2id[t])
++        else:
++            print(f"skip {t}")
++
++    return ids
++
++
++def compute_rms(features):
++    return np.sqrt(np.mean(np.square(features)))
++
++
++def get_timestamps(num_steps, t_shift=1):
++    steps = np.linspace(0, 1, num_steps + 1)
++    if t_shift != 1:
++        steps = t_shift * steps / (1 + (t_shift - 1) * steps)
++
++    return steps.tolist()
++
++
++def trim_leading_silence_energy(samples, frame_size=2048, hop=512, energy_thresh=0.5):
++    energies = [
++        np.sum(np.abs(samples[i : i + frame_size]) ** 2)
++        for i in range(0, len(samples) - frame_size, hop)
++    ]
++    #  print(energies)
++    # First frame whose energy exceeds threshold
++    frame_index = next((i for i, e in enumerate(energies) if e > energy_thresh), 0)
++    frame_index = max(frame_index - 3, 0)
++    start_sample = frame_index * hop
++    return samples[start_sample:]
++
++
++def main():
++    vocoder = OnnxVocosModel("./vocos_24khz.onnx")
++
++    prompt_text = ", ! , "
++    prompt_wav_filename = "news-female.wav"
++
++    prompt_text = ", , , ."
++    prompt_wav_filename = "news-female-2.wav"
++
++    prompt_text = ", . ."
++    prompt_wav_filename = "leijun-1.wav"
++
++    prompt_ids = get_phones(prompt_text)
++
++    text = ", . . , ."
++
++    ids = get_phones(text)
++
++    data, sample_rate = sf.read(
++        prompt_wav_filename,
++        always_2d=True,
++        dtype="float32",
++    )
++    data = data[:, 0]  # use only the first channel
++    samples = np.ascontiguousarray(data)
++    if sample_rate != 24000:
++        import librosa
++
++        samples = librosa.resample(
++            samples,
++            orig_sr=sample_rate,
++            target_sr=24000,
++        )
++        sample_rate = 24000
++
++    assert len(samples.shape) == 1, samples.shape
++
++    rms = compute_rms(samples)
++    print("rms", rms)
++
++    target_rms = 0.1
++    if rms < target_rms:
++        samples = samples * target_rms / rms
++    new_rms = compute_rms(samples)
++
++    print("new_rms", new_rms)
++
++    prompt_features = compute_features(samples)
++    print("features.shape", prompt_features.shape)
++
++    feat_scale = 0.1
++    prompt_features = prompt_features * feat_scale
++
++    model = OnnxModel(
++        text_encoder_path="./text_encoder_int8.onnx",
++        fm_decoder_path="./fm_decoder_int8.onnx",
++    )
++
++    tokens = np.array([ids], dtype=np.int64)
++    assert len(tokens.shape) == 2, tokens.shape
++
++    prompt_tokens = np.array([prompt_ids], dtype=np.int64)
++    assert len(prompt_tokens.shape) == 2, prompt_tokens.shape
++    prompt_features_len = np.array(prompt_features.shape[0], dtype=np.int64)
++    speed = np.array(1.0, dtype=np.float32)
++
++    print(tokens.shape, prompt_tokens.shape, prompt_features_len)
++
++    text_condition = model.run_text_encoder(
++        tokens=tokens,
++        prompt_tokens=prompt_tokens,
++        prompt_features_len=prompt_features_len,
++        speed=speed,
++    )
++
++    x = np.random.randn(*text_condition.shape).astype(np.float32)
++
++    speech_condition = np.pad(
++        prompt_features,
++        pad_width=((0, x.shape[1] - prompt_features.shape[0]), (0, 0)),
++        mode="constant",
++        constant_values=0,
++    )[None].astype(np.float32)
++
++    print(speech_condition.shape, prompt_features.shape)
++
++    guidance_scale = np.array(1.0, dtype=np.float32)
++
++    num_steps = 8
++    steps = get_timestamps(num_steps=num_steps, t_shift=0.5)
++    for i in range(num_steps):
++        t = np.array(steps[i], dtype=np.float32)
++        v = model.run_fm_decoder(
++            t=t,
++            x=x,
++            text_condition=text_condition,
++            speech_condition=speech_condition,
++            guidance_scale=guidance_scale,
++        )
++        x = x + v * (steps[i + 1] - steps[i])
++    print("prompt_features", prompt_features.shape)
++    x = x[:, prompt_features.shape[0] :]
++    print("x", x.shape)
++
++    x = x / feat_scale
++    mel = x.transpose(0, 2, 1)
++    mag, x, y = vocoder(mel)
++    print("mag", mag.shape, x.shape, y.shape)
++
++    stft_result = knf.StftResult(
++        real=(mag * x)[0].transpose().reshape(-1).tolist(),
++        imag=(mag * y)[0].transpose().reshape(-1).tolist(),
++        num_frames=mag.shape[2],
++    )
++    config = knf.StftConfig(
++        n_fft=1024,
++        hop_length=256,
++        win_length=1024,
++        window_type="hann",
++        center=True,
++        pad_mode="reflect",
++        normalized=False,
++    )
++    istft = knf.IStft(config)
++    audio_vocos = istft(stft_result)
++
++    audio_vocos = np.array(audio_vocos)
++    audio_vocos = trim_leading_silence_energy(audio_vocos)
++
++    #  if rms < target_rms:
++    #      audio_vocos = audio_vocos / target_rms * rms
++
++    sf.write("generated.wav", audio_vocos, sample_rate, "PCM_16")
++
++
++if __name__ == "__main__":
++    main()
+
+commit 7faab3deeb7ae7f105f8b9444e55fd1fab0b3516
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Dec 10 18:19:35 2025 +0800
+
+    Publish WASM spaces for MatchaTTS Chinese+English model (#2885)
+
+diff --git a/.github/workflows/wasm-simd-hf-space-de-tts.yaml b/.github/workflows/wasm-simd-hf-space-de-tts.yaml
+deleted file mode 100644
+index cf265378..00000000
+--- a/.github/workflows/wasm-simd-hf-space-de-tts.yaml
++++ /dev/null
+@@ -1,169 +0,0 @@
+-name: wasm-simd-hf-space-de-tts
+-
+-on:
+-  push:
+-    branches:
+-      - wasm
+-    tags:
+-      - 'v[0-9]+.[0-9]+.[0-9]+*'
+-
+-  workflow_dispatch:
+-
+-concurrency:
+-  group: wasm-simd-hf-space-de-tts-${{ github.ref }}
+-  cancel-in-progress: true
+-
+-jobs:
+-  wasm-simd-hf-space-de-tts:
+-    runs-on: ${{ matrix.os }}
+-    strategy:
+-      fail-fast: false
+-      matrix:
+-        os: [ubuntu-latest]
+-
+-    steps:
+-      - uses: actions/checkout@v4
+-        with:
+-          fetch-depth: 0
+-
+-      - name: Update version
+-        shell: bash
+-        run: |
+-          ./new-release.sh
+-          git diff .
+-
+-      - name: Install emsdk
+-        uses: mymindstorm/setup-emsdk@v14
+-        with:
+-          version: 3.1.53
+-          actions-cache-folder: 'emsdk-cache'
+-
+-      - name: View emsdk version
+-        shell: bash
+-        run: |
+-          emcc -v
+-          echo "--------------------"
+-          emcc --check
+-
+-      - name: Download model files
+-        shell: bash
+-        run: |
+-          cd wasm/tts/assets
+-          ls -lh
+-          echo "----------"
+-          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
+-          tar xf vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
+-          rm vits-piper-de_DE-thorsten_emotional-medium.tar.bz2
+-
+-          mv -v vits-piper-de_DE-thorsten_emotional-medium/de_DE-thorsten_emotional-medium.onnx ./model.onnx
+-          mv -v vits-piper-de_DE-thorsten_emotional-medium/tokens.txt ./
+-          mv vits-piper-de_DE-thorsten_emotional-medium/espeak-ng-data ./
+-
+-          rm -rf vits-piper-de_DE-thorsten_emotional-medium
+-
+-          ls -lh
+-
+-      - name: Build sherpa-onnx for WebAssembly
+-        shell: bash
+-        run: |
+-          ./build-wasm-simd-tts.sh
+-
+-      - name: collect files
+-        shell: bash
+-        run: |
+-          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-          mv build-wasm-simd-tts/install/bin/wasm/tts sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
+-          ls -lh sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
+-          tar cjfv sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts.tar.bz2 ./sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts
+-
+-      - name: Upload wasm files
+-        uses: actions/upload-artifact@v4
+-        with:
+-          name: sherpa-onnx-wasm-simd-de-tts
+-          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
+-
+-      - name: Release
+-        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+-        uses: svenstaro/upload-release-action@v2
+-        with:
+-          file_glob: true
+-          overwrite: true
+-          file: ./*.tar.bz2
+-
+-      - name: Publish to ModelScope
+-        # if: false
+-        env:
+-          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+-        uses: nick-fields/retry@v2
+-        with:
+-          max_attempts: 20
+-          timeout_seconds: 200
+-          shell: bash
+-          command: |
+-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-            git config --global user.email "csukuangfj@gmail.com"
+-            git config --global user.name "Fangjun Kuang"
+-
+-            rm -rf ms
+-            export GIT_LFS_SKIP_SMUDGE=1
+-            export GIT_CLONE_PROTECTION_ACTIVE=false
+-
+-            git clone http://www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de.git ms
+-
+-            cd ms
+-            rm -fv *.js
+-            rm -fv *.data
+-
+-            git fetch
+-            git pull
+-            git merge -m "merge remote" --ff origin main
+-
+-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts/* .
+-
+-            git status
+-            git lfs track "*.data"
+-            git lfs track "*.wasm"
+-            ls -lh
+-
+-            git add .
+-            git commit -m "update model"
+-            git push http://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de.git
+-
+-      - name: Publish to huggingface
+-        env:
+-          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+-        uses: nick-fields/retry@v2
+-        with:
+-          max_attempts: 20
+-          timeout_seconds: 200
+-          shell: bash
+-          command: |
+-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-            git config --global user.email "csukuangfj@gmail.com"
+-            git config --global user.name "Fangjun Kuang"
+-
+-            rm -rf huggingface
+-            export GIT_LFS_SKIP_SMUDGE=1
+-            export GIT_CLONE_PROTECTION_ACTIVE=false
+-
+-            git clone https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de huggingface
+-            cd huggingface
+-            rm -fv *.js
+-            rm -fv *.data
+-            git fetch
+-            git pull
+-            git merge -m "merge remote" --ff origin main
+-
+-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-de-tts/* .
+-
+-            git status
+-            git lfs track "*.data"
+-            git lfs track "*.wasm"
+-            ls -lh
+-
+-            git add .
+-            git commit -m "update model"
+-            git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de main
+diff --git a/.github/workflows/wasm-simd-hf-space-en-tts.yaml b/.github/workflows/wasm-simd-hf-space-en-tts.yaml
+deleted file mode 100644
+index b639706a..00000000
+--- a/.github/workflows/wasm-simd-hf-space-en-tts.yaml
++++ /dev/null
+@@ -1,165 +0,0 @@
+-name: wasm-simd-hf-space-en-tts
+-
+-on:
+-  push:
+-    branches:
+-      - wasm
+-    tags:
+-      - 'v[0-9]+.[0-9]+.[0-9]+*'
+-
+-  workflow_dispatch:
+-
+-concurrency:
+-  group: wasm-simd-hf-space-en-tts-${{ github.ref }}
+-  cancel-in-progress: true
+-
+-jobs:
+-  wasm-simd-hf-space-en-tts:
+-    runs-on: ${{ matrix.os }}
+-    strategy:
+-      fail-fast: false
+-      matrix:
+-        os: [ubuntu-latest]
+-
+-    steps:
+-      - uses: actions/checkout@v4
+-        with:
+-          fetch-depth: 0
+-
+-      - name: Update version
+-        shell: bash
+-        run: |
+-          ./new-release.sh
+-          git diff .
+-
+-      - name: Install emsdk
+-        uses: mymindstorm/setup-emsdk@v14
+-        with:
+-          version: 3.1.53
+-          actions-cache-folder: 'emsdk-cache'
+-
+-      - name: View emsdk version
+-        shell: bash
+-        run: |
+-          emcc -v
+-          echo "--------------------"
+-          emcc --check
+-
+-      - name: Download model files
+-        shell: bash
+-        run: |
+-          cd wasm/tts/assets
+-          ls -lh
+-          echo "----------"
+-          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-en_US-libritts_r-medium.tar.bz2
+-          tar xf vits-piper-en_US-libritts_r-medium.tar.bz2
+-          rm vits-piper-en_US-libritts_r-medium.tar.bz2
+-          mv vits-piper-en_US-libritts_r-medium/en_US-libritts_r-medium.onnx ./model.onnx
+-          mv vits-piper-en_US-libritts_r-medium/tokens.txt ./
+-          mv vits-piper-en_US-libritts_r-medium/espeak-ng-data ./
+-          rm -rf vits-piper-en_US-libritts_r-medium
+-
+-          ls -lh
+-
+-      - name: Build sherpa-onnx for WebAssembly
+-        shell: bash
+-        run: |
+-          ./build-wasm-simd-tts.sh
+-
+-      - name: collect files
+-        shell: bash
+-        run: |
+-          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-          mv build-wasm-simd-tts/install/bin/wasm/tts sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
+-          ls -lh sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
+-          tar cjfv sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts.tar.bz2 ./sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts
+-
+-      - name: Upload wasm files
+-        uses: actions/upload-artifact@v4
+-        with:
+-          name: sherpa-onnx-wasm-simd-en-tts
+-          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
+-
+-      - name: Release
+-        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+-        uses: svenstaro/upload-release-action@v2
+-        with:
+-          file_glob: true
+-          overwrite: true
+-          file: ./*.tar.bz2
+-
+-      - name: Publish to ModelScope
+-        # if: false
+-        env:
+-          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
+-        uses: nick-fields/retry@v2
+-        with:
+-          max_attempts: 20
+-          timeout_seconds: 200
+-          shell: bash
+-          command: |
+-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-            git config --global user.email "csukuangfj@gmail.com"
+-            git config --global user.name "Fangjun Kuang"
+-
+-            rm -rf ms
+-            export GIT_LFS_SKIP_SMUDGE=1
+-            export GIT_CLONE_PROTECTION_ACTIVE=false
+-
+-            git clone https://www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en.git ms
+-            cd ms
+-            rm -fv *.js
+-            rm -fv *.data
+-            git fetch
+-            git pull
+-            git merge -m "merge remote" --ff origin main
+-
+-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts/* .
+-
+-            git status
+-            git lfs track "*.data"
+-            git lfs track "*.wasm"
+-            ls -lh
+-
+-            git add .
+-            git commit -m "update model"
+-            git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en.git
+-
+-      - name: Publish to huggingface
+-        env:
+-          HF_TOKEN: ${{ secrets.HF_TOKEN }}
+-        uses: nick-fields/retry@v2
+-        with:
+-          max_attempts: 20
+-          timeout_seconds: 200
+-          shell: bash
+-          command: |
+-            SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
+-
+-            git config --global user.email "csukuangfj@gmail.com"
+-            git config --global user.name "Fangjun Kuang"
+-
+-            rm -rf huggingface
+-            export GIT_LFS_SKIP_SMUDGE=1
+-            export GIT_CLONE_PROTECTION_ACTIVE=false
+-
+-            git clone https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en huggingface
+-            cd huggingface
+-            rm -fv *.js
+-            rm -fv *.data
+-            git fetch
+-            git pull
+-            git merge -m "merge remote" --ff origin main
+-
+-            cp -v ../sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-en-tts/* .
+-
+-            git status
+-            git lfs track "*.data"
+-            git lfs track "*.wasm"
+-            ls -lh
+-
+-            git add .
+-            git commit -m "update model"
+-            git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en main
+diff --git a/.github/workflows/wasm-simd-hf-space-tts.yaml b/.github/workflows/wasm-simd-hf-space-tts.yaml
+new file mode 100644
+index 00000000..f2522774
+--- /dev/null
++++ b/.github/workflows/wasm-simd-hf-space-tts.yaml
+@@ -0,0 +1,102 @@
++name: wasm-simd-hf-space-tts
++
++on:
++  push:
++    branches:
++      - wasm
++    tags:
++      - 'v[0-9]+.[0-9]+.[0-9]+*'
++
++  workflow_dispatch:
++
++concurrency:
++  group: wasm-simd-hf-space-tts${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  wasm-simd-hf-space-tts:
++    name: ${{ matrix.index }}/${{ matrix.total }}
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        total: ["4"]
++        index: ["0", "1", "2", "3"]
++
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Update version
++        shell: bash
++        run: |
++          ./new-release.sh
++          git diff .
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          python3 -m pip install --upgrade pip jinja2
++
++      - name: Install emsdk
++        uses: mymindstorm/setup-emsdk@v14
++        with:
++          version: 3.1.53
++          actions-cache-folder: 'emsdk-cache'
++
++      - name: View emsdk version
++        shell: bash
++        run: |
++          emcc -v
++          echo "--------------------"
++          emcc --check
++
++      - name: Generate build script
++        shell: bash
++        run: |
++          cd scripts/wasm
++
++          total=${{ matrix.total }}
++          index=${{ matrix.index }}
++
++          ./generate-tts.py --total $total --index $index
++
++          chmod +x run-tts.sh
++          mv -v ./run-tts.sh ../..
++
++      - name: Show build scripts
++        shell: bash
++        run: |
++          cat ./run-tts.sh
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: run-tts-${{ matrix.index }}
++          path: ./run-tts.sh
++
++      - name: Build sherpa-onnx for WebAssembly
++        shell: bash
++        env:
++          MS_TOKEN: ${{ secrets.MODEL_SCOPE_GIT_TOKEN }}
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        run: |
++          ./run-tts.sh
++
++      - name: Release
++        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: ./*.tar.bz2
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.19
++
++      - name: Upload wasm files
++        uses: actions/upload-artifact@v4
++        with:
++          name: sherpa-onnx-wasm-simd-tts-${{ matrix.index }}
++          path: ./sherpa-onnx-wasm-simd-*.tar.bz2
+diff --git a/README.md b/README.md
+index 72a3079e..6cd6d6f4 100644
+--- a/README.md
++++ b/README.md
+@@ -139,8 +139,11 @@ We also have spaces built using WebAssembly. They are listed below:
+ |VAD + speech recognition (English + Chinese, ) with Paraformer-large          |[Click me][wasm-hf-vad-asr-zh-en-paraformer-large]| [][wasm-ms-vad-asr-zh-en-paraformer-large]|
+ |VAD + speech recognition (English + Chinese, ) with Paraformer-small          |[Click me][wasm-hf-vad-asr-zh-en-paraformer-small]| [][wasm-ms-vad-asr-zh-en-paraformer-small]|
+ |VAD + speech recognition () with [Dolphin][Dolphin]-base          |[Click me][wasm-hf-vad-asr-multi-lang-dolphin-base]| [][wasm-ms-vad-asr-multi-lang-dolphin-base]|
+-|Speech synthesis (English)                                                                  |[Click me][wasm-hf-tts-piper-en]| [][wasm-ms-tts-piper-en]|
+-|Speech synthesis (German)                                                                   |[Click me][wasm-hf-tts-piper-de]| [][wasm-ms-tts-piper-de]|
++|Speech synthesis (Piper, English)                                                                  |[Click me][wasm-hf-tts-piper-en]| [][wasm-ms-tts-piper-en]|
++|Speech synthesis (Piper, German)                                                                   |[Click me][wasm-hf-tts-piper-de]| [][wasm-ms-tts-piper-de]|
++|Speech synthesis (Matcha, Chinese)                                                                  |[Click me][wasm-hf-tts-matcha-zh]| [][wasm-ms-tts-matcha-zh]|
++|Speech synthesis (Matcha, English)                                                                  |[Click me][wasm-hf-tts-matcha-en]| [][wasm-ms-tts-matcha-en]|
++|Speech synthesis (Matcha, Chinese+English)                                                          |[Click me][wasm-hf-tts-matcha-zh-en]| [][wasm-ms-tts-matcha-zh-en]|
+ |Speaker diarization                                                                         |[Click me][wasm-hf-speaker-diarization]|[][wasm-ms-speaker-diarization]|
+ 
+ </details>
+@@ -495,6 +498,12 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
+ [wasm-ms-vad-asr-multi-lang-dolphin-base]: https://modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc
+ [wasm-hf-vad-asr-multi-lang-dolphin-base]: https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc
+ 
++[wasm-hf-tts-matcha-zh-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-zh-en-tts-matcha
++[wasm-hf-tts-matcha-zh]: https://huggingface.co/spaces/k2-fsa/web-assembly-zh-tts-matcha
++[wasm-ms-tts-matcha-zh-en]: https://modelscope.cn/studios/csukuangfj/web-assembly-zh-en-tts-matcha
++[wasm-ms-tts-matcha-zh]: https://modelscope.cn/studios/csukuangfj/web-assembly-zh-tts-matcha
++[wasm-hf-tts-matcha-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-en-tts-matcha
++[wasm-ms-tts-matcha-en]: https://modelscope.cn/studios/csukuangfj/web-assembly-en-tts-matcha
+ [wasm-hf-tts-piper-en]: https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en
+ [wasm-ms-tts-piper-en]: https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en
+ [wasm-hf-tts-piper-de]: https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de
+diff --git a/scripts/wasm/generate-tts.py b/scripts/wasm/generate-tts.py
+new file mode 100755
+index 00000000..b2b41f03
+--- /dev/null
++++ b/scripts/wasm/generate-tts.py
+@@ -0,0 +1,189 @@
++#!/usr/bin/env python3
++
++import argparse
++from dataclasses import dataclass
++
++import jinja2
++
++
++def get_args():
++    parser = argparse.ArgumentParser()
++    parser.add_argument(
++        "--total",
++        type=int,
++        default=1,
++        help="Number of runners",
++    )
++    parser.add_argument(
++        "--index",
++        type=int,
++        default=0,
++        help="Index of the current runner",
++    )
++    return parser.parse_args()
++
++
++@dataclass
++class Model:
++    model_name: str
++    hf: str  # huggingface space name
++    ms: str  # modelscope space name
++    cmd: str = ""
++
++
++def get_models():
++    models = [
++        Model(
++            model_name="vits-piper-de_DE-thorsten_emotional-medium",
++            hf="k2-fsa/web-assembly-tts-sherpa-onnx-de",
++            ms="k2-fsa/web-assembly-tts-sherpa-onnx-de",
++            cmd="""
++            pushd $model_name
++
++            mv -v *.onnx ../
++            mv -v tokens.txt ../
++            mv -v espeak-ng-data ../
++            popd
++
++
++            git checkout .
++
++            rm -rf $model_name
++            git diff
++            """,
++        ),
++        Model(
++            model_name="vits-piper-en_US-libritts_r-medium",
++            hf="k2-fsa/web-assembly-tts-sherpa-onnx-en",
++            ms="k2-fsa/web-assembly-tts-sherpa-onnx-en",
++            cmd="""
++            pushd $model_name
++
++            mv -v *.onnx ../
++            mv -v tokens.txt ../
++            mv -v espeak-ng-data ../
++            popd
++
++
++            git checkout .
++
++            rm -rf $model_name
++            git diff
++            """,
++        ),
++        Model(
++            model_name="matcha-icefall-zh-en",
++            hf="k2-fsa/web-assembly-zh-en-tts-matcha",
++            ms="csukuangfj/web-assembly-zh-en-tts-matcha",
++            cmd="""
++            pushd $model_name
++
++            mv -v *.fst ../
++            mv -v *.onnx ../
++            mv -v tokens.txt ../
++            mv -v lexicon.txt ../
++            mv -v espeak-ng-data ../
++            popd
++
++            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
++
++            git checkout .
++            sed -i.bak 's/let type = 0/let type = 1/g' ../sherpa-onnx-tts.js
++
++            rm -rf $model_name
++            git diff
++            """,
++        ),
++        Model(
++            model_name="matcha-icefall-zh-baker",
++            hf="k2-fsa/web-assembly-zh-tts-matcha",
++            ms="csukuangfj/web-assembly-zh-tts-matcha",
++            cmd="""
++            pushd $model_name
++
++            mv -v *.fst ../
++            mv -v *.onnx ../
++            mv -v tokens.txt ../
++            mv -v lexicon.txt ../
++            popd
++
++            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-22khz-univ.onnx
++
++
++            git checkout .
++            sed -i.bak 's/let type = 0/let type = 2/g' ../sherpa-onnx-tts.js
++
++            rm -rf $model_name
++            git diff
++            """,
++        ),
++        Model(
++            model_name="matcha-icefall-en_US-ljspeech",
++            hf="k2-fsa/web-assembly-en-tts-matcha",
++            ms="csukuangfj/web-assembly-en-tts-matcha",
++            cmd="""
++            pushd $model_name
++
++            mv -v *.onnx ../
++            mv -v tokens.txt ../
++            mv -v espeak-ng-data ../
++            popd
++
++            curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-22khz-univ.onnx
++
++
++            git checkout .
++            sed -i.bak 's/let type = 0/let type = 3/g' ../sherpa-onnx-tts.js
++
++            rm -rf $model_name
++            git diff
++            """,
++        ),
++    ]
++    return models
++
++
++def main():
++    args = get_args()
++    index = args.index
++    total = args.total
++    assert 0 <= index < total, (index, total)
++
++    all_model_list = get_models()
++
++    num_models = len(all_model_list)
++
++    num_per_runner = num_models // total
++    if num_per_runner <= 0:
++        raise ValueError(f"num_models: {num_models}, num_runners: {total}")
++
++    start = index * num_per_runner
++    end = start + num_per_runner
++
++    remaining = num_models - args.total * num_per_runner
++
++    print(f"{index}/{total}: {start}-{end}/{num_models}")
++
++    d = dict()
++    d["model_list"] = all_model_list[start:end]
++    if index < remaining:
++        s = args.total * num_per_runner + index
++        d["model_list"].append(all_model_list[s])
++        print(f"{s}/{num_models}")
++
++    filename_list = [
++        "./run-tts.sh",
++    ]
++    for filename in filename_list:
++        environment = jinja2.Environment()
++        with open(f"{filename}.in") as f:
++            s = f.read()
++        template = environment.from_string(s)
++
++        s = template.render(**d)
++        with open(filename, "w") as f:
++            print(s, file=f)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/wasm/generate-vad-asr.py b/scripts/wasm/generate-vad-asr.py
+index 209ed24d..f20881f5 100755
+--- a/scripts/wasm/generate-vad-asr.py
++++ b/scripts/wasm/generate-vad-asr.py
+@@ -2,7 +2,6 @@
+ 
+ import argparse
+ from dataclasses import dataclass
+-from typing import List, Optional
+ 
+ import jinja2
+ 
+diff --git a/scripts/wasm/run-tts.sh.in b/scripts/wasm/run-tts.sh.in
+new file mode 100644
+index 00000000..7063a37a
+--- /dev/null
++++ b/scripts/wasm/run-tts.sh.in
+@@ -0,0 +1,90 @@
++#!/usr/bin/env bash
++#
++# Build WebAssembly APPs for huggingface spaces and modelscope spaces
++
++set -ex
++
++log() {
++  # This function is from espnet
++  local fname=${BASH_SOURCE[1]##*/}
++  echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
++}
++
++SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++
++{% for model in model_list %}
++model_name={{ model.model_name }}
++hf_name={{ model.hf }}
++ms_name={{ model.ms }}
++
++pushd wasm/tts
++git checkout .
++rm -rf assets
++mkdir assets
++cd assets
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/${model_name}.tar.bz2
++tar xvf ${model_name}.tar.bz2
++rm ${model_name}.tar.bz2
++
++{{ model.cmd }}
++
++popd
++
++ls -lh wasm/tts/assets
++
++rm -rf build-wasm-simd-tts/install
++rm -rf build-wasm-simd-tts/wasm
++
++./build-wasm-simd-tts.sh
++
++dst=sherpa-onnx-wasm-simd-${SHERPA_ONNX_VERSION}-${model_name}
++mv build-wasm-simd-tts/install/bin/wasm/tts $dst
++ls -lh $dst
++tar cjfv $dst.tar.bz2 ./$dst
++ls -lh *.tar.bz2
++
++git config --global user.email "csukuangfj@gmail.com"
++git config --global user.name "Fangjun Kuang"
++
++export GIT_LFS_SKIP_SMUDGE=1
++export GIT_CLONE_PROTECTION_ACTIVE=false
++
++rm -rf ms
++git clone https://www.modelscope.cn/studios/$ms_name.git ms
++
++cd ms
++cp -v ../$dst/* .
++
++git status
++git lfs track "*.data"
++git lfs track "*.wasm"
++ls -lh
++
++git add .
++git commit -m "update model" || true
++git push https://oauth2:${MS_TOKEN}@www.modelscope.cn/studios/$ms_name.git || true
++cd ..
++rm -rf ms
++
++rm -rf huggingface
++
++git clone https://huggingface.co/spaces/$hf_name huggingface
++cd huggingface
++cp -v ../$dst/* .
++
++git status
++git lfs track "*.data"
++git lfs track "*.wasm"
++ls -lh
++
++git add .
++git commit -m "update model" || true
++git push https://csukuangfj:$HF_TOKEN@huggingface.co/spaces/$hf_name main || true
++cd ..
++rm -rf huggingface
++rm -rf $dst
++
++ls -lh *.tar.bz2
++
++{% endfor %}
+diff --git a/wasm/asr/app-asr.js b/wasm/asr/app-asr.js
+index 94725057..987449cb 100644
+--- a/wasm/asr/app-asr.js
++++ b/wasm/asr/app-asr.js
+@@ -15,7 +15,7 @@ let resultList = [];
+ clearBtn.onclick = function() {
+   resultList = [];
+   textArea.value = getDisplayResult();
+-  textArea.scrollTop = textArea.scrollHeight; // auto scroll
++  textArea.scrollTop = textArea.scrollHeight;  // auto scroll
+ };
+ 
+ function getDisplayResult() {
+@@ -48,9 +48,21 @@ Module.locateFile = function(path, scriptDirectory = '') {
+ Module.setStatus = function(status) {
+   console.log(`status ${status}`);
+   const statusElement = document.getElementById('status');
+-  if (status == "Running...") {
++  if (status == 'Running...') {
+     status = 'Model downloaded. Initializing recongizer...'
+   }
++
++  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
++  if (downloadMatch) {
++    const downloaded = BigInt(downloadMatch[1]);
++    const total = BigInt(downloadMatch[2]);
++    const percent =
++        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
++    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
++        downloadMatch[2]})`;
++    console.log(`here ${status}`)
++  }
++
+   statusElement.textContent = status;
+   if (status === '') {
+     statusElement.style.display = 'none';
+@@ -80,11 +92,11 @@ let audioCtx;
+ let mediaStream;
+ 
+ let expectedSampleRate = 16000;
+-let recordSampleRate; // the sampleRate of the microphone
+-let recorder = null;  // the microphone
+-let leftchannel = []; // TODO: Use a single channel
++let recordSampleRate;  // the sampleRate of the microphone
++let recorder = null;   // the microphone
++let leftchannel = [];  // TODO: Use a single channel
+ 
+-let recordingLength = 0; // number of samples so far
++let recordingLength = 0;  // number of samples so far
+ 
+ let recognizer = null;
+ let recognizer_stream = null;
+@@ -93,11 +105,11 @@ if (navigator.mediaDevices.getUserMedia) {
+   console.log('getUserMedia supported.');
+ 
+   // see https://w3c.github.io/mediacapture-main/#dom-mediadevices-getusermedia
+-  const constraints = {audio : true};
++  const constraints = {audio: true};
+ 
+   let onSuccess = function(stream) {
+     if (!audioCtx) {
+-      audioCtx = new AudioContext({sampleRate : 16000});
++      audioCtx = new AudioContext({sampleRate: 16000});
+     }
+     console.log(audioCtx);
+     recordSampleRate = audioCtx.sampleRate;
+@@ -160,7 +172,7 @@ if (navigator.mediaDevices.getUserMedia) {
+       }
+ 
+       textArea.value = getDisplayResult();
+-      textArea.scrollTop = textArea.scrollHeight; // auto scroll
++      textArea.scrollTop = textArea.scrollHeight;  // auto scroll
+ 
+       let buf = new Int16Array(samples.length);
+       for (var i = 0; i < samples.length; ++i) {
+@@ -247,8 +259,9 @@ if (navigator.mediaDevices.getUserMedia) {
+     };
+   };
+ 
+-  let onError = function(
+-      err) { console.log('The following error occured: ' + err); };
++  let onError = function(err) {
++    console.log('The following error occured: ' + err);
++  };
+ 
+   navigator.mediaDevices.getUserMedia(constraints).then(onSuccess, onError);
+ } else {
+@@ -281,22 +294,22 @@ function toWav(samples) {
+ 
+   // http://soundfile.sapp.org/doc/WaveFormat/
+   //                   F F I R
+-  view.setUint32(0, 0x46464952, true);              // chunkID
+-  view.setUint32(4, 36 + samples.length * 2, true); // chunkSize
++  view.setUint32(0, 0x46464952, true);               // chunkID
++  view.setUint32(4, 36 + samples.length * 2, true);  // chunkSize
+   //                   E V A W
+-  view.setUint32(8, 0x45564157, true); // format
+-                                       //
++  view.setUint32(8, 0x45564157, true);  // format
++                                        //
+   //                      t m f
+-  view.setUint32(12, 0x20746d66, true);             // subchunk1ID
+-  view.setUint32(16, 16, true);                     // subchunk1Size, 16 for PCM
+-  view.setUint32(20, 1, true);                      // audioFormat, 1 for PCM
+-  view.setUint16(22, 1, true);                      // numChannels: 1 channel
+-  view.setUint32(24, expectedSampleRate, true);     // sampleRate
+-  view.setUint32(28, expectedSampleRate * 2, true); // byteRate
+-  view.setUint16(32, 2, true);                      // blockAlign
+-  view.setUint16(34, 16, true);                     // bitsPerSample
+-  view.setUint32(36, 0x61746164, true);             // Subchunk2ID
+-  view.setUint32(40, samples.length * 2, true);     // subchunk2Size
++  view.setUint32(12, 0x20746d66, true);          // subchunk1ID
++  view.setUint32(16, 16, true);                  // subchunk1Size, 16 for PCM
++  view.setUint32(20, 1, true);                   // audioFormat, 1 for PCM
++  view.setUint16(22, 1, true);                   // numChannels: 1 channel
++  view.setUint32(24, expectedSampleRate, true);  // sampleRate
++  view.setUint32(28, expectedSampleRate * 2, true);  // byteRate
++  view.setUint16(32, 2, true);                       // blockAlign
++  view.setUint16(34, 16, true);                      // bitsPerSample
++  view.setUint32(36, 0x61746164, true);              // Subchunk2ID
++  view.setUint32(40, samples.length * 2, true);      // subchunk2Size
+ 
+   let offset = 44;
+   for (let i = 0; i < samples.length; ++i) {
+@@ -304,7 +317,7 @@ function toWav(samples) {
+     offset += 2;
+   }
+ 
+-  return new Blob([ view ], {type : 'audio/wav'});
++  return new Blob([view], {type: 'audio/wav'});
+ }
+ 
+ // this function is copied from
+diff --git a/wasm/tts/CMakeLists.txt b/wasm/tts/CMakeLists.txt
+index bc1da0fd..a560289a 100644
+--- a/wasm/tts/CMakeLists.txt
++++ b/wasm/tts/CMakeLists.txt
+@@ -2,7 +2,7 @@ if(NOT $ENV{SHERPA_ONNX_IS_USING_BUILD_WASM_SH})
+   message(FATAL_ERROR "Please use ./build-wasm-simd-tts.sh to build for wasm TTS")
+ endif()
+ 
+-if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/assets/model.onnx")
++if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/assets/tokens.txt")
+   message(FATAL_ERROR "Please read ${CMAKE_CURRENT_SOURCE_DIR}/assets/README.md before you continue")
+ endif()
+ 
+diff --git a/wasm/tts/app-tts.js b/wasm/tts/app-tts.js
+index c87e70f9..624c974c 100644
+--- a/wasm/tts/app-tts.js
++++ b/wasm/tts/app-tts.js
+@@ -26,9 +26,21 @@ Module.locateFile = function(path, scriptDirectory = '') {
+ Module.setStatus = function(status) {
+   console.log(`status ${status}`);
+   const statusElement = document.getElementById('status');
+-  if (status == "Running...") {
++  if (status == 'Running...') {
+     status = 'Model downloaded. Initializing text to speech model...'
+   }
++
++  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
++  if (downloadMatch) {
++    const downloaded = BigInt(downloadMatch[1]);
++    const total = BigInt(downloadMatch[2]);
++    const percent =
++        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
++    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
++        downloadMatch[2]})`;
++    console.log(`here ${status}`)
++  }
++
+   statusElement.textContent = status;
+   if (status === '') {
+     statusElement.style.display = 'none';
+@@ -57,7 +69,9 @@ Module.onRuntimeInitialized = function() {
+   generateBtn.disabled = false;
+ };
+ 
+-speedInput.oninput = function() { speedValue.innerHTML = this.value; };
++speedInput.oninput = function() {
++  speedValue.innerHTML = this.value;
++};
+ 
+ generateBtn.onclick = function() {
+   let speakerId = speakerIdInput.value;
+@@ -89,12 +103,12 @@ generateBtn.onclick = function() {
+   console.log('text', text);
+ 
+   let audio =
+-      tts.generate({text : text, sid : speakerId, speed : speedInput.value});
++      tts.generate({text: text, sid: speakerId, speed: speedInput.value});
+ 
+   console.log(audio.samples.length, audio.sampleRate);
+ 
+   if (!audioCtx) {
+-    audioCtx = new AudioContext({sampleRate : tts.sampleRate});
++    audioCtx = new AudioContext({sampleRate: tts.sampleRate});
+   }
+ 
+   const buffer = audioCtx.createBuffer(1, audio.samples.length, tts.sampleRate);
+@@ -175,22 +189,22 @@ function toWav(floatSamples, sampleRate) {
+ 
+   // http://soundfile.sapp.org/doc/WaveFormat/
+   //                   F F I R
+-  view.setUint32(0, 0x46464952, true);              // chunkID
+-  view.setUint32(4, 36 + samples.length * 2, true); // chunkSize
++  view.setUint32(0, 0x46464952, true);               // chunkID
++  view.setUint32(4, 36 + samples.length * 2, true);  // chunkSize
+   //                   E V A W
+-  view.setUint32(8, 0x45564157, true); // format
+-                                       //
++  view.setUint32(8, 0x45564157, true);  // format
++                                        //
+   //                      t m f
+-  view.setUint32(12, 0x20746d66, true);         // subchunk1ID
+-  view.setUint32(16, 16, true);                 // subchunk1Size, 16 for PCM
+-  view.setUint32(20, 1, true);                  // audioFormat, 1 for PCM
+-  view.setUint16(22, 1, true);                  // numChannels: 1 channel
+-  view.setUint32(24, sampleRate, true);         // sampleRate
+-  view.setUint32(28, sampleRate * 2, true);     // byteRate
+-  view.setUint16(32, 2, true);                  // blockAlign
+-  view.setUint16(34, 16, true);                 // bitsPerSample
+-  view.setUint32(36, 0x61746164, true);         // Subchunk2ID
+-  view.setUint32(40, samples.length * 2, true); // subchunk2Size
++  view.setUint32(12, 0x20746d66, true);          // subchunk1ID
++  view.setUint32(16, 16, true);                  // subchunk1Size, 16 for PCM
++  view.setUint32(20, 1, true);                   // audioFormat, 1 for PCM
++  view.setUint16(22, 1, true);                   // numChannels: 1 channel
++  view.setUint32(24, sampleRate, true);          // sampleRate
++  view.setUint32(28, sampleRate * 2, true);      // byteRate
++  view.setUint16(32, 2, true);                   // blockAlign
++  view.setUint16(34, 16, true);                  // bitsPerSample
++  view.setUint32(36, 0x61746164, true);          // Subchunk2ID
++  view.setUint32(40, samples.length * 2, true);  // subchunk2Size
+ 
+   let offset = 44;
+   for (let i = 0; i < samples.length; ++i) {
+@@ -198,5 +212,5 @@ function toWav(floatSamples, sampleRate) {
+     offset += 2;
+   }
+ 
+-  return new Blob([ view ], {type : 'audio/wav'});
++  return new Blob([view], {type: 'audio/wav'});
+ }
+diff --git a/wasm/tts/sherpa-onnx-tts.js b/wasm/tts/sherpa-onnx-tts.js
+index a1c4d4bf..e24bbc3f 100644
+--- a/wasm/tts/sherpa-onnx-tts.js
++++ b/wasm/tts/sherpa-onnx-tts.js
+@@ -543,17 +543,17 @@ class OfflineTts {
+ }
+ 
+ function createOfflineTts(Module, myConfig) {
+-  const offlineTtsVitsModelConfig = {
+-    model: './model.onnx',
++  const vits = {
++    model: '',
+     lexicon: '',
+-    tokens: './tokens.txt',
+-    dataDir: './espeak-ng-data',
++    tokens: '',
++    dataDir: '',
+     noiseScale: 0.667,
+     noiseScaleW: 0.8,
+     lengthScale: 1.0,
+   };
+ 
+-  const offlineTtsMatchaModelConfig = {
++  const matcha = {
+     acousticModel: '',
+     vocoder: '',
+     lexicon: '',
+@@ -581,9 +581,48 @@ function createOfflineTts(Module, myConfig) {
+     lengthScale: 1.0,
+   };
+ 
++  let ruleFsts = '';
++
++  let type = 0;
++  switch (type) {
++    case 0:
++      // vits
++      vits.model = './model.onnx';
++      vits.tokens = './tokens.txt';
++      vits.dataDir = './espeak-ng-data';
++      break;
++    case 1:
++      // matcha zh-en
++      // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
++      matcha.acousticModel = './model-steps-3.onnx';
++      matcha.vocoder = './vocos-16khz-univ.onnx';
++      matcha.lexicon = './lexicon.txt';
++      matcha.tokens = './tokens.txt';
++      matcha.dataDir = './espeak-ng-data';
++      ruleFsts = './phone-zh.fst,./date-zh.fst,./number-zh.fst';
++      break;
++    case 2:
++      // matcha zh
++      // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese/matcha-icefall-zh-baker.html
++      matcha.acousticModel = './model-steps-3.onnx';
++      matcha.vocoder = './vocos-22khz-univ.onnx';
++      matcha.lexicon = './lexicon.txt';
++      matcha.tokens = './tokens.txt';
++      ruleFsts = './phone.fst,./date.fst,./number.fst';
++      break;
++    case 3:
++      // matcha en
++      // https://k2-fsa.github.io/sherpa/onnx/tts/all/English/matcha-icefall-en_US-ljspeech.html
++      matcha.acousticModel = './model-steps-3.onnx';
++      matcha.vocoder = './vocos-22khz-univ.onnx';
++      matcha.tokens = './tokens.txt';
++      matcha.dataDir = './espeak-ng-data';
++      break;
++  }
++
+   const offlineTtsModelConfig = {
+-    offlineTtsVitsModelConfig: offlineTtsVitsModelConfig,
+-    offlineTtsMatchaModelConfig: offlineTtsMatchaModelConfig,
++    offlineTtsVitsModelConfig: vits,
++    offlineTtsMatchaModelConfig: matcha,
+     offlineTtsKokoroModelConfig: offlineTtsKokoroModelConfig,
+     offlineTtsKittenModelConfig: offlineTtsKittenModelConfig,
+     numThreads: 1,
+@@ -593,7 +632,7 @@ function createOfflineTts(Module, myConfig) {
+ 
+   let offlineTtsConfig = {
+     offlineTtsModelConfig: offlineTtsModelConfig,
+-    ruleFsts: '',
++    ruleFsts: ruleFsts,
+     ruleFars: '',
+     maxNumSentences: 1,
+   }
+diff --git a/wasm/vad-asr/app-vad-asr.js b/wasm/vad-asr/app-vad-asr.js
+index 258753e8..5eaf5d20 100644
+--- a/wasm/vad-asr/app-vad-asr.js
++++ b/wasm/vad-asr/app-vad-asr.js
+@@ -140,6 +140,18 @@ Module.setStatus = function(status) {
+   if (status == 'Running...') {
+     status = 'Model downloaded. Initializing recongizer...'
+   }
++
++  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
++  if (downloadMatch) {
++    const downloaded = BigInt(downloadMatch[1]);
++    const total = BigInt(downloadMatch[2]);
++    const percent =
++        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
++    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
++        downloadMatch[2]})`;
++    console.log(`here ${status}`)
++  }
++
+   statusElement.textContent = status;
+   if (status === '') {
+     statusElement.style.display = 'none';
+diff --git a/wasm/vad/app-vad.js b/wasm/vad/app-vad.js
+index 45e8fe4b..e4ec4ba6 100644
+--- a/wasm/vad/app-vad.js
++++ b/wasm/vad/app-vad.js
+@@ -56,6 +56,18 @@ Module.setStatus = function(status) {
+   if (status == 'Running...') {
+     status = 'Model downloaded. Initializing vad...'
+   }
++
++  const downloadMatch = status.match(/Downloading data... \((\d+)\/(\d+)\)/);
++  if (downloadMatch) {
++    const downloaded = BigInt(downloadMatch[1]);
++    const total = BigInt(downloadMatch[2]);
++    const percent =
++        total === 0 ? 0.00 : Number((downloaded * 10000n) / total) / 100;
++    status = `Downloading data... ${percent.toFixed(2)}% (${downloadMatch[1]}/${
++        downloadMatch[2]})`;
++    console.log(`here ${status}`)
++  }
++
+   statusElement.textContent = status;
+   if (status === '') {
+     statusElement.style.display = 'none';
+
+commit afa59281c10fa294e91bfc6b627a402b1ec5a592
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Wed Dec 10 15:57:56 2025 +0800
+
+    Build APKs for MatchaTTS Chinese+English (#2882)
+
+diff --git a/.gitignore b/.gitignore
+index 6fdcc69a..612f82b2 100755
+--- a/.gitignore
++++ b/.gitignore
+@@ -164,3 +164,4 @@ sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+ build-riscv64-linux-gnu-spacemit/
+ spacemit-toolchain*
+ sherpa-onnx-qnn-*
++matcha-icefall-*
+diff --git a/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt b/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
+index 07b543e3..1cdb6ab5 100644
+--- a/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
++++ b/android/SherpaOnnxTts/app/src/main/java/com/k2fsa/sherpa/onnx/MainActivity.kt
+@@ -294,6 +294,15 @@ class MainActivity : AppCompatActivity() {
+         // dataDir = "kokoro-multi-lang-v1_0/espeak-ng-data"
+         // isKitten = true
+ 
++        // Example 12
++        // matcha-icefall-zh-en
++        // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
++        // modelDir = "matcha-icefall-zh-en"
++        // acousticModelName = "model-steps-3.onnx"
++        // vocoder = "vocos-16khz-univ.onnx"    // Vocoder should be downloaded separately; place in the **root directory of your resources folder**, not under modelDir.
++        // dataDir = "matcha-icefall-zh-en/espeak-ng-data"
++        // lexicon = "lexicon.txt"
++
+         if (dataDir != null) {
+             val newDir = copyDataDir(dataDir!!)
+             dataDir = "$newDir/$dataDir"
+diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
+index 38ce8096..f388b5f4 100644
+--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
++++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
+@@ -44,6 +44,7 @@ import androidx.compose.ui.unit.dp
+ import com.k2fsa.sherpa.onnx.tts.engine.ui.theme.SherpaOnnxTtsEngineTheme
+ import kotlinx.coroutines.CoroutineScope
+ import kotlinx.coroutines.Dispatchers
++import kotlinx.coroutines.SupervisorJob
+ import kotlinx.coroutines.channels.Channel
+ import kotlinx.coroutines.launch
+ import kotlinx.coroutines.withContext
+@@ -64,7 +65,9 @@ class MainActivity : ComponentActivity() {
+ 
+     private var stopped: Boolean = false
+ 
+-    private var samplesChannel = Channel<FloatArray>()
++    private var samplesChannel = Channel<FloatArray>(capacity = 128)
++    private val scope = CoroutineScope(Dispatchers.IO + SupervisorJob())
++
+ 
+     override fun onCreate(savedInstanceState: Bundle?) {
+         super.onCreate(savedInstanceState)
+@@ -177,8 +180,16 @@ class MainActivity : ComponentActivity() {
+                                                 rtfText = ""
+                                                 Log.i(TAG, "Started with text $testText")
+ 
+-                                                CoroutineScope(Dispatchers.IO).launch {
++                                                scope.launch {
+                                                     for (samples in samplesChannel) {
++                                                        if (samples.isEmpty()) {
++                                                            break
++                                                        }
++
++                                                        Log.i(
++                                                            TAG,
++                                                            "Received ${samples.count()} samples"
++                                                        )
+                                                         track.write(
+                                                             samples,
+                                                             0,
+@@ -189,10 +200,14 @@ class MainActivity : ComponentActivity() {
+                                                             break
+                                                         }
+                                                     }
++                                                    Log.i(TAG, "Draining the channel")
+ 
+-                                                    for (s in samplesChannel) {
+-                                                        // drain the channel
++                                                    // drain remaining
++                                                    while (!samplesChannel.isEmpty) {
++                                                        samplesChannel.tryReceive().getOrNull()
+                                                     }
++                                                    Log.i(TAG, "Channel drained")
++
+                                                 }
+ 
+                                                 CoroutineScope(Dispatchers.Default).launch {
+@@ -222,6 +237,12 @@ class MainActivity : ComponentActivity() {
+                                                         elapsed / audioDuration
+                                                     )
+ 
++                                                    scope.launch {
++                                                        Log.i(TAG, "send 0 samples")
++                                                            samplesChannel.send(FloatArray(0))
++                                                        Log.i(TAG, "send 0 samples done")
++                                                    }
++
+                                                     val filename =
+                                                         application.filesDir.absolutePath + "/generated.wav"
+ 
+@@ -237,8 +258,10 @@ class MainActivity : ComponentActivity() {
+                                                             playEnabled = true
+                                                             rtfText = RTF
+                                                         }
++
++
+                                                     }
+-                                                }.start()
++                                                }
+                                             }
+                                         }) {
+                                         Text("Start")
+@@ -311,8 +334,10 @@ class MainActivity : ComponentActivity() {
+     private fun callback(samples: FloatArray): Int {
+         if (!stopped) {
+             val samplesCopy = samples.copyOf()
+-            CoroutineScope(Dispatchers.IO).launch {
+-                samplesChannel.send(samplesCopy)
++            scope.launch {
++                Log.i(TAG, "callback called with ${samplesCopy.count()} samples")
++                val ok = samplesChannel.trySend(samplesCopy).isSuccess
++                Log.i(TAG, "callback called with $ok")
+             }
+             return 1
+         } else {
+diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
+index 078b0948..649b7dce 100644
+--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
++++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
+@@ -176,6 +176,16 @@ object TtsEngine {
+         // dataDir = "kitten-nano-en-v0_1-fp16/espeak-ng-data"
+         // lang = "eng"
+         // isKitten = true
++
++        // Example 12
++        // matcha-icefall-zh-en
++        // https://k2-fsa.github.io/sherpa/onnx/tts/all/Chinese-English/matcha-icefall-zh-en.html
++        // modelDir = "matcha-icefall-zh-en"
++        // acousticModelName = "model-steps-3.onnx"
++        // vocoder = "vocos-16khz-univ.onnx"
++        // dataDir = "matcha-icefall-zh-en/espeak-ng-data"
++        // lexicon = "lexicon.txt"
++        // lang = "zho"
+     }
+ 
+     fun createTts(context: Context) {
+diff --git a/scripts/apk/build-apk-tts-engine.sh.in b/scripts/apk/build-apk-tts-engine.sh.in
+index 341bc6f7..b3abfd08 100644
+--- a/scripts/apk/build-apk-tts-engine.sh.in
++++ b/scripts/apk/build-apk-tts-engine.sh.in
+@@ -113,6 +113,10 @@ if [[ $model_dir == vits-melo-tts-zh_en ]]; then
+   lang=zh_en
+ fi
+ 
++if [[ $model_dir == matcha-icefall-zh-en ]]; then
++  lang=zh_en
++fi
++
+ if [[ $model_dir == kokoro-multi-lang-v1_0 || $model_dir == kokoro-multi-lang-v1_1 || $model_dir == kokoro-int8-multi-lang-v1_1 ]]; then
+   lang=zh_en
+ fi
+diff --git a/scripts/apk/build-apk-tts.sh.in b/scripts/apk/build-apk-tts.sh.in
+index 42e2811b..bdb3b77f 100644
+--- a/scripts/apk/build-apk-tts.sh.in
++++ b/scripts/apk/build-apk-tts.sh.in
+@@ -107,6 +107,10 @@ if [[ $model_dir == vits-melo-tts-zh_en ]]; then
+   lang=zh_en
+ fi
+ 
++if [[ $model_dir == matcha-icefall-zh-en ]]; then
++  lang=zh_en
++fi
++
+ if [[ $model_dir == kokoro-multi-lang-v1_0 || $model_dir == kokoro-multi-lang-v1_1 || $model_dir == kokoro-int8-multi-lang-v1_1 ]]; then
+   lang=zh_en
+ fi
+diff --git a/scripts/apk/generate-tts-apk-script.py b/scripts/apk/generate-tts-apk-script.py
+index 8ea80d35..756bb8ac 100755
+--- a/scripts/apk/generate-tts-apk-script.py
++++ b/scripts/apk/generate-tts-apk-script.py
+@@ -411,7 +411,6 @@ def get_vits_models() -> List[TtsModel]:
+             or "melo-tts" in m.model_dir
+         ):
+             s = s[:-1]
+-            m.dict_dir = m.model_dir + "/dict"
+         else:
+             m.rule_fars = f"{m.model_dir}/rule.far"
+ 
+@@ -440,15 +439,30 @@ def get_matcha_models() -> List[TtsModel]:
+             model_dir="matcha-icefall-zh-baker",
+             acoustic_model_name="model-steps-3.onnx",
+             lang="zh",
++            lexicon="lexicon.txt",
+         )
+     ]
+     rule_fsts = ["phone.fst", "date.fst", "number.fst"]
+     for m in chinese_models:
+         s = [f"{m.model_dir}/{r}" for r in rule_fsts]
+         m.rule_fsts = ",".join(s)
+-        m.dict_dir = m.model_dir + "/dict"
+         m.vocoder = "vocos-22khz-univ.onnx"
+ 
++    chinese_english_models = [
++        TtsModel(
++            model_dir="matcha-icefall-zh-en",
++            acoustic_model_name="model-steps-3.onnx",
++            lang="zh",
++            lexicon="lexicon.txt",
++        )
++    ]
++    rule_fsts_zh = ["phone-zh.fst", "date-zh.fst", "number-zh.fst"]
++    for m in chinese_english_models:
++        s = [f"{m.model_dir}/{r}" for r in rule_fsts_zh]
++        m.rule_fsts = ",".join(s)
++        m.vocoder = "vocos-16khz-univ.onnx"
++        m.data_dir = f"{m.model_dir}/espeak-ng-data"
++
+     english_persian_models = [
+         TtsModel(
+             model_dir="matcha-icefall-en_US-ljspeech",
+@@ -470,7 +484,7 @@ def get_matcha_models() -> List[TtsModel]:
+         m.data_dir = f"{m.model_dir}/espeak-ng-data"
+         m.vocoder = "vocos-22khz-univ.onnx"
+ 
+-    return chinese_models + english_persian_models
++    return chinese_models + english_persian_models + chinese_english_models
+ 
+ 
+ def get_kokoro_models() -> List[TtsModel]:
+@@ -507,7 +521,6 @@ def get_kokoro_models() -> List[TtsModel]:
+     ]
+     for m in multi_lingual_models:
+         m.data_dir = f"{m.model_dir}/espeak-ng-data"
+-        m.dict_dir = f"{m.model_dir}/dict"
+         m.voices = "voices.bin"
+         m.lexicon = f"{m.model_dir}/lexicon-us-en.txt,{m.model_dir}/lexicon-zh.txt"
+         m.rule_fsts = f"{m.model_dir}/phone-zh.fst,{m.model_dir}/date-zh.fst,{m.model_dir}/number-zh.fst"
+
+commit 9fdd5099362a64e9933584a3121544707ed2e08f
+Author: zhouyong <769721723@qq.com>
+Date:   Tue Dec 9 21:36:06 2025 +0800
+
+    Optimize streaming output results when VAD does not detect human voice for a long time (#2876)
+    
+    Co-authored-by: zhouyong <yonga.zhou@archermind.com>
+
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+index e6d5cb3d..ae5e1bd8 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+@@ -143,6 +143,7 @@ fun HomeScreen() {
+                     var startTime = System.currentTimeMillis()
+                     var lastText = ""
+                     var added = false
++                    var speechStartOffset = 0
+ 
+ 
+                     while (isStarted) {
+@@ -162,6 +163,11 @@ fun HomeScreen() {
+                                 offset += windowSize
+                                 if (!isSpeechStarted && SimulateStreamingAsr.vad.isSpeechDetected()) {
+                                     isSpeechStarted = true
++                                    // offset 0.25s
++                                    speechStartOffset = offset - 6400
++                                    if(speechStartOffset < 0) {
++                                        speechStartOffset = 0
++                                    }
+                                     startTime = System.currentTimeMillis()
+                                 }
+                             }
+@@ -172,7 +178,7 @@ fun HomeScreen() {
+                                 // You can change it to some other value
+                                 val stream = SimulateStreamingAsr.recognizer.createStream()
+                                 stream.acceptWaveform(
+-                                    buffer.subList(0, offset).toFloatArray(),
++                                    buffer.subList(speechStartOffset, offset).toFloatArray(),
+                                     sampleRateInHz
+                                 )
+                                 SimulateStreamingAsr.recognizer.decode(stream)
+
+commit d5b381ccc4c29446a385458d5a35b14018e3a22e
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Tue Dec 9 21:32:42 2025 +0800
+
+    Export models to Ascend 910B4 (#2878)
+
+diff --git a/.github/scripts/export-ascend/generate_paraformer.py b/.github/scripts/export-ascend/generate_paraformer.py
+index fa22d64b..396460bf 100755
+--- a/.github/scripts/export-ascend/generate_paraformer.py
++++ b/.github/scripts/export-ascend/generate_paraformer.py
+@@ -5,7 +5,7 @@ import itertools
+ import json
+ from dataclasses import asdict, dataclass
+ 
+-from generate_zipformer_ctc_20250703 import get_image
++from generate_zipformer_ctc_20250703 import get_cann_version, get_image, get_soc_version
+ 
+ 
+ @dataclass
+@@ -26,8 +26,8 @@ class Config:
+ 
+ 
+ def main():
+-    cann_version = ["7.0", "8.0", "8.2"]
+-    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    cann_version = get_cann_version()
++    soc_version = get_soc_version()
+     framework_list = ["FunASR", "WSChuan-ASR"]
+ 
+     configs = [
+diff --git a/.github/scripts/export-ascend/generate_sense_voice.py b/.github/scripts/export-ascend/generate_sense_voice.py
+index f1ecbf39..6c54a0a4 100755
+--- a/.github/scripts/export-ascend/generate_sense_voice.py
++++ b/.github/scripts/export-ascend/generate_sense_voice.py
+@@ -5,7 +5,7 @@ import itertools
+ import json
+ from dataclasses import asdict, dataclass
+ 
+-from generate_zipformer_ctc_20250703 import get_image
++from generate_zipformer_ctc_20250703 import get_image, get_soc_version, get_cann_version
+ 
+ 
+ @dataclass
+@@ -26,8 +26,8 @@ class Config:
+ 
+ 
+ def main():
+-    cann_version = ["7.0", "8.0", "8.2"]
+-    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    cann_version = get_cann_version()
++    soc_version = get_soc_version()
+     framework_list = ["FunASR", "WSYue-ASR"]
+ 
+     configs = [
+diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+index 511bfee1..f1865d5d 100755
+--- a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
++++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+@@ -32,6 +32,16 @@ def get_image(cann: str, soc_version: str):
+         raise ValueError(f"Unsupported soc_version {soc_version}")
+ 
+ 
++def get_soc_version():
++    soc_version = ["910B", "910B2", "910B3", "910B4", "310P3"]
++    return soc_version
++
++
++def get_cann_version():
++    cann_version = ["7.0", "8.0", "8.2"]
++    return cann_version
++
++
+ @dataclass
+ class Config:
+     # 7.0, 8.0, 8.2
+@@ -49,8 +59,8 @@ class Config:
+ 
+ 
+ def main():
+-    cann_version = ["7.0", "8.0", "8.2"]
+-    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    cann_version = get_cann_version()
++    soc_version = get_soc_version()
+     input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+ 
+     configs = [
+diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+index 63587194..9b8d62fc 100644
+--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
++++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - refactor-ascend-export-script
++      - ascend-910b4-2
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -26,8 +26,8 @@ jobs:
+         id: set-matrix
+         run: |
+           # outputting for debugging purposes
+-          python3 .github/scripts/export-ascend/generate_sense_voice.py
+-          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
++          python3 .github/scripts/export-ascend/generate_paraformer.py
++          MATRIX=$(python3 .github/scripts/export-ascend/generate_paraformer.py)
+ 
+           # deprecated
+           # echo "::set-output name=matrix::${MATRIX}"
+@@ -75,6 +75,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           echo "CANN environment:"
+           which atc || echo "atc not found"
+           atc --help
+@@ -129,6 +132,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           soc_version=${{ matrix.soc_version }}
+           cann=${{ matrix.cann }}
+ 
+@@ -222,6 +228,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           soc_version=${{ matrix.soc_version }}
+           cann=${{ matrix.cann }}
+ 
+diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+index 0be4e277..efc87861 100644
+--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
++++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - refactor-ascend-export-script
++      - ascend-910b4-2
+   workflow_dispatch:
+ 
+ concurrency:
+diff --git a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+index 4169f7c0..af517075 100644
+--- a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
++++ b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+@@ -3,7 +3,7 @@ name: export-zipformer-ctc-to-ascend-npu-20250703
+ on:
+   push:
+     branches:
+-      - export-zipformer-ctc-ascend
++      - ascend-910b4-2
+   workflow_dispatch:
+ 
+ concurrency:
+
+commit 8fac37f7d10f74945eda983e019f6deaacbb88b6
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Tue Dec 9 17:55:19 2025 +0800
+
+    Load QNN context binary for faster startup (#2877)
+
+diff --git a/.github/scripts/export-qnn/__init__.py b/.github/scripts/export-qnn/__init__.py
+new file mode 100644
+index 00000000..e69de29b
+diff --git a/.github/scripts/export-qnn/device_info.py b/.github/scripts/export-qnn/device_info.py
+new file mode 120000
+index 00000000..57618a93
+--- /dev/null
++++ b/.github/scripts/export-qnn/device_info.py
+@@ -0,0 +1 @@
++../../../scripts/qnn/device_info.py
+\ No newline at end of file
+diff --git a/.github/scripts/export-qnn/generate_config.py b/.github/scripts/export-qnn/generate_config.py
+new file mode 120000
+index 00000000..fbdbd361
+--- /dev/null
++++ b/.github/scripts/export-qnn/generate_config.py
+@@ -0,0 +1 @@
++../../../scripts/qnn/generate_config.py
+\ No newline at end of file
+diff --git a/.github/scripts/export-qnn/generate_sense_voice.py b/.github/scripts/export-qnn/generate_sense_voice.py
+new file mode 100755
+index 00000000..8a3e1778
+--- /dev/null
++++ b/.github/scripts/export-qnn/generate_sense_voice.py
+@@ -0,0 +1,47 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import json
++
++from device_info import soc_info_dict
++from dataclasses import asdict, dataclass
++import itertools
++
++
++@dataclass
++class Config:
++    soc: str  # SM8850
++    soc_id: int  # 87
++    arch: str  # v81
++    input_in_seconds: str
++    framework: str
++
++
++def main():
++
++    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++    framework_list = ["FunASR", "WSYue-ASR"]
++
++    configs = []
++
++    for name, soc in soc_info_dict.items():
++        for num_seconds, framework in itertools.product(
++            input_in_seconds, framework_list
++        ):
++            configs.append(
++                Config(
++                    soc=name,
++                    soc_id=soc.model.value,
++                    arch=soc.info.arch.name,
++                    input_in_seconds=num_seconds,
++                    framework=framework,
++                )
++            )
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/workflows/export-sense-voice-to-qnn.yaml b/.github/workflows/export-sense-voice-to-qnn.yaml
+index 73c8a4ec..db88a409 100644
+--- a/.github/workflows/export-sense-voice-to-qnn.yaml
++++ b/.github/workflows/export-sense-voice-to-qnn.yaml
+@@ -3,7 +3,7 @@ name: export-sense-voice-to-qnn
+ on:
+   push:
+     branches:
+-      - export-sense-voice-qnn-2
++      - qnn-binary-2
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -11,25 +11,45 @@ concurrency:
+   cancel-in-progress: true
+ 
+ jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-qnn/generate_sense_voice.py
++          MATRIX=$(python3 .github/scripts/export-qnn/generate_sense_voice.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
+   export-sense-voice-to-qnn:
++    needs: generate_build_matrix
+     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+-    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }}
+-    runs-on: ${{ matrix.os }}
++    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }} ${{ matrix.soc }}
++    runs-on: ubuntu-22.04
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [ubuntu-22.04]
+-        python-version: ["3.10"]
+-        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
+-        framework: ["FunASR", "WSYue-ASR"]
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+ 
+     steps:
+       - uses: actions/checkout@v4
+ 
+-      - name: Setup Python ${{ matrix.python-version }}
++      - name: Setup Python 3.10
+         uses: actions/setup-python@v5
+         with:
+-          python-version: ${{ matrix.python-version }}
++          python-version: "3.10"
+ 
+       - name: Display NDK HOME
+         shell: bash
+@@ -37,6 +57,11 @@ jobs:
+           echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
+           ls -lh ${ANDROID_NDK_LATEST_HOME}
+ 
++      - name: Create directories
++        shell: bash
++        run: |
++          mkdir so binary
++
+       - name: Create Python virtual environment
+         shell: bash
+         run: |
+@@ -54,13 +79,13 @@ jobs:
+       - name: Download toolkit
+         shell: bash
+         run: |
+-          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
+-          ls -lh v2.33.0.250327.zip
++          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.40.0.251030.zip
++          ls -lh v2.40.0.251030.zip
+ 
+       - name: Unzip toolkit
+         shell: bash
+         run: |
+-          unzip v2.33.0.250327.zip
++          unzip v2.40.0.251030.zip
+ 
+       - name: Show
+         shell: bash
+@@ -82,7 +107,7 @@ jobs:
+ 
+           ls -lh qairt
+ 
+-          cd qairt/2.33.0.250327/bin
++          cd qairt/2.40.0.251030/bin
+           source envsetup.sh
+ 
+           yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
+@@ -92,7 +117,7 @@ jobs:
+         run: |
+           source py310/bin/activate
+ 
+-          cd qairt/2.33.0.250327/bin
++          cd qairt/2.40.0.251030/bin
+           source envsetup.sh
+ 
+           python3 -m pip install \
+@@ -147,7 +172,7 @@ jobs:
+         run: |
+           source py310/bin/activate
+ 
+-          pushd qairt/2.33.0.250327/bin
++          pushd qairt/2.40.0.251030/bin
+           source envsetup.sh
+           popd
+ 
+@@ -158,7 +183,7 @@ jobs:
+         run: |
+           source py310/bin/activate
+ 
+-          pushd qairt/2.33.0.250327/bin
++          pushd qairt/2.40.0.251030/bin
+           source envsetup.sh
+           popd
+ 
+@@ -169,7 +194,7 @@ jobs:
+         run: |
+           source py310/bin/activate
+ 
+-          pushd qairt/2.33.0.250327/bin
++          pushd qairt/2.40.0.251030/bin
+           source envsetup.sh
+           popd
+ 
+@@ -181,12 +206,13 @@ jobs:
+         run: |
+           source py310/bin/activate
+ 
+-          pushd qairt/2.33.0.250327/bin
++          pushd qairt/2.40.0.251030/bin
+           source envsetup.sh
+           popd
+ 
+           export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
+           export LDFLAGS="-Wl,-z,max-page-size=16384"
++          dir=$PWD
+ 
+           cd scripts/sense-voice/qnn
+ 
+@@ -269,8 +295,48 @@ jobs:
+ 
+           readelf -lW model_libs/*/lib*.so
+ 
++          echo "Generate context binary"
++
++          $dir/scripts/qnn/generate_config.py  \
++            --soc ${{ matrix.soc }} \
++            --graph-name "model_${t}_seconds_quantized" \
++            --output-dir ./my-config \
++            --qnn-sdk-root $QNN_SDK_ROOT
++
++          ls -lh my-config
++
++          head -n 1000 my-config/*.json
++
++          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
++            --output_dir ./binary \
++            --binary_file model \
++            --config_file ./my-config/htp_backend_extensions.json
++
++          ls -lh binary/
++
+           echo "collect results"
+ 
++          d=sherpa-onnx-qnn-${{ matrix.soc}}-binary-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v README.md $d
++          cp -v LICENSE $d
++          cp -v binary/model.bin $d/
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++
++          echo "num_frames=$num_frames" > $d/info.txt
++
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++          mv *.tar.bz2 ../../../binary/
++
++
+           for p in x86_64-linux-clang aarch64-android; do
+             if [[ $p == x86_64-linux-clang ]]; then
+               d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-linux-x64
+@@ -302,15 +368,17 @@ jobs:
+           echo "----show---"
+           ls -lh *.tar.bz2
+ 
+-          mv *.tar.bz2 ../../..
++          mv *.tar.bz2 ../../../so/
++
+ 
+       - name: Run SenseVoice from WSYue-ASR
+         if: matrix.framework == 'WSYue-ASR'
+         shell: bash
+         run: |
++          dir=$PWD
+           source py310/bin/activate
+ 
+-          pushd qairt/2.33.0.250327/bin
++          pushd qairt/2.40.0.251030/bin
+           source envsetup.sh
+           popd
+ 
+@@ -416,6 +484,44 @@ jobs:
+ 
+           readelf -lW model_libs/*/lib*.so
+ 
++          $dir/scripts/qnn/generate_config.py  \
++            --soc ${{ matrix.soc }} \
++            --graph-name "model_${t}_seconds_quantized" \
++            --output-dir ./my-config \
++            --qnn-sdk-root $QNN_SDK_ROOT
++
++          ls -lh my-config
++
++          head -n 1000 my-config/*.json
++
++          $QNN_SDK_ROOT/bin/x86_64-linux-clang/qnn-context-binary-generator \
++            --backend $QNN_SDK_ROOT/lib/x86_64-linux-clang/libQnnHtp.so \
++            --model ./model_libs/x86_64-linux-clang/libmodel-$t-seconds-quantized.so \
++            --output_dir ./binary \
++            --binary_file model \
++            --config_file ./my-config/htp_backend_extensions.json
++
++          ls -lh binary/
++
++          echo "collect results"
++
++          d=sherpa-onnx-qnn-${{ matrix.soc }}-binary-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v README.md $d
++          cp -v binary/model.bin $d/
++          cp -v tokens.txt $d
++          cp -v *.wav $d/test_wavs
++
++          echo "num_frames=$num_frames" > $d/info.txt
++
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++          mv *.tar.bz2 ../../../binary/
++
+           echo "collect results"
+           for p in x86_64-linux-clang aarch64-android; do
+             if [[ $p == x86_64-linux-clang ]]; then
+@@ -447,22 +553,42 @@ jobs:
+           echo "----show---"
+           ls -lh *.tar.bz2
+ 
+-          mv *.tar.bz2 ../../..
++          mv *.tar.bz2 ../../../so/
+ 
+       - uses: actions/upload-artifact@v4
+         with:
+-          name: ${{ matrix.framework }}-${{ matrix.input_in_seconds }}-seconds
++          name: ${{ matrix.framework }}-${{ matrix.soc }}-${{ matrix.input_in_seconds }}-seconds
+           path: ./scripts/sense-voice/qnn/*.json
+ 
++      - name: Release
++        if: github.repository_owner == 'csukuangfj' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn
++
+       - name: Release
+         if: github.repository_owner == 'csukuangfj'
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+-          file: ./*.tar.bz2
++          file: ./binary/*.tar.bz2
+           overwrite: true
+           repo_name: k2-fsa/sherpa-onnx
+           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn-binary
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa' && matrix.soc == 'SM8850'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./so/*.tar.bz2
++          overwrite: true
+           tag: asr-models-qnn
+ 
+       - name: Release
+@@ -470,6 +596,6 @@ jobs:
+         uses: svenstaro/upload-release-action@v2
+         with:
+           file_glob: true
+-          file: ./*.tar.bz2
++          file: ./binary/*.tar.bz2
+           overwrite: true
+-          tag: asr-models-qnn
++          tag: asr-models-qnn-binary
+diff --git a/.gitignore b/.gitignore
+index 813b50a2..6fdcc69a 100755
+--- a/.gitignore
++++ b/.gitignore
+@@ -163,3 +163,4 @@ sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+ sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+ build-riscv64-linux-gnu-spacemit/
+ spacemit-toolchain*
++sherpa-onnx-qnn-*
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+index 470850c9..8a09d884 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+@@ -132,9 +132,15 @@ object SimulateStreamingAsr {
+                 config.modelConfig.tokens =
+                     copyAssetToInternalStorage(config.modelConfig.tokens, context)
+ 
+-                if (config.modelConfig.senseVoice.model.isNotEmpty()) {
+-                    config.modelConfig.senseVoice.model =
+-                        copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
++                if (config.modelConfig.senseVoice.model.isNotEmpty() || assetExists(
++                        context.assets,
++                        path = config.modelConfig.senseVoice.qnnConfig.contextBinary
++                    )
++                ) {
++                    if (config.modelConfig.senseVoice.model.isNotEmpty()) {
++                        config.modelConfig.senseVoice.model =
++                            copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
++                    }
+ 
+                     config.modelConfig.senseVoice.qnnConfig.contextBinary =
+                         copyAssetToInternalStorage(
+diff --git a/scripts/qnn/__init__.py b/scripts/qnn/__init__.py
+new file mode 100644
+index 00000000..e69de29b
+diff --git a/scripts/qnn/device_info.py b/scripts/qnn/device_info.py
+new file mode 100755
+index 00000000..03135f2c
+--- /dev/null
++++ b/scripts/qnn/device_info.py
+@@ -0,0 +1,112 @@
++#!/usr/bin/env python3
++from dataclasses import dataclass
++from enum import IntEnum, unique
++
++"""
++See also
++https://docs.qualcomm.com/doc/80-63442-10/topic/QNN_general_overview.html#supported-snapdragon-devices
++
++SA8255 soc_id    52 dsp_arch     v73 vtcm_size (MB)      8
++SA8295 soc_id    39 dsp_arch     v68 vtcm_size (MB)      8
++SM8350 soc_id    35 dsp_arch     v68 vtcm_size (MB)      4
++SM8450 soc_id    36 dsp_arch     v69 vtcm_size (MB)      8
++SM8475 soc_id    42 dsp_arch     v69 vtcm_size (MB)      8
++SM8550 soc_id    43 dsp_arch     v73 vtcm_size (MB)      8
++SM8650 soc_id    57 dsp_arch     v75 vtcm_size (MB)      8
++SM8750 soc_id    69 dsp_arch     v79 vtcm_size (MB)      8
++SM8850 soc_id    87 dsp_arch     v81 vtcm_size (MB)      8
++SSG2115P soc_id  46 dsp_arch     v73 vtcm_size (MB)      2
++SSG2125P soc_id  58 dsp_arch     v73 vtcm_size (MB)      2
++SXR1230P soc_id  45 dsp_arch     v73 vtcm_size (MB)      2
++SXR2230P soc_id  53 dsp_arch     v69 vtcm_size (MB)      8
++SXR2330P soc_id  75 dsp_arch     v79 vtcm_size (MB)      8
++QCS9100 soc_id   77 dsp_arch     v73 vtcm_size (MB)      8
++SAR2230P soc_id  95 dsp_arch     v81 vtcm_size (MB)      4
++SW6100 soc_id    96 dsp_arch     v81 vtcm_size (MB)      4
++"""
++
++
++@unique
++class Chipset(IntEnum):
++    # see https://github.com/pytorch/executorch/blob/main/backends/qualcomm/serialization/qc_schema.py#L41
++    # SA8255, soc_id 52,  dsp_arch v73
++    SA8255 = 52  # v73
++    SA8295 = 39  # v68
++    SM8350 = 35  # v68
++    SM8450 = 36  # v69
++    SM8475 = 42  # v69
++    SM8550 = 43  # v73
++    SM8650 = 57  # v75
++    SM8750 = 69  # v79
++    SM8850 = 87  # v81
++    #  SSG2115P = 46  # v73
++    #  SSG2125P = 58  # v73
++    #  SXR1230P = 45  # v73
++    #  SXR2230P = 53  # v69
++    #  SXR2330P = 75  # v79
++    QCS9100 = 77  # v73
++    #  SAR2230P = 95  # v81
++    #  SW6100 = 96  # v81
++
++
++@unique
++class HtpArch(IntEnum):
++    v68 = 68
++    v69 = 69
++    v73 = 73
++    v75 = 75
++    v79 = 79
++    v81 = 81
++    v87 = 87
++
++
++@dataclass
++class HtpInfo:
++    arch: HtpArch
++    vtcm_size_in_mb: int
++
++
++@dataclass
++class SocInfo:
++    model: Chipset
++    info: HtpInfo
++
++
++soc_info_list = [
++    SocInfo(Chipset.SA8255, HtpInfo(HtpArch.v73, 8)),
++    SocInfo(Chipset.SA8295, HtpInfo(HtpArch.v68, 8)),
++    SocInfo(Chipset.SM8350, HtpInfo(HtpArch.v68, 4)),
++    SocInfo(Chipset.SM8450, HtpInfo(HtpArch.v69, 8)),
++    SocInfo(Chipset.SM8475, HtpInfo(HtpArch.v69, 8)),
++    SocInfo(Chipset.SM8550, HtpInfo(HtpArch.v73, 8)),
++    SocInfo(Chipset.SM8650, HtpInfo(HtpArch.v75, 8)),
++    SocInfo(Chipset.SM8750, HtpInfo(HtpArch.v79, 8)),
++    SocInfo(Chipset.SM8850, HtpInfo(HtpArch.v81, 8)),
++    #  SocInfo(Chipset.SSG2115P, HtpInfo(HtpArch.v73, 2)),
++    #  SocInfo(Chipset.SSG2125P, HtpInfo(HtpArch.v73, 2)),
++    #  SocInfo(Chipset.SXR1230P, HtpInfo(HtpArch.v73, 2)),
++    #  SocInfo(Chipset.SXR2230P, HtpInfo(HtpArch.v69, 8)),
++    #  SocInfo(Chipset.SXR2330P, HtpInfo(HtpArch.v79, 8)),
++    SocInfo(Chipset.QCS9100, HtpInfo(HtpArch.v73, 8)),
++    #  SocInfo(Chipset.SAR2230P, HtpInfo(HtpArch.v81, 4)),
++    #  SocInfo(Chipset.SW6100, HtpInfo(HtpArch.v81, 4)),
++]
++
++soc_info_dict = {soc.model.name: soc for soc in soc_info_list}
++
++
++def _test():
++    for soc in soc_info_list:
++        print(
++            soc.model.name,
++            "soc_id\t",
++            soc.model.value,
++            "dsp_arch\t",
++            soc.info.arch.name,
++            "vtcm_size (MB)\t",
++            soc.info.vtcm_size_in_mb,
++        )
++
++
++if __name__ == "__main__":
++    _test()
+diff --git a/scripts/qnn/generate_config.py b/scripts/qnn/generate_config.py
+new file mode 100755
+index 00000000..cb985c85
+--- /dev/null
++++ b/scripts/qnn/generate_config.py
+@@ -0,0 +1,122 @@
++#!/usr/bin/env python3
++
++# see
++# https://github.com/MollySophia/rwkv-qualcomm/blob/2a82c641c90ee130cbd7038ca7449b2fa818de71/utils/htp_devices_config.py
++# https://docs.qualcomm.com/bundle/publicresource/topics/80-64748-1/model_prep_linux.html#QNN-HTP-context-binary
++
++import argparse
++import json
++from pathlib import Path
++
++from device_info import soc_info_dict
++
++
++def get_args():
++    parser = argparse.ArgumentParser()
++    parser.add_argument(
++        "--soc",
++        type=str,
++        required=True,
++        help="SM8850, SA8295, etc",
++    )
++
++    parser.add_argument(
++        "--graph-name",
++        type=str,
++        required=True,
++        help="Graph name",
++    )
++
++    parser.add_argument(
++        "--output-dir",
++        type=str,
++        required=True,
++        help="Output directory to save the generated json files",
++    )
++
++    parser.add_argument(
++        "--qnn-sdk-root",
++        type=str,
++        required=True,
++        help="Path to qnn sdk",
++    )
++
++    return parser.parse_args()
++
++
++def generate_config(
++    soc_name: str,
++    graph_name: str,
++    output_dir: str,
++    qnn_sdk_root: str,
++):
++    if soc_name not in soc_info_dict:
++        raise ValueError(
++            f"Unsupported SOC {soc_name}. Supported: - {sorted(list(soc_info_dict.keys()))}"
++        )
++    soc = soc_info_dict[soc_name]
++
++    output_dir = Path(output_dir).absolute()
++    output_dir.mkdir(parents=True, exist_ok=True)
++
++    htp_backend_extensions_data = {
++        "backend_extensions": {
++            "shared_library_path": f"{qnn_sdk_root}/lib/x86_64-linux-clang/libQnnHtpNetRunExtensions.so",
++            "config_file_path": f"{output_dir}/htp_config.json",
++        }
++    }
++
++    htp_backend_config_data = {
++        "graphs": [
++            {
++                "vtcm_mb": soc.info.vtcm_size_in_mb,
++                "O": 3,
++                "graph_names": [graph_name],
++            }
++        ],
++        "devices": [
++            {
++                "device_id": 0,
++                "soc_id": soc.model.value,
++                "dsp_arch": soc.info.arch.name,
++                "cores": [
++                    {
++                        "core_id": 0,
++                        "perf_profile": "burst",
++                        "rpc_control_latency": 200,
++                    }
++                ],
++            }
++        ],
++    }
++
++    with open(str(output_dir / "htp_backend_extensions.json"), "w") as f:
++        json.dump(htp_backend_extensions_data, f, indent=4)
++
++    with open(str(output_dir / "htp_config.json"), "w") as f:
++        json.dump(htp_backend_config_data, f, indent=4)
++
++
++def _test():
++    qnn_sdk_root = "/home/fangjun/open-source/qairt/2.40.0.251030"
++    generate_config(
++        soc_name="SM8850",
++        graph_name="model_10_seconds_quantized",
++        output_dir="./tmp",
++        qnn_sdk_root=qnn_sdk_root,
++    )
++
++
++if __name__ == "__main__":
++    #  _test()
++
++    args = get_args()
++    print(vars(args))
++    generate_config(
++        soc_name=args.soc,
++        graph_name=args.graph_name,
++        output_dir=args.output_dir,
++        qnn_sdk_root=args.qnn_sdk_root,
++    )
++
++# ./generate_config.py  --soc SM8850 --graph-name abc --output-dir ./tmp2 --qnn-sdk-root $QNN_SDK_ROOT
+diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
+index 7d536750..49189f9f 100644
+--- a/sherpa-onnx/csrc/offline-model-config.cc
++++ b/sherpa-onnx/csrc/offline-model-config.cc
+@@ -135,7 +135,8 @@ bool OfflineModelConfig::Validate() const {
+     return wenet_ctc.Validate();
+   }
+ 
+-  if (!sense_voice.model.empty()) {
++  if (!sense_voice.model.empty() ||
++      !sense_voice.qnn_config.context_binary.empty()) {
+     return sense_voice.Validate();
+   }
+ 
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index a473a717..a58b34ef 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -171,7 +171,8 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ 
+   if (config.model_config.provider == "qnn") {
+ #if SHERPA_ONNX_ENABLE_QNN
+-    if (!config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty() ||
++        !config.model_config.sense_voice.qnn_config.context_binary.empty()) {
+       return std::make_unique<
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+           config);
+@@ -491,7 +492,8 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ 
+   if (config.model_config.provider == "qnn") {
+ #if SHERPA_ONNX_ENABLE_QNN
+-    if (!config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty() ||
++        !config.model_config.sense_voice.qnn_config.context_binary.empty()) {
+       return std::make_unique<
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
+           mgr, config);
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+index ddadbfb9..ae5482fc 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+@@ -29,9 +29,16 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
+ }
+ 
+ bool OfflineSenseVoiceModelConfig::Validate() const {
+-  if (!FileExists(model)) {
+-    SHERPA_ONNX_LOGE("SenseVoice model '%s' does not exist", model.c_str());
+-    return false;
++  if (qnn_config.context_binary.empty()) {
++    if (model.empty()) {
++      SHERPA_ONNX_LOGE("Please provide a senseVoice model");
++      return false;
++    }
++
++    if (!FileExists(model)) {
++      SHERPA_ONNX_LOGE("SenseVoice model '%s' does not exist", model.c_str());
++      return false;
++    }
+   }
+ 
+   if (!language.empty()) {
+@@ -46,7 +53,18 @@ bool OfflineSenseVoiceModelConfig::Validate() const {
+     }
+   }
+ 
+-  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
++  if (model.empty() && !qnn_config.context_binary.empty()) {
++    // we require that the context_binary exists
++    if (!FileExists(qnn_config.context_binary)) {
++      SHERPA_ONNX_LOGE(
++          "Model is empty, but you provide a context binary that does not "
++          "exist");
++      return false;
++    }
++  }
++
++  if (EndsWith(model, ".so") || EndsWith(model, ".bin") ||
++      (model.empty() && !qnn_config.context_binary.empty())) {
+     return qnn_config.Validate();
+   }
+ 
+diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+index b1b4bd8e..f27dfbc5 100644
+--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
++++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+@@ -1113,6 +1113,24 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                 tokens = "$modelDir/tokens.txt",
+             )
+         }
++
++        9022 -> {
++            // for my Xiaomi 17 Pro
++            val modelDir = "sherpa-onnx-qnn-SM8850-binary-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    qnnConfig = QnnConfig(
++                        // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
+     }
+     return null
+ }
+
+commit b229e031c54f3c25f3677003fe01c8a83cf77c16
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Mon Dec 8 14:48:42 2025 +0800
+
+    Remove unused lock file (#2875)
+
+diff --git a/dart-api-examples/streaming-asr/pubspec.lock b/dart-api-examples/streaming-asr/pubspec.lock
+deleted file mode 100644
+index 349b3b46..00000000
+--- a/dart-api-examples/streaming-asr/pubspec.lock
++++ /dev/null
+@@ -1,432 +0,0 @@
+-# Generated by pub
+-# See https://dart.dev/tools/pub/glossary#lockfile
+-packages:
+-  _fe_analyzer_shared:
+-    dependency: transitive
+-    description:
+-      name: _fe_analyzer_shared
+-      sha256: "0b2f2bd91ba804e53a61d757b986f89f1f9eaed5b11e4b2f5a2468d86d6c9fc7"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "67.0.0"
+-  analyzer:
+-    dependency: transitive
+-    description:
+-      name: analyzer
+-      sha256: "37577842a27e4338429a1cbc32679d508836510b056f1eedf0c8d20e39c1383d"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "6.4.1"
+-  args:
+-    dependency: "direct main"
+-    description:
+-      name: args
+-      sha256: "7cf60b9f0cc88203c5a190b4cd62a99feea42759a7fa695010eb5de1c0b2252a"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.5.0"
+-  async:
+-    dependency: transitive
+-    description:
+-      name: async
+-      sha256: "947bfcf187f74dbc5e146c9eb9c0f10c9f8b30743e341481c1e2ed3ecc18c20c"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.11.0"
+-  boolean_selector:
+-    dependency: transitive
+-    description:
+-      name: boolean_selector
+-      sha256: "6cfb5af12253eaf2b368f07bacc5a80d1301a071c73360d746b7f2e32d762c66"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.1"
+-  characters:
+-    dependency: transitive
+-    description:
+-      name: characters
+-      sha256: "04a925763edad70e8443c99234dc3328f442e811f1d8fd1a72f1c8ad0f69a605"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.3.0"
+-  collection:
+-    dependency: transitive
+-    description:
+-      name: collection
+-      sha256: ee67cb0715911d28db6bf4af1026078bd6f0128b07a5f66fb2ed94ec6783c09a
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.18.0"
+-  convert:
+-    dependency: transitive
+-    description:
+-      name: convert
+-      sha256: "0f08b14755d163f6e2134cb58222dd25ea2a2ee8a195e53983d57c075324d592"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.1.1"
+-  coverage:
+-    dependency: transitive
+-    description:
+-      name: coverage
+-      sha256: "3945034e86ea203af7a056d98e98e42a5518fff200d6e8e6647e1886b07e936e"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.8.0"
+-  crypto:
+-    dependency: transitive
+-    description:
+-      name: crypto
+-      sha256: ff625774173754681d66daaf4a448684fb04b78f902da9cb3d308c19cc5e8bab
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.0.3"
+-  ffi:
+-    dependency: transitive
+-    description:
+-      name: ffi
+-      sha256: "493f37e7df1804778ff3a53bd691d8692ddf69702cf4c1c1096a2e41b4779e21"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.2"
+-  file:
+-    dependency: transitive
+-    description:
+-      name: file
+-      sha256: "5fc22d7c25582e38ad9a8515372cd9a93834027aacf1801cf01164dac0ffa08c"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "7.0.0"
+-  flutter:
+-    dependency: transitive
+-    description: flutter
+-    source: sdk
+-    version: "0.0.0"
+-  frontend_server_client:
+-    dependency: transitive
+-    description:
+-      name: frontend_server_client
+-      sha256: f64a0333a82f30b0cca061bc3d143813a486dc086b574bfb233b7c1372427694
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "4.0.0"
+-  glob:
+-    dependency: transitive
+-    description:
+-      name: glob
+-      sha256: "0e7014b3b7d4dac1ca4d6114f82bf1782ee86745b9b42a92c9289c23d8a0ab63"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.2"
+-  http_multi_server:
+-    dependency: transitive
+-    description:
+-      name: http_multi_server
+-      sha256: "97486f20f9c2f7be8f514851703d0119c3596d14ea63227af6f7a481ef2b2f8b"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.2.1"
+-  http_parser:
+-    dependency: transitive
+-    description:
+-      name: http_parser
+-      sha256: "2aa08ce0341cc9b354a498388e30986515406668dbcc4f7c950c3e715496693b"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "4.0.2"
+-  io:
+-    dependency: transitive
+-    description:
+-      name: io
+-      sha256: "2ec25704aba361659e10e3e5f5d672068d332fc8ac516421d483a11e5cbd061e"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.0.4"
+-  js:
+-    dependency: transitive
+-    description:
+-      name: js
+-      sha256: c1b2e9b5ea78c45e1a0788d29606ba27dc5f71f019f32ca5140f61ef071838cf
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.7.1"
+-  lints:
+-    dependency: "direct dev"
+-    description:
+-      name: lints
+-      sha256: cbf8d4b858bb0134ef3ef87841abdf8d63bfc255c266b7bf6b39daa1085c4290
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.0.0"
+-  logging:
+-    dependency: transitive
+-    description:
+-      name: logging
+-      sha256: "623a88c9594aa774443aa3eb2d41807a48486b5613e67599fb4c41c0ad47c340"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.2.0"
+-  matcher:
+-    dependency: transitive
+-    description:
+-      name: matcher
+-      sha256: d2323aa2060500f906aa31a895b4030b6da3ebdcc5619d14ce1aada65cd161cb
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.12.16+1"
+-  material_color_utilities:
+-    dependency: transitive
+-    description:
+-      name: material_color_utilities
+-      sha256: "0e0a020085b65b6083975e499759762399b4475f766c21668c4ecca34ea74e5a"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.8.0"
+-  meta:
+-    dependency: transitive
+-    description:
+-      name: meta
+-      sha256: "7687075e408b093f36e6bbf6c91878cc0d4cd10f409506f7bc996f68220b9136"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.12.0"
+-  mime:
+-    dependency: transitive
+-    description:
+-      name: mime
+-      sha256: "2e123074287cc9fd6c09de8336dae606d1ddb88d9ac47358826db698c176a1f2"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.0.5"
+-  node_preamble:
+-    dependency: transitive
+-    description:
+-      name: node_preamble
+-      sha256: "6e7eac89047ab8a8d26cf16127b5ed26de65209847630400f9aefd7cd5c730db"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.0.2"
+-  package_config:
+-    dependency: transitive
+-    description:
+-      name: package_config
+-      sha256: "1c5b77ccc91e4823a5af61ee74e6b972db1ef98c2ff5a18d3161c982a55448bd"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.0"
+-  path:
+-    dependency: "direct main"
+-    description:
+-      name: path
+-      sha256: "087ce49c3f0dc39180befefc60fdb4acd8f8620e5682fe2476afd0b3688bb4af"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.9.0"
+-  pool:
+-    dependency: transitive
+-    description:
+-      name: pool
+-      sha256: "20fe868b6314b322ea036ba325e6fc0711a22948856475e2c2b6306e8ab39c2a"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.5.1"
+-  pub_semver:
+-    dependency: transitive
+-    description:
+-      name: pub_semver
+-      sha256: "40d3ab1bbd474c4c2328c91e3a7df8c6dd629b79ece4c4bd04bee496a224fb0c"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.4"
+-  shelf:
+-    dependency: transitive
+-    description:
+-      name: shelf
+-      sha256: ad29c505aee705f41a4d8963641f91ac4cee3c8fad5947e033390a7bd8180fa4
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.4.1"
+-  shelf_packages_handler:
+-    dependency: transitive
+-    description:
+-      name: shelf_packages_handler
+-      sha256: "89f967eca29607c933ba9571d838be31d67f53f6e4ee15147d5dc2934fee1b1e"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.0.2"
+-  shelf_static:
+-    dependency: transitive
+-    description:
+-      name: shelf_static
+-      sha256: a41d3f53c4adf0f57480578c1d61d90342cd617de7fc8077b1304643c2d85c1e
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.1.2"
+-  shelf_web_socket:
+-    dependency: transitive
+-    description:
+-      name: shelf_web_socket
+-      sha256: "9ca081be41c60190ebcb4766b2486a7d50261db7bd0f5d9615f2d653637a84c1"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.0.4"
+-  sherpa_onnx:
+-    dependency: "direct main"
+-    description:
+-      name: sherpa_onnx
+-      sha256: e45894f81e7c854ca96d678bcab5303036e884a7c90e9a6c4ec04c7b1ee215a8
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.9.29"
+-  sky_engine:
+-    dependency: transitive
+-    description: flutter
+-    source: sdk
+-    version: "0.0.99"
+-  source_map_stack_trace:
+-    dependency: transitive
+-    description:
+-      name: source_map_stack_trace
+-      sha256: "84cf769ad83aa6bb61e0aa5a18e53aea683395f196a6f39c4c881fb90ed4f7ae"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.1"
+-  source_maps:
+-    dependency: transitive
+-    description:
+-      name: source_maps
+-      sha256: "708b3f6b97248e5781f493b765c3337db11c5d2c81c3094f10904bfa8004c703"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.10.12"
+-  source_span:
+-    dependency: transitive
+-    description:
+-      name: source_span
+-      sha256: "53e943d4206a5e30df338fd4c6e7a077e02254531b138a15aec3bd143c1a8b3c"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.10.0"
+-  stack_trace:
+-    dependency: transitive
+-    description:
+-      name: stack_trace
+-      sha256: "73713990125a6d93122541237550ee3352a2d84baad52d375a4cad2eb9b7ce0b"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.11.1"
+-  stream_channel:
+-    dependency: transitive
+-    description:
+-      name: stream_channel
+-      sha256: ba2aa5d8cc609d96bbb2899c28934f9e1af5cddbd60a827822ea467161eb54e7
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.2"
+-  string_scanner:
+-    dependency: transitive
+-    description:
+-      name: string_scanner
+-      sha256: "556692adab6cfa87322a115640c11f13cb77b3f076ddcc5d6ae3c20242bedcde"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.2.0"
+-  term_glyph:
+-    dependency: transitive
+-    description:
+-      name: term_glyph
+-      sha256: a29248a84fbb7c79282b40b8c72a1209db169a2e0542bce341da992fe1bc7e84
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.2.1"
+-  test:
+-    dependency: "direct dev"
+-    description:
+-      name: test
+-      sha256: "7ee446762c2c50b3bd4ea96fe13ffac69919352bd3b4b17bac3f3465edc58073"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.25.2"
+-  test_api:
+-    dependency: transitive
+-    description:
+-      name: test_api
+-      sha256: "9955ae474176f7ac8ee4e989dadfb411a58c30415bcfb648fa04b2b8a03afa7f"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.7.0"
+-  test_core:
+-    dependency: transitive
+-    description:
+-      name: test_core
+-      sha256: "2bc4b4ecddd75309300d8096f781c0e3280ca1ef85beda558d33fcbedc2eead4"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.6.0"
+-  typed_data:
+-    dependency: transitive
+-    description:
+-      name: typed_data
+-      sha256: facc8d6582f16042dd49f2463ff1bd6e2c9ef9f3d5da3d9b087e244a7b564b3c
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.3.2"
+-  vector_math:
+-    dependency: transitive
+-    description:
+-      name: vector_math
+-      sha256: "80b3257d1492ce4d091729e3a67a60407d227c27241d6927be0130c98e741803"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.1.4"
+-  vm_service:
+-    dependency: transitive
+-    description:
+-      name: vm_service
+-      sha256: f652077d0bdf60abe4c1f6377448e8655008eef28f128bc023f7b5e8dfeb48fc
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "14.2.4"
+-  watcher:
+-    dependency: transitive
+-    description:
+-      name: watcher
+-      sha256: "3d2ad6751b3c16cf07c7fca317a1413b3f26530319181b37e3b9039b84fc01d8"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.1.0"
+-  web:
+-    dependency: transitive
+-    description:
+-      name: web
+-      sha256: "97da13628db363c635202ad97068d47c5b8aa555808e7a9411963c533b449b27"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "0.5.1"
+-  web_socket_channel:
+-    dependency: transitive
+-    description:
+-      name: web_socket_channel
+-      sha256: "58c6666b342a38816b2e7e50ed0f1e261959630becd4c879c4f26bfa14aa5a42"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "2.4.5"
+-  webkit_inspection_protocol:
+-    dependency: transitive
+-    description:
+-      name: webkit_inspection_protocol
+-      sha256: "87d3f2333bb240704cd3f1c6b5b7acd8a10e7f0bc28c28dcf14e782014f4a572"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "1.2.1"
+-  yaml:
+-    dependency: transitive
+-    description:
+-      name: yaml
+-      sha256: "75769501ea3489fca56601ff33454fe45507ea3bfb014161abc3b43ae25989d5"
+-      url: "https://pub.dev"
+-    source: hosted
+-    version: "3.1.2"
+-sdks:
+-  dart: ">=3.4.0 <4.0.0"
+-  flutter: ">=3.3.0"
+
+commit 132c91d7dcf0763eb0c3a518ad968b784682c568
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Mon Dec 8 13:32:18 2025 +0800
+
+    Refactor Paraformer Impl (#2874)
+    
+    This pull request refactors the Paraformer offline speech recognizer by introducing a generic templated implementation. This change consolidates the code for different hardware backends like RKNN and Ascend, promoting code reuse and simplifying future maintenance and extension of Paraformer support across various platforms. The previous platform-specific implementation files have been either removed or converted into this new templated structure.
+
+diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
+deleted file mode 100644
+index 2e1bf68a..00000000
+--- a/sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
++++ /dev/null
+@@ -1,121 +0,0 @@
+-// sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
+-#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
+-
+-#include <memory>
+-#include <utility>
+-#include <vector>
+-
+-#include "sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-model-config.h"
+-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+-#include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/symbol-table.h"
+-
+-namespace sherpa_onnx {
+-
+-// defined in ../offline-recognizer-paraformer-impl.h
+-OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
+-                                 const SymbolTable &sym_table);
+-
+-class OfflineRecognizerParaformerAscendImpl : public OfflineRecognizerImpl {
+- public:
+-  explicit OfflineRecognizerParaformerAscendImpl(
+-      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(config),
+-        config_(config),
+-        symbol_table_(config_.model_config.tokens),
+-        model_(std::make_unique<OfflineParaformerModelAscend>(
+-            config.model_config)) {
+-    if (config.decoding_method != "greedy_search") {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  template <typename Manager>
+-  OfflineRecognizerParaformerAscendImpl(Manager *mgr,
+-                                        const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(mgr, config),
+-        config_(config),
+-        symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineParaformerModelAscend>(
+-            mgr, config.model_config)) {
+-    if (config.decoding_method != "greedy_search") {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
+-  }
+-
+-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    for (int32_t i = 0; i < n; ++i) {
+-      DecodeOneStream(ss[i]);
+-    }
+-  }
+-
+-  OfflineRecognizerConfig GetConfig() const override { return config_; }
+-
+- private:
+-  void InitFeatConfig() {
+-    config_.feat_config.normalize_samples = false;
+-    config_.feat_config.window_type = "hamming";
+-    config_.feat_config.high_freq = 0;
+-    config_.feat_config.snip_edges = true;
+-  }
+-
+-  void DecodeOneStream(OfflineStream *s) const {
+-    std::vector<float> f = s->GetFrames();
+-
+-    std::vector<float> logits = model_->Run(std::move(f));
+-    if (logits.empty()) {
+-      SHERPA_ONNX_LOGE("No speech detected");
+-      return;
+-    }
+-
+-    int32_t vocab_size = model_->VocabSize();
+-    int32_t num_tokens = logits.size() / vocab_size;
+-
+-    int32_t eos_id = symbol_table_["</s>"];
+-
+-    OfflineParaformerDecoderResult r;
+-    const float *p = logits.data();
+-    for (int32_t i = 0; i < num_tokens; ++i) {
+-      auto max_idx = static_cast<int64_t>(
+-          std::distance(p, std::max_element(p, p + vocab_size)));
+-
+-      if (max_idx == eos_id) {
+-        break;
+-      }
+-      r.tokens.push_back(max_idx);
+-      p += vocab_size;
+-    }
+-
+-    auto result = Convert(r, symbol_table_);
+-    result.text = ApplyInverseTextNormalization(std::move(result.text));
+-    result.text = ApplyHomophoneReplacer(std::move(result.text));
+-    s->SetResult(result);
+-  }
+-
+- private:
+-  OfflineRecognizerConfig config_;
+-  SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineParaformerModelAscend> model_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_PARAFORMER_ASCEND_IMPL_H_
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index bde04386..a473a717 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -30,6 +30,7 @@
+ #include "sherpa-onnx/csrc/offline-recognizer-fire-red-asr-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-moonshine-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-transducer-impl.h"
+@@ -38,7 +39,7 @@
+ #include "sherpa-onnx/csrc/text-utils.h"
+ 
+ #if SHERPA_ONNX_ENABLE_RKNN
+-#include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
++#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
+ #include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
+ #endif
+ 
+@@ -51,7 +52,7 @@
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+-#include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
++#include "sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.h"
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
+ #include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
+ #endif
+@@ -72,7 +73,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
+           config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+-      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelRknn>>(
++          config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice and Paraformer models are currently supported "
+@@ -144,7 +147,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
+           config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+-      return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelAscend>>(
++          config);
+     } else if (!config.model_config.zipformer_ctc.model.empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(config);
+     } else {
+@@ -387,7 +392,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
+           mgr, config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+-      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelRknn>>(
++          mgr, config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice and Paraformer models are currently supported "
+@@ -459,8 +466,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+           OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
+           mgr, config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+-      return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
+-                                                                     config);
++      return std::make_unique<
++          OfflineRecognizerParaformerTplImpl<OfflineParaformerModelAscend>>(
++          mgr, config);
+     } else if (!config.model_config.zipformer_ctc.model.empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(mgr,
+                                                                        config);
+diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h b/sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
+similarity index 77%
+rename from sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+rename to sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
+index 28fc17a6..a6b264cb 100644
+--- a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
+@@ -1,9 +1,9 @@
+-// sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
++// sherpa-onnx/csrc/offline-recognizer-paraformer-tpl-impl.h
+ //
+ // Copyright (c)  2025  Xiaomi Corporation
+ 
+-#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+-#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_
+ 
+ #include <memory>
+ #include <utility>
+@@ -13,7 +13,6 @@
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
+ #include "sherpa-onnx/csrc/symbol-table.h"
+ 
+ namespace sherpa_onnx {
+@@ -22,15 +21,15 @@ namespace sherpa_onnx {
+ OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
+                                  const SymbolTable &sym_table);
+ 
+-class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
++template <typename ParaformerModel>
++class OfflineRecognizerParaformerTplImpl : public OfflineRecognizerImpl {
+  public:
+-  explicit OfflineRecognizerParaformerRknnImpl(
++  explicit OfflineRecognizerParaformerTplImpl(
+       const OfflineRecognizerConfig &config)
+       : OfflineRecognizerImpl(config),
+         config_(config),
+         symbol_table_(config_.model_config.tokens),
+-        model_(
+-            std::make_unique<OfflineParaformerModelRknn>(config.model_config)) {
++        model_(std::make_unique<ParaformerModel>(config.model_config)) {
+     if (config.decoding_method != "greedy_search") {
+       SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                        config.decoding_method.c_str());
+@@ -41,13 +40,12 @@ class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
+   }
+ 
+   template <typename Manager>
+-  OfflineRecognizerParaformerRknnImpl(Manager *mgr,
+-                                      const OfflineRecognizerConfig &config)
++  OfflineRecognizerParaformerTplImpl(Manager *mgr,
++                                     const OfflineRecognizerConfig &config)
+       : OfflineRecognizerImpl(mgr, config),
+         config_(config),
+         symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineParaformerModelRknn>(
+-            mgr, config.model_config)) {
++        model_(std::make_unique<ParaformerModel>(mgr, config.model_config)) {
+     if (config.decoding_method != "greedy_search") {
+       SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+                        config.decoding_method.c_str());
+@@ -113,9 +111,9 @@ class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
+  private:
+   OfflineRecognizerConfig config_;
+   SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineParaformerModelRknn> model_;
++  std::unique_ptr<ParaformerModel> model_;
+ };
+ 
+ }  // namespace sherpa_onnx
+ 
+-#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_PARAFORMER_TPL_IMPL_H_
+
+commit f201a29fa53354b52d5bfc56f70ae697a607640c
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Mon Dec 8 12:54:12 2025 +0800
+
+    Refactor sense voice impl (#2873)
+    
+    This PR refactors the SenseVoice offline recognizer implementations to eliminate code duplication across multiple hardware backends. Previously, each hardware provider (RKNN, Axera, AXCL, Ascend, QNN) had its own nearly identical implementation file. The refactoring consolidates all these implementations into a single template class OfflineRecognizerSenseVoiceTplImpl parameterized by the model type.
+
+diff --git a/build-axcl-linux-aarch64.sh b/build-axcl-linux-aarch64.sh
+index 7b767cae..e3e7e563 100755
+--- a/build-axcl-linux-aarch64.sh
++++ b/build-axcl-linux-aarch64.sh
+@@ -21,7 +21,11 @@ set -ex
+ 
+ # Before you run this file, make sure you have first cloned
+ # https://github.com/Abandon-ht/axcl_bsp_sdk
+-# and set the environment variable SHERPA_ONNX_AXERA_PATH
++# and set the environment variable SHERPA_ONNX_AXCL_SDK_ROOT
++
++if [ -d ./axcl_bsp_sdk ]; then
++  AXCL_SDK_ROOT=/star-fj/fangjun/open-source/sherpa-onnx/axcl_bsp_sdk/out
++fi
+ 
+ if [ -z "$AXCL_SDK_ROOT" ]; then
+   AXCL_SDK_ROOT=/home/m5stack/Workspace/kaldi/sherpa-onnx/axcl_bsp_sdk/out
+@@ -114,7 +118,7 @@ cmake \
+   -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
+   ..
+ 
+-make VERBOSE=1 -j22
++make VERBOSE=1 -j2
+ make install/strip
+ 
+ # Enable it if only needed
+diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+deleted file mode 100644
+index 53da0300..00000000
+--- a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
++++ /dev/null
+@@ -1,142 +0,0 @@
+-// sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
+-#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
+-
+-#include <memory>
+-#include <utility>
+-#include <vector>
+-
+-#include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-model-config.h"
+-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+-#include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+-#include "sherpa-onnx/csrc/symbol-table.h"
+-
+-namespace sherpa_onnx {
+-
+-// defined in ../offline-recognizer-sense-voice-impl.h
+-OfflineRecognitionResult ConvertSenseVoiceResult(
+-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor);
+-
+-class OfflineRecognizerSenseVoiceAscendImpl : public OfflineRecognizerImpl {
+- public:
+-  explicit OfflineRecognizerSenseVoiceAscendImpl(
+-      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(config),
+-        config_(config),
+-        symbol_table_(config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelAscend>(
+-            config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  template <typename Manager>
+-  OfflineRecognizerSenseVoiceAscendImpl(Manager *mgr,
+-                                        const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(mgr, config),
+-        config_(config),
+-        symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelAscend>(
+-            mgr, config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
+-  }
+-
+-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    for (int32_t i = 0; i < n; ++i) {
+-      DecodeOneStream(ss[i]);
+-    }
+-  }
+-
+-  OfflineRecognizerConfig GetConfig() const override { return config_; }
+-
+- private:
+-  void InitFeatConfig() {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+-    config_.feat_config.window_type = "hamming";
+-    config_.feat_config.high_freq = 0;
+-    config_.feat_config.snip_edges = true;
+-  }
+-
+-  void DecodeOneStream(OfflineStream *s) const {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    std::vector<float> f = s->GetFrames();
+-
+-    int32_t language = 0;
+-    if (config_.model_config.sense_voice.language.empty()) {
+-      language = 0;
+-    } else if (meta_data.lang2id.count(
+-                   config_.model_config.sense_voice.language)) {
+-      language =
+-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+-    } else {
+-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+-                       config_.model_config.sense_voice.language.c_str());
+-    }
+-
+-    int32_t text_norm = config_.model_config.sense_voice.use_itn
+-                            ? meta_data.with_itn_id
+-                            : meta_data.without_itn_id;
+-
+-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+-    if (logits.empty()) {
+-      return;
+-    }
+-
+-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+-
+-    auto result =
+-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+-
+-    int32_t frame_shift_ms = 10;
+-    int32_t subsampling_factor = meta_data.window_shift;
+-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+-                                     subsampling_factor);
+-
+-    r.text = ApplyInverseTextNormalization(std::move(r.text));
+-    r.text = ApplyHomophoneReplacer(std::move(r.text));
+-    s->SetResult(r);
+-  }
+-
+- private:
+-  OfflineRecognizerConfig config_;
+-  SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineSenseVoiceModelAscend> model_;
+-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_SENSE_VOICE_ASCEND_IMPL_H_
+diff --git a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+deleted file mode 100644
+index 59062ac4..00000000
+--- a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
++++ /dev/null
+@@ -1,138 +0,0 @@
+-// sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+-#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+-
+-#include <memory>
+-#include <utility>
+-#include <vector>
+-
+-#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-model-config.h"
+-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+-#include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+-#include "sherpa-onnx/csrc/symbol-table.h"
+-
+-namespace sherpa_onnx {
+-
+-// defined in ../online-recognizer-sense-voice-impl.h
+-OfflineRecognitionResult ConvertSenseVoiceResult(
+-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor);
+-
+-class OfflineRecognizerSenseVoiceAxclImpl : public OfflineRecognizerImpl {
+- public:
+-  explicit OfflineRecognizerSenseVoiceAxclImpl(
+-      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(config),
+-        config_(config),
+-        symbol_table_(config_.model_config.tokens),
+-        model_(
+-            std::make_unique<OfflineSenseVoiceModelAxcl>(config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  template <typename Manager>
+-  OfflineRecognizerSenseVoiceAxclImpl(Manager *mgr,
+-                                      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(mgr, config),
+-        config_(config),
+-        symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelAxcl>(
+-            mgr, config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
+-  }
+-
+-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    for (int32_t i = 0; i < n; ++i) {
+-      DecodeOneStream(ss[i]);
+-    }
+-  }
+-
+-  OfflineRecognizerConfig GetConfig() const override { return config_; }
+-
+- private:
+-  void InitFeatConfig() {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+-    config_.feat_config.window_type = "hamming";
+-    config_.feat_config.high_freq = 0;
+-    config_.feat_config.snip_edges = true;
+-  }
+-
+-  void DecodeOneStream(OfflineStream *s) const {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    std::vector<float> f = s->GetFrames();
+-
+-    int32_t language = 0;
+-    if (config_.model_config.sense_voice.language.empty()) {
+-      language = 0;
+-    } else if (meta_data.lang2id.count(
+-                   config_.model_config.sense_voice.language)) {
+-      language =
+-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+-    } else {
+-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+-                       config_.model_config.sense_voice.language.c_str());
+-    }
+-
+-    int32_t text_norm = config_.model_config.sense_voice.use_itn
+-                            ? meta_data.with_itn_id
+-                            : meta_data.without_itn_id;
+-
+-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+-
+-    auto result =
+-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+-
+-    int32_t frame_shift_ms = 10;
+-    int32_t subsampling_factor = meta_data.window_shift;
+-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+-                                     subsampling_factor);
+-
+-    r.text = ApplyInverseTextNormalization(std::move(r.text));
+-    r.text = ApplyHomophoneReplacer(std::move(r.text));
+-    s->SetResult(r);
+-  }
+-
+- private:
+-  OfflineRecognizerConfig config_;
+-  SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineSenseVoiceModelAxcl> model_;
+-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+deleted file mode 100644
+index 37d9ad2c..00000000
+--- a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
++++ /dev/null
+@@ -1,138 +0,0 @@
+-// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-axera-impl.h
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+-#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+-
+-#include <memory>
+-#include <utility>
+-#include <vector>
+-
+-#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-model-config.h"
+-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+-#include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+-#include "sherpa-onnx/csrc/symbol-table.h"
+-
+-namespace sherpa_onnx {
+-
+-// defined in ../offline-recognizer-sense-voice-impl.h
+-OfflineRecognitionResult ConvertSenseVoiceResult(
+-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor);
+-
+-class OfflineRecognizerSenseVoiceAxeraImpl : public OfflineRecognizerImpl {
+- public:
+-  explicit OfflineRecognizerSenseVoiceAxeraImpl(
+-      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(config),
+-        config_(config),
+-        symbol_table_(config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
+-            config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  template <typename Manager>
+-  OfflineRecognizerSenseVoiceAxeraImpl(Manager *mgr,
+-                                       const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(mgr, config),
+-        config_(config),
+-        symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
+-            mgr, config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
+-  }
+-
+-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    for (int32_t i = 0; i < n; ++i) {
+-      DecodeOneStream(ss[i]);
+-    }
+-  }
+-
+-  OfflineRecognizerConfig GetConfig() const override { return config_; }
+-
+- private:
+-  void InitFeatConfig() {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+-    config_.feat_config.window_type = "hamming";
+-    config_.feat_config.high_freq = 0;
+-    config_.feat_config.snip_edges = true;
+-  }
+-
+-  void DecodeOneStream(OfflineStream *s) const {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    std::vector<float> f = s->GetFrames();
+-
+-    int32_t language = 0;
+-    if (config_.model_config.sense_voice.language.empty()) {
+-      language = 0;
+-    } else if (meta_data.lang2id.count(
+-                   config_.model_config.sense_voice.language)) {
+-      language =
+-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+-    } else {
+-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+-                       config_.model_config.sense_voice.language.c_str());
+-    }
+-
+-    int32_t text_norm = config_.model_config.sense_voice.use_itn
+-                            ? meta_data.with_itn_id
+-                            : meta_data.without_itn_id;
+-
+-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+-
+-    auto result =
+-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+-
+-    int32_t frame_shift_ms = 10;
+-    int32_t subsampling_factor = meta_data.window_shift;
+-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+-                                     subsampling_factor);
+-
+-    r.text = ApplyInverseTextNormalization(std::move(r.text));
+-    r.text = ApplyHomophoneReplacer(std::move(r.text));
+-    s->SetResult(r);
+-  }
+-
+- private:
+-  OfflineRecognizerConfig config_;
+-  SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineSenseVoiceModelAxera> model_;
+-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 683fbddc..bde04386 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -31,6 +31,7 @@
+ #include "sherpa-onnx/csrc/offline-recognizer-moonshine-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-paraformer-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-sense-voice-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-transducer-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-transducer-nemo-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-whisper-impl.h"
+@@ -38,26 +39,26 @@
+ 
+ #if SHERPA_ONNX_ENABLE_RKNN
+ #include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
+-#include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
++#include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_AXERA
+-#include "sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h"
++#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_AXCL
+-#include "sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h"
++#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
+-#include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
++#include "sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.h"
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_QNN
+-#include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
+ #include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
++#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
+ #endif
+ 
+ namespace sherpa_onnx {
+@@ -67,7 +68,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "rknn") {
+ #if SHERPA_ONNX_ENABLE_RKNN
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
++          config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
+     } else {
+@@ -90,7 +93,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "axera") {
+ #if SHERPA_ONNX_ENABLE_AXERA
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxera>>(
++          config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice models are currently supported by Axera NPU for "
+@@ -111,7 +116,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "axcl") {
+ #if SHERPA_ONNX_ENABLE_AXCL
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxcl>>(
++          config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice models are currently supported by axcl for "
+@@ -133,7 +140,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "ascend") {
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
++          config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
+     } else if (!config.model_config.zipformer_ctc.model.empty()) {
+@@ -158,7 +167,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "qnn") {
+ #if SHERPA_ONNX_ENABLE_QNN
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
++          config);
+     } else if (!config.model_config.zipformer_ctc.model.empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
+     } else {
+@@ -372,7 +383,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "rknn") {
+ #if SHERPA_ONNX_ENABLE_RKNN
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(mgr, config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelRknn>>(
++          mgr, config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
+     } else {
+@@ -395,8 +408,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "axera") {
+ #if SHERPA_ONNX_ENABLE_AXERA
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(mgr,
+-                                                                    config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxera>>(
++          mgr, config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice models are currently supported by Axera NPU for "
+@@ -417,7 +431,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "axcl") {
+ #if SHERPA_ONNX_ENABLE_AXCL
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(mgr, config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAxcl>>(
++          mgr, config);
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice models are currently supported by axcl for "
+@@ -439,8 +455,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "ascend") {
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(mgr,
+-                                                                     config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelAscend>>(
++          mgr, config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
+                                                                      config);
+@@ -467,7 +484,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+   if (config.model_config.provider == "qnn") {
+ #if SHERPA_ONNX_ENABLE_QNN
+     if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
++      return std::make_unique<
++          OfflineRecognizerSenseVoiceTplImpl<OfflineSenseVoiceModelQnn>>(
++          mgr, config);
+     } else if (!config.model_config.zipformer_ctc.model.empty()) {
+       return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
+                                                                     config);
+@@ -770,8 +789,8 @@ OfflineRecognizerImpl::OfflineRecognizerImpl(
+         itn_list_.push_back(
+             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
+       }  // for (; !reader->Done(); reader->Next())
+-    }    // for (const auto &f : files)
+-  }      // if (!config.rule_fars.empty())
++    }  // for (const auto &f : files)
++  }  // if (!config.rule_fars.empty())
+ 
+   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
+     auto hr_config = config.hr;
+diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+similarity index 84%
+rename from sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+rename to sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+index 7c793ed9..bdaeb855 100644
+--- a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+@@ -1,9 +1,9 @@
+-// sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
++// sherpa-onnx/csrc/offline-recognizer-sense-voice-tpl-impl.h
+ //
+ // Copyright (c)  2025  Xiaomi Corporation
+ 
+-#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+-#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
+ 
+ #include <memory>
+ #include <utility>
+@@ -13,7 +13,6 @@
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
+ #include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+ #include "sherpa-onnx/csrc/symbol-table.h"
+ 
+@@ -24,15 +23,15 @@ OfflineRecognitionResult ConvertSenseVoiceResult(
+     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+     int32_t frame_shift_ms, int32_t subsampling_factor);
+ 
+-class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
++template <typename SenseVoiceModel>
++class OfflineRecognizerSenseVoiceTplImpl : public OfflineRecognizerImpl {
+  public:
+-  explicit OfflineRecognizerSenseVoiceQnnImpl(
++  explicit OfflineRecognizerSenseVoiceTplImpl(
+       const OfflineRecognizerConfig &config)
+       : OfflineRecognizerImpl(config),
+         config_(config),
+         symbol_table_(config_.model_config.tokens),
+-        model_(
+-            std::make_unique<OfflineSenseVoiceModelQnn>(config.model_config)) {
++        model_(std::make_unique<SenseVoiceModel>(config.model_config)) {
+     const auto &meta_data = model_->GetModelMetadata();
+     if (config.decoding_method == "greedy_search") {
+       decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+@@ -47,13 +46,12 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
+   }
+ 
+   template <typename Manager>
+-  OfflineRecognizerSenseVoiceQnnImpl(Manager *mgr,
++  OfflineRecognizerSenseVoiceTplImpl(Manager *mgr,
+                                      const OfflineRecognizerConfig &config)
+       : OfflineRecognizerImpl(mgr, config),
+         config_(config),
+         symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelQnn>(
+-            mgr, config.model_config)) {
++        model_(std::make_unique<SenseVoiceModel>(mgr, config.model_config)) {
+     const auto &meta_data = model_->GetModelMetadata();
+     if (config.decoding_method == "greedy_search") {
+       decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+@@ -111,6 +109,10 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
+                             : meta_data.without_itn_id;
+ 
+     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    if (logits.empty()) {
++      return;
++    }
++
+     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+ 
+     auto result =
+@@ -129,10 +131,10 @@ class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
+  private:
+   OfflineRecognizerConfig config_;
+   SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineSenseVoiceModelQnn> model_;
++  std::unique_ptr<SenseVoiceModel> model_;
+   std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+ };
+ 
+ }  // namespace sherpa_onnx
+ 
+-#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_RECOGNIZER_SENSE_VOICE_TPL_IMPL_H_
+diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+deleted file mode 100644
+index 8daccec0..00000000
+--- a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
++++ /dev/null
+@@ -1,142 +0,0 @@
+-// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+-//
+-// Copyright (c)  2025  Xiaomi Corporation
+-
+-#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_
+-#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_
+-
+-#include <memory>
+-#include <utility>
+-#include <vector>
+-
+-#include "sherpa-onnx/csrc/macros.h"
+-#include "sherpa-onnx/csrc/offline-model-config.h"
+-#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+-#include "sherpa-onnx/csrc/offline-recognizer.h"
+-#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
+-#include "sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h"
+-#include "sherpa-onnx/csrc/symbol-table.h"
+-
+-namespace sherpa_onnx {
+-
+-// defined in ../online-recognizer-sense-voice-impl.h
+-OfflineRecognitionResult ConvertSenseVoiceResult(
+-    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+-    int32_t frame_shift_ms, int32_t subsampling_factor);
+-
+-class OfflineRecognizerSenseVoiceRknnImpl : public OfflineRecognizerImpl {
+- public:
+-  explicit OfflineRecognizerSenseVoiceRknnImpl(
+-      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(config),
+-        config_(config),
+-        symbol_table_(config_.model_config.tokens),
+-        model_(
+-            std::make_unique<OfflineSenseVoiceModelRknn>(config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  template <typename Manager>
+-  OfflineRecognizerSenseVoiceRknnImpl(Manager *mgr,
+-                                      const OfflineRecognizerConfig &config)
+-      : OfflineRecognizerImpl(mgr, config),
+-        config_(config),
+-        symbol_table_(mgr, config_.model_config.tokens),
+-        model_(std::make_unique<OfflineSenseVoiceModelRknn>(
+-            mgr, config.model_config)) {
+-    const auto &meta_data = model_->GetModelMetadata();
+-    if (config.decoding_method == "greedy_search") {
+-      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
+-          meta_data.blank_id);
+-    } else {
+-      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
+-                       config.decoding_method.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    InitFeatConfig();
+-  }
+-
+-  std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
+-  }
+-
+-  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    for (int32_t i = 0; i < n; ++i) {
+-      DecodeOneStream(ss[i]);
+-    }
+-  }
+-
+-  OfflineRecognizerConfig GetConfig() const override { return config_; }
+-
+- private:
+-  void InitFeatConfig() {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    config_.feat_config.normalize_samples = meta_data.normalize_samples;
+-    config_.feat_config.window_type = "hamming";
+-    config_.feat_config.high_freq = 0;
+-    config_.feat_config.snip_edges = true;
+-  }
+-
+-  void DecodeOneStream(OfflineStream *s) const {
+-    const auto &meta_data = model_->GetModelMetadata();
+-
+-    std::vector<float> f = s->GetFrames();
+-
+-    int32_t language = 0;
+-    if (config_.model_config.sense_voice.language.empty()) {
+-      language = 0;
+-    } else if (meta_data.lang2id.count(
+-                   config_.model_config.sense_voice.language)) {
+-      language =
+-          meta_data.lang2id.at(config_.model_config.sense_voice.language);
+-    } else {
+-      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
+-                       config_.model_config.sense_voice.language.c_str());
+-    }
+-
+-    int32_t text_norm = config_.model_config.sense_voice.use_itn
+-                            ? meta_data.with_itn_id
+-                            : meta_data.without_itn_id;
+-
+-    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
+-    if (logits.empty()) {
+-      return;
+-    }
+-
+-    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+-
+-    auto result =
+-        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
+-
+-    int32_t frame_shift_ms = 10;
+-    int32_t subsampling_factor = meta_data.window_shift;
+-    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
+-                                     subsampling_factor);
+-
+-    r.text = ApplyInverseTextNormalization(std::move(r.text));
+-    r.text = ApplyHomophoneReplacer(std::move(r.text));
+-    s->SetResult(r);
+-  }
+-
+- private:
+-  OfflineRecognizerConfig config_;
+-  SymbolTable symbol_table_;
+-  std::unique_ptr<OfflineSenseVoiceModelRknn> model_;
+-  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
+-};
+-
+-}  // namespace sherpa_onnx
+-
+-#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_SENSE_VOICE_RKNN_IMPL_H_
+
+commit 5af7603e1a2c19ba03a492e4f9f6d9b491bce1d2
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Mon Dec 8 12:17:31 2025 +0800
+
+    Add CI for Axera NPU (#2872)
+
+diff --git a/.github/workflows/axcl-linux-aarch64.yaml b/.github/workflows/axcl-linux-aarch64.yaml
+new file mode 100644
+index 00000000..f47419f5
+--- /dev/null
++++ b/.github/workflows/axcl-linux-aarch64.yaml
+@@ -0,0 +1,238 @@
++name: axcl-linux-aarch64
++
++on:
++  push:
++    branches:
++      - master
++    tags:
++      - 'v[0-9]+.[0-9]+.[0-9]+*'
++    paths:
++      - '.github/workflows/axcl-linux-aarch64.yaml'
++      - 'cmake/**'
++      - 'sherpa-onnx/csrc/*'
++      - 'sherpa-onnx/csrc/axcl/*'
++      - 'sherpa-onnx/c-api/*'
++      - 'toolchains/aarch64-linux-gnu.toolchain.cmake'
++
++  workflow_dispatch:
++
++concurrency:
++  group: axcl-linux-aarch64-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  axcl_linux_aarch64:
++    runs-on: ubuntu-22.04-arm
++    name: axcl npu
++    strategy:
++      fail-fast: false
++
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Update version
++        shell: bash
++        run: |
++          ./new-release.sh
++          git diff .
++
++      - name: Download SDK
++        shell: bash
++        run: |
++          git clone --depth 1 https://github.com/Abandon-ht/axcl_bsp_sdk
++          mv axcl_bsp_sdk/out sdk_dir
++
++          ls -lh sdk_dir/include
++          echo "---"
++          ls -lh sdk_dir/bsp
++          echo "---"
++          ls -lh sdk_dir/lib
++
++      - name: ccache
++        uses: hendrikmuhs/ccache-action@v1.2
++        with:
++          key: axcl-linux-aarch64
++
++      - name: Build sherpa-onnx
++        uses: addnab/docker-run-action@v3
++        with:
++            image: quay.io/pypa/manylinux_2_28_aarch64
++            # image: quay.io/pypa/manylinux2014_aarch64 # won't work
++            options: |
++              --volume ${{ github.workspace }}/:/k2-fsa/sherpa-onnx
++            shell: bash
++            run: |
++              uname -a
++              which gcc
++
++              gcc --version
++              g++ --version
++
++              cmake --version
++
++
++              cd /k2-fsa/sherpa-onnx/
++
++              export AXCL_SDK_ROOT=$PWD/sdk_dir
++              echo "AXCL_SDK_ROOT: $AXCL_SDK_ROOT"
++              export CPLUS_INCLUDE_PATH="$AXCL_SDK_ROOT/include:$AXCL_SDK_ROOT/bsp:$CPLUS_INCLUDE_PATH"
++              export SHERPA_ONNX_AXCL_LIB_DIR="$AXCL_SDK_ROOT/lib"
++
++              echo "pwd"
++
++              ls -lh
++
++              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
++              pushd alsa-lib
++              ./gitcompile
++              popd
++
++              ls -lh $PWD/alsa-lib/src/.libs
++
++              strings $PWD/alsa-lib/src/.libs/libasound.so.2.0.0 | grep "^GLIBC"
++
++              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
++              export C_INCLUDE_PATH=$PWD/alsa-lib/include:$C_INCLUDE_PATH
++              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
++              p=$PWD
++
++              export SHERPA_ONNX_ENABLE_ALSA=1
++
++              mkdir build
++              cd build
++
++              cmake \
++                -DALSA_INCLUDE_DIR=$p/alsa-lib/include \
++                -DALSA_LIBRARY=$p/alsa-lib/src/.libs/libasound.so \
++                -DBUILD_SHARED_LIBS=ON \
++                -DCMAKE_INSTALL_PREFIX=./install \
++                -DSHERPA_ONNX_ENABLE_AXCL=ON \
++                ..
++
++              make -j4 install
++
++              rm -rf install/lib/pkgconfig
++              rm -fv install/lib/cargs.h
++              rm -fv install/lib/libcargs.so
++
++      - name: Display system info
++        shell: bash
++        run: |
++          uname -a
++          gcc --version
++          g++ --version
++
++      - name: Display generated files
++        shell: bash
++        run: |
++          export AXCL_SDK_ROOT=$PWD/sdk_dir
++          export LD_LIBRARY_PATH=$AXCL_SDK_ROOT/lib:$LD_LIBRARY_PATH
++
++          ls -lh $AXCL_SDK_ROOT/lib/
++
++          cd build/install
++
++          ls -lh bin
++
++          echo "---"
++
++          ls -lh lib
++
++          file bin/sherpa-onnx
++
++          readelf -d bin/sherpa-onnx
++
++          ldd bin/sherpa-onnx
++
++          echo "---"
++          strings bin/sherpa-onnx | grep "^GLIBC"
++
++      - name: Copy files
++        shell: bash
++        run: |
++          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++          suffix=shared
++
++          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-axcl-linux-aarch64-$suffix
++          mkdir $dst
++
++          cp -a build/install/bin $dst/
++
++          mkdir -p $dst/lib
++          cp -v build/install/lib/lib*.so $dst/lib/
++
++          ls -lh build/install/lib
++          ls -lh build/install/bin
++
++          ls -lh $dst/bin/
++          echo "strip"
++          strip $dst/bin/*
++
++          echo "after strip"
++          ls -lh $dst/bin/
++
++          tree $dst
++
++          tar cjvf ${dst}.tar.bz2 $dst
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: sherpa-onnx-axcl-linux-aarch64-shared
++          path: sherpa-onnx-*linux-aarch64*.tar.bz2
++
++      # https://huggingface.co/docs/hub/spaces-github-actions
++      - name: Publish to huggingface
++        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            rm -rf huggingface
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++            GIT_LFS_SKIP_SMUDGE=1 git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
++
++            cd huggingface
++            dst=axcl-linux-aarch64/$SHERPA_ONNX_VERSION
++            mkdir -p $dst
++
++            cp -v ../sherpa-onnx-*axcl*-*.tar.bz2 $dst
++
++            git status
++            git lfs track "*.bz2"
++
++            git add .
++
++            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-axcl-linux-aarch64.tar.bz2"
++
++            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
++
++      - name: Release pre-compiled binaries and libs for linux aarch64
++        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-aarch64*.tar.bz2
++
++      - name: Release pre-compiled binaries and libs for linux aarch64
++        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-aarch64*.tar.bz2
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: v1.12.19
+diff --git a/.github/workflows/axera-linux-aarch64.yaml b/.github/workflows/axera-linux-aarch64.yaml
+new file mode 100644
+index 00000000..a60780bd
+--- /dev/null
++++ b/.github/workflows/axera-linux-aarch64.yaml
+@@ -0,0 +1,251 @@
++name: axera-linux-aarch64
++
++on:
++  push:
++    branches:
++      - master
++    tags:
++      - 'v[0-9]+.[0-9]+.[0-9]+*'
++    paths:
++      - '.github/workflows/axera-linux-aarch64.yaml'
++      - 'cmake/**'
++      - 'sherpa-onnx/csrc/*'
++      - 'sherpa-onnx/csrc/axera/*'
++      - 'sherpa-onnx/c-api/*'
++      - 'toolchains/aarch64-linux-gnu.toolchain.cmake'
++
++  workflow_dispatch:
++
++concurrency:
++  group: axera-linux-aarch64-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  axera_linux_aarch64:
++    runs-on: ubuntu-22.04-arm
++    name: axera npu
++    strategy:
++      fail-fast: false
++      matrix:
++        include:
++          - soc: ax650
++          - soc: ax630c
++          - soc: ax620q
++
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Update version
++        shell: bash
++        run: |
++          ./new-release.sh
++          git diff .
++
++      - name: Download SDK
++        shell: bash
++        run: |
++          soc=${{ matrix.soc }}
++          if [[ $soc == ax650 ]]; then
++            version=1.45.0_p39
++            curl -SL -O https://github.com/AXERA-TECH/ax650n_bsp_sdk/archive/refs/tags/v$version.zip
++            unzip -qq v$version.zip
++
++            mv $PWD/ax650n_bsp_sdk-$version/msp/out sdk_dir
++          elif [[ $soc == ax630c || $soc == ax620q ]]; then
++            version=2.0.0_P7
++            curl -SL -O https://github.com/AXERA-TECH/ax620e_bsp_sdk/archive/refs/tags/v2.0.0_P7.zip
++            unzip -qq v$version.zip
++            mv $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc sdk_dir
++
++          fi
++
++      - name: ccache
++        uses: hendrikmuhs/ccache-action@v1.2
++        with:
++          key: axera-${{ matrix.soc }}-linux-aarch64
++
++      - name: Build sherpa-onnx
++        uses: addnab/docker-run-action@v3
++        with:
++            image: quay.io/pypa/manylinux_2_28_aarch64
++            # image: quay.io/pypa/manylinux2014_aarch64 # won't work
++            options: |
++              --volume ${{ github.workspace }}/:/k2-fsa/sherpa-onnx
++            shell: bash
++            run: |
++              uname -a
++              which gcc
++
++              gcc --version
++              g++ --version
++
++              cmake --version
++
++
++              cd /k2-fsa/sherpa-onnx/
++
++              export AXERA_SDK_ROOT=$PWD/sdk_dir
++              echo "AXERA_SDK_ROOT: $AXERA_SDK_ROOT"
++              export CPLUS_INCLUDE_PATH="$AXERA_SDK_ROOT/include:$CPLUS_INCLUDE_PATH"
++              export SHERPA_ONNX_AXERA_LIB_DIR="$AXERA_SDK_ROOT/lib"
++
++              echo "pwd"
++
++              ls -lh
++
++              git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
++              pushd alsa-lib
++              ./gitcompile
++              popd
++
++              ls -lh $PWD/alsa-lib/src/.libs
++
++              strings $PWD/alsa-lib/src/.libs/libasound.so.2.0.0 | grep "^GLIBC"
++
++              export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
++              export C_INCLUDE_PATH=$PWD/alsa-lib/include:$C_INCLUDE_PATH
++              export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
++              p=$PWD
++
++              export SHERPA_ONNX_ENABLE_ALSA=1
++
++              mkdir build
++              cd build
++
++              cmake \
++                -DALSA_INCLUDE_DIR=$p/alsa-lib/include \
++                -DALSA_LIBRARY=$p/alsa-lib/src/.libs/libasound.so \
++                -DBUILD_SHARED_LIBS=ON \
++                -DCMAKE_INSTALL_PREFIX=./install \
++                -DSHERPA_ONNX_ENABLE_AXERA=ON \
++                ..
++
++              make -j4 install
++
++              rm -rf install/lib/pkgconfig
++              rm -fv install/lib/cargs.h
++              rm -fv install/lib/libcargs.so
++
++      - name: Display system info
++        shell: bash
++        run: |
++          uname -a
++          gcc --version
++          g++ --version
++
++      - name: Display generated files
++        shell: bash
++        run: |
++          export AXERA_SDK_ROOT=$PWD/sdk_dir
++          export LD_LIBRARY_PATH=$AXERA_SDK_ROOT/lib:$LD_LIBRARY_PATH
++
++          ls -lh $AXERA_SDK_ROOT/lib/
++
++          cd build/install
++
++          ls -lh bin
++
++          echo "---"
++
++          ls -lh lib
++
++          file bin/sherpa-onnx
++
++          readelf -d bin/sherpa-onnx
++
++          ldd bin/sherpa-onnx
++
++          echo "---"
++          strings bin/sherpa-onnx | grep "^GLIBC"
++
++      - name: Copy files
++        shell: bash
++        run: |
++          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++          suffix=shared
++
++          soc=${{ matrix.soc }}
++
++          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-axera-$soc-linux-aarch64-$suffix
++          mkdir $dst
++
++          cp -a build/install/bin $dst/
++
++          mkdir -p $dst/lib
++          cp -v build/install/lib/lib*.so $dst/lib/
++
++          ls -lh build/install/lib
++          ls -lh build/install/bin
++
++          ls -lh $dst/bin/
++          echo "strip"
++          strip $dst/bin/*
++
++          echo "after strip"
++          ls -lh $dst/bin/
++
++          tree $dst
++
++          tar cjvf ${dst}.tar.bz2 $dst
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: sherpa-onnx-axera-${{ matrix.soc }}-linux-aarch64-shared
++          path: sherpa-onnx-*linux-aarch64*.tar.bz2
++
++      # https://huggingface.co/docs/hub/spaces-github-actions
++      - name: Publish to huggingface
++        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            rm -rf huggingface
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++            GIT_LFS_SKIP_SMUDGE=1 git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
++
++            cd huggingface
++            dst=axera-linux-aarch64/$SHERPA_ONNX_VERSION/${{ matrix.soc }}
++            mkdir -p $dst
++
++            cp -v ../sherpa-onnx-*axera*-*.tar.bz2 $dst
++
++            git status
++            git lfs track "*.bz2"
++
++            git add .
++
++            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-axera-${{ matrix.soc }}-linux-aarch64.tar.bz2"
++
++            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
++
++      - name: Release pre-compiled binaries and libs for linux aarch64
++        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-aarch64*.tar.bz2
++
++      - name: Release pre-compiled binaries and libs for linux aarch64
++        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-aarch64*.tar.bz2
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: v1.12.19
+
+commit 6c9d1940224d712a2db927500b3822aee0cc1aa3
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Sun Dec 7 14:02:22 2025 +0800
+
+    Update README to include Axera NPU (#2870)
+
+diff --git a/README.md b/README.md
+index 6eb4cd4e..72a3079e 100644
+--- a/README.md
++++ b/README.md
+@@ -51,6 +51,10 @@ It also supports WebAssembly.
+ |-------------------------------------|-----------------------------------|-----------------------------|
+ |                                   |                                 |                           |
+ 
++| [4. Axera NPU][axera-npu] |
++|---------------------------|
++|                         |
++
+ [Join our discord](https://discord.gg/fJdxzg2VbG)
+ 
+ 
+@@ -582,3 +586,4 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
+ [rknpu-doc]: https://k2-fsa.github.io/sherpa/onnx/rknn/index.html
+ [qnn-doc]: https://k2-fsa.github.io/sherpa/onnx/qnn/index.html
+ [ascend-doc]: https://k2-fsa.github.io/sherpa/onnx/ascend/index.html
++[axera-npu]: https://axera-tech.com/Skill/166.html
+
+commit 1e742cee3089febbbdc7b1792abe6b530eed4b9f
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Sun Dec 7 13:08:31 2025 +0800
+
+    Refactor axcl examples. (#2867)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index ceedb01c..2a7e7433 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -206,8 +206,13 @@ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_AXCL)
+   list(APPEND sources
++    ./axcl/axcl-engine-guard.cc
++    ./axcl/axcl-engine-io-guard.cc
++    ./axcl/axcl-engine-io-info-guard.cc
++    ./axcl/axcl-manager.cc
++    ./axcl/axcl-model.cc
+     ./axcl/offline-sense-voice-model-axcl.cc
+-    ./axcl/ax_model_runner_axcl.cc
++    ./axcl/utils.cc
+   )
+ endif()
+ 
+diff --git a/sherpa-onnx/csrc/ascend/utils.h b/sherpa-onnx/csrc/ascend/utils.h
+index 2aaadf27..0a3aaf7c 100644
+--- a/sherpa-onnx/csrc/ascend/utils.h
++++ b/sherpa-onnx/csrc/ascend/utils.h
+@@ -19,7 +19,10 @@ class Acl {
+   ~Acl();
+ 
+   Acl(const Acl &) = delete;
+-  const Acl &operator=(const Acl &) = delete;
++  Acl &operator=(const Acl &) = delete;
++
++  Acl(Acl &&) = delete;
++  Acl &operator=(Acl &&) = delete;
+ 
+  private:
+   bool initialized_ = false;
+@@ -32,7 +35,10 @@ class AclContext {
+   ~AclContext();
+ 
+   AclContext(const AclContext &) = delete;
+-  const AclContext &operator=(const AclContext &) = delete;
++  AclContext &operator=(const AclContext &) = delete;
++
++  AclContext(AclContext &&) = delete;
++  AclContext &operator=(AclContext &&) = delete;
+ 
+   aclrtContext Get() const;
+   operator aclrtContext() { return context_; }
+@@ -49,7 +55,10 @@ class AclDevicePtr {
+   ~AclDevicePtr();
+ 
+   AclDevicePtr(const AclDevicePtr &) = delete;
+-  const AclDevicePtr &operator=(const AclDevicePtr &) = delete;
++  AclDevicePtr &operator=(const AclDevicePtr &) = delete;
++
++  AclDevicePtr(AclDevicePtr &&) = delete;
++  AclDevicePtr &operator=(AclDevicePtr &&) = delete;
+ 
+   void *Get() const { return p_; }
+   operator void *() { return p_; }
+@@ -68,7 +77,10 @@ class AclModelDesc {
+   ~AclModelDesc();
+ 
+   AclModelDesc(const AclModelDesc &) = delete;
+-  const AclModelDesc &operator=(const AclModelDesc &) = delete;
++  AclModelDesc &operator=(const AclModelDesc &) = delete;
++
++  AclModelDesc(AclModelDesc &&) = delete;
++  AclModelDesc &operator=(AclModelDesc &&) = delete;
+ 
+   aclmdlDesc *Get() const { return p_; }
+   operator aclmdlDesc *() const { return p_; }
+@@ -90,7 +102,10 @@ class AclModel {
+   operator uint32_t() const { return model_id_; }
+ 
+   AclModel(const AclModel &) = delete;
+-  const AclModel &operator=(const AclModel &) = delete;
++  AclModel &operator=(const AclModel &) = delete;
++
++  AclModel(AclModel &&) = delete;
++  AclModel &operator=(AclModel &&) = delete;
+ 
+   std::string GetInfo() const;
+ 
+@@ -135,6 +150,9 @@ class AclMdlDataset {
+   AclMdlDataset(const AclMdlDataset &) = delete;
+   AclMdlDataset &operator=(const AclMdlDataset &) = delete;
+ 
++  AclMdlDataset(AclMdlDataset &&) = delete;
++  AclMdlDataset &operator=(AclMdlDataset &&) = delete;
++
+   void AddBuffer(aclDataBuffer *buffer) const;
+   void SetTensorDesc(aclTensorDesc *tensor_desc, size_t index) const;
+ 
+@@ -153,6 +171,9 @@ class AclDataBuffer {
+   AclDataBuffer(const AclDataBuffer &) = delete;
+   AclDataBuffer &operator=(const AclDataBuffer &) = delete;
+ 
++  AclDataBuffer(AclDataBuffer &&) = delete;
++  AclDataBuffer &operator=(AclDataBuffer &&) = delete;
++
+   aclDataBuffer *Get() const { return p_; }
+   operator aclDataBuffer *() const { return p_; }
+ 
+@@ -169,6 +190,9 @@ class AclTensorDesc {
+   AclTensorDesc(const AclTensorDesc &) = delete;
+   AclTensorDesc &operator=(const AclTensorDesc &) = delete;
+ 
++  AclTensorDesc(AclTensorDesc &&) = delete;
++  AclTensorDesc &operator=(AclTensorDesc &&) = delete;
++
+   aclTensorDesc *Get() const { return p_; }
+   operator aclTensorDesc *() const { return p_; }
+ 
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+deleted file mode 100644
+index 42baad0d..00000000
+--- a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
++++ /dev/null
+@@ -1,148 +0,0 @@
+-// sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-#pragma once
+-#include <map>
+-#include <stdexcept>
+-#include <string>
+-#include <vector>
+-
+-typedef enum _color_space_e {
+-  ax_color_space_unknown,
+-  ax_color_space_nv12,
+-  ax_color_space_nv21,
+-  ax_color_space_bgr,
+-  ax_color_space_rgb,
+-} ax_color_space_e;
+-
+-typedef struct {
+-  std::string sName;
+-  unsigned int nIdx;
+-  std::vector<unsigned int> vShape;
+-  int nSize;
+-  unsigned long long phyAddr;
+-  void *pVirAddr;
+-} ax_runner_tensor_t;
+-
+-class ax_runner_base {
+- public:
+-  std::vector<ax_runner_tensor_t> moutput_tensors;
+-  std::vector<ax_runner_tensor_t> minput_tensors;
+-
+-  std::vector<std::vector<ax_runner_tensor_t>> mgroup_output_tensors;
+-  std::vector<std::vector<ax_runner_tensor_t>> mgroup_input_tensors;
+-
+-  std::map<std::string, ax_runner_tensor_t> map_output_tensors;
+-  std::map<std::string, ax_runner_tensor_t> map_input_tensors;
+-
+-  std::map<std::string, std::vector<ax_runner_tensor_t>>
+-      map_group_output_tensors;
+-  std::map<std::string, std::vector<ax_runner_tensor_t>>
+-      map_group_input_tensors;
+-
+-  bool _auto_sync_before_inference = true;
+-  bool _auto_sync_after_inference = true;
+-
+-  float cost_host_to_device = 0;
+-  float cost_inference = 0;
+-  float cost_device_to_host = 0;
+-
+- public:
+-  virtual int init(const char *model_file) = 0;
+-  virtual int init(char *model_buffer, size_t model_size) = 0;
+-
+-  virtual void deinit() = 0;
+-
+-  float get_inference_time() { return cost_inference; }
+-
+-  int get_num_inputs() { return minput_tensors.size(); };
+-  int get_num_outputs() { return moutput_tensors.size(); };
+-
+-  const ax_runner_tensor_t &get_input(int idx) { return minput_tensors[idx]; }
+-  const ax_runner_tensor_t *get_inputs_ptr() { return minput_tensors.data(); }
+-  const ax_runner_tensor_t &get_input(std::string name) {
+-    if (map_input_tensors.size() == 0) {
+-      for (size_t i = 0; i < minput_tensors.size(); i++) {
+-        map_input_tensors[minput_tensors[i].sName] = minput_tensors[i];
+-      }
+-    }
+-    if (map_input_tensors.find(name) == map_input_tensors.end()) {
+-      throw std::runtime_error("input tensor not found: " + name);
+-    }
+-
+-    return map_input_tensors[name];
+-  }
+-
+-  const ax_runner_tensor_t &get_input(int grpid, int idx) {
+-    return mgroup_input_tensors[grpid][idx];
+-  }
+-  const ax_runner_tensor_t *get_inputs_ptr(int grpid) {
+-    return mgroup_input_tensors[grpid].data();
+-  }
+-  const ax_runner_tensor_t &get_input(int grpid, std::string name) {
+-    if (map_group_input_tensors.size() == 0) {
+-      for (size_t i = 0; i < mgroup_input_tensors.size(); i++) {
+-        for (size_t j = 0; j < mgroup_input_tensors[i].size(); j++) {
+-          map_group_input_tensors[mgroup_input_tensors[i][j].sName].push_back(
+-              mgroup_input_tensors[i][j]);
+-        }
+-      }
+-    }
+-    if (map_group_input_tensors.find(name) == map_group_input_tensors.end()) {
+-      throw std::runtime_error("input tensor not found: " + name);
+-    }
+-    return map_group_input_tensors[name][grpid];
+-    // return map_input_tensors[name];
+-  }
+-
+-  const ax_runner_tensor_t &get_output(int idx) { return moutput_tensors[idx]; }
+-  const ax_runner_tensor_t *get_outputs_ptr() { return moutput_tensors.data(); }
+-  const ax_runner_tensor_t &get_output(std::string name) {
+-    if (map_output_tensors.size() == 0) {
+-      for (size_t i = 0; i < moutput_tensors.size(); i++) {
+-        map_output_tensors[moutput_tensors[i].sName] = moutput_tensors[i];
+-      }
+-    }
+-    if (map_output_tensors.find(name) == map_output_tensors.end()) {
+-      throw std::runtime_error("output tensor not found: " + name);
+-    }
+-
+-    return map_output_tensors[name];
+-  }
+-
+-  const ax_runner_tensor_t &get_output(int grpid, int idx) {
+-    return mgroup_output_tensors[grpid][idx];
+-  }
+-  const ax_runner_tensor_t *get_outputs_ptr(int grpid) {
+-    return mgroup_output_tensors[grpid].data();
+-  }
+-  const ax_runner_tensor_t &get_output(int grpid, std::string name) {
+-    if (map_group_output_tensors.size() == 0) {
+-      for (size_t i = 0; i < mgroup_output_tensors.size(); i++) {
+-        for (size_t j = 0; j < mgroup_output_tensors[i].size(); j++) {
+-          map_group_output_tensors[mgroup_output_tensors[i][j].sName].push_back(
+-              mgroup_output_tensors[i][j]);
+-        }
+-      }
+-    }
+-    if (map_group_output_tensors.find(name) == map_group_output_tensors.end()) {
+-      throw std::runtime_error("input tensor not found: " + name);
+-    }
+-    return map_group_output_tensors[name][grpid];
+-  }
+-
+-  virtual int get_algo_width() = 0;
+-  virtual int get_algo_height() = 0;
+-  virtual ax_color_space_e get_color_space() = 0;
+-
+-  void set_auto_sync_before_inference(bool sync) {
+-    _auto_sync_before_inference = sync;
+-  }
+-  void set_auto_sync_after_inference(bool sync) {
+-    _auto_sync_after_inference = sync;
+-  }
+-
+-  virtual int inference() = 0;
+-  virtual int inference(int grpid) = 0;
+-};
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+deleted file mode 100644
+index 7f3733a9..00000000
+--- a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
++++ /dev/null
+@@ -1,470 +0,0 @@
+-// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-#include "ax_model_runner_axcl.hpp"
+-
+-#include <axcl.h>
+-#include <fcntl.h>
+-#include <string.h>
+-
+-#include <fstream>
+-#include <memory>
+-
+-typedef enum {
+-  AX_ENGINE_ABST_DEFAULT = 0,
+-  AX_ENGINE_ABST_CACHED = 1,
+-} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
+-
+-typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
+-                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
+-    INPUT_OUTPUT_ALLOC_STRATEGY;
+-
+-static void print_io_info(std::vector<ax_runner_tensor_t> &input,
+-                          std::vector<ax_runner_tensor_t> &output) {
+-  printf("\ninput size: %ld\n", input.size());
+-  for (size_t i = 0; i < input.size(); ++i) {
+-    // print shape info,like [batchsize x channel x height x width]
+-    auto &info = input[i];
+-    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
+-    for (size_t s = 0; s < info.vShape.size(); s++) {
+-      printf("%d", info.vShape[s]);
+-      if (s != info.vShape.size() - 1) {
+-        printf(" x ");
+-      }
+-    }
+-    printf("\e[0m\n\n");
+-  }
+-
+-  printf("\noutput size: %ld\n", output.size());
+-  for (size_t i = 0; i < output.size(); ++i) {
+-    // print shape info,like [batchsize x channel x height x width]
+-    auto &info = output[i];
+-    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
+-    for (size_t s = 0; s < info.vShape.size(); s++) {
+-      printf("%d", info.vShape[s]);
+-      if (s != info.vShape.size() - 1) {
+-        printf(" x ");
+-      }
+-    }
+-    printf("\e[0m\n\n");
+-  }
+-}
+-
+-static bool read_file(const char *fn, std::vector<unsigned char> &data) {
+-  FILE *fp = fopen(fn, "r");
+-  if (fp != nullptr) {
+-    fseek(fp, 0L, SEEK_END);
+-    auto len = ftell(fp);
+-    fseek(fp, 0, SEEK_SET);
+-    data.clear();
+-    size_t read_size = 0;
+-    if (len > 0) {
+-      data.resize(len);
+-      read_size = fread(data.data(), 1, len, fp);
+-    }
+-    fclose(fp);
+-    return read_size == (size_t)len;
+-  }
+-  return false;
+-}
+-
+-typedef struct {
+-  int nIndex;
+-  int nSize;
+-  void *pBuf;
+-  void *pVirAddr;
+-
+-  std::string Name;
+-
+-  axclrtEngineIODims dims;
+-} AXCL_IO_BUF_T;
+-
+-typedef struct {
+-  uint32_t nInputSize;
+-  uint32_t nOutputSize;
+-  AXCL_IO_BUF_T *pInputs;
+-  AXCL_IO_BUF_T *pOutputs;
+-} AXCL_IO_DATA_T;
+-
+-static void free_io_index(AXCL_IO_BUF_T *pBuf, size_t index) {
+-  for (size_t i = 0; i < index; ++i) {
+-    axclrtFree(pBuf[i].pBuf);
+-  }
+-}
+-
+-static void free_io(AXCL_IO_DATA_T *io_data) {
+-  for (size_t j = 0; j < io_data->nInputSize; ++j) {
+-    axclrtFree(io_data->pInputs[j].pBuf);
+-    free(io_data->pInputs[j].pVirAddr);
+-  }
+-  for (size_t j = 0; j < io_data->nOutputSize; ++j) {
+-    axclrtFree(io_data->pOutputs[j].pBuf);
+-    free(io_data->pOutputs[j].pVirAddr);
+-  }
+-  delete[] io_data->pInputs;
+-  delete[] io_data->pOutputs;
+-}
+-
+-static inline int prepare_io(int grpid, axclrtEngineIOInfo io_info,
+-                             axclrtEngineIO io, AXCL_IO_DATA_T *io_data,
+-                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
+-  memset(io_data, 0, sizeof(AXCL_IO_DATA_T));
+-
+-  auto inputNum = axclrtEngineGetNumInputs(io_info);
+-  auto outputNum = axclrtEngineGetNumOutputs(io_info);
+-  io_data->nInputSize = inputNum;
+-  io_data->nOutputSize = outputNum;
+-  io_data->pInputs = new AXCL_IO_BUF_T[inputNum];
+-  io_data->pOutputs = new AXCL_IO_BUF_T[outputNum];
+-
+-  // 1. alloc inputs
+-  for (uint32_t i = 0; i < inputNum; i++) {
+-    auto bufSize = axclrtEngineGetInputSizeByIndex(io_info, grpid, i);
+-    void *devPtr = nullptr;
+-    axclError ret = 0;
+-    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
+-      ret = axclrtMalloc(&devPtr, bufSize,
+-                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+-    } else {
+-      ret = axclrtMallocCached(
+-          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+-    }
+-
+-    if (ret != 0) {
+-      free_io_index(io_data->pInputs, i);
+-      fprintf(stderr, "Malloc input(index: %d, size: %ld) failed! ret=0x%x\n",
+-              i, bufSize, ret);
+-      return -1;
+-    }
+-    std::vector<char> tmp(bufSize, 0);
+-    axclrtMemcpy(devPtr, tmp.data(), bufSize,
+-                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
+-    // axclrtMemset(devPtr, 0, bufSize);
+-
+-    axclrtEngineIODims dims;
+-    ret = axclrtEngineGetInputDims(io_info, grpid, i, &dims);
+-    if (ret != 0) {
+-      free_io_index(io_data->pInputs, i);
+-      fprintf(stderr, "Get input dims(index: %d) failed! ret=0x%x\n", i, ret);
+-      return -1;
+-    }
+-
+-    io_data->pInputs[i].nIndex = i;
+-    io_data->pInputs[i].nSize = bufSize;
+-    io_data->pInputs[i].pBuf = devPtr;
+-    io_data->pInputs[i].dims = dims;
+-    io_data->pInputs[i].Name = axclrtEngineGetInputNameByIndex(io_info, i);
+-    io_data->pInputs[i].pVirAddr = malloc(bufSize);
+-    memset(io_data->pInputs[i].pVirAddr, 0, bufSize);
+-    ret = axclrtEngineSetInputBufferByIndex(io, i, devPtr, bufSize);
+-    if (ret != 0) {
+-      free_io_index(io_data->pInputs, i);
+-      fprintf(stderr,
+-              "Set input buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
+-              bufSize, ret);
+-      return -1;
+-    }
+-  }
+-
+-  // 2. alloc outputs
+-  for (uint32_t i = 0; i < outputNum; i++) {
+-    auto bufSize = axclrtEngineGetOutputSizeByIndex(io_info, grpid, i);
+-    void *devPtr = NULL;
+-    axclError ret = 0;
+-    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
+-      ret = axclrtMalloc(&devPtr, bufSize,
+-                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+-    } else {
+-      ret = axclrtMallocCached(
+-          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
+-    }
+-
+-    if (ret != 0) {
+-      free_io_index(io_data->pOutputs, i);
+-      fprintf(stderr, "Malloc output(index: %d, size: %ld) failed! ret=0x%x\n",
+-              i, bufSize, ret);
+-      return -1;
+-    }
+-    std::vector<char> tmp(bufSize, 0);
+-    axclrtMemcpy(devPtr, tmp.data(), bufSize,
+-                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
+-    axclrtEngineIODims dims;
+-    ret = axclrtEngineGetOutputDims(io_info, grpid, i, &dims);
+-    if (ret != 0) {
+-      free_io_index(io_data->pOutputs, i);
+-      fprintf(stderr, "Get output dims(index: %d) failed! ret=0x%x\n", i, ret);
+-      return -1;
+-    }
+-
+-    io_data->pOutputs[i].nIndex = i;
+-    io_data->pOutputs[i].nSize = bufSize;
+-    io_data->pOutputs[i].pBuf = devPtr;
+-    io_data->pOutputs[i].dims = dims;
+-    io_data->pOutputs[i].Name = axclrtEngineGetOutputNameByIndex(io_info, i);
+-    io_data->pOutputs[i].pVirAddr = malloc(bufSize);
+-    memset(io_data->pOutputs[i].pVirAddr, 0, bufSize);
+-    ret = axclrtEngineSetOutputBufferByIndex(io, i, devPtr, bufSize);
+-    if (ret != 0) {
+-      free_io_index(io_data->pOutputs, i);
+-      fprintf(stderr,
+-              "Set output buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
+-              bufSize, ret);
+-      return -1;
+-    }
+-  }
+-
+-  return 0;
+-}
+-
+-struct ax_joint_runner_axcl_handle_t {
+-  uint64_t handle = 0;
+-  uint64_t context = 0;
+-  axclrtEngineIOInfo io_info = 0;
+-  std::vector<axclrtEngineIO> ios;
+-  std::vector<AXCL_IO_DATA_T> io_datas;
+-
+-  // int algo_width, algo_height;
+-  // int algo_colorformat;
+-};
+-
+-int ax_runner_axcl::sub_init() {
+-  // 4. create context
+-  int ret = axclrtEngineCreateContext(m_handle->handle, &m_handle->context);
+-  if (0 != ret) {
+-    fprintf(stderr, "axclrtEngineCreateContext failed.\n");
+-    return ret;
+-  }
+-  fprintf(stdout, "axclrtEngineCreateContextt is done. \n");
+-
+-  // 5. set io
+-
+-  ret = axclrtEngineGetIOInfo(m_handle->handle, &m_handle->io_info);
+-  if (0 != ret) {
+-    fprintf(stderr, "axclrtEngineGetIOInfo failed.\n");
+-    return ret;
+-  }
+-  fprintf(stdout, "axclrtEngineGetIOInfo is done. \n");
+-
+-  ret = axclrtEngineGetShapeGroupsCount(m_handle->io_info, &group_count);
+-  if (ret != 0) {
+-    axclrtEngineUnload(m_handle->handle);
+-    return ret;
+-  }
+-
+-  // 6. alloc io
+-  if (!_parepare_io) {
+-    m_handle->ios.resize(group_count);
+-    m_handle->io_datas.resize(group_count);
+-    mgroup_input_tensors.resize(group_count);
+-    mgroup_output_tensors.resize(group_count);
+-
+-    memset(&m_handle->io_datas[0], 0, sizeof(AXCL_IO_DATA_T) * group_count);
+-
+-    auto malloc_strategy =
+-        std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_DEFAULT);
+-
+-    for (int grpid = 0; grpid < group_count; grpid++) {
+-      ret = axclrtEngineCreateIO(m_handle->io_info, &m_handle->ios[grpid]);
+-      if (ret != 0) {
+-        axclrtEngineUnload(m_handle->handle);
+-        fprintf(stderr, "Create io failed. ret=0x%x\n", ret);
+-        return -1;
+-      }
+-
+-      ret = prepare_io(grpid, m_handle->io_info, m_handle->ios[grpid],
+-                       &m_handle->io_datas[grpid], malloc_strategy);
+-      if (ret != 0) {
+-        free_io(&m_handle->io_datas[grpid]);
+-        axclrtEngineDestroyIO(m_handle->ios[grpid]);
+-        axclrtEngineUnload(m_handle->handle);
+-
+-        fprintf(stderr, "prepare_io failed.\n");
+-        return ret;
+-      }
+-    }
+-
+-    for (int grpid = 0; grpid < group_count; grpid++) {
+-      // auto &io_info = m_handle->io_info[grpid];
+-      auto &io_data = m_handle->io_datas[grpid];
+-      for (uint32_t i = 0; i < io_data.nOutputSize; i++) {
+-        ax_runner_tensor_t tensor;
+-        tensor.nIdx = i;
+-        tensor.sName = std::string(io_data.pOutputs[i].Name);
+-        tensor.nSize = io_data.pOutputs[i].nSize;
+-        for (int32_t j = 0; j < io_data.pOutputs[i].dims.dimCount; j++) {
+-          tensor.vShape.push_back(io_data.pOutputs[i].dims.dims[j]);
+-        }
+-        tensor.phyAddr = (unsigned long long)io_data.pOutputs[i].pBuf;
+-        tensor.pVirAddr = io_data.pOutputs[i].pVirAddr;
+-        mgroup_output_tensors[grpid].push_back(tensor);
+-      }
+-
+-      for (size_t i = 0; i < io_data.nInputSize; i++) {
+-        ax_runner_tensor_t tensor;
+-        tensor.nIdx = i;
+-        tensor.sName = std::string(io_data.pInputs[i].Name);
+-        tensor.nSize = io_data.pInputs[i].nSize;
+-        for (int32_t j = 0; j < io_data.pInputs[i].dims.dimCount; j++) {
+-          tensor.vShape.push_back(io_data.pInputs[i].dims.dims[j]);
+-        }
+-        tensor.phyAddr = (unsigned long long)io_data.pInputs[i].pBuf;
+-        tensor.pVirAddr = io_data.pInputs[i].pVirAddr;
+-        mgroup_input_tensors[grpid].push_back(tensor);
+-      }
+-    }
+-
+-    moutput_tensors = mgroup_output_tensors[0];
+-    minput_tensors = mgroup_input_tensors[0];
+-    _parepare_io = true;
+-  } else {
+-  }
+-  // for (int grpid = 0; grpid < group_count; grpid++) {
+-  //   printf("\ngrpid: %d\n", grpid);
+-  //   print_io_info(mgroup_input_tensors[grpid], mgroup_output_tensors[grpid]);
+-  //   printf("==================================================\n\n");
+-  // }
+-
+-  return ret;
+-}
+-
+-int ax_runner_axcl::init(const char *model_file) {
+-  std::vector<unsigned char> model_buffer;
+-  if (!read_file(model_file, model_buffer)) {
+-    fprintf(stderr, "read_file failed.\n");
+-    return -1;
+-  }
+-  auto ret = init((char *)model_buffer.data(), model_buffer.size());
+-  return ret;
+-}
+-
+-int ax_runner_axcl::init(char *model_buffer, size_t model_size) {
+-  if (!m_handle) {
+-    m_handle = new ax_joint_runner_axcl_handle_t;
+-  }
+-  memset((void *)m_handle, 0, sizeof(ax_joint_runner_axcl_handle_t));
+-
+-  // 3. create handle
+-  void *devMem = nullptr;
+-  axclrtMalloc(&devMem, model_size, AXCL_MEM_MALLOC_NORMAL_ONLY);
+-
+-  // 4. copy model to device
+-  axclrtMemcpy(devMem, model_buffer, model_size, AXCL_MEMCPY_HOST_TO_DEVICE);
+-
+-  int ret = axclrtEngineLoadFromMem(devMem, model_size, &m_handle->handle);
+-  if (0 != ret) {
+-    fprintf(stderr, "AX_ENGINE_CreateHandle");
+-    return ret;
+-  }
+-  axclrtFree(devMem);
+-
+-  return sub_init();
+-}
+-
+-void ax_runner_axcl::release() {
+-  if (m_handle && m_handle->handle) {
+-    for (int grpid = 0; grpid < group_count; grpid++) {
+-      free_io(&m_handle->io_datas[grpid]);
+-      axclrtEngineDestroyIO(m_handle->ios[grpid]);
+-    }
+-
+-    axclrtEngineUnload(m_handle->handle);
+-    m_handle->handle = 0;
+-  }
+-
+-  if (m_handle) {
+-    delete m_handle;
+-    m_handle = nullptr;
+-  }
+-
+-  minput_tensors.clear();
+-  moutput_tensors.clear();
+-
+-  map_input_tensors.clear();
+-  map_output_tensors.clear();
+-
+-  mgroup_input_tensors.clear();
+-  mgroup_output_tensors.clear();
+-
+-  map_group_input_tensors.clear();
+-  map_group_output_tensors.clear();
+-}
+-
+-void ax_runner_axcl::deinit() {
+-  if (m_handle && m_handle->handle) {
+-    axclrtEngineUnload(m_handle->handle);
+-    m_handle->handle = 0;
+-  }
+-}
+-
+-int ax_runner_axcl::get_algo_width() {
+-  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
+-    return minput_tensors[0].vShape[2];
+-  }
+-  return -1;
+-}
+-int ax_runner_axcl::get_algo_height() {
+-  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
+-    return minput_tensors[0].vShape[1];
+-  }
+-  return -1;
+-}
+-
+-int ax_runner_axcl::set_input(int grpid, int idx,
+-                              unsigned long long int phy_addr,
+-                              unsigned long size) {
+-  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid], idx,
+-                                           (void *)phy_addr, size);
+-}
+-int ax_runner_axcl::set_output(int grpid, int idx,
+-                               unsigned long long int phy_addr,
+-                               unsigned long size) {
+-  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid], idx,
+-                                            (void *)phy_addr, size);
+-}
+-
+-int ax_runner_axcl::set_input(int grpid, std::string name,
+-                              unsigned long long int phy_addr,
+-                              unsigned long size) {
+-  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid],
+-                                           get_input(grpid, name).nIdx,
+-                                           (void *)phy_addr, size);
+-}
+-
+-int ax_runner_axcl::set_output(int grpid, std::string name,
+-                               unsigned long long int phy_addr,
+-                               unsigned long size) {
+-  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid],
+-                                            get_output(grpid, name).nIdx,
+-                                            (void *)phy_addr, size);
+-}
+-
+-ax_color_space_e ax_runner_axcl::get_color_space() {
+-  return ax_color_space_unknown;
+-}
+-
+-int ax_runner_axcl::inference() { return inference(0); }
+-
+-int ax_runner_axcl::inference(int grpid) {
+-  if (_auto_sync_before_inference)
+-    for (size_t i = 0; i < mgroup_input_tensors[grpid].size(); i++)
+-      axclrtMemcpy((void *)mgroup_input_tensors[grpid][i].phyAddr,
+-                   mgroup_input_tensors[grpid][i].pVirAddr,
+-                   mgroup_input_tensors[grpid][i].nSize,
+-                   AXCL_MEMCPY_HOST_TO_DEVICE);
+-
+-  auto ret = axclrtEngineExecute(m_handle->handle, m_handle->context, grpid,
+-                                 m_handle->ios[grpid]);
+-  if (ret != 0) {
+-    fprintf(stderr, "axclrtEngineExecute failed. ret=0x%x\n", ret);
+-    return ret;
+-  }
+-
+-  if (_auto_sync_after_inference)
+-    for (size_t i = 0; i < mgroup_output_tensors[grpid].size(); i++)
+-      axclrtMemcpy(mgroup_output_tensors[grpid][i].pVirAddr,
+-                   (void *)mgroup_output_tensors[grpid][i].phyAddr,
+-                   mgroup_output_tensors[grpid][i].nSize,
+-                   AXCL_MEMCPY_DEVICE_TO_HOST);
+-  return 0;
+-}
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+deleted file mode 100644
+index 4aba11b0..00000000
+--- a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
++++ /dev/null
+@@ -1,40 +0,0 @@
+-// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-#pragma once
+-#include "ax_model_runner.hpp"
+-
+-class ax_runner_axcl : public ax_runner_base {
+- protected:
+-  struct ax_joint_runner_axcl_handle_t *m_handle = nullptr;
+-  int group_count = 0;
+-  bool _parepare_io = false;
+-
+-  int sub_init();
+-
+- public:
+-  int init(const char *model_file) override;
+-  int init(char *model_buffer, size_t model_size) override;
+-
+-  void release();
+-  void deinit() override;
+-
+-  int get_algo_width() override;
+-  int get_algo_height() override;
+-  ax_color_space_e get_color_space() override;
+-
+-  int set_input(int grpid, int idx, unsigned long long int phy_addr,
+-                unsigned long size);
+-  int set_output(int grpid, int idx, unsigned long long int phy_addr,
+-                 unsigned long size);
+-
+-  int set_input(int grpid, std::string name, unsigned long long int phy_addr,
+-                unsigned long size);
+-  int set_output(int grpid, std::string name, unsigned long long int phy_addr,
+-                 unsigned long size);
+-
+-  // int inference(ax_image_t *pstFrame) override;
+-  int inference() override;
+-  int inference(int grpid) override;
+-};
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
+new file mode 100644
+index 00000000..706095b6
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-guard.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/axcl-engine-guard.h"
++
++#include <cstdint>
++
++#include "axcl.h"  // NOLINT
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++AxclEngineGuard::AxclEngineGuard(
++    axclrtEngineVNpuKind npuKind /*= AXCL_VNPU_DISABLE*/) {
++  axclError ret = axclrtEngineInit(npuKind);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("Failed to call axclrtEngineInit(). Return code is: %d",
++                     static_cast<int32_t>(ret));
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  initialized_ = true;
++}
++
++AxclEngineGuard::~AxclEngineGuard() {
++  if (initialized_) {
++    auto ret = axclrtEngineFinalize();
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineFinalize(). Return code is: %d",
++          static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-guard.h
+new file mode 100644
+index 00000000..15fed786
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-guard.h
+@@ -0,0 +1,27 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-guard.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
++#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
++#include "axcl.h"  // NOLINT
++
++namespace sherpa_onnx {
++
++class AxclEngineGuard {
++ public:
++  explicit AxclEngineGuard(axclrtEngineVNpuKind npuKind = AXCL_VNPU_DISABLE);
++  ~AxclEngineGuard();
++
++  AxclEngineGuard(const AxclEngineGuard &) = delete;
++  AxclEngineGuard &operator=(const AxclEngineGuard &) = delete;
++  AxclEngineGuard(AxclEngineGuard &&) = delete;
++  AxclEngineGuard &operator=(AxclEngineGuard &&) = delete;
++
++ private:
++  bool initialized_ = false;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_GUARD_H_
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
+new file mode 100644
+index 00000000..f05b0131
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-io-guard.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h"
++
++#include <cstdint>
++
++#include "axcl.h"  // NOLINT
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++AxclEngineIOGuard::AxclEngineIOGuard(axclrtEngineIOInfo io_info) {
++  axclError ret = axclrtEngineCreateIO(io_info, &io_);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE(
++        "Failed to call axclrtEngineCreateIO(). Return code is: %d",
++        static_cast<int32_t>(ret));
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  initialized_ = true;
++}
++
++AxclEngineIOGuard::~AxclEngineIOGuard() {
++  if (initialized_) {
++    auto ret = axclrtEngineDestroyIO(io_);
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineDestroyIO(). Return code is: %d",
++          static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
+new file mode 100644
+index 00000000..6401f0a8
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
+@@ -0,0 +1,30 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
++#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
++#include "axcl.h"  // NOLINT
++
++namespace sherpa_onnx {
++
++class AxclEngineIOGuard {
++ public:
++  explicit AxclEngineIOGuard(axclrtEngineIOInfo io_info);
++  ~AxclEngineIOGuard();
++
++  AxclEngineIOGuard(const AxclEngineIOGuard &) = delete;
++  AxclEngineIOGuard &operator=(const AxclEngineIOGuard &) = delete;
++  AxclEngineIOGuard(AxclEngineIOGuard &&) = delete;
++  AxclEngineIOGuard &operator=(AxclEngineIOGuard &&) = delete;
++
++  operator axclrtEngineIO() { return io_; }
++
++ private:
++  bool initialized_ = false;
++  axclrtEngineIO io_ = nullptr;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_GUARD_H_
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
+new file mode 100644
+index 00000000..e2e7a124
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h"
++
++#include <cstdint>
++
++#include "axcl.h"  // NOLINT
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++AxclEngineIOInfoGuard::AxclEngineIOInfoGuard(uint64_t model_id) {
++  axclError ret = axclrtEngineGetIOInfo(model_id, &io_info_);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE(
++        "Failed to call axclrtEngineGetIOInfo(). Return code is: %d",
++        static_cast<int32_t>(ret));
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  initialized_ = true;
++}
++
++AxclEngineIOInfoGuard::~AxclEngineIOInfoGuard() {
++  if (initialized_) {
++    auto ret = axclrtEngineDestroyIOInfo(io_info_);
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineDestroyIOInfo(). Return code is: %d",
++          static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
+new file mode 100644
+index 00000000..0926b1b1
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
+@@ -0,0 +1,32 @@
++// sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
++#define SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
++#include <cstdint>
++
++#include "axcl.h"  // NOLINT
++
++namespace sherpa_onnx {
++
++class AxclEngineIOInfoGuard {
++ public:
++  explicit AxclEngineIOInfoGuard(uint64_t model_id);
++  ~AxclEngineIOInfoGuard();
++
++  AxclEngineIOInfoGuard(const AxclEngineIOInfoGuard &) = delete;
++  AxclEngineIOInfoGuard &operator=(const AxclEngineIOInfoGuard &) = delete;
++  AxclEngineIOInfoGuard(AxclEngineIOInfoGuard &&) = delete;
++  AxclEngineIOInfoGuard &operator=(AxclEngineIOInfoGuard &&) = delete;
++
++  operator axclrtEngineIOInfo() { return io_info_; }
++
++ private:
++  bool initialized_ = false;
++  axclrtEngineIOInfo io_info_ = nullptr;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_ENGINE_IO_INFO_GUARD_H_
+diff --git a/sherpa-onnx/csrc/axcl/axcl-manager.cc b/sherpa-onnx/csrc/axcl/axcl-manager.cc
+new file mode 100644
+index 00000000..2166fb2f
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-manager.cc
+@@ -0,0 +1,45 @@
++// sherpa-onnx/csrc/axcl/axcl-manager.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/axcl-manager.h"
++
++#include <cstdint>
++
++#include "axcl.h"  // NOLINT
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++std::mutex AxclManager::mutex_;
++
++int32_t AxclManager::count_{0};
++
++AxclManager::AxclManager(const char *config /*= nullptr*/) {
++  std::lock_guard<std::mutex> lock(mutex_);
++  if (count_ == 0) {
++    auto ret = axclInit(config);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call axclInit(). Return code: %d",
++                       static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  ++count_;
++}
++
++AxclManager::~AxclManager() {
++  std::lock_guard<std::mutex> lock(mutex_);
++  if (--count_ == 0) {
++    auto ret = axclFinalize();
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call axclFinalize(). Return code: %d",
++                       static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/axcl-manager.h b/sherpa-onnx/csrc/axcl/axcl-manager.h
+new file mode 100644
+index 00000000..ce349d93
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-manager.h
+@@ -0,0 +1,29 @@
++// sherpa-onnx/csrc/axcl/axcl-manager.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
++#define SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
++
++#include <cstdint>
++#include <mutex>
++
++namespace sherpa_onnx {
++
++class AxclManager {
++ public:
++  explicit AxclManager(const char *config = nullptr);
++  ~AxclManager();
++
++  AxclManager(const AxclManager &) = delete;
++  AxclManager &operator=(const AxclManager &) = delete;
++
++  AxclManager(AxclManager &&) = delete;
++  AxclManager &operator=(AxclManager &&) = delete;
++
++ private:
++  static std::mutex mutex_;
++  static int32_t count_;
++};
++}  // namespace sherpa_onnx
++#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_MANAGER_H_
+diff --git a/sherpa-onnx/csrc/axcl/axcl-model.cc b/sherpa-onnx/csrc/axcl/axcl-model.cc
+new file mode 100644
+index 00000000..85a3b8a8
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-model.cc
+@@ -0,0 +1,441 @@
++// sherpa-onnx/csrc/axcl/axcl-model.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/axcl-model.h"
++
++#include <memory>
++#include <string>
++#include <vector>
++
++#include "axcl.h"  // NOLINT
++#include "sherpa-onnx/csrc/axcl/axcl-engine-guard.h"
++#include "sherpa-onnx/csrc/axcl/axcl-engine-io-guard.h"
++#include "sherpa-onnx/csrc/axcl/axcl-engine-io-info-guard.h"
++#include "sherpa-onnx/csrc/axcl/axcl-manager.h"
++#include "sherpa-onnx/csrc/axcl/utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++/*
++Initialization step:
++
++1. AxclInit()
++2. set device
++3. init engine
++4. axclrtEngineLoadFromMem or axclrtEngineLoadFromFile
++5. axclrtEngineCreateContext
++ */
++
++class AxclModel::Impl {
++ public:
++  Impl(const std::string &filename, int32_t device_id) {
++    if (!SetDevice(device_id)) {
++      return;
++    }
++
++    InitEngine();
++
++    axclError ret = axclrtEngineLoadFromFile(filename.c_str(), &model_id_);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineLoadFromFile() with file: %s. Return "
++          "code is: %d",
++          filename.c_str(), static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    model_loaded_ = true;
++
++    PostInit();
++  }
++
++  Impl(const void *cpu_buf, size_t buf_len_in_bytes, int32_t device_id) {
++    if (!SetDevice(device_id)) {
++      return;
++    }
++
++    InitEngine();
++
++    {
++      AxclDevicePtr device_ptr(buf_len_in_bytes, AXCL_MEM_MALLOC_NORMAL_ONLY);
++      auto ret = axclrtMemcpy(device_ptr, cpu_buf, buf_len_in_bytes,
++                              AXCL_MEMCPY_HOST_TO_DEVICE);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE("Failed to call axclrtMemcpy(). Return code is: %d",
++                         static_cast<int32_t>(ret));
++        return;
++      }
++
++      ret = axclrtEngineLoadFromMem(device_ptr, buf_len_in_bytes, &model_id_);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineLoadFromMem(). Return code is: %d",
++            static_cast<int32_t>(ret));
++        return;
++      }
++    }
++
++    model_loaded_ = true;
++
++    PostInit();
++  }
++
++  ~Impl() {
++    if (model_loaded_) {
++      axclError ret = axclrtEngineUnload(model_id_);
++
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineUnload(). Return code is: %d",
++            static_cast<int32_t>(ret));
++        SHERPA_ONNX_EXIT(-1);
++      }
++    }
++  }
++
++  const std::vector<std::string> &InputTensorNames() const {
++    return input_tensor_names_;
++  }
++  const std::vector<std::string> &OutputTensorNames() const {
++    return output_tensor_names_;
++  }
++
++  std::vector<int32_t> TensorShape(const std::string &name) const {
++    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
++      if (input_tensor_names_[i] == name) {
++        return input_tensor_shapes_[i];
++      }
++    }
++
++    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
++      if (output_tensor_names_[i] == name) {
++        return output_tensor_shapes_[i];
++      }
++    }
++
++    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
++    return {};
++  }
++
++  int32_t TensorSizeInBytes(const std::string &name) const {
++    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
++      if (input_tensor_names_[i] == name) {
++        return input_tensors_[i].Size();
++      }
++    }
++
++    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
++      if (output_tensor_names_[i] == name) {
++        return output_tensors_[i].Size();
++      }
++    }
++
++    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
++    return 0;
++  }
++
++  bool HasTensor(const std::string &name) const {
++    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
++      if (input_tensor_names_[i] == name) {
++        return true;
++      }
++    }
++
++    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
++      if (output_tensor_names_[i] == name) {
++        return true;
++      }
++    }
++
++    return false;
++  }
++
++  template <typename T>
++  bool SetInputTensorData(const std::string &name, const T *p,
++                          int32_t n) const {
++    for (size_t i = 0; i < input_tensor_names_.size(); ++i) {
++      if (input_tensor_names_[i] == name) {
++        if (n * sizeof(T) != input_tensors_[i].Size()) {
++          SHERPA_ONNX_LOGE("Expected size: %zu, given: %zu",
++                           input_tensors_[i].Size(), n * sizeof(T));
++          return false;
++        }
++
++        auto ret =
++            axclrtMemcpy(input_tensors_[i].Get(), p, input_tensors_[i].Size(),
++                         AXCL_MEMCPY_HOST_TO_DEVICE);
++        if (ret != 0) {
++          SHERPA_ONNX_LOGE(
++              "Failed to call axclrtMemcpy(). tensor name: '%s', return code: "
++              "%d",
++              name.c_str(), static_cast<int32_t>(ret));
++          return false;
++        }
++
++        return true;
++      }
++    }
++
++    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
++
++    return false;
++  }
++
++  std::vector<float> GetOutputTensorData(const std::string &name) const {
++    for (size_t i = 0; i < output_tensor_names_.size(); ++i) {
++      if (output_tensor_names_[i] == name) {
++        size_t bytes = output_tensors_[i].Size();
++        std::vector<float> out(bytes / sizeof(float));
++
++        auto ret = axclrtMemcpy(out.data(), output_tensors_[i].Get(), bytes,
++                                AXCL_MEMCPY_DEVICE_TO_HOST);
++        if (ret != 0) {
++          SHERPA_ONNX_LOGE(
++              "Failed to call axclrtMemcpy(). tensor name: '%s', return code: "
++              "%d",
++              name.c_str(), static_cast<int32_t>(ret));
++          return {};
++        }
++
++        return out;
++      }
++    }
++
++    SHERPA_ONNX_LOGE("Found no tensor with name: '%s'", name.c_str());
++
++    return {};
++  }
++
++  bool Run() const {
++    uint32_t group = 0;
++    auto ret =
++        axclrtEngineExecute(model_id_, context_id_, group, *engine_io_guard_);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call axclrtEngineExecute(), return code: %d",
++                       static_cast<int32_t>(ret));
++      return false;
++    }
++    return true;
++  }
++
++  bool IsInitialized() const { return model_loaded_; }
++
++ private:
++  bool SetDevice(int32_t device_id) {
++    axclrtDeviceList lst;
++    auto ret = axclrtGetDeviceList(&lst);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtGetDeviceList(). Return code is: %d",
++          static_cast<int32_t>(ret));
++      return false;
++    }
++
++    if (lst.num == 0) {
++      SHERPA_ONNX_LOGE("Found 0 device.");
++      return false;
++    }
++
++    // device_id counts from 0
++    if (device_id < 0 || device_id >= lst.num) {
++      SHERPA_ONNX_LOGE("Invalid device_id: %d. Valid range: 0-%d", device_id,
++                       lst.num - 1);
++      return false;
++    }
++
++    ret = axclrtSetDevice(lst.devices[device_id]);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call axclrtSetDevice(). Return code is: %d",
++                       static_cast<int32_t>(ret));
++      return false;
++    }
++
++    return true;
++  }
++
++  void InitEngine() { engine_guard_ = std::make_unique<AxclEngineGuard>(); }
++
++  void PostInit() {
++    InitContext();
++
++    io_info_guard_ = std::make_unique<AxclEngineIOInfoGuard>(model_id_);
++
++    int32_t count = 0;
++    auto ret = axclrtEngineGetShapeGroupsCount(*io_info_guard_, &count);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineGetShapeGroupsCount(). Return code is: "
++          "%d",
++          static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (count != 1) {
++      SHERPA_ONNX_LOGE("Only support 1 group at present. Given: %d", count);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    engine_io_guard_ = std::make_unique<AxclEngineIOGuard>(*io_info_guard_);
++
++    InitInput();
++    InitOutput();
++  }
++
++  void InitContext() {
++    // Note(fangjun): No need to destroy context_id_
++    auto ret = axclrtEngineCreateContext(model_id_, &context_id_);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE(
++          "Failed to call axclrtEngineCreateContext(). Return code is: %d",
++          static_cast<int32_t>(ret));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  void InitInput() {
++    uint32_t group = 0;
++
++    int32_t num_inputs = axclrtEngineGetNumInputs(*io_info_guard_);
++
++    input_tensor_names_.resize(num_inputs);
++    input_tensor_shapes_.reserve(num_inputs);
++
++    for (int32_t i = 0; i < num_inputs; ++i) {
++      size_t size_in_bytes =
++          axclrtEngineGetInputSizeByIndex(*io_info_guard_, group, i);
++      input_tensors_.emplace_back(size_in_bytes, AXCL_MEM_MALLOC_HUGE_FIRST);
++
++      axclrtEngineIODims dims;
++      auto ret = axclrtEngineGetInputDims(*io_info_guard_, group, i, &dims);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineGetInputDims(). Return code is: %d",
++            static_cast<int32_t>(ret));
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      input_tensor_shapes_.emplace_back(dims.dims, dims.dims + dims.dimCount);
++
++      input_tensor_names_[i] =
++          axclrtEngineGetInputNameByIndex(*io_info_guard_, i);
++
++      ret = axclrtEngineSetInputBufferByIndex(*engine_io_guard_, i,
++                                              input_tensors_[i], size_in_bytes);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineSetInputBufferByIndex(). Return code "
++            "is: %d",
++            static_cast<int32_t>(ret));
++        SHERPA_ONNX_EXIT(-1);
++      }
++    }
++  }
++
++  void InitOutput() {
++    uint32_t group = 0;
++
++    int32_t num_outputs = axclrtEngineGetNumOutputs(*io_info_guard_);
++
++    output_tensor_names_.resize(num_outputs);
++    output_tensor_shapes_.reserve(num_outputs);
++
++    for (int32_t i = 0; i < num_outputs; ++i) {
++      auto size_in_bytes =
++          axclrtEngineGetOutputSizeByIndex(*io_info_guard_, group, i);
++      output_tensors_.emplace_back(size_in_bytes, AXCL_MEM_MALLOC_HUGE_FIRST);
++
++      axclrtEngineIODims dims;
++      auto ret = axclrtEngineGetOutputDims(*io_info_guard_, group, i, &dims);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineGetOutputDims(). Return code is: %d",
++            static_cast<int32_t>(ret));
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      output_tensor_shapes_.emplace_back(dims.dims, dims.dims + dims.dimCount);
++      output_tensor_names_[i] =
++          axclrtEngineGetOutputNameByIndex(*io_info_guard_, i);
++
++      ret = axclrtEngineSetOutputBufferByIndex(
++          *engine_io_guard_, i, output_tensors_[i], size_in_bytes);
++      if (ret != 0) {
++        SHERPA_ONNX_LOGE(
++            "Failed to call axclrtEngineSetOutputBufferByIndex(). Return code "
++            "is: %d",
++            static_cast<int32_t>(ret));
++        SHERPA_ONNX_EXIT(-1);
++      }
++    }
++  }
++
++ private:
++  AxclManager manager_;
++  std::unique_ptr<AxclEngineGuard> engine_guard_;
++  std::unique_ptr<AxclEngineIOGuard> engine_io_guard_;
++  std::unique_ptr<AxclEngineIOInfoGuard> io_info_guard_;
++
++  bool model_loaded_ = false;
++  uint64_t model_id_ = 0;
++  uint64_t context_id_ = 0;
++
++  std::vector<std::string> input_tensor_names_;
++  std::vector<std::string> output_tensor_names_;
++
++  std::vector<AxclDevicePtr> input_tensors_;
++  std::vector<AxclDevicePtr> output_tensors_;
++
++  std::vector<std::vector<int32_t>> input_tensor_shapes_;
++  std::vector<std::vector<int32_t>> output_tensor_shapes_;
++};
++
++AxclModel::AxclModel(const std::string &filename, int32_t device_id /*= 0*/)
++    : impl_(std::make_unique<Impl>(filename, device_id)) {}
++
++AxclModel::AxclModel(const void *cpu_buf, size_t buf_len_in_bytes,
++                     int32_t device_id /*= 0*/)
++    : impl_(std::make_unique<Impl>(cpu_buf, buf_len_in_bytes, device_id)) {}
++
++AxclModel::~AxclModel() = default;
++
++const std::vector<std::string> &AxclModel::InputTensorNames() const {
++  return impl_->InputTensorNames();
++}
++const std::vector<std::string> &AxclModel::OutputTensorNames() const {
++  return impl_->OutputTensorNames();
++}
++
++std::vector<int32_t> AxclModel::TensorShape(const std::string &name) const {
++  return impl_->TensorShape(name);
++}
++
++int32_t AxclModel::TensorSizeInBytes(const std::string &name) const {
++  return impl_->TensorSizeInBytes(name);
++}
++
++bool AxclModel::HasTensor(const std::string &name) const {
++  return impl_->HasTensor(name);
++}
++
++bool AxclModel::SetInputTensorData(const std::string &name, const float *p,
++                                   int32_t n) const {
++  return impl_->SetInputTensorData(name, p, n);
++}
++
++bool AxclModel::SetInputTensorData(const std::string &name, const int32_t *p,
++                                   int32_t n) const {
++  return impl_->SetInputTensorData(name, p, n);
++}
++
++std::vector<float> AxclModel::GetOutputTensorData(
++    const std::string &name) const {
++  return impl_->GetOutputTensorData(name);
++}
++
++bool AxclModel::Run() const { return impl_->Run(); }
++
++bool AxclModel::IsInitialized() const { return impl_->IsInitialized(); }
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/axcl-model.h b/sherpa-onnx/csrc/axcl/axcl-model.h
+new file mode 100644
+index 00000000..e6685bae
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/axcl-model.h
+@@ -0,0 +1,49 @@
++// sherpa-onnx/csrc/axcl/axcl-model.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
++#define SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
++
++#include <cstdint>
++#include <memory>
++#include <string>
++#include <vector>
++
++namespace sherpa_onnx {
++
++class AxclModel {
++ public:
++  explicit AxclModel(const std::string &filename, int32_t device_id = 0);
++
++  AxclModel(const void *cpu_buf, size_t buf_len_in_bytes,
++            int32_t device_id = 0);
++  ~AxclModel();
++
++  const std::vector<std::string> &InputTensorNames() const;
++  const std::vector<std::string> &OutputTensorNames() const;
++
++  std::vector<int32_t> TensorShape(const std::string &name) const;
++  int32_t TensorSizeInBytes(const std::string &name) const;
++
++  bool HasTensor(const std::string &name) const;
++
++  bool SetInputTensorData(const std::string &name, const float *p,
++                          int32_t n) const;
++
++  bool SetInputTensorData(const std::string &name, const int32_t *p,
++                          int32_t n) const;
++
++  std::vector<float> GetOutputTensorData(const std::string &name) const;
++
++  bool Run() const;
++  bool IsInitialized() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_AXCL_MODEL_H_
+diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+index 54461590..db8dd643 100644
+--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
++++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+@@ -7,10 +7,11 @@
+ #include <algorithm>
+ #include <array>
+ #include <cstring>
++#include <memory>
+ #include <utility>
+ #include <vector>
+ 
+-#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
++#include "sherpa-onnx/csrc/axcl/axcl-model.h"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+@@ -18,17 +19,18 @@ namespace sherpa_onnx {
+ 
+ class OfflineSenseVoiceModelAxcl::Impl {
+  public:
+-  ~Impl() { runner_.release(); }
+-
+   explicit Impl(const OfflineModelConfig &config) : config_(config) {
+-    auto buf = ReadFile(config_.sense_voice.model);
+-    Init(buf.data(), buf.size());
++    model_ = std::make_unique<AxclModel>(config_.sense_voice.model);
++
++    PostInit();
+   }
+ 
+   template <typename Manager>
+   Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+     auto buf = ReadFile(mgr, config_.sense_voice.model);
+-    Init(buf.data(), buf.size());
++    model_ = std::make_unique<AxclModel>(buf.data(), buf.size());
++
++    PostInit();
+   }
+ 
+   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+@@ -40,86 +42,23 @@ class OfflineSenseVoiceModelAxcl::Impl {
+     features = ApplyLFR(std::move(features));
+     std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
+ 
+-    // input 0: features
+-    auto &in0 = runner_.get_input(0);
+-    size_t bytes0 = in0.nSize;
+-    if (bytes0 != features.size() * sizeof(float)) {
+-      SHERPA_ONNX_LOGE(
+-          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
+-          in0.nSize, features.size() * sizeof(float));
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-    std::memcpy(in0.pVirAddr, features.data(), bytes0);
+-
+-    auto &in1 = runner_.get_input(1);
+-    size_t bytes1 = in1.nSize;
+-    if (bytes1 != prompt.size() * sizeof(int32_t)) {
+-      SHERPA_ONNX_LOGE(
+-          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
+-          in1.nSize, prompt.size() * sizeof(int32_t));
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-    std::memcpy(in1.pVirAddr, prompt.data(), bytes1);
+-
+-    int ret = runner_.inference();
+-    if (ret != 0) {
+-      SHERPA_ONNX_LOGE("ax_runner_axcl inference failed, ret = %d", ret);
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+-    // output 0
+-    auto &out0 = runner_.get_output(0);
+-    size_t out_elems = out0.nSize / sizeof(float);
+-    std::vector<float> out(out_elems);
+-    std::memcpy(out.data(), out0.pVirAddr, out0.nSize);
+-    return out;
++    model_->SetInputTensorData("x", features.data(), features.size());
++    model_->SetInputTensorData("prompt", prompt.data(), prompt.size());
++    model_->Run();
++    return model_->GetOutputTensorData("logits");
+   }
+ 
+  private:
+-  void Init(void *model_data, size_t model_data_length) {
+-    {
+-      if (auto ret = axclInit(0); 0 != ret) {
+-        fprintf(stderr, "Init AXCL failed{0x%8x}.\n", ret);
+-        return;
+-      }
+-      axclrtDeviceList lst;
+-      if (const auto ret = axclrtGetDeviceList(&lst);
+-          0 != ret || 0 == lst.num) {
+-        fprintf(stderr,
+-                "Get AXCL device failed{0x%8x}, find total %d device.\n", ret,
+-                lst.num);
+-        return;
+-      }
+-      if (const auto ret = axclrtSetDevice(lst.devices[0]); 0 != ret) {
+-        fprintf(stderr, "Set AXCL device failed{0x%8x}.\n", ret);
+-        return;
+-      }
+-      int ret = axclrtEngineInit(AXCL_VNPU_DISABLE);
+-      if (0 != ret) {
+-        fprintf(stderr, "axclrtEngineInit %d\n", ret);
+-        return;
+-      }
+-    }
+-
+-    int ret =
+-        runner_.init(reinterpret_cast<char *>(model_data), model_data_length);
+-    if (ret != 0) {
+-      SHERPA_ONNX_LOGE("Init ax_runner_axcl failed, ret = %d", ret);
++  void PostInit() {
++    if (!model_->IsInitialized()) {
++      SHERPA_ONNX_LOGE("Failed to initialize the model with '%s'",
++                       config_.sense_voice.model.c_str());
+       SHERPA_ONNX_EXIT(-1);
+     }
+ 
+-    auto &in0 = runner_.get_input(0);
+-    if (in0.vShape.size() < 2) {
+-      SHERPA_ONNX_LOGE(
+-          "Input tensor rank is too small (rank = %zu). Shape vector is empty "
+-          "or has only 1 dim.",
+-          in0.vShape.size());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-    num_input_frames_ = in0.vShape[1];
++    num_input_frames_ = model_->TensorShape("x")[1];
+ 
+     if (config_.debug) {
+-      SHERPA_ONNX_LOGE("Axcl SenseVoice model init done with ax_runner_axcl.");
+       SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
+     }
+   }
+@@ -156,7 +95,7 @@ class OfflineSenseVoiceModelAxcl::Impl {
+ 
+  private:
+   OfflineModelConfig config_;
+-  ax_runner_axcl runner_;
++  std::unique_ptr<AxclModel> model_;
+   OfflineSenseVoiceModelMetaData meta_data_;
+   int32_t num_input_frames_ = -1;
+ };
+@@ -193,4 +132,4 @@ template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
+     NativeResourceManager *mgr, const OfflineModelConfig &config);
+ #endif
+ 
+-}  // namespace sherpa_onnx
+\ No newline at end of file
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+index 11a7503b..2ef47ee8 100644
+--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
++++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+@@ -8,8 +8,6 @@
+ #include <memory>
+ #include <vector>
+ 
+-#include "axcl.h"  // NOLINT
+-#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+ 
+diff --git a/sherpa-onnx/csrc/axcl/utils.cc b/sherpa-onnx/csrc/axcl/utils.cc
+new file mode 100644
+index 00000000..0dec25bc
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/utils.cc
+@@ -0,0 +1,41 @@
++// sherpa-onnx/csrc/axcl/utils.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axcl/utils.h"
++
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++AxclDevicePtr::AxclDevicePtr(
++    size_t size,
++    axclrtMemMallocPolicy policy /*= AXCL_MEM_MALLOC_HUGE_FIRST*/) {
++  auto ret = axclrtMalloc(&p_, size, policy);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("Failed to call axclrtMalloc(). Return code: %d",
++                     static_cast<int32_t>(ret));
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  size_ = size;
++}
++
++void AxclDevicePtr::Release() {
++  if (!p_) {
++    return;
++  }
++
++  auto ret = axclrtFree(p_);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("Failed to call axclrtFree(). Return code: %d",
++                     static_cast<int32_t>(ret));
++    SHERPA_ONNX_EXIT(-1);
++  }
++  p_ = nullptr;
++  size_ = 0;
++}
++
++AxclDevicePtr::~AxclDevicePtr() { Release(); }
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axcl/utils.h b/sherpa-onnx/csrc/axcl/utils.h
+new file mode 100644
+index 00000000..e3ea50f8
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/utils.h
+@@ -0,0 +1,57 @@
++// sherpa-onnx/csrc/axcl/utils.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_UTILS_H_
++#define SHERPA_ONNX_CSRC_AXCL_UTILS_H_
++
++#include "axcl.h"  // NOLINT
++
++namespace sherpa_onnx {
++
++class AxclDevicePtr {
++ public:
++  explicit AxclDevicePtr(
++      size_t size, axclrtMemMallocPolicy policy = AXCL_MEM_MALLOC_HUGE_FIRST);
++
++  ~AxclDevicePtr();
++
++  AxclDevicePtr(const AxclDevicePtr &) = delete;
++  AxclDevicePtr &operator=(const AxclDevicePtr &) = delete;
++
++  AxclDevicePtr(AxclDevicePtr &&other) {
++    p_ = other.p_;
++    size_ = other.size_;
++
++    other.p_ = nullptr;
++    other.size_ = 0;
++  }
++  AxclDevicePtr &operator=(AxclDevicePtr &&other) {
++    if (this == &other) {
++      return *this;
++    }
++    Release();
++    p_ = other.p_;
++    size_ = other.size_;
++
++    other.p_ = nullptr;
++    other.size_ = 0;
++
++    return *this;
++  }
++
++  void Release();
++
++  void *Get() const { return p_; }
++  operator void *() { return p_; }
++
++  size_t Size() const { return size_; }
++
++ private:
++  void *p_ = nullptr;
++  size_t size_ = 0;  // in bytes
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_UTILS_H_
+
+commit 2202e2a59ba7687dd007afcd0b3050a3af7e5d6c
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Fri Dec 5 20:44:58 2025 +0800
+
+    Release v1.12.19 (#2868)
+
+diff --git a/.github/workflows/windows-x64-jni.yaml b/.github/workflows/windows-x64-jni.yaml
+index 2f9682f5..bf1e5468 100644
+--- a/.github/workflows/windows-x64-jni.yaml
++++ b/.github/workflows/windows-x64-jni.yaml
+@@ -81,9 +81,7 @@ jobs:
+           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-win-x64-jni
+           mkdir -p $dst
+ 
+-          cp -a build/install/bin $dst/ || true
+           cp -a build/install/lib $dst/ || true
+-          cp -a build/install/include $dst/ || true
+ 
+           tar cjvf ${dst}.tar.bz2 $dst
+ 
+diff --git a/CHANGELOG.md b/CHANGELOG.md
+index 78bf0cb7..439b0db9 100644
+--- a/CHANGELOG.md
++++ b/CHANGELOG.md
+@@ -1,3 +1,24 @@
++## 1.12.19
++
++* Fix building without TTS for C API (#2838)
++* [ZipVoice] Fix english tokenization error (#2834)
++* Add simulate streaming ASR Python example for Paraformer (#2839)
++* Fix building JNI for Windows (#2840)
++* Avoid NaN in NeMo speaker embedding models. (#2844)
++* Add spacemit ort ep for spacemit riscv cpus (#2837)
++* Add token-level confidence scores (ys_probs) for offline transducer models (#2843)
++* Fix token log probabilities in offline transducer modified beam search decoder (#2846)
++* Support AXERA ax630, ax650, and axcl backends. (#2849)
++* Refactor axera npu examples (#2850)
++* Fix matcha tts zh-en model (#2851)
++* Fix the English part for Matcha TTS. (#2853)
++* Refactor text-utils (#2855)
++* Fix matcha tts (#2856)
++* Add a space between English words for Matcha zh-en TTS (#2858)
++* Fix punctuations in matcha zh-en tts (#2859)
++* Upload matcha tts zh-en model (#2865)
++* Fix the discrepancy with the Silero VAD isSpeech logic (#2863)
++
+ ## 1.12.18
+ 
+ * Fix building wheels (#2786)
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 20f63f71..84be55ea 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -14,7 +14,7 @@ project(sherpa-onnx)
+ # Remember to update
+ # ./CHANGELOG.md
+ # ./new-release.sh
+-set(SHERPA_ONNX_VERSION "1.12.18")
++set(SHERPA_ONNX_VERSION "1.12.19")
+ 
+ # Disable warning about
+ #
+diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
+index 214a87cd..2ac752f2 100644
+--- a/android/SherpaOnnx/app/build.gradle
++++ b/android/SherpaOnnx/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
+index 214a87cd..2ac752f2 100644
+--- a/android/SherpaOnnx2Pass/app/build.gradle
++++ b/android/SherpaOnnx2Pass/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
+index 7d1505bd..d7942a39 100644
+--- a/android/SherpaOnnxAar/README.md
++++ b/android/SherpaOnnxAar/README.md
+@@ -4,8 +4,8 @@
+ git clone https://github.com/k2-fsa/sherpa-onnx
+ cd sherpa-onnx
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-v1.12.18-android.tar.bz2
+-tar xvf sherpa-onnx-v1.12.18-android.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-v1.12.19-android.tar.bz2
++tar xvf sherpa-onnx-v1.12.19-android.tar.bz2
+ 
+ cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
+ cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
+@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
+ 
+ ./gradlew :sherpa_onnx:assembleRelease
+ ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
+-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.18.aar
++cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.19.aar
+ ```
+diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+index cf40dd2a..dafdd504 100644
+--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+index 27ad7ac2..29f894c1 100644
+--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
+         minSdk = 26
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
+index f9d0c62b..c0b95fdf 100644
+--- a/android/SherpaOnnxJavaDemo/app/build.gradle
++++ b/android/SherpaOnnxJavaDemo/app/build.gradle
+@@ -9,8 +9,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 28
+         targetSdk 34
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+@@ -34,5 +34,5 @@ dependencies {
+     implementation 'pub.devrel:easypermissions:3.0.0'
+     implementation 'androidx.core:core-ktx:1.7.0'
+     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
+-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.18'
++    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.19'
+ }
+diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
+index 214a87cd..2ac752f2 100644
+--- a/android/SherpaOnnxKws/app/build.gradle
++++ b/android/SherpaOnnxKws/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+index 43b998bf..571a805f 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+index 3a5e95cc..ac309729 100644
+--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
+         minSdk = 28
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+@@ -58,7 +58,7 @@ dependencies {
+     implementation(libs.compose.foundation)
+     implementation(libs.activity.compose)
+     implementation(libs.core.splashscreen)
+-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.18")
++    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.19")
+     androidTestImplementation(platform(libs.compose.bom))
+     androidTestImplementation(libs.ui.test.junit4)
+     debugImplementation(libs.ui.tooling)
+diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+index 34f1fcd5..fe528faa 100644
+--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+index 1af89d4a..12e210ee 100644
+--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+index 41a2bdf9..fc0c03e1 100644
+--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.slid"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
+index b1151b94..0b6ba6c0 100644
+--- a/android/SherpaOnnxTts/app/build.gradle
++++ b/android/SherpaOnnxTts/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+index 53ad4569..6a8978b9 100644
+--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
++++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251127
+-        versionName = "1.12.18"
++        versionCode = 20251205
++        versionName = "1.12.19"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
+index b65311a6..66bf1f41 100644
+--- a/android/SherpaOnnxVad/app/build.gradle
++++ b/android/SherpaOnnxVad/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
+index b65311a6..66bf1f41 100644
+--- a/android/SherpaOnnxVadAsr/app/build.gradle
++++ b/android/SherpaOnnxVadAsr/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
+index 45e27edd..3a29c0b1 100644
+--- a/android/SherpaOnnxWebSocket/app/build.gradle
++++ b/android/SherpaOnnxWebSocket/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251127
+-        versionName "1.12.18"
++        versionCode 20251205
++        versionName "1.12.19"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/build-ios-shared.sh b/build-ios-shared.sh
+index a52287f5..df5c6037 100755
+--- a/build-ios-shared.sh
++++ b/build-ios-shared.sh
+@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
+ 	<key>CFBundlePackageType</key>
+ 	<string>FMWK</string>
+ 	<key>CFBundleShortVersionString</key>
+-	<string>1.12.18</string>
++	<string>1.12.19</string>
+ 	<key>CFBundleSupportedPlatforms</key>
+ 	<array>
+ 		<string>iPhoneOS</string>
+diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
+index 9eec66cc..b77879a4 100644
+--- a/dart-api-examples/add-punctuations/pubspec.yaml
++++ b/dart-api-examples/add-punctuations/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
+index c48cb85a..396260c2 100644
+--- a/dart-api-examples/audio-tagging/pubspec.yaml
++++ b/dart-api-examples/audio-tagging/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
+index 4b90ab48..04eeb417 100644
+--- a/dart-api-examples/keyword-spotter/pubspec.yaml
++++ b/dart-api-examples/keyword-spotter/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
+index 4f89b81f..fcae6b01 100644
+--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
+index 1850039f..8a1fadb3 100644
+--- a/dart-api-examples/speaker-diarization/pubspec.yaml
++++ b/dart-api-examples/speaker-diarization/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
+index e5122ffa..ede5c6e4 100644
+--- a/dart-api-examples/speaker-identification/pubspec.yaml
++++ b/dart-api-examples/speaker-identification/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+index 0292fd08..f42abe91 100644
+--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
++++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
+index 90ff0b20..0564cda2 100644
+--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
++++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
+index f6abcf2f..377dd7bb 100644
+--- a/dart-api-examples/streaming-asr/pubspec.yaml
++++ b/dart-api-examples/streaming-asr/pubspec.yaml
+@@ -11,7 +11,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
+index d551987f..cb072fd3 100644
+--- a/dart-api-examples/tts/pubspec.yaml
++++ b/dart-api-examples/tts/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+index de635d33..d581ebee 100644
+--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
+index 83a6e030..403fd353 100644
+--- a/dart-api-examples/vad/pubspec.yaml
++++ b/dart-api-examples/vad/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+index c3588163..70fbb929 100644
+--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
++++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.18
++version: 1.12.19
+ 
+ topics:
+   - speech-recognition
+@@ -31,7 +31,7 @@ dependencies:
+   record: 6.0.0
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
+index 0a259b1d..e3527e8a 100644
+--- a/flutter-examples/streaming_asr/pubspec.yaml
++++ b/flutter-examples/streaming_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.18
++version: 1.12.19
+ 
+ topics:
+   - speech-recognition
+@@ -30,7 +30,7 @@ dependencies:
+   record: ^6.1.2
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
+index 21d23eb4..d98a541b 100644
+--- a/flutter-examples/tts/pubspec.yaml
++++ b/flutter-examples/tts/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none' # Remove this line if you wish to publish to pub.dev
+ 
+-version: 1.12.18
++version: 1.12.19
+ 
+ environment:
+   sdk: ">=2.17.0 <4.0.0"
+@@ -18,7 +18,7 @@ dependencies:
+   cupertino_icons: ^1.0.6
+   path_provider: ^2.1.3
+   path: ^1.9.0
+-  sherpa_onnx: ^1.12.18
++  sherpa_onnx: ^1.12.19
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   url_launcher: 6.2.6
+diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
+index 3491fead..b65ccfeb 100644
+--- a/flutter/sherpa_onnx/pubspec.yaml
++++ b/flutter/sherpa_onnx/pubspec.yaml
+@@ -17,7 +17,7 @@ topics:
+   - voice-activity-detection
+ 
+ # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+-version: 1.12.18
++version: 1.12.19
+ 
+ homepage: https://github.com/k2-fsa/sherpa-onnx
+ 
+@@ -30,23 +30,23 @@ dependencies:
+   flutter:
+     sdk: flutter
+ 
+-  sherpa_onnx_android: ^1.12.18
++  sherpa_onnx_android: ^1.12.19
+   # sherpa_onnx_android:
+   #   path: ../sherpa_onnx_android
+ 
+-  sherpa_onnx_macos: ^1.12.18
++  sherpa_onnx_macos: ^1.12.19
+   # sherpa_onnx_macos:
+   #   path: ../sherpa_onnx_macos
+ 
+-  sherpa_onnx_linux: ^1.12.18
++  sherpa_onnx_linux: ^1.12.19
+   # sherpa_onnx_linux:
+   #   path: ../sherpa_onnx_linux
+ 
+-  sherpa_onnx_windows: ^1.12.18
++  sherpa_onnx_windows: ^1.12.19
+   # sherpa_onnx_windows:
+   #   path: ../sherpa_onnx_windows
+ 
+-  sherpa_onnx_ios: ^1.12.18
++  sherpa_onnx_ios: ^1.12.19
+   # sherpa_onnx_ios:
+   #   path: ../sherpa_onnx_ios
+ 
+diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+index a4b7b606..ffd952bf 100644
+--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
++++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+@@ -7,7 +7,7 @@
+ # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_ios'
+-  s.version          = '1.12.18'
++  s.version          = '1.12.19'
+   s.summary          = 'A new Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ A new Flutter FFI plugin project.
+diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+index dcd8f6b5..ec1ea5fd 100644
+--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
++++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+@@ -4,7 +4,7 @@
+ #
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_macos'
+-  s.version          = '1.12.18'
++  s.version          = '1.12.19'
+   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ sherpa-onnx Flutter FFI plugin project.
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+index b04e4790..2a5e5dbf 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+@@ -1,7 +1,7 @@
+ /**
+  * Use these variables when you tailor your ArkTS code. They must be of the const type.
+  */
+-export const HAR_VERSION = '1.12.18';
++export const HAR_VERSION = '1.12.19';
+ export const BUILD_MODE_NAME = 'debug';
+ export const DEBUG = true;
+ export const TARGET_NAME = 'default';
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+index 9467e26d..dde223ef 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
+ 
+ ```
+   "dependencies": {
+-    "sherpa_onnx": "1.12.18",
++    "sherpa_onnx": "1.12.19",
+   },
+ ```
+ 
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+index 7438ad28..f3e43a76 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+@@ -1,6 +1,6 @@
+ {
+   "name": "sherpa_onnx",
+-  "version": "1.12.18",
++  "version": "1.12.19",
+   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
+   "main": "Index.ets",
+   "author": "The next-gen Kaldi team",
+diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+index 4fc965d4..c703cba9 100644
+--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.18"
++    "sherpa_onnx": "1.12.19"
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+index 88735c1c..21ccbcef 100644
+--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.18",
++    "sherpa_onnx": "1.12.19",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+index 88735c1c..21ccbcef 100644
+--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.18",
++    "sherpa_onnx": "1.12.19",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+index 88735c1c..21ccbcef 100644
+--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.18",
++    "sherpa_onnx": "1.12.19",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+index dff1eb6b..af0510b9 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
++++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+@@ -1,6 +1,6 @@
+ # Introduction
+ 
+-Please download ./sherpa_onnx-v1.12.18.har
++Please download ./sherpa_onnx-v1.12.19.har
+ from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
+ 
+ Hint: For users who have no access to huggingface, please use
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+index cf0d6dee..c24b1baf 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+@@ -7,7 +7,7 @@
+   "license": "",
+   "dependencies": {
+     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
+-    "sherpa_onnx": "1.12.18",
++    "sherpa_onnx": "1.12.19",
+   }
+ }
+ 
+diff --git a/jitpack.yml b/jitpack.yml
+index bd8c3c5d..5591d3ac 100644
+--- a/jitpack.yml
++++ b/jitpack.yml
+@@ -2,8 +2,8 @@ jdk:
+   - openjdk17
+ 
+ before_install:
+-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-1.12.18.aar
++  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-1.12.19.aar
+ 
+ install:
+-  - FILE="-Dfile=sherpa-onnx-1.12.18.aar"
+-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.18 -Dpackaging=aar -DgeneratePom=true
++  - FILE="-Dfile=sherpa-onnx-1.12.19.aar"
++  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.19 -Dpackaging=aar -DgeneratePom=true
+diff --git a/mfc-examples/README.md b/mfc-examples/README.md
+index 683034ac..970c9cc3 100644
+--- a/mfc-examples/README.md
++++ b/mfc-examples/README.md
+@@ -5,9 +5,9 @@ for speech recognition.
+ 
+ |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
+ |---------|--------------------|-------------------|------------|
+-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x86-v1.12.18.exe)| Non-streaming speech recognition|
+-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x86-v1.12.18.exe)| Streaming speech recognition|
+-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x86-v1.12.18.exe)| Non-streaming text to speech|
++|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-asr-x86-v1.12.19.exe)| Non-streaming speech recognition|
++|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-streaming-asr-x86-v1.12.19.exe)| Streaming speech recognition|
++|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x64-v1.12.19.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.19/sherpa-onnx-non-streaming-tts-x86-v1.12.19.exe)| Non-streaming text to speech|
+ 
+ Caution: You need to use Windows and install Visual Studio 2022 in order to
+ compile it.
+diff --git a/new-release.sh b/new-release.sh
+index 5d4a89c5..4d2d2280 100755
+--- a/new-release.sh
++++ b/new-release.sh
+@@ -2,11 +2,11 @@
+ 
+ set -ex
+ 
+-old_version_code=20251113
+-new_version_code=20251127
++old_version_code=20251127
++new_version_code=20251205
+ 
+-old_version="1\.12\.17"
+-new_version="1\.12\.18"
++old_version="1\.12\.18"
++new_version="1\.12\.19"
+ 
+ replace_str="s/$old_version/$new_version/g"
+ 
+diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
+index 3555e6ed..d24f1105 100644
+--- a/nodejs-addon-examples/package.json
++++ b/nodejs-addon-examples/package.json
+@@ -1,5 +1,5 @@
+ {
+   "dependencies": {
+-    "sherpa-onnx-node": "^1.12.18"
++    "sherpa-onnx-node": "^1.12.19"
+   }
+ }
+diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
+index db474390..af3538c6 100644
+--- a/nodejs-examples/package.json
++++ b/nodejs-examples/package.json
+@@ -2,7 +2,7 @@
+   "dependencies": {
+     "mic": "^2.1.2",
+     "naudiodon2": "^2.4.0",
+-    "sherpa-onnx": "^1.12.18",
++    "sherpa-onnx": "^1.12.19",
+     "wav": "^1.0.2"
+   }
+ }
+diff --git a/pom.xml b/pom.xml
+index f54d3c84..0d84b015 100644
+--- a/pom.xml
++++ b/pom.xml
+@@ -4,7 +4,7 @@
+     <modelVersion>4.0.0</modelVersion>
+     <groupId>com.k2fsa.sherpa.onnx</groupId>
+     <artifactId>sherpa-onnx-android</artifactId>
+-    <version>1.12.18</version>
++    <version>1.12.19</version>
+     <url>https://github.com/k2-fsa/sherpa-onnx</url>
+     <packaging>pom</packaging>
+     <description>First Android Library</description>
+diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
+index c86f3727..e9a561da 100644
+--- a/scripts/wheel/sherpa-onnx-bin/setup.py
++++ b/scripts/wheel/sherpa-onnx-bin/setup.py
+@@ -13,7 +13,7 @@ print("bin_files", bin_files)
+ 
+ setup(
+     name="sherpa-onnx-bin",
+-    version="1.12.18",
++    version="1.12.19",
+     description="Binary executables for sherpa-onnx",
+     author="The sherpa-onnx development team",
+     url="https://github.com/k2-fsa/sherpa-onnx",
+@@ -23,7 +23,7 @@ setup(
+     packages=[],
+     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
+     install_requires=[
+-        "sherpa-onnx-core==1.12.18",
++        "sherpa-onnx-core==1.12.19",
+     ],
+     classifiers=[
+         "Programming Language :: Python :: 3",
+diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
+index 456e2b7f..d053d293 100644
+--- a/scripts/wheel/sherpa-onnx-core/setup.py
++++ b/scripts/wheel/sherpa-onnx-core/setup.py
+@@ -23,7 +23,7 @@ def get_binaries():
+ 
+ setup(
+     name="sherpa-onnx-core",
+-    version="1.12.18",
++    version="1.12.19",
+     description="Core shared libraries for sherpa-onnx",
+     packages=["sherpa_onnx"],
+     include_package_data=True,
+diff --git a/setup.py b/setup.py
+index 329bddd4..63991827 100644
+--- a/setup.py
++++ b/setup.py
+@@ -109,7 +109,7 @@ setuptools.setup(
+         ],
+     },
+     license="Apache licensed, as found in the LICENSE file",
+-    install_requires=["sherpa-onnx-core==1.12.18"] if need_split_package() else None,
++    install_requires=["sherpa-onnx-core==1.12.19"] if need_split_package() else None,
+ )
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
+diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
+index e118dcc5..315d3d2e 100644
+--- a/sherpa-onnx/csrc/version.cc
++++ b/sherpa-onnx/csrc/version.cc
+@@ -7,17 +7,17 @@
+ namespace sherpa_onnx {
+ 
+ const char *GetGitDate() {
+-  static const char *date = "Thu Nov 27 15:24:39 2025";
++  static const char *date = "Fri Dec 5 11:45:41 2025";
+   return date;
+ }
+ 
+ const char *GetGitSha1() {
+-  static const char *sha1 = "7d1d2270";
++  static const char *sha1 = "e6a6599f";
+   return sha1;
+ }
+ 
+ const char *GetVersionStr() {
+-  static const char *version = "1.12.18";
++  static const char *version = "1.12.19";
+   return version;
+ }
+ 
+
+commit 32fbd3a6d68e8859348747e77e8b6e8a985119a1
+Author: ming030890 <67713085+ming030890@users.noreply.github.com>
+Date:   Fri Dec 5 04:00:29 2025 +0000
+
+    Fix the discrepancy with the Silero VAD isSpeech logic (#2863)
+
+diff --git a/sherpa-onnx/csrc/silero-vad-model-config.cc b/sherpa-onnx/csrc/silero-vad-model-config.cc
+index 2623a179..6f015e55 100644
+--- a/sherpa-onnx/csrc/silero-vad-model-config.cc
++++ b/sherpa-onnx/csrc/silero-vad-model-config.cc
+@@ -43,6 +43,12 @@ void SileroVadModelConfig::Register(ParseOptions *po) {
+       "512, 1024, 1536 samples for 16000 sample rate and 256, 512, 768 samples "
+       "for 8000 sample rate. Values other than these may affect model "
+       "performance!");
++
++  po->Register(
++    "silero-vad-neg-threshold", &neg_threshold,
++    "Negative threshold (noise threshold). If < 0, defaults to "
++    "(threshold - 0.15) with lower bound 0.01."
++  );
+ }
+ 
+ bool SileroVadModelConfig::Validate() const {
+@@ -110,7 +116,8 @@ std::string SileroVadModelConfig::ToString() const {
+   os << "min_silence_duration=" << min_silence_duration << ", ";
+   os << "min_speech_duration=" << min_speech_duration << ", ";
+   os << "max_speech_duration=" << max_speech_duration << ", ";
+-  os << "window_size=" << window_size << ")";
++  os << "window_size=" << window_size << ", ";
++  os << "neg_threshold=" << neg_threshold << ")";
+ 
+   return os.str();
+ }
+diff --git a/sherpa-onnx/csrc/silero-vad-model-config.h b/sherpa-onnx/csrc/silero-vad-model-config.h
+index 95b32e6b..e99c8c78 100644
+--- a/sherpa-onnx/csrc/silero-vad-model-config.h
++++ b/sherpa-onnx/csrc/silero-vad-model-config.h
+@@ -31,6 +31,12 @@ struct SileroVadModelConfig {
+   // the threshold value is reset to its original value.
+   float max_speech_duration = 20;  // in seconds
+ 
++  // Negative (exit) threshold for transitioning from speech  silence.
++  // If left as a negative value, the default Silero rule applies:
++  //     neg_threshold = max(threshold - 0.15f, 0.01f)
++  // This prevents the exit threshold from becoming negative when threshold < 0.15.
++  float neg_threshold = -1;
++
+   SileroVadModelConfig() = default;
+ 
+   void Register(ParseOptions *po);
+diff --git a/sherpa-onnx/csrc/silero-vad-model.cc b/sherpa-onnx/csrc/silero-vad-model.cc
+index 00a0f399..5b1bb27a 100644
+--- a/sherpa-onnx/csrc/silero-vad-model.cc
++++ b/sherpa-onnx/csrc/silero-vad-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/silero-vad-model.h"
+ 
++#include <algorithm>
+ #include <memory>
+ #include <string>
+ #include <utility>
+@@ -131,7 +132,13 @@ class SileroVadModel::Impl {
+       return false;
+     }
+ 
+-    if ((prob > threshold - 0.15) && triggered_) {
++    float neg_threshold;
++    if (config_.silero_vad.neg_threshold < 0) {
++        neg_threshold = std::max(threshold - 0.15f, 0.01f);
++    } else {
++        neg_threshold = std::max(config_.silero_vad.neg_threshold, 0.01f);
++    }
++    if ((prob > neg_threshold) && triggered_) {
+       // speaking
+       return true;
+     }
+
+commit e6a6599f59ca2dc8ef2e818995d741a4121c4757
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Fri Dec 5 11:45:41 2025 +0800
+
+    Upload matcha tts zh-en model (#2865)
+
+diff --git a/.github/workflows/export-matcha-zh-en.yaml b/.github/workflows/export-matcha-zh-en.yaml
+new file mode 100644
+index 00000000..154be1b4
+--- /dev/null
++++ b/.github/workflows/export-matcha-zh-en.yaml
+@@ -0,0 +1,163 @@
++name: export-matcha-zh-en-to-onnx
++
++on:
++  push:
++    branches:
++      - matcha-zh-en
++
++  workflow_dispatch:
++
++concurrency:
++  group: export-matcha-zh-en-to-onnx-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  export-matcha-zh-en-to-onnx:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: export matcha zh-en ${{ matrix.version }}
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        python-version: ["3.10"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python ${{ matrix.python-version }}
++        uses: actions/setup-python@v5
++        with:
++          python-version: ${{ matrix.python-version }}
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          pip install "numpy<=1.26.4" pypinyin soundfile \
++            sherpa-onnx -f https://k2-fsa.github.io/sherpa/onnx/cpu.html
++
++      - name: Generate samples
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        shell: bash
++        run: |
++          cd scripts/matcha-tts/zh-en
++
++          git config --global user.email "csukuangfj@gmail.com"
++          git config --global user.name "Fangjun Kuang"
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/matcha-icefall-zh-en.tar.bz2
++          tar xvf matcha-icefall-zh-en.tar.bz2
++          rm matcha-icefall-zh-en.tar.bz2
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
++
++          git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-tts-samples hf
++          mkdir -p ./hf/matcha/icefall-zh-en/mp3
++
++          ./generate_samples.py
++
++          pushd hf
++          git pull
++          git add .
++          git commit -m 'add samples for matcha tts zh en'
++          git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-tts-samples main
++          popd
++          rm -rf hf
++
++          ls -lh
++
++      - name: Run
++        shell: bash
++        run: |
++          cd scripts/matcha-tts/zh-en
++          curl -SL -O https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/model-steps-3.onnx
++          curl -SL -O https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocab_tts.txt
++
++          ./generate_tokens.py
++          ./generate_lexicon.py
++
++          curl -SL -o date-zh.fst https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/date.fst
++          curl -SL -o number-zh.fst  https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/number.fst
++          curl -SL -o phone-zh.fst https://huggingface.co/csukuangfj/icefall-tts-aishell3-vits-low-2024-04-06/resolve/main/data/phone.fst
++
++      - name: Collect results ${{ matrix.version }}
++        shell: bash
++        run: |
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/espeak-ng-data.tar.bz2
++          tar xf espeak-ng-data.tar.bz2
++          rm espeak-ng-data.tar.bz2
++
++          src=scripts/matcha-tts/zh-en
++          dst=matcha-icefall-zh-en
++
++          mkdir $dst
++
++          cp -a espeak-ng-data $dst/
++
++          cp -v $src/tokens.txt $dst
++          cp -v $src/lexicon.txt $dst
++          cp -v $src/model-steps-3.onnx $dst
++          cp -v $src/README.md $dst
++          cp -v $src/*.fst $dst
++
++          tar cjfv $dst.tar.bz2 $dst
++
++          ls -lh $dst.tar.bz2
++
++      - name: Publish to huggingface
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            rm -rf huggingface
++            export GIT_LFS_SKIP_SMUDGE=1
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++
++            git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/matcha-icefall-zh-en huggingface
++            cd huggingface
++            rm -rf ./*
++            git fetch
++            git pull
++
++            git lfs track "cmn_dict"
++            git lfs track "ru_dict" af_dict ar_dict da_dict en_dict fa_dict hu_dict ia_dict it_dict lb_dict phondata ta_dict ur_dict yue_dict
++
++            cp -a ../matcha-icefall-zh-en/* ./
++
++            git lfs track "*.onnx"
++            git add .
++
++            ls -lh
++
++            git status
++
++            git commit -m "add models"
++            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/matcha-icefall-zh-en main || true
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: tts-models
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          tag: tts-models
+diff --git a/scripts/matcha-tts/zh-en/README.md b/scripts/matcha-tts/zh-en/README.md
+index 0fba2f75..e46b40e4 100644
+--- a/scripts/matcha-tts/zh-en/README.md
++++ b/scripts/matcha-tts/zh-en/README.md
+@@ -6,9 +6,10 @@ https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/summary
+ Note that you have to use
+ vocos-16khz-univ.onnx
+ 
+-You can download it from 
++You can download it from
+  https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocos-16khz-univ.onnx
+ or
++ https://github.com/k2-fsa/sherpa-onnx/releases/download/vocoder-models/vocos-16khz-univ.onnx
+ 
+ ```
+ {'am': './model-steps-3.onnx', 'vocoder': './vocos-16khz-univ.onnx', 'tokens': './tokens.txt', 'lexicon': './lexicon.txt', 'text': '. It supports both English ', 'out_wav': 'generated.wav'}
+diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
+index 4722dc58..21392359 100755
+--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
++++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
+@@ -45,12 +45,13 @@ def main():
+             if key in user_defined:
+                 continue
+             tokens = pinyin(key, style=Style.TONE3, neutral_tone_with_five=True)
++
+             for i in range(len(tokens)):
+-                if tokens[i] == "shei2":
+-                    tokens[i] = "shui2"
++                if tokens[i][0] == "shei2":
++                    tokens[i][0] = "shui2"
+ 
+-                if tokens[i][-1] not in ("1", "2", "3", "4", "5"):
+-                    tokens[i] += "1"
++                if tokens[i][0][-1] not in ("1", "2", "3", "4", "5"):
++                    tokens[i][0] += "1"
+ 
+             flatten = [t[0] for t in tokens]
+ 
+diff --git a/scripts/matcha-tts/zh-en/generate_samples.py b/scripts/matcha-tts/zh-en/generate_samples.py
+new file mode 100755
+index 00000000..81df8d9c
+--- /dev/null
++++ b/scripts/matcha-tts/zh-en/generate_samples.py
+@@ -0,0 +1,40 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++"""
++Generate samples for
++https://k2-fsa.github.io/sherpa/onnx/tts/all/
++"""
++
++
++import sherpa_onnx
++import soundfile as sf
++
++config = sherpa_onnx.OfflineTtsConfig(
++    model=sherpa_onnx.OfflineTtsModelConfig(
++        matcha=sherpa_onnx.OfflineTtsMatchaModelConfig(
++            acoustic_model="matcha-icefall-zh-en/model-steps-3.onnx",
++            vocoder="vocos-16khz-univ.onnx",
++            lexicon="matcha-icefall-zh-en/lexicon.txt",
++            tokens="matcha-icefall-zh-en/tokens.txt",
++            data_dir="matcha-icefall-zh-en/espeak-ng-data",
++        ),
++        num_threads=2,
++    ),
++    max_num_sentences=1,
++    rule_fsts="./matcha-icefall-zh-en/phone-zh.fst,./matcha-icefall-zh-en/date-zh.fst,./matcha-icefall-zh-en/number-zh.fst",
++)
++
++if not config.validate():
++    raise ValueError("Please check your config")
++
++tts = sherpa_onnx.OfflineTts(config)
++text = "machine learningartificial intelligencevocationParis; 2025124110189202512043123456"
++
++
++audio = tts.generate(text, sid=0, speed=1.0)
++
++sf.write(
++    "./hf/matcha/icefall-zh-en/mp3/0.mp3",
++    audio.samples,
++    samplerate=audio.sample_rate,
++)
+
+commit ab479d8e01d580d86f666d5d5fac278e2a86b6fd
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Thu Dec 4 16:17:01 2025 +0800
+
+    Fix punctuations in matcha zh-en tts (#2859)
+
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index e7059c99..873918d0 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -197,7 +197,7 @@ class MatchaTtsLexicon::Impl {
+   std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
+     std::string text = _text;
+     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
+-        {"", ","}, {"", ","}, {"", ";"}, {"", ":"},
++        {"", ","}, {"", ","}, {"", ";"}, {"", ","},   {":", ","},
+         {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
+     };
+     for (const auto &p : replace_str_pairs) {
+
+commit 5ea95ee047cc7d337ed1802ce9e7f6cf0c82f5b6
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Thu Dec 4 16:07:08 2025 +0800
+
+    Add a space between English words for Matcha zh-en TTS (#2858)
+
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 1e08d478..e7059c99 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+ 
++#include <ctype.h>
++
+ #include <algorithm>
+ #include <fstream>
+ #include <memory>
+@@ -281,7 +283,10 @@ class MatchaTtsLexicon::Impl {
+ 
+     PhraseMatcher matcher(&all_words_, words, debug_);
+ 
++    int32_t blank = token2id_.at(" ");
++
+     std::vector<int32_t> ids;
++    std::string last_word;
+     for (const std::string &w : matcher) {
+       ids = ConvertWordToIds(w);
+ 
+@@ -291,9 +296,15 @@ class MatchaTtsLexicon::Impl {
+ #else
+         SHERPA_ONNX_LOGE("Ignore OOV '%s'", w.c_str());
+ #endif
++
++        last_word = w;
+         continue;
+       }
+ 
++      if (!last_word.empty() && isalpha(last_word[0])) {
++        this_sentence.push_back(blank);
++      }
++
+       this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
+ 
+       if (IsPunct(w)) {
+@@ -312,6 +323,8 @@ class MatchaTtsLexicon::Impl {
+         ans.emplace_back(std::move(this_sentence));
+         this_sentence = {};
+       }
++
++      last_word = w;
+     }  // for (const std::string &w : matcher)
+ 
+     if (!this_sentence.empty()) {
+
+commit 7cfded8fafbc259fd138434caa183d50397d6ccc
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Thu Dec 4 13:50:26 2025 +0800
+
+    Fix matcha tts (#2856)
+
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+index 2d1255ce..92d7efff 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+@@ -431,10 +431,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+ 
+     std::vector<int64_t> x;
+     x.reserve(num_tokens);
++    for (const auto &k : tokens) {
++      x.insert(x.end(), k.begin(), k.end());
++    }
++
+     if (config_.model.debug) {
+-      for (const auto &k : tokens) {
+-        x.insert(x.end(), k.begin(), k.end());
+-      }
+       std::ostringstream oss;
+       for (int32_t i : x) {
+         oss << i << ", ";
+
+commit 5e7e2525661ddc46dc48c096777cd43cc6b69f4d
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Thu Dec 4 12:06:52 2025 +0800
+
+    Refactor text-utils (#2855)
+    
+    This pull request refactors text utility functions by introducing a generic Join function in text-utils and removing a specialized JoinTokensNoSpace function.
+
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 6248cb64..1e08d478 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -92,14 +92,6 @@ std::vector<std::string> ConvertPhonemesToUTF8(
+   return out;
+ }
+ 
+-std::string JoinTokensNoSpace(const std::vector<std::string> &tokens) {
+-  std::string out;
+-  for (const auto &t : tokens) {
+-    out += t;
+-  }
+-  return out;
+-}
+-
+ std::string ApplyReplacements(std::string s) {
+   for (const auto &p : kReplacements) {
+     const std::string &from = p.first;
+@@ -131,7 +123,7 @@ std::vector<std::string> SplitTokensUTF8(const std::string &s) {
+ std::vector<std::string> ProcessPhonemes(
+     const std::vector<std::vector<char32_t>> &phonemes) {
+   auto tokens = ConvertPhonemesToUTF8(phonemes);
+-  std::string joined = JoinTokensNoSpace(tokens);
++  std::string joined = Join(tokens);
+   std::string replaced = ApplyReplacements(joined);
+   return SplitTokensUTF8(replaced);
+ }
+diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
+index 993f2bb0..3c759abe 100644
+--- a/sherpa-onnx/csrc/text-utils.cc
++++ b/sherpa-onnx/csrc/text-utils.cc
+@@ -733,6 +733,17 @@ std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size) {
+   return ans;
+ }
+ 
++std::string Join(const std::vector<std::string> &ss, const std::string &delim) {
++  std::ostringstream oss;
++  if (!ss.empty()) {
++    oss << ss[0];
++    for (size_t i = 1; i < ss.size(); ++i) {
++      oss << delim << ss[i];
++    }
++  }
++  return oss.str();
++}
++
+ std::u32string Utf8ToUtf32(const std::string &str) {
+   std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
+   return conv.from_bytes(str);
+diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
+index 3f0f80d5..d1f5ce39 100644
+--- a/sherpa-onnx/csrc/text-utils.h
++++ b/sherpa-onnx/csrc/text-utils.h
+@@ -151,6 +151,9 @@ bool Contains(const std::string &haystack, const std::string &needle);
+ 
+ std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size);
+ 
++std::string Join(const std::vector<std::string> &ss,
++                 const std::string &delim = "");
++
+ // Converts a UTF-8 std::string to a UTF-32 std::u32string
+ std::u32string Utf8ToUtf32(const std::string &str);
+ 
+
+commit 2493d1f9be7cddb397b7aa4e340b9072a0ac0618
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Thu Dec 4 11:18:15 2025 +0800
+
+    Fix the English part for Matcha TTS. (#2853)
+
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 21726791..6248cb64 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -5,7 +5,6 @@
+ #include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+ 
+ #include <algorithm>
+-#include <codecvt>
+ #include <fstream>
+ #include <memory>
+ #include <regex>  // NOLINT
+@@ -38,6 +37,107 @@
+ 
+ namespace sherpa_onnx {
+ 
++namespace {
++// code in this anonymous namespace is written by ChatGPT
++//
++// Please see https://github.com/k2-fsa/sherpa-onnx/pull/2853
++// for why we need to do the replacement
++static const std::vector<std::pair<std::string, std::string>> kReplacements = {
++    {"", ""}, {"", ""},
++
++    {"e", "A"}, {"a", "I"}, {"", "Y"},
++    {"o", "O"}, {"", "O"}, {"a", "W"},
++
++    {"t", ""}, {"d", ""},
++
++    {"", ""},
++
++    {"g", ""},  {"r", ""},
++
++    {"e", ""},
++};
++
++std::string Utf32ToUtf8(char32_t cp) {
++  std::string out;
++
++  if (cp <= 0x7F) {
++    out.push_back(static_cast<char>(cp));
++  } else if (cp <= 0x7FF) {
++    out.push_back(static_cast<char>(0xC0 | (cp >> 6)));
++    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
++  } else if (cp <= 0xFFFF) {
++    out.push_back(static_cast<char>(0xE0 | (cp >> 12)));
++    out.push_back(static_cast<char>(0x80 | ((cp >> 6) & 0x3F)));
++    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
++  } else {
++    out.push_back(static_cast<char>(0xF0 | (cp >> 18)));
++    out.push_back(static_cast<char>(0x80 | ((cp >> 12) & 0x3F)));
++    out.push_back(static_cast<char>(0x80 | ((cp >> 6) & 0x3F)));
++    out.push_back(static_cast<char>(0x80 | (cp & 0x3F)));
++  }
++
++  return out;
++}
++
++std::vector<std::string> ConvertPhonemesToUTF8(
++    const std::vector<std::vector<char32_t>> &phonemes) {
++  std::vector<std::string> out;
++
++  for (const auto &word : phonemes) {
++    for (char32_t cp : word) {
++      out.push_back(Utf32ToUtf8(cp));
++    }
++  }
++
++  return out;
++}
++
++std::string JoinTokensNoSpace(const std::vector<std::string> &tokens) {
++  std::string out;
++  for (const auto &t : tokens) {
++    out += t;
++  }
++  return out;
++}
++
++std::string ApplyReplacements(std::string s) {
++  for (const auto &p : kReplacements) {
++    const std::string &from = p.first;
++    const std::string &to = p.second;
++
++    size_t pos = 0;
++    while ((pos = s.find(from, pos)) != std::string::npos) {
++      s.replace(pos, from.size(), to);
++      pos += to.size();
++    }
++  }
++  return s;
++}
++
++std::vector<std::string> SplitTokensUTF8(const std::string &s) {
++  std::vector<std::string> out;
++
++  for (size_t i = 0; i < s.size();) {
++    unsigned char c = s[i];
++    size_t len = (c < 0x80) ? 1 : (c < 0xE0) ? 2 : (c < 0xF0) ? 3 : 4;
++
++    out.push_back(s.substr(i, len));
++    i += len;
++  }
++
++  return out;
++}
++
++std::vector<std::string> ProcessPhonemes(
++    const std::vector<std::vector<char32_t>> &phonemes) {
++  auto tokens = ConvertPhonemesToUTF8(phonemes);
++  std::string joined = JoinTokensNoSpace(tokens);
++  std::string replaced = ApplyReplacements(joined);
++  return SplitTokensUTF8(replaced);
++}
++
++}  // namespace
++
+ void CallPhonemizeEspeak(const std::string &text,
+                          piper::eSpeakPhonemeConfig &config,  // NOLINT
+                          std::vector<std::vector<piper::Phoneme>> *phonemes);
+@@ -246,21 +346,22 @@ class MatchaTtsLexicon::Impl {
+           }
+         }
+       } else {
+-        SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
++        if (debug_) {
++          SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
++        }
+         // use espeak
+         piper::eSpeakPhonemeConfig config;
+         config.voice = "en-us";
+         std::vector<std::vector<piper::Phoneme>> phonemes;
+         CallPhonemizeEspeak(w, config, &phonemes);
+-        for (const auto &ps : phonemes) {
+-          for (const auto &p : ps) {
+-            if (phoneme2id_.count(p)) {
+-              ans.push_back(phoneme2id_.at(p));
+-            } else {
+-              SHERPA_ONNX_LOGE(
+-                  "Skip unknown phonemes. Unicode codepoint: \\U+%04x. for %s",
+-                  static_cast<uint32_t>(p), w.c_str());
+-            }
++
++        auto pp = ProcessPhonemes(phonemes);
++
++        for (const auto &p : pp) {
++          if (token2id_.count(p)) {
++            ans.push_back(token2id_.at(p));
++          } else {
++            SHERPA_ONNX_LOGE("Skip token: %s", p.c_str());
+           }
+         }
+       }
+@@ -290,29 +391,6 @@ class MatchaTtsLexicon::Impl {
+         id2token_[p.second] = p.first;
+       }
+     }
+-
+-    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
+-    std::u32string s;
+-    for (const auto &p : token2id_) {
+-      if ((p.first.front() == '<' && p.first.back() == '>') ||
+-          p.first.back() == '1' || p.first.back() == '2' ||
+-          p.first.back() == '3' || p.first.back() == '4' ||
+-          p.first.back() == '5') {
+-        continue;
+-      }
+-      s = conv.from_bytes(p.first);
+-
+-      if (s.size() != 1) {
+-        SHERPA_ONNX_LOGE("Error for token %s with id %d", p.first.c_str(),
+-                         p.second);
+-        SHERPA_ONNX_EXIT(-1);
+-      }
+-
+-      char32_t c = s[0];
+-      if (!phoneme2id_.count(c)) {
+-        phoneme2id_.insert({c, p.second});
+-      }
+-    }
+   }
+ 
+   void InitLexicon(const std::string &lexicon) {
+@@ -390,7 +468,6 @@ class MatchaTtsLexicon::Impl {
+ 
+   // tokens.txt is saved in token2id_
+   std::unordered_map<std::string, int32_t> token2id_;
+-  std::unordered_map<char32_t, int32_t> phoneme2id_;
+ 
+   std::unordered_map<int32_t, std::string> id2token_;
+ 
+
+commit a60d6d0b31e2b17d6e2490ac1311e18a4cd3f3ae
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Wed Dec 3 19:20:12 2025 +0800
+
+    Fix matcha tts zh-en model (#2851)
+
+diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
+index 15bc5f44..4722dc58 100755
+--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
++++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
+@@ -1,6 +1,6 @@
+ #!/usr/bin/env python3
+ 
+-from pypinyin import Style, lazy_pinyin, load_phrases_dict, phrases_dict, pinyin_dict
++from pypinyin import Style, pinyin, load_phrases_dict, phrases_dict, pinyin_dict
+ 
+ load_phrases_dict(
+     {
+@@ -28,7 +28,8 @@ def main():
+                 continue
+ 
+             w = chr(key)
+-            tokens = lazy_pinyin(w, style=Style.TONE3, tone_sandhi=True)[0]
++            tokens = pinyin(w, style=Style.TONE3, neutral_tone_with_five=True)[0][0]
++
+             if tokens == "shei2":
+                 tokens = "shui2"
+ 
+@@ -43,7 +44,7 @@ def main():
+         for key in phrases:
+             if key in user_defined:
+                 continue
+-            tokens = lazy_pinyin(key, style=Style.TONE3, tone_sandhi=True)
++            tokens = pinyin(key, style=Style.TONE3, neutral_tone_with_five=True)
+             for i in range(len(tokens)):
+                 if tokens[i] == "shei2":
+                     tokens[i] = "shui2"
+@@ -51,7 +52,9 @@ def main():
+                 if tokens[i][-1] not in ("1", "2", "3", "4", "5"):
+                     tokens[i] += "1"
+ 
+-            tokens = " ".join(tokens)
++            flatten = [t[0] for t in tokens]
++
++            tokens = " ".join(flatten)
+ 
+             f.write(f"{key} {tokens}\n")
+ 
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index b5247d6e..21726791 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -266,10 +266,6 @@ class MatchaTtsLexicon::Impl {
+       }
+     }
+ 
+-    if (IsAlphaOrPunct(w.front())) {
+-      ans.push_back(token2id_.at(" "));
+-    }
+-
+     if (debug_) {
+       std::ostringstream os;
+       os << w << ": ";
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+index 3588b2d2..2d1255ce 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+@@ -431,8 +431,16 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+ 
+     std::vector<int64_t> x;
+     x.reserve(num_tokens);
+-    for (const auto &k : tokens) {
+-      x.insert(x.end(), k.begin(), k.end());
++    if (config_.model.debug) {
++      for (const auto &k : tokens) {
++        x.insert(x.end(), k.begin(), k.end());
++      }
++      std::ostringstream oss;
++      for (int32_t i : x) {
++        oss << i << ", ";
++      }
++      oss << "\n";
++      SHERPA_ONNX_LOGE("%s\n", oss.str().c_str());
+     }
+ 
+     auto memory_info =
+
+commit 172c906aee1086d4da1cd5cba08255119457406c
+Author: Fangjun Kuang <csukuangfj@mails.tsinghua.edu.cn>
+Date:   Wed Dec 3 17:06:55 2025 +0800
+
+    Refactor axera npu examples (#2850)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index a9fd3c06..ceedb01c 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -198,10 +198,10 @@ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_AXERA)
+   list(APPEND sources
++    ./axera/ax-engine-guard.cc
+     ./axera/offline-sense-voice-model-axera.cc
+     ./axera/utils.cc
+   )
+-
+ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_AXCL)
+@@ -209,7 +209,6 @@ if(SHERPA_ONNX_ENABLE_AXCL)
+     ./axcl/offline-sense-voice-model-axcl.cc
+     ./axcl/ax_model_runner_axcl.cc
+   )
+-
+ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN OR SHERPA_ONNX_ENABLE_AXERA OR SHERPA_ONNX_ENABLE_AXCL)
+diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+index 542d0e4f..7143d1ce 100644
+--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+@@ -9,7 +9,7 @@
+ #include <algorithm>
+ #include <array>
+ #include <memory>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+index 10391ee4..11a7503b 100644
+--- a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
++++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+@@ -8,7 +8,7 @@
+ #include <memory>
+ #include <vector>
+ 
+-#include "axcl.h"
++#include "axcl.h"  // NOLINT
+ #include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+@@ -36,4 +36,4 @@ class OfflineSenseVoiceModelAxcl {
+ 
+ }  // namespace sherpa_onnx
+ 
+-#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
+\ No newline at end of file
++#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
+diff --git a/sherpa-onnx/csrc/axera/ax-engine-guard.cc b/sherpa-onnx/csrc/axera/ax-engine-guard.cc
+new file mode 100644
+index 00000000..12331bd6
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/ax-engine-guard.cc
+@@ -0,0 +1,51 @@
++// sherpa-onnx/csrc/axera/ax-engine-guard.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/axera/ax-engine-guard.h"
++
++#include <cstring>
++
++#include "ax_engine_api.h"  // NOLINT
++#include "ax_sys_api.h"     // NOLINT
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++thread_local int32_t AxEngineGuard::count_ = 0;
++
++AxEngineGuard::AxEngineGuard() {
++  if (count_ == 0) {
++    auto ret = AX_SYS_Init();
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call AX_SYS_Init. ret code: %d",
++                       static_cast<int32_t>(ret));
++
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    AX_ENGINE_NPU_ATTR_T npu_attr;
++    memset(&npu_attr, 0, sizeof(npu_attr));
++    npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
++    ret = AX_ENGINE_Init(&npu_attr);
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to call AX_ENGINE_Init. ret code: %d",
++                       static_cast<int32_t>(ret));
++
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  ++count_;
++}
++
++AxEngineGuard::~AxEngineGuard() {
++  --count_;
++  if (count_ == 0) {
++    AX_ENGINE_Deinit();
++    AX_SYS_Deinit();
++  }
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axera/ax-engine-guard.h b/sherpa-onnx/csrc/axera/ax-engine-guard.h
+new file mode 100644
+index 00000000..907563fb
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/ax-engine-guard.h
+@@ -0,0 +1,28 @@
++// sherpa-onnx/csrc/axera/ax-engine-guard.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
++#define SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
++#include <cstdint>
++
++namespace sherpa_onnx {
++
++class AxEngineGuard {
++ public:
++  AxEngineGuard();
++  ~AxEngineGuard();
++
++  AxEngineGuard(const AxEngineGuard &) = delete;
++  AxEngineGuard &operator=(const AxEngineGuard &) = delete;
++
++  AxEngineGuard(AxEngineGuard &&) = delete;
++  AxEngineGuard &operator=(AxEngineGuard &&) = delete;
++
++ private:
++  static thread_local int32_t count_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXERA_AX_ENGINE_GUARD_H_
+diff --git a/sherpa-onnx/csrc/axera/io.hpp b/sherpa-onnx/csrc/axera/io.hpp
+deleted file mode 100644
+index 3d3f2535..00000000
+--- a/sherpa-onnx/csrc/axera/io.hpp
++++ /dev/null
+@@ -1,255 +0,0 @@
+-// sherpa-onnx/csrc/axera/io.hpp
+-//
+-// This file is adapted from AXERA's ax-samples project.
+-// See the original BSD 3-Clause license below.
+-//
+-// Copyright (c)  2025  M5Stack Technology CO LTD
+-
+-/*
+- * AXERA is pleased to support the open source community by making ax-samples
+- * available.
+- *
+- * Copyright (c) 2022, AXERA Semiconductor (Shanghai) Co., Ltd. All rights
+- * reserved.
+- *
+- * Licensed under the BSD 3-Clause License (the "License"); you may not use this
+- * file except in compliance with the License. You may obtain a copy of the
+- * License at
+- *
+- * https://opensource.org/licenses/BSD-3-Clause
+- *
+- * Unless required by applicable law or agreed to in writing, software
+- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+- * License for the specific language governing permissions and limitations under
+- * the License.
+- */
+-/*
+- * Author: AXERA Corporation
+- */
+-
+-#pragma once
+-
+-#include <ax_engine_api.h>
+-#include <ax_sys_api.h>
+-
+-#include <cstdio>
+-#include <cstring>
+-#include <map>
+-#include <utility>
+-#include <vector>
+-
+-#define AX_CMM_ALIGN_SIZE 128
+-
+-inline const char *AX_CMM_SESSION_NAME = "ax-samples-cmm";
+-
+-typedef enum {
+-  AX_ENGINE_ABST_DEFAULT = 0,
+-  AX_ENGINE_ABST_CACHED = 1,
+-} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
+-
+-typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
+-                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
+-    INPUT_OUTPUT_ALLOC_STRATEGY;
+-
+-#define SAMPLE_AX_ENGINE_DEAL_HANDLE        \
+-  if (0 != ret) {                           \
+-    return AX_ENGINE_DestroyHandle(handle); \
+-  }
+-
+-#define SAMPLE_AX_ENGINE_DEAL_HANDLE_IO     \
+-  if (0 != ret) {                           \
+-    middleware::free_io(&io_data);          \
+-    return AX_ENGINE_DestroyHandle(handle); \
+-  }
+-
+-namespace middleware {
+-
+-inline void free_io_index(AX_ENGINE_IO_BUFFER_T *io_buf, size_t index) {
+-  for (int i = 0; i < (int)index; ++i) {
+-    AX_ENGINE_IO_BUFFER_T *pBuf = io_buf + i;
+-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+-  }
+-}
+-
+-inline void free_io(AX_ENGINE_IO_T *io) {
+-  for (size_t j = 0; j < io->nInputSize; ++j) {
+-    AX_ENGINE_IO_BUFFER_T *pBuf = io->pInputs + j;
+-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+-  }
+-  for (size_t j = 0; j < io->nOutputSize; ++j) {
+-    AX_ENGINE_IO_BUFFER_T *pBuf = io->pOutputs + j;
+-    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
+-  }
+-  delete[] io->pInputs;
+-  delete[] io->pOutputs;
+-}
+-
+-static inline int prepare_io(AX_ENGINE_IO_INFO_T *info, AX_ENGINE_IO_T *io_data,
+-                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
+-  memset(io_data, 0, sizeof(*io_data));
+-  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[info->nInputSize];
+-  memset(io_data->pInputs, 0, sizeof(AX_ENGINE_IO_BUFFER_T) * info->nInputSize);
+-  io_data->nInputSize = info->nInputSize;
+-
+-  auto ret = 0;
+-  for (int i = 0; i < (int)info->nInputSize; ++i) {
+-    auto meta = info->pInputs[i];
+-    auto buffer = &io_data->pInputs[i];
+-    if (strategy.first == AX_ENGINE_ABST_CACHED) {
+-      ret = AX_SYS_MemAllocCached(
+-          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
+-          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
+-    } else {
+-      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
+-                            meta.nSize, AX_CMM_ALIGN_SIZE,
+-                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
+-    }
+-
+-    if (ret != 0) {
+-      free_io_index(io_data->pInputs, i);
+-      fprintf(
+-          stderr,
+-          "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
+-          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+-      return ret;
+-    }
+-    // fprintf(stderr, "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes
+-    // }. \n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+-  }
+-
+-  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[info->nOutputSize];
+-  memset(io_data->pOutputs, 0,
+-         sizeof(AX_ENGINE_IO_BUFFER_T) * info->nOutputSize);
+-  io_data->nOutputSize = info->nOutputSize;
+-  for (int i = 0; i < (int)info->nOutputSize; ++i) {
+-    auto meta = info->pOutputs[i];
+-    auto buffer = &io_data->pOutputs[i];
+-    buffer->nSize = meta.nSize;
+-    if (strategy.second == AX_ENGINE_ABST_CACHED) {
+-      ret = AX_SYS_MemAllocCached(
+-          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
+-          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
+-    } else {
+-      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
+-                            meta.nSize, AX_CMM_ALIGN_SIZE,
+-                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
+-    }
+-    if (ret != 0) {
+-      fprintf(
+-          stderr,
+-          "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
+-          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+-      free_io_index(io_data->pInputs, io_data->nInputSize);
+-      free_io_index(io_data->pOutputs, i);
+-      return ret;
+-    }
+-    // fprintf(stderr, "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes
+-    // }.\n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
+-  }
+-
+-  return 0;
+-}
+-
+-static int push_input(const std::vector<uint8_t> &data, AX_ENGINE_IO_T *io_t,
+-                      AX_ENGINE_IO_INFO_T *info_t) {
+-  if (info_t->nInputSize != 1) {
+-    fprintf(stderr, "Only support Input size == 1 current now");
+-    return -1;
+-  }
+-
+-  if (data.size() != info_t->pInputs[0].nSize) {
+-    fprintf(stderr,
+-            "The input data size is not matched with tensor {name: %s, size: "
+-            "%d}.\n",
+-            info_t->pInputs[0].pName, info_t->pInputs[0].nSize);
+-    return -1;
+-  }
+-
+-  memcpy(io_t->pInputs[0].pVirAddr, data.data(), data.size());
+-
+-  return 0;
+-}
+-
+-static void print_io_info(AX_ENGINE_IO_INFO_T *io_info) {
+-  static std::map<AX_ENGINE_DATA_TYPE_T, const char *> data_type = {
+-      {AX_ENGINE_DT_UNKNOWN, "UNKNOWN"},
+-      {AX_ENGINE_DT_UINT8, "UINT8"},
+-      {AX_ENGINE_DT_UINT16, "UINT16"},
+-      {AX_ENGINE_DT_FLOAT32, "FLOAT32"},
+-      {AX_ENGINE_DT_SINT16, "SINT16"},
+-      {AX_ENGINE_DT_SINT8, "SINT8"},
+-      {AX_ENGINE_DT_SINT32, "SINT32"},
+-      {AX_ENGINE_DT_UINT32, "UINT32"},
+-      {AX_ENGINE_DT_FLOAT64, "FLOAT64"},
+-      {AX_ENGINE_DT_UINT10_PACKED, "UINT10_PACKED"},
+-      {AX_ENGINE_DT_UINT12_PACKED, "UINT12_PACKED"},
+-      {AX_ENGINE_DT_UINT14_PACKED, "UINT14_PACKED"},
+-      {AX_ENGINE_DT_UINT16_PACKED, "UINT16_PACKED"},
+-  };
+-
+-  static std::map<AX_ENGINE_COLOR_SPACE_T, const char *> color_type = {
+-      {AX_ENGINE_CS_FEATUREMAP, "FEATUREMAP"},
+-      {AX_ENGINE_CS_RAW8, "RAW8"},
+-      {AX_ENGINE_CS_RAW10, "RAW10"},
+-      {AX_ENGINE_CS_RAW12, "RAW12"},
+-      {AX_ENGINE_CS_RAW14, "RAW14"},
+-      {AX_ENGINE_CS_RAW16, "RAW16"},
+-      {AX_ENGINE_CS_NV12, "NV12"},
+-      {AX_ENGINE_CS_NV21, "NV21"},
+-      {AX_ENGINE_CS_RGB, "RGB"},
+-      {AX_ENGINE_CS_BGR, "BGR"},
+-      {AX_ENGINE_CS_RGBA, "RGBA"},
+-      {AX_ENGINE_CS_GRAY, "GRAY"},
+-      {AX_ENGINE_CS_YUV444, "YUV444"},
+-  };
+-  printf("\ninput size: %d\n", io_info->nInputSize);
+-  for (uint32_t i = 0; i < io_info->nInputSize; ++i) {
+-    // print shape info,like [batchsize x channel x height x width]
+-    auto &info = io_info->pInputs[i];
+-    printf("    name: \e[1;32m%8s", info.pName);
+-
+-    std::string dt = "unknown";
+-    if (data_type.find(info.eDataType) != data_type.end()) {
+-      dt = data_type[info.eDataType];
+-      printf(" \e[1;34m[%s] ", dt.c_str());
+-    } else {
+-      printf(" \e[1;31m[%s] ", dt.c_str());
+-    }
+-
+-    std::string ct = "unknown";
+-    if (info.pExtraMeta &&
+-        color_type.find(info.pExtraMeta->eColorSpace) != color_type.end()) {
+-      ct = color_type[info.pExtraMeta->eColorSpace];
+-      printf("\e[1;34m[%s]", ct.c_str());
+-    } else {
+-      printf("\e[1;31m[%s]", ct.c_str());
+-    }
+-    printf(" \n        \e[1;31m");
+-
+-    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
+-      printf("%d", info.pShape[s]);
+-      if (s != info.nShapeSize - 1) {
+-        printf(" x ");
+-      }
+-    }
+-    printf("\e[0m\n\n");
+-  }
+-
+-  printf("\noutput size: %d\n", io_info->nOutputSize);
+-  for (uint32_t i = 0; i < io_info->nOutputSize; ++i) {
+-    // print shape info,like [batchsize x channel x height x width]
+-    auto &info = io_info->pOutputs[i];
+-    printf("    name: \e[1;32m%8s \e[1;34m[%s]\e[0m\n        \e[1;31m",
+-           info.pName, data_type[info.eDataType]);
+-    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
+-      printf("%d", info.pShape[s]);
+-      if (s != info.nShapeSize - 1) {
+-        printf(" x ");
+-      }
+-    }
+-    printf("\e[0m\n\n");
+-  }
+-}
+-
+-}  // namespace middleware
+diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+index 039976e1..37d9ad2c 100644
+--- a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
++++ b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+@@ -19,7 +19,7 @@
+ 
+ namespace sherpa_onnx {
+ 
+-// defined in ../online-recognizer-sense-voice-impl.h
++// defined in ../offline-recognizer-sense-voice-impl.h
+ OfflineRecognitionResult ConvertSenseVoiceResult(
+     const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
+     int32_t frame_shift_ms, int32_t subsampling_factor);
+diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+index 7e964791..904c0800 100644
+--- a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
++++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <array>
+ #include <cstring>
++#include <mutex>
+ #include <utility>
+ #include <vector>
+ 
+@@ -19,7 +20,9 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
+-#include "sherpa-onnx/csrc/axera/io.hpp"
++#include "ax_engine_api.h"  // NOLINT
++#include "ax_sys_api.h"     // NOLINT
++#include "sherpa-onnx/csrc/axera/ax-engine-guard.h"
+ #include "sherpa-onnx/csrc/axera/utils.h"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+@@ -29,7 +32,7 @@ namespace sherpa_onnx {
+ class OfflineSenseVoiceModelAxera::Impl {
+  public:
+   ~Impl() {
+-    middleware::free_io(&io_data_);
++    FreeIO(&io_data_);
+     if (handle_) {
+       AX_ENGINE_DestroyHandle(handle_);
+     }
+@@ -52,15 +55,13 @@ class OfflineSenseVoiceModelAxera::Impl {
+ 
+   std::vector<float> Run(std::vector<float> features, int32_t language,
+                          int32_t text_norm) {
++    // TODO(fangjun): Support multi clients
++    std::lock_guard<std::mutex> lock(mutex_);
++
+     features = ApplyLFR(std::move(features));
+ 
+     std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
+ 
+-    if (!io_info_ || io_info_->nInputSize < 1) {
+-      SHERPA_ONNX_LOGE("Axera model expects at least 1 input tensor");
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+     const auto &in0_meta = io_info_->pInputs[0];
+     size_t bytes0 = in0_meta.nSize;
+ 
+@@ -73,19 +74,15 @@ class OfflineSenseVoiceModelAxera::Impl {
+ 
+     std::memcpy(io_data_.pInputs[0].pVirAddr, features.data(), bytes0);
+ 
+-    //   io_info_->nInputSize >= 2
+-    //   io_info_->pInputs[1].nSize == prompt.size() * sizeof(int32_t)
+-    if (io_info_->nInputSize >= 2) {
+-      const auto &in1_meta = io_info_->pInputs[1];
+-      size_t bytes1 = in1_meta.nSize;
+-      if (bytes1 != prompt.size() * sizeof(int32_t)) {
+-        SHERPA_ONNX_LOGE(
+-            "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
+-            in1_meta.nSize, prompt.size() * sizeof(int32_t));
+-        SHERPA_ONNX_EXIT(-1);
+-      }
+-      std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
++    const auto &in1_meta = io_info_->pInputs[1];
++    size_t bytes1 = in1_meta.nSize;
++    if (bytes1 != prompt.size() * sizeof(int32_t)) {
++      SHERPA_ONNX_LOGE(
++          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
++          in1_meta.nSize, prompt.size() * sizeof(int32_t));
++      SHERPA_ONNX_EXIT(-1);
+     }
++    std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
+ 
+     auto ret = AX_ENGINE_RunSync(handle_, &io_data_);
+     if (ret != 0) {
+@@ -93,11 +90,6 @@ class OfflineSenseVoiceModelAxera::Impl {
+       SHERPA_ONNX_EXIT(-1);
+     }
+ 
+-    if (io_info_->nOutputSize < 1) {
+-      SHERPA_ONNX_LOGE("Axera model has no output tensor");
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-
+     const auto &out_meta = io_info_->pOutputs[0];
+     auto &out_buf = io_data_.pOutputs[0];
+ 
+@@ -111,17 +103,13 @@ class OfflineSenseVoiceModelAxera::Impl {
+ 
+  private:
+   void Init(void *model_data, size_t model_data_length) {
+-    InitEngine(config_.debug);
+-
+     InitContext(model_data, model_data_length, config_.debug, &handle_);
+ 
+     InitInputOutputAttrs(handle_, config_.debug, &io_info_);
+ 
+-    std::memset(&io_data_, 0, sizeof(io_data_));
+-
+     PrepareIO(io_info_, &io_data_, config_.debug);
+ 
+-    if (!io_info_ || io_info_->nInputSize == 0 || !io_info_->pInputs) {
++    if (!io_info_ || io_info_->nInputSize != 2 || !io_info_->pInputs) {
+       SHERPA_ONNX_LOGE("No input tensor in Axera model");
+       SHERPA_ONNX_EXIT(-1);
+     }
+@@ -134,6 +122,11 @@ class OfflineSenseVoiceModelAxera::Impl {
+     }
+     num_input_frames_ = in0.pShape[1];
+ 
++    if (io_info_->nOutputSize != 1) {
++      SHERPA_ONNX_LOGE("Axera sense voice model expected only 1 output tensor");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
+     if (config_.debug) {
+       SHERPA_ONNX_LOGE("Axera SenseVoice model init done.");
+       SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
+@@ -173,6 +166,9 @@ class OfflineSenseVoiceModelAxera::Impl {
+   }
+ 
+  private:
++  std::mutex mutex_;
++  AxEngineGuard ax_engine_guard_;
++
+   OfflineModelConfig config_;
+   AX_ENGINE_HANDLE handle_ = nullptr;
+   AX_ENGINE_IO_INFO_T *io_info_ = nullptr;
+@@ -213,4 +209,4 @@ template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
+     NativeResourceManager *mgr, const OfflineModelConfig &config);
+ #endif
+ 
+-}  // namespace sherpa_onnx
+\ No newline at end of file
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+index dcbb2381..5ecdcd64 100644
+--- a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
++++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+@@ -8,8 +8,6 @@
+ #include <memory>
+ #include <vector>
+ 
+-#include "ax_engine_api.h"
+-#include "ax_sys_api.h"
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+ 
+@@ -36,4 +34,4 @@ class OfflineSenseVoiceModelAxera {
+ 
+ }  // namespace sherpa_onnx
+ 
+-#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
+\ No newline at end of file
++#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
+diff --git a/sherpa-onnx/csrc/axera/utils.cc b/sherpa-onnx/csrc/axera/utils.cc
+index c42f2a6a..13441cef 100644
+--- a/sherpa-onnx/csrc/axera/utils.cc
++++ b/sherpa-onnx/csrc/axera/utils.cc
+@@ -9,97 +9,142 @@
+ #include <sstream>
+ #include <string>
+ #include <utility>
+-#include <vector>
+ 
+-#include "sherpa-onnx/csrc/axera/io.hpp"
++#include "ax_engine_api.h"   // NOLINT
++#include "ax_engine_type.h"  // NOLINT
++#include "ax_sys_api.h"      // NOLINT
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+ 
++#define SHERPA_ONNX_TO_STRING(type) \
++  case type:                        \
++    return #type
++
+ namespace sherpa_onnx {
+ 
+-void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
+-                       int32_t height, int32_t width, float *dst) {
+-  for (int32_t i = 0; i < n; ++i) {
+-    for (int32_t h = 0; h < height; ++h) {
+-      for (int32_t w = 0; w < width; ++w) {
+-        for (int32_t c = 0; c < channel; ++c) {
+-          dst[i * height * width * channel + h * width * channel + w * channel +
+-              c] = src[i * height * width * channel + c * height * width +
+-                       h * width + w];
+-        }
+-      }
+-    }
+-  }
+-}
++static constexpr int32_t kCmnAlignSize = 128;
++static const char *kSherpaOnnxAxeraSessionName = "sherpa-onnx-axera";
+ 
+-std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
++static std::string VectorToString(AX_S32 *arr, AX_U8 n) {
+   std::ostringstream os;
+-  os << "{";
+-  if (!io_info) {
+-    os << "null AX_ENGINE_IO_INFO_T}";
+-    return os.str();
++  std::string sep;
++  os << "[";
++  for (AX_U8 i = 0; i < n; ++i) {
++    os << sep << arr[i];
++    sep = ", ";
+   }
++  os << "]";
+ 
+-  os << "nInputSize: " << io_info->nInputSize;
+-  os << ", nOutputSize: " << io_info->nOutputSize;
+-  os << ", nMaxBatchSize: " << io_info->nMaxBatchSize;
+-  os << ", bDynamicBatchSize: "
+-     << (io_info->bDynamicBatchSize ? "true" : "false");
+-  os << "}";
+   return os.str();
+ }
+ 
+-std::unordered_map<std::string, std::string> Parse(const char *custom_string,
+-                                                   bool debug /*= false*/) {
+-  std::unordered_map<std::string, std::string> ans;
+-  if (!custom_string) {
+-    SHERPA_ONNX_LOGE("Parse: custom_string is null");
+-    SHERPA_ONNX_EXIT(-1);
++static const char *AxEngineDataTypeToString(AX_ENGINE_DATA_TYPE_T type) {
++  switch (type) {
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UNKNOWN);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT8);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT16);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_FLOAT32);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT16);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT8);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_SINT32);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT32);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_FLOAT64);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT10_PACKED);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT12_PACKED);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT14_PACKED);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_DT_UINT16_PACKED);
++    default:
++      return "Unknown data type";
+   }
++}
+ 
+-  std::vector<std::string> fields;
+-  SplitStringToVector(custom_string, ";", false, &fields);
+-  std::vector<std::string> tmp;
+-
+-  for (const auto &f : fields) {
+-    tmp.clear();
+-    SplitStringToVector(f, "=", false, &tmp);
+-    if (tmp.size() != 2) {
+-      SHERPA_ONNX_LOGE("Invalid custom string %s for %s", custom_string,
+-                       f.c_str());
+-      SHERPA_ONNX_EXIT(-1);
+-    }
+-    ans[std::move(tmp[0])] = std::move(tmp[1]);
++static const char *AxEngineTensorLayoutToString(
++    AX_ENGINE_TENSOR_LAYOUT_T layout) {
++  switch (layout) {
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_UNKNOWN);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_NHWC);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_TENSOR_LAYOUT_NCHW);
++    default:
++      return "Unknown data layout";
+   }
++}
+ 
+-  if (debug) {
+-    for (const auto &p : ans) {
+-      SHERPA_ONNX_LOGE("%s: %s", p.first.c_str(), p.second.c_str());
+-    }
++static const char *AxEngineMemoryTypeToString(AX_ENGINE_MEMORY_TYPE_T type) {
++  switch (type) {
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_PHYSICAL);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_VIRTUAL);
++    SHERPA_ONNX_TO_STRING(AX_ENGINE_MT_OCM);
++    default:
++      return "Unknown memory type";
+   }
+-  return ans;
+ }
+ 
+-void InitEngine(bool debug) {
+-  AX_SYS_Init();
+-#ifdef AXERA_TARGET_CHIP_AX620E
+-  auto ret = AX_ENGINE_Init();
+-#else
+-  AX_ENGINE_NPU_ATTR_T npu_attr;
+-  memset(&npu_attr, 0, sizeof(npu_attr));
+-  npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
+-  auto ret = AX_ENGINE_Init(&npu_attr);
+-#endif
+-  if (ret != 0) {
+-    SHERPA_ONNX_LOGE("AX_ENGINE_Init failed, ret = %d", ret);
+-    SHERPA_ONNX_EXIT(-1);
++/*
++num_inputs: 2
++num_outputs: 1
++max_bach_size: 1
++dynamic_bach_size: false
++---input 0---
++ name: x
++ shape: [1, 167, 560]
++ layout: AX_ENGINE_TENSOR_LAYOUT_NCHW
++ memory_type: AX_ENGINE_MT_PHYSICAL
++ data_type: AX_ENGINE_DT_FLOAT32
++ n_size (number of bytes): 374080
++---input 1---
++ name: prompt
++ shape: [4]
++ layout: AX_ENGINE_TENSOR_LAYOUT_NCHW
++ memory_type: AX_ENGINE_MT_PHYSICAL
++ data_type: AX_ENGINE_DT_SINT32
++ n_size (number of bytes): 16
++
++---output 0---
++ name: logits
++ shape: [1, 171, 25055]
++ layout: AX_ENGINE_TENSOR_LAYOUT_UNKNOWN
++ memory_type: AX_ENGINE_MT_PHYSICAL
++ data_type: AX_ENGINE_DT_FLOAT32
++ n_size: 17137620
++ */
++static std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
++  std::ostringstream os;
++  os << "num_inputs: " << io_info->nInputSize << "\n";
++  os << "num_outputs: " << io_info->nOutputSize << "\n";
++  os << "max_bach_size: " << io_info->nMaxBatchSize << "\n";
++  os << "dynamic_bach_size: " << (io_info->bDynamicBatchSize ? "true" : "false")
++     << "\n";
++
++  for (AX_U32 i = 0; i < io_info->nInputSize; ++i) {
++    const auto &input = io_info->pInputs[i];
++    os << "---input " << i << "---\n";
++    os << " name: " << input.pName << "\n";
++    os << " shape: " << VectorToString(input.pShape, input.nShapeSize) << "\n";
++    os << " layout: " << AxEngineTensorLayoutToString(input.eLayout) << "\n";
++    os << " memory_type: " << AxEngineMemoryTypeToString(input.eMemoryType)
++       << "\n";
++    os << " data_type: " << AxEngineDataTypeToString(input.eDataType) << "\n";
++    os << " n_size (number of bytes): " << input.nSize << "\n";
+   }
+-  if (debug) {
+-    SHERPA_ONNX_LOGE("AX_ENGINE_Init done.");
++  os << "\n";
++
++  for (AX_U32 i = 0; i < io_info->nOutputSize; ++i) {
++    const auto &output = io_info->pOutputs[i];
++    os << "---output " << i << "---\n";
++    os << " name: " << output.pName << "\n";
++    os << " shape: " << VectorToString(output.pShape, output.nShapeSize)
++       << "\n";
++    os << " layout: " << AxEngineTensorLayoutToString(output.eLayout) << "\n";
++    os << " memory_type: " << AxEngineMemoryTypeToString(output.eMemoryType)
++       << "\n";
++    os << " data_type: " << AxEngineDataTypeToString(output.eDataType) << "\n";
++    os << " n_size: " << output.nSize << "\n";
+   }
++
++  return os.str();
+ }
+ 
+-void InitContext(void *model_data, size_t model_data_length, bool debug,
++void InitContext(const void *model_data, size_t model_data_length, bool debug,
+                  AX_ENGINE_HANDLE *handle) {
+   if (!handle) {
+     SHERPA_ONNX_LOGE("InitContext: handle is null");
+@@ -111,9 +156,9 @@ void InitContext(void *model_data, size_t model_data_length, bool debug,
+     SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle failed, ret = %d", ret);
+     SHERPA_ONNX_EXIT(-1);
+   }
++
+   if (debug) {
+-    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p",
+-                     (void *)(*handle));
++    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p", *handle);
+   }
+ 
+   ret = AX_ENGINE_CreateContext(*handle);
+@@ -121,6 +166,7 @@ void InitContext(void *model_data, size_t model_data_length, bool debug,
+     SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext failed, ret = %d", ret);
+     SHERPA_ONNX_EXIT(-1);
+   }
++
+   if (debug) {
+     SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext done.");
+   }
+@@ -133,6 +179,7 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+     SHERPA_ONNX_EXIT(-1);
+   }
+ 
++  // Note(fangjun): No need to free *io_info
+   auto ret = AX_ENGINE_GetIOInfo(handle, io_info);
+   if (ret != 0) {
+     SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo failed, ret = %d", ret);
+@@ -141,8 +188,7 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+ 
+   if (debug) {
+     SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo done.");
+-    SHERPA_ONNX_LOGE("IO_INFO: %s", ToString(*io_info).c_str());
+-    middleware::print_io_info(*io_info);
++    SHERPA_ONNX_LOGE("IO_INFO:\n%s", ToString(*io_info).c_str());
+   }
+ }
+ 
+@@ -153,17 +199,69 @@ void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
+     SHERPA_ONNX_EXIT(-1);
+   }
+ 
+-  auto ret = middleware::prepare_io(
+-      io_info, io_data,
+-      std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_CACHED));
+-  if (ret != 0) {
+-    SHERPA_ONNX_LOGE("middleware::prepare_io failed, ret = %d", ret);
+-    SHERPA_ONNX_EXIT(-1);
++  memset(io_data, 0, sizeof(AX_ENGINE_IO_T));
++
++  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[io_info->nInputSize];
++
++  memset(io_data->pInputs, 0,
++         sizeof(AX_ENGINE_IO_BUFFER_T) * io_info->nInputSize);
++
++  io_data->nInputSize = io_info->nInputSize;
++
++  for (AX_U32 i = 0; i < io_info->nInputSize; ++i) {
++    const auto &input = io_info->pInputs[i];
++    auto &buffer = io_data->pInputs[i];
++
++    buffer.nSize = input.nSize;
++
++    auto ret = AX_SYS_MemAlloc(
++        reinterpret_cast<AX_U64 *>(&buffer.phyAddr), &buffer.pVirAddr,
++        input.nSize, kCmnAlignSize,
++        reinterpret_cast<const AX_S8 *>(kSherpaOnnxAxeraSessionName));
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to allocate memory for Input %d",
++                       static_cast<int32_t>(i));
++      SHERPA_ONNX_EXIT(-1);
++    }
+   }
+ 
+-  if (debug) {
+-    SHERPA_ONNX_LOGE("PrepareIO (middleware::prepare_io) done.");
++  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[io_info->nOutputSize];
++
++  memset(io_data->pOutputs, 0,
++         sizeof(AX_ENGINE_IO_BUFFER_T) * io_info->nOutputSize);
++
++  io_data->nOutputSize = io_info->nOutputSize;
++
++  for (AX_U32 i = 0; i < io_info->nOutputSize; ++i) {
++    const auto &output = io_info->pOutputs[i];
++    auto &buffer = io_data->pOutputs[i];
++    buffer.nSize = output.nSize;
++    auto ret = AX_SYS_MemAllocCached(
++        reinterpret_cast<AX_U64 *>(&buffer.phyAddr), &buffer.pVirAddr,
++        output.nSize, kCmnAlignSize,
++        reinterpret_cast<const AX_S8 *>(kSherpaOnnxAxeraSessionName));
++
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Failed to allocate memory for Output %d",
++                       static_cast<int32_t>(i));
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++}
++
++void FreeIO(AX_ENGINE_IO_T *io_data) {
++  for (AX_U32 i = 0; i < io_data->nInputSize; ++i) {
++    auto &buf = io_data->pInputs[i];
++    AX_SYS_MemFree(buf.phyAddr, buf.pVirAddr);
++  }
++
++  for (AX_U32 i = 0; i < io_data->nOutputSize; ++i) {
++    auto &buf = io_data->pOutputs[i];
++    AX_SYS_MemFree(buf.phyAddr, buf.pVirAddr);
+   }
++  delete[] io_data->pInputs;
++  delete[] io_data->pOutputs;
+ }
+ 
+-}  // namespace sherpa_onnx
+\ No newline at end of file
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/axera/utils.h b/sherpa-onnx/csrc/axera/utils.h
+index 039eefc8..b1896fed 100644
+--- a/sherpa-onnx/csrc/axera/utils.h
++++ b/sherpa-onnx/csrc/axera/utils.h
+@@ -5,25 +5,13 @@
+ #ifndef SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+ #define SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+ 
+-#include <string>
+-#include <unordered_map>
+-#include <vector>
++#include <cstddef>
+ 
+-#include "ax_engine_api.h"
++#include "ax_engine_api.h"  // NOLINT
+ 
+ namespace sherpa_onnx {
+ 
+-void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
+-                       int32_t height, int32_t width, float *dst);
+-
+-std::string ToString(const AX_ENGINE_IO_INFO_T *io_info);
+-
+-std::unordered_map<std::string, std::string> Parse(const char *custom_string,
+-                                                   bool debug = false);
+-
+-void InitEngine(bool debug);
+-
+-void InitContext(void *model_data, size_t model_data_length, bool debug,
++void InitContext(const void *model_data, size_t model_data_length, bool debug,
+                  AX_ENGINE_HANDLE *handle);
+ 
+ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+@@ -32,6 +20,8 @@ void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
+ void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
+                bool debug);
+ 
++void FreeIO(AX_ENGINE_IO_T *io_data);
++
+ }  // namespace sherpa_onnx
+ 
+ #endif  // SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index ae45d304..683fbddc 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -89,14 +89,14 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ 
+   if (config.model_config.provider == "axera") {
+ #if SHERPA_ONNX_ENABLE_AXERA
+-    if (config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
++    } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
+-          "by axera for non-streaming ASR.");
++          "Only SenseVoice models are currently supported by Axera NPU for "
++          "non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+-    } else if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -110,15 +110,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ 
+   if (config.model_config.provider == "axcl") {
+ #if SHERPA_ONNX_ENABLE_AXCL
+-    if (config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
++    } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
+-          "by axcl for non-streaming ASR.");
++          "Only SenseVoice models are currently supported by axcl for "
++          "non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+-    } else if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
+     }
++
+ #else
+     SHERPA_ONNX_LOGE(
+         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
+@@ -391,6 +392,50 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #endif
+   }
+ 
++  if (config.model_config.provider == "axera") {
++#if SHERPA_ONNX_ENABLE_AXERA
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(mgr,
++                                                                    config);
++    } else {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported by Axera NPU for "
++          "non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    }
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXERA=ON if you "
++        "want to use axera. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/axera/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
++  if (config.model_config.provider == "axcl") {
++#if SHERPA_ONNX_ENABLE_AXCL
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(mgr, config);
++    } else {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported by axcl for "
++          "non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    }
++
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
++        "want to use axcl. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/axcl/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
+   if (config.model_config.provider == "ascend") {
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+     if (!config.model_config.sense_voice.model.empty()) {
+diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+index 2427d412..8cb39349 100644
+--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+@@ -54,12 +54,11 @@ OfflineTransducerGreedySearchDecoder::Decode(Ort::Value encoder_out,
+       if (blank_penalty_ > 0.0) {
+         p_logit[0] -= blank_penalty_;  // assuming blank id is 0
+       }
+-      
++
+       LogSoftmax(p_logit, vocab_size);
+ 
+       auto y = static_cast<int32_t>(std::distance(
+-          p_logit,
+-          std::max_element(p_logit, p_logit + vocab_size)));
++          p_logit, std::max_element(p_logit, p_logit + vocab_size)));
+ 
+       float log_prob = p_logit[y];
+ 
+diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+index 335b0dec..60b3a99e 100644
+--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+@@ -137,15 +137,14 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
+           new_hyp.ys.push_back(new_token);
+           new_hyp.timestamps.push_back(t);
+ 
+-          // Store the token log probability (subtract prev log_prob to get original)
++          // Store the token log probability (subtract prev log_prob to get
++          // original)
+           float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
+           new_hyp.ys_probs.push_back(token_log_prob);
+ 
+           if (context_graphs[i] != nullptr) {
+-            auto context_res =
+-                context_graphs[i]->ForwardOneStep(context_state,
+-                  new_token,
+-                  false /* non-strict mode */);
++            auto context_res = context_graphs[i]->ForwardOneStep(
++                context_state, new_token, false /* non-strict mode */);
+             context_score = std::get<0>(context_res);
+             new_hyp.context_state = std::get<1>(context_res);
+           }
+diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
+index d3d223c3..cba83f1a 100755
+--- a/sherpa-onnx/csrc/session.cc
++++ b/sherpa-onnx/csrc/session.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <algorithm>
+ #include <string>
++#include <unordered_map>
+ #include <utility>
+ #include <vector>
+ 
+@@ -259,17 +260,22 @@ Ort::SessionOptions GetSessionOptionsImpl(
+       SHERPA_ONNX_LOGE("Set InterOpNumThreads to 1");
+       sess_opts.SetInterOpNumThreads(1);
+       SHERPA_ONNX_LOGE("Set SPACEMIT_EP_INTRA_THREAD_NUM to %d", num_threads);
+-      provider_options.insert(
+-          std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM", std::to_string(num_threads)));
+-      OrtStatus* sts = Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
++      provider_options.insert(std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM",
++                                             std::to_string(num_threads)));
++      OrtStatus *sts =
++          Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
+       if (sts) {
+         const auto &api = Ort::GetApi();
+         const char *msg = api.GetErrorMessage(sts);
+-        SHERPA_ONNX_LOGE("Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu", msg);
++        SHERPA_ONNX_LOGE(
++            "Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu",
++            msg);
+         api.ReleaseStatus(sts);
+       }
+ #else
+-      SHERPA_ONNX_LOGE("SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback to cpu!");
++      SHERPA_ONNX_LOGE(
++          "SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback "
++          "to cpu!");
+ #endif
+       break;
+     }
+diff --git a/sherpa-onnx/csrc/wave-reader-test.cc b/sherpa-onnx/csrc/wave-reader-test.cc
+index 286aab06..d63cba2a 100644
+--- a/sherpa-onnx/csrc/wave-reader-test.cc
++++ b/sherpa-onnx/csrc/wave-reader-test.cc
+@@ -7,6 +7,7 @@
+ #include <cstdio>
+ #include <fstream>
+ #include <string>
++#include <vector>
+ 
+ #if defined(_WIN32)
+ #include <windows.h>
+@@ -23,7 +24,7 @@ class TempFile {
+  public:
+   TempFile() : TempFile("") {}
+ 
+-  explicit TempFile(const std::string& suffix) {
++  explicit TempFile(const std::string &suffix) {
+ #if defined(_WIN32)
+     char temp_path[MAX_PATH];
+     char temp_file[MAX_PATH];
+@@ -54,7 +55,7 @@ class TempFile {
+     }
+   }
+ 
+-  const char* path() const { return path_.c_str(); }
++  const char *path() const { return path_.c_str(); }
+ 
+  private:
+   std::string path_;
+@@ -70,15 +71,10 @@ TEST(WaveReader, TestNonWavFile) {
+     // (webm files typically start with EBML header: 0x1a45dfa3)
+     const unsigned char webm_header[] = {
+         0x1a, 0x45, 0xdf, 0xa3,  // EBML header signature (NOT RIFF)
+-        0x01, 0x00, 0x00, 0x00,
+-        0x00, 0x00, 0x00, 0x1f,
+-        0x42, 0x86, 0x81, 0x01,
++        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x1f, 0x42, 0x86, 0x81, 0x01,
+         // Add some more bytes to make it look like a real file
+-        0x42, 0xf7, 0x81, 0x01,
+-        0x42, 0xf2, 0x81, 0x04,
+-        'w', 'e', 'b', 'm'
+-    };
+-    out.write(reinterpret_cast<const char*>(webm_header), sizeof(webm_header));
++        0x42, 0xf7, 0x81, 0x01, 0x42, 0xf2, 0x81, 0x04, 'w', 'e', 'b', 'm'};
++    out.write(reinterpret_cast<const char *>(webm_header), sizeof(webm_header));
+   }
+ 
+   // Test C++ API - should not segfault
+@@ -113,12 +109,15 @@ TEST(WaveReader, TestTruncatedWaveFile) {
+     std::ofstream out(temp_file.path(), std::ios::binary);
+     // Write only partial WAV header (less than 44 bytes required)
+     const unsigned char partial_wav[] = {
+-        'R', 'I', 'F', 'F',  // chunk_id
+-        0x00, 0x00, 0x00, 0x00,  // chunk_size
+-        'W', 'A', 'V', 'E'  // format
+-        // Missing the rest of the header
++        'R',  'I',  'F',
++        'F',  // chunk_id
++        0x00, 0x00, 0x00,
++        0x00,  // chunk_size
++        'W',  'A',  'V',
++        'E'  // format
++             // Missing the rest of the header
+     };
+-    out.write(reinterpret_cast<const char*>(partial_wav), sizeof(partial_wav));
++    out.write(reinterpret_cast<const char *>(partial_wav), sizeof(partial_wav));
+   }
+ 
+   // Test C++ API - should not segfault
+
+commit 89b7e9f42712bbd1767acb2b83a3dc119f877b6a
+Author: Abandon-ht <64671747+Abandon-ht@users.noreply.github.com>
+Date:   Wed Dec 3 11:31:30 2025 +0800
+
+    Support AXERA ax630, ax650, and axcl backends. (#2849)
+
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 10467632..20f63f71 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -60,6 +60,8 @@ option(SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE "True to use pre-i
+ option(SHERPA_ONNX_ENABLE_SANITIZER "Whether to enable ubsan and asan" OFF)
+ option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUGGEST_BUILD_BINARIES})
+ option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
++option(SHERPA_ONNX_ENABLE_AXERA "Whether to build for Axera NPU " OFF)
++option(SHERPA_ONNX_ENABLE_AXCL "Whether to build for Axcl NPU " OFF)
+ option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
+ option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
+ option(SHERPA_ONNX_ENABLE_SPACEMIT "Whether to build for SpacemiT CPUs " OFF)
+@@ -180,6 +182,8 @@ message(STATUS "SHERPA_ONNX_USE_PRE_INSTALLED_ONNXRUNTIME_IF_AVAILABLE ${SHERPA_
+ message(STATUS "SHERPA_ONNX_ENABLE_SANITIZER: ${SHERPA_ONNX_ENABLE_SANITIZER}")
+ message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAMPLES}")
+ message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
++message(STATUS "SHERPA_ONNX_ENABLE_AXERA: ${SHERPA_ONNX_ENABLE_AXERA}")
++message(STATUS "SHERPA_ONNX_ENABLE_AXCL: ${SHERPA_ONNX_ENABLE_AXCL}")
+ message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
+ message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
+ message(STATUS "SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
+@@ -306,6 +310,14 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+   add_definitions(-DSHERPA_ONNX_ENABLE_RKNN=1)
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_AXERA)
++  add_definitions(-DSHERPA_ONNX_ENABLE_AXERA=1)
++endif()
++
++if(SHERPA_ONNX_ENABLE_AXCL)
++  add_definitions(-DSHERPA_ONNX_ENABLE_AXCL=1)
++endif()
++
+ if(SHERPA_ONNX_ENABLE_QNN)
+   add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
+ endif()
+diff --git a/build-axcl-linux-aarch64.sh b/build-axcl-linux-aarch64.sh
+new file mode 100755
+index 00000000..7b767cae
+--- /dev/null
++++ b/build-axcl-linux-aarch64.sh
+@@ -0,0 +1,124 @@
++#!/usr/bin/env bash
++#
++# Usage of this file
++#
++# Refer to the "How to build static libraries and statically linked binaries"
++# section at:
++#   https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html
++#
++# Use the following toolchain:
++#   aarch64-none-linux-gnu-gcc
++#   (GNU Toolchain for the A-profile Architecture 10.3-2021.07 (arm-10.29))
++#   version 10.3.1 20210621
++#
++# Note: Do NOT set:
++#   export BUILD_SHARED_LIBS=OFF
++#
++# Usage of this file
++#
++
++set -ex
++
++# Before you run this file, make sure you have first cloned
++# https://github.com/Abandon-ht/axcl_bsp_sdk
++# and set the environment variable SHERPA_ONNX_AXERA_PATH
++
++if [ -z "$AXCL_SDK_ROOT" ]; then
++  AXCL_SDK_ROOT=/home/m5stack/Workspace/kaldi/sherpa-onnx/axcl_bsp_sdk/out
++  echo "Please set AXCL_SDK_ROOT to your Axcl SDK path, e.g.:"
++  echo "  export AXCL_SDK_ROOT=$PWD/axcl_bsp_sdk/out"
++  exit 1
++fi
++
++if [ ! -d "$AXCL_SDK_ROOT" ]; then
++  echo "AXCL_SDK_ROOT ($AXCL_SDK_ROOT) does not exist"
++  exit 1
++fi
++
++if [ ! -f "$AXCL_SDK_ROOT/include/axcl.h" ]; then
++  echo "$AXCL_SDK_ROOT/include/axcl.h does not exist"
++  exit 1
++fi
++
++if [ ! -f "$AXCL_SDK_ROOT/lib/libaxcl_comm.so" ]; then
++  echo "$AXCL_SDK_ROOT/lib/libaxcl_comm.so does not exist"
++  exit 1
++fi
++
++export CPLUS_INCLUDE_PATH="$AXCL_SDK_ROOT/include:$AXCL_SDK_ROOT/bsp:$CPLUS_INCLUDE_PATH"
++export SHERPA_ONNX_AXCL_LIB_DIR="$AXCL_SDK_ROOT/lib"
++
++if command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
++  ln -svf $(which aarch64-none-linux-gnu-gcc) ./aarch64-linux-gnu-gcc
++  ln -svf $(which aarch64-none-linux-gnu-g++) ./aarch64-linux-gnu-g++
++  export PATH=$PWD:$PATH
++fi
++
++if ! command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
++  echo "Please install a toolchain for cross-compiling."
++  echo "You can refer to: "
++  echo "  https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html"
++  echo "for help."
++  exit 1
++fi
++
++
++dir=$PWD/build-axcl-linux-aarch64
++mkdir -p $dir
++
++cd $dir
++
++if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
++  echo "Start to cross-compile alsa-lib"
++  if [ ! -d alsa-lib ]; then
++    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
++  fi
++  # If it shows:
++  #  ./gitcompile: line 79: libtoolize: command not found
++  # Please use:
++  #  sudo apt-get install libtool m4 automake
++  #
++  pushd alsa-lib
++  CC=aarch64-linux-gnu-gcc ./gitcompile --host=aarch64-linux-gnu
++  popd
++  echo "Finish cross-compiling alsa-lib"
++fi
++
++export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
++export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
++
++if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
++  # By default, use shared link
++  BUILD_SHARED_LIBS=ON
++fi
++
++cmake \
++  -DALSA_INCLUDE_DIR=$PWD/alsa-lib/include \
++  -DALSA_LIBRARY=$PWD/alsa-lib/src/.libs/libasound.so \
++  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
++  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
++  -DBUILD_ESPEAK_NG_EXE=OFF \
++  -DBUILD_ESPEAK_NG_TESTS=OFF \
++  -DCMAKE_INSTALL_PREFIX=./install \
++  -DCMAKE_BUILD_TYPE=Release \
++  -DSHERPA_ONNX_ENABLE_GPU=OFF \
++  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
++  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
++  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
++  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
++  -DSHERPA_ONNX_ENABLE_PORTAUDIO=ON \
++  -DSHERPA_ONNX_ENABLE_JNI=OFF \
++  -DSHERPA_ONNX_ENABLE_C_API=ON \
++  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
++  -DSHERPA_ONNX_ENABLE_AXCL=ON \
++  -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
++  ..
++
++make VERBOSE=1 -j22
++make install/strip
++
++# Enable it if only needed
++# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
++
++# See also
++# https://github.com/airockchip/rknn-toolkit2/blob/master/rknpu2/examples/rknn_api_demo/build-linux.sh
+diff --git a/build-axera-linux-aarch64.sh b/build-axera-linux-aarch64.sh
+new file mode 100755
+index 00000000..a6716712
+--- /dev/null
++++ b/build-axera-linux-aarch64.sh
+@@ -0,0 +1,210 @@
++#!/usr/bin/env bash
++#
++# Usage of this file
++#
++# Refer to the "How to build static libraries and statically linked binaries"
++# section at:
++#   https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html
++#
++# Use the following toolchain:
++#   aarch64-none-linux-gnu-gcc
++#   (GNU Toolchain for the A-profile Architecture 10.3-2021.07 (arm-10.29))
++#   version 10.3.1 20210621
++#
++# Note: Do NOT set:
++#   export BUILD_SHARED_LIBS=OFF
++#
++# Usage of this file
++#
++# ./build-axera-linux-aarch64.sh ax650
++# ./build-axera-linux-aarch64.sh ax630c
++# ./build-axera-linux-aarch64.sh ax620q
++
++set -ex
++
++SUPPORTED_TARGETS=("ax650" "ax630c" "ax620q")
++
++function print_info() {
++    echo -e "\033[32m[INFO]\033[0m $1"
++}
++
++function print_error() {
++    echo -e "\033[31m[ERROR]\033[0m $1"
++}
++
++function print_warn() {
++    echo -e "\033[33m[WARN]\033[0m $1"
++}
++
++function usage() {
++    print_info "Usage: $0 <axera_target_chip>"
++    print_info "Supported chips: ${SUPPORTED_TARGETS[*]}"
++    print_info "Example: $0 ax650"
++    print_info "Example: $0 ax630c"
++    print_info "Example: $0 ax620q"
++}
++
++function download_650_bsp_sdk() {
++  local version=1.45.0_p39
++  if [ -d ax650n_bsp_sdk-$version ]; then
++    echo $PWD/ax650n_bsp_sdk-$version/msp/out
++    return 0
++  fi
++
++  # 166 MB
++  if [ ! -f v$version.zip ]; then
++    wget https://github.com/AXERA-TECH/ax650n_bsp_sdk/archive/refs/tags/v$version.zip
++  fi
++
++  unzip -qq v$version.zip
++
++  echo $PWD/ax650n_bsp_sdk-$version/msp/out
++
++  return 0
++}
++
++function download_620e_bsp_sdk() {
++  local version=2.0.0_P7
++  if [ -d ax620e_bsp_sdk-$version ]; then
++    echo $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc
++    return 0
++  fi
++
++  # 166 MB
++  if [ ! -f v$version.zip ]; then
++    wget https://github.com/AXERA-TECH/ax620e_bsp_sdk/archive/refs/tags/v2.0.0_P7.zip
++  fi
++
++  unzip -qq v$version.zip
++
++  echo $PWD/ax620e_bsp_sdk-$version/msp/out/arm64_glibc
++
++  return 0
++}
++
++if [ $# -ne 1 ]; then
++    print_error "Error: You need to provide the axera target chip"
++    usage
++    exit 1
++fi
++
++target_chip=$(echo "$1" | tr '[:upper:]' '[:lower:]')
++
++if ! [[ " ${SUPPORTED_TARGETS[*]} " =~ " ${target_chip} " ]]; then
++    print_error "Unsupported target chip '$target_chip'!"
++    print_info "Supported target chips are ${SUPPORTED_TARGETS[*]}"
++    exit 1
++fi
++
++
++if [ -z "$AXERA_SDK_ROOT" ]; then
++  case "$target_chip" in
++    ax650)
++      AXERA_SDK_ROOT=$(download_650_bsp_sdk)
++      ;;
++    ax630c|ax620q)
++      AXERA_SDK_ROOT=$(download_620e_bsp_sdk)
++      ;;
++    *)
++      print_error "Unsupported target chip $target_chip"
++      exit 1
++      ;;
++  esac
++fi
++
++echo "AXERA_SDK_ROOT: $AXERA_SDK_ROOT"
++
++if [ ! -d "$AXERA_SDK_ROOT" ]; then
++  echo "AXERA_SDK_ROOT ($AXERA_SDK_ROOT) does not exist"
++  exit 1
++fi
++
++if [ ! -f "$AXERA_SDK_ROOT/include/ax_engine_api.h" ]; then
++  echo "$AXERA_SDK_ROOT/include/ax_engine_api.h does not exist"
++  exit 1
++fi
++
++if [ ! -f "$AXERA_SDK_ROOT/lib/libax_engine.so" ]; then
++  echo "$AXERA_SDK_ROOT/lib/libax_engine.so does not exist"
++  exit 1
++fi
++
++export CPLUS_INCLUDE_PATH="$AXERA_SDK_ROOT/include:$CPLUS_INCLUDE_PATH"
++
++export SHERPA_ONNX_AXERA_LIB_DIR="$AXERA_SDK_ROOT/lib"
++
++if command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
++  ln -svf $(which aarch64-none-linux-gnu-gcc) ./aarch64-linux-gnu-gcc
++  ln -svf $(which aarch64-none-linux-gnu-g++) ./aarch64-linux-gnu-g++
++  export PATH=$PWD:$PATH
++fi
++
++if ! command -v aarch64-none-linux-gnu-gcc  &> /dev/null; then
++  echo "Please install a toolchain for cross-compiling."
++  echo "You can refer to: "
++  echo "  https://k2-fsa.github.io/sherpa/onnx/install/aarch64-embedded-linux.html"
++  echo "for help."
++  exit 1
++fi
++
++
++dir=$PWD/build-axera-linux-aarch64-$target_chip
++mkdir -p $dir
++
++cd $dir
++
++if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
++  echo "Start to cross-compile alsa-lib"
++  if [ ! -d alsa-lib ]; then
++    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
++  fi
++  # If it shows:
++  #  ./gitcompile: line 79: libtoolize: command not found
++  # Please use:
++  #  sudo apt-get install libtool m4 automake
++  #
++  # If it shows plantuml: command not found
++  # Please use
++  #   sudo apt-get install plantuml
++  pushd alsa-lib
++  CC=aarch64-linux-gnu-gcc ./gitcompile --host=aarch64-linux-gnu
++  popd
++  echo "Finish cross-compiling alsa-lib"
++fi
++
++export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
++export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
++
++if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
++  # By default, use shared link
++  BUILD_SHARED_LIBS=ON
++fi
++
++cmake \
++  -DALSA_INCLUDE_DIR=$PWD/alsa-lib/include \
++  -DALSA_LIBRARY=$PWD/alsa-lib/src/.libs/libasound.so \
++  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
++  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
++  -DBUILD_ESPEAK_NG_EXE=OFF \
++  -DBUILD_ESPEAK_NG_TESTS=OFF \
++  -DCMAKE_INSTALL_PREFIX=./install \
++  -DCMAKE_BUILD_TYPE=Release \
++  -DSHERPA_ONNX_ENABLE_GPU=OFF \
++  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
++  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
++  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
++  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
++  -DSHERPA_ONNX_ENABLE_PORTAUDIO=ON \
++  -DSHERPA_ONNX_ENABLE_JNI=OFF \
++  -DSHERPA_ONNX_ENABLE_C_API=ON \
++  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
++  -DSHERPA_ONNX_ENABLE_AXERA=ON \
++  -DCMAKE_TOOLCHAIN_FILE=../toolchains/aarch64-linux-gnu.toolchain.cmake \
++  ..
++
++make VERBOSE=1 -j2
++make install/strip
++
++
++# Enable it if only needed
++# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 9e6e5647..a9fd3c06 100755
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -196,7 +196,23 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+ 
+ endif()
+ 
+-if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN)
++if(SHERPA_ONNX_ENABLE_AXERA)
++  list(APPEND sources
++    ./axera/offline-sense-voice-model-axera.cc
++    ./axera/utils.cc
++  )
++
++endif()
++
++if(SHERPA_ONNX_ENABLE_AXCL)
++  list(APPEND sources
++    ./axcl/offline-sense-voice-model-axcl.cc
++    ./axcl/ax_model_runner_axcl.cc
++  )
++
++endif()
++
++if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN OR SHERPA_ONNX_ENABLE_AXERA OR SHERPA_ONNX_ENABLE_AXCL)
+   list(APPEND sources
+     ./rknn/offline-ctc-greedy-search-decoder-rknn.cc
+   )
+@@ -327,6 +343,38 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+   endif()
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_AXERA)
++  if(DEFINED ENV{SHERPA_ONNX_AXERA_LIB_DIR})
++    target_link_libraries(sherpa-onnx-core
++      -L$ENV{SHERPA_ONNX_AXERA_LIB_DIR}
++      -lax_engine
++      -lax_interpreter
++      -lax_sys
++      -lpthread
++    )
++  else()
++    target_link_libraries(sherpa-onnx-core
++      ax_engine
++      ax_interpreter
++      ax_sys
++      pthread
++    )
++  endif()
++endif()
++
++if(SHERPA_ONNX_ENABLE_AXCL)
++  if(DEFINED ENV{SHERPA_ONNX_AXCL_LIB_DIR})
++    target_link_libraries(sherpa-onnx-core
++      -L$ENV{SHERPA_ONNX_AXCL_LIB_DIR}
++      -laxcl_rt
++      )
++  else()
++    target_link_libraries(sherpa-onnx-core
++      axcl_rt
++    )
++  endif()
++endif()
++
+ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+     target_include_directories(sherpa-onnx-core PRIVATE ${ASCEND_TOOLKIT_HOME}/include)
+     target_link_libraries(sherpa-onnx-core
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+new file mode 100644
+index 00000000..42baad0d
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/ax_model_runner.hpp
+@@ -0,0 +1,148 @@
++// sherpa-onnx/csrc/axcl/ax_model_runner.hpp
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#pragma once
++#include <map>
++#include <stdexcept>
++#include <string>
++#include <vector>
++
++typedef enum _color_space_e {
++  ax_color_space_unknown,
++  ax_color_space_nv12,
++  ax_color_space_nv21,
++  ax_color_space_bgr,
++  ax_color_space_rgb,
++} ax_color_space_e;
++
++typedef struct {
++  std::string sName;
++  unsigned int nIdx;
++  std::vector<unsigned int> vShape;
++  int nSize;
++  unsigned long long phyAddr;
++  void *pVirAddr;
++} ax_runner_tensor_t;
++
++class ax_runner_base {
++ public:
++  std::vector<ax_runner_tensor_t> moutput_tensors;
++  std::vector<ax_runner_tensor_t> minput_tensors;
++
++  std::vector<std::vector<ax_runner_tensor_t>> mgroup_output_tensors;
++  std::vector<std::vector<ax_runner_tensor_t>> mgroup_input_tensors;
++
++  std::map<std::string, ax_runner_tensor_t> map_output_tensors;
++  std::map<std::string, ax_runner_tensor_t> map_input_tensors;
++
++  std::map<std::string, std::vector<ax_runner_tensor_t>>
++      map_group_output_tensors;
++  std::map<std::string, std::vector<ax_runner_tensor_t>>
++      map_group_input_tensors;
++
++  bool _auto_sync_before_inference = true;
++  bool _auto_sync_after_inference = true;
++
++  float cost_host_to_device = 0;
++  float cost_inference = 0;
++  float cost_device_to_host = 0;
++
++ public:
++  virtual int init(const char *model_file) = 0;
++  virtual int init(char *model_buffer, size_t model_size) = 0;
++
++  virtual void deinit() = 0;
++
++  float get_inference_time() { return cost_inference; }
++
++  int get_num_inputs() { return minput_tensors.size(); };
++  int get_num_outputs() { return moutput_tensors.size(); };
++
++  const ax_runner_tensor_t &get_input(int idx) { return minput_tensors[idx]; }
++  const ax_runner_tensor_t *get_inputs_ptr() { return minput_tensors.data(); }
++  const ax_runner_tensor_t &get_input(std::string name) {
++    if (map_input_tensors.size() == 0) {
++      for (size_t i = 0; i < minput_tensors.size(); i++) {
++        map_input_tensors[minput_tensors[i].sName] = minput_tensors[i];
++      }
++    }
++    if (map_input_tensors.find(name) == map_input_tensors.end()) {
++      throw std::runtime_error("input tensor not found: " + name);
++    }
++
++    return map_input_tensors[name];
++  }
++
++  const ax_runner_tensor_t &get_input(int grpid, int idx) {
++    return mgroup_input_tensors[grpid][idx];
++  }
++  const ax_runner_tensor_t *get_inputs_ptr(int grpid) {
++    return mgroup_input_tensors[grpid].data();
++  }
++  const ax_runner_tensor_t &get_input(int grpid, std::string name) {
++    if (map_group_input_tensors.size() == 0) {
++      for (size_t i = 0; i < mgroup_input_tensors.size(); i++) {
++        for (size_t j = 0; j < mgroup_input_tensors[i].size(); j++) {
++          map_group_input_tensors[mgroup_input_tensors[i][j].sName].push_back(
++              mgroup_input_tensors[i][j]);
++        }
++      }
++    }
++    if (map_group_input_tensors.find(name) == map_group_input_tensors.end()) {
++      throw std::runtime_error("input tensor not found: " + name);
++    }
++    return map_group_input_tensors[name][grpid];
++    // return map_input_tensors[name];
++  }
++
++  const ax_runner_tensor_t &get_output(int idx) { return moutput_tensors[idx]; }
++  const ax_runner_tensor_t *get_outputs_ptr() { return moutput_tensors.data(); }
++  const ax_runner_tensor_t &get_output(std::string name) {
++    if (map_output_tensors.size() == 0) {
++      for (size_t i = 0; i < moutput_tensors.size(); i++) {
++        map_output_tensors[moutput_tensors[i].sName] = moutput_tensors[i];
++      }
++    }
++    if (map_output_tensors.find(name) == map_output_tensors.end()) {
++      throw std::runtime_error("output tensor not found: " + name);
++    }
++
++    return map_output_tensors[name];
++  }
++
++  const ax_runner_tensor_t &get_output(int grpid, int idx) {
++    return mgroup_output_tensors[grpid][idx];
++  }
++  const ax_runner_tensor_t *get_outputs_ptr(int grpid) {
++    return mgroup_output_tensors[grpid].data();
++  }
++  const ax_runner_tensor_t &get_output(int grpid, std::string name) {
++    if (map_group_output_tensors.size() == 0) {
++      for (size_t i = 0; i < mgroup_output_tensors.size(); i++) {
++        for (size_t j = 0; j < mgroup_output_tensors[i].size(); j++) {
++          map_group_output_tensors[mgroup_output_tensors[i][j].sName].push_back(
++              mgroup_output_tensors[i][j]);
++        }
++      }
++    }
++    if (map_group_output_tensors.find(name) == map_group_output_tensors.end()) {
++      throw std::runtime_error("input tensor not found: " + name);
++    }
++    return map_group_output_tensors[name][grpid];
++  }
++
++  virtual int get_algo_width() = 0;
++  virtual int get_algo_height() = 0;
++  virtual ax_color_space_e get_color_space() = 0;
++
++  void set_auto_sync_before_inference(bool sync) {
++    _auto_sync_before_inference = sync;
++  }
++  void set_auto_sync_after_inference(bool sync) {
++    _auto_sync_after_inference = sync;
++  }
++
++  virtual int inference() = 0;
++  virtual int inference(int grpid) = 0;
++};
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+new file mode 100644
+index 00000000..7f3733a9
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
+@@ -0,0 +1,470 @@
++// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.cc
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#include "ax_model_runner_axcl.hpp"
++
++#include <axcl.h>
++#include <fcntl.h>
++#include <string.h>
++
++#include <fstream>
++#include <memory>
++
++typedef enum {
++  AX_ENGINE_ABST_DEFAULT = 0,
++  AX_ENGINE_ABST_CACHED = 1,
++} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
++
++typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
++                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
++    INPUT_OUTPUT_ALLOC_STRATEGY;
++
++static void print_io_info(std::vector<ax_runner_tensor_t> &input,
++                          std::vector<ax_runner_tensor_t> &output) {
++  printf("\ninput size: %ld\n", input.size());
++  for (size_t i = 0; i < input.size(); ++i) {
++    // print shape info,like [batchsize x channel x height x width]
++    auto &info = input[i];
++    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
++    for (size_t s = 0; s < info.vShape.size(); s++) {
++      printf("%d", info.vShape[s]);
++      if (s != info.vShape.size() - 1) {
++        printf(" x ");
++      }
++    }
++    printf("\e[0m\n\n");
++  }
++
++  printf("\noutput size: %ld\n", output.size());
++  for (size_t i = 0; i < output.size(); ++i) {
++    // print shape info,like [batchsize x channel x height x width]
++    auto &info = output[i];
++    printf("    name: \e[1;32m%8s \e[0m\n        \e[1;31m", info.sName.c_str());
++    for (size_t s = 0; s < info.vShape.size(); s++) {
++      printf("%d", info.vShape[s]);
++      if (s != info.vShape.size() - 1) {
++        printf(" x ");
++      }
++    }
++    printf("\e[0m\n\n");
++  }
++}
++
++static bool read_file(const char *fn, std::vector<unsigned char> &data) {
++  FILE *fp = fopen(fn, "r");
++  if (fp != nullptr) {
++    fseek(fp, 0L, SEEK_END);
++    auto len = ftell(fp);
++    fseek(fp, 0, SEEK_SET);
++    data.clear();
++    size_t read_size = 0;
++    if (len > 0) {
++      data.resize(len);
++      read_size = fread(data.data(), 1, len, fp);
++    }
++    fclose(fp);
++    return read_size == (size_t)len;
++  }
++  return false;
++}
++
++typedef struct {
++  int nIndex;
++  int nSize;
++  void *pBuf;
++  void *pVirAddr;
++
++  std::string Name;
++
++  axclrtEngineIODims dims;
++} AXCL_IO_BUF_T;
++
++typedef struct {
++  uint32_t nInputSize;
++  uint32_t nOutputSize;
++  AXCL_IO_BUF_T *pInputs;
++  AXCL_IO_BUF_T *pOutputs;
++} AXCL_IO_DATA_T;
++
++static void free_io_index(AXCL_IO_BUF_T *pBuf, size_t index) {
++  for (size_t i = 0; i < index; ++i) {
++    axclrtFree(pBuf[i].pBuf);
++  }
++}
++
++static void free_io(AXCL_IO_DATA_T *io_data) {
++  for (size_t j = 0; j < io_data->nInputSize; ++j) {
++    axclrtFree(io_data->pInputs[j].pBuf);
++    free(io_data->pInputs[j].pVirAddr);
++  }
++  for (size_t j = 0; j < io_data->nOutputSize; ++j) {
++    axclrtFree(io_data->pOutputs[j].pBuf);
++    free(io_data->pOutputs[j].pVirAddr);
++  }
++  delete[] io_data->pInputs;
++  delete[] io_data->pOutputs;
++}
++
++static inline int prepare_io(int grpid, axclrtEngineIOInfo io_info,
++                             axclrtEngineIO io, AXCL_IO_DATA_T *io_data,
++                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
++  memset(io_data, 0, sizeof(AXCL_IO_DATA_T));
++
++  auto inputNum = axclrtEngineGetNumInputs(io_info);
++  auto outputNum = axclrtEngineGetNumOutputs(io_info);
++  io_data->nInputSize = inputNum;
++  io_data->nOutputSize = outputNum;
++  io_data->pInputs = new AXCL_IO_BUF_T[inputNum];
++  io_data->pOutputs = new AXCL_IO_BUF_T[outputNum];
++
++  // 1. alloc inputs
++  for (uint32_t i = 0; i < inputNum; i++) {
++    auto bufSize = axclrtEngineGetInputSizeByIndex(io_info, grpid, i);
++    void *devPtr = nullptr;
++    axclError ret = 0;
++    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
++      ret = axclrtMalloc(&devPtr, bufSize,
++                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
++    } else {
++      ret = axclrtMallocCached(
++          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
++    }
++
++    if (ret != 0) {
++      free_io_index(io_data->pInputs, i);
++      fprintf(stderr, "Malloc input(index: %d, size: %ld) failed! ret=0x%x\n",
++              i, bufSize, ret);
++      return -1;
++    }
++    std::vector<char> tmp(bufSize, 0);
++    axclrtMemcpy(devPtr, tmp.data(), bufSize,
++                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
++    // axclrtMemset(devPtr, 0, bufSize);
++
++    axclrtEngineIODims dims;
++    ret = axclrtEngineGetInputDims(io_info, grpid, i, &dims);
++    if (ret != 0) {
++      free_io_index(io_data->pInputs, i);
++      fprintf(stderr, "Get input dims(index: %d) failed! ret=0x%x\n", i, ret);
++      return -1;
++    }
++
++    io_data->pInputs[i].nIndex = i;
++    io_data->pInputs[i].nSize = bufSize;
++    io_data->pInputs[i].pBuf = devPtr;
++    io_data->pInputs[i].dims = dims;
++    io_data->pInputs[i].Name = axclrtEngineGetInputNameByIndex(io_info, i);
++    io_data->pInputs[i].pVirAddr = malloc(bufSize);
++    memset(io_data->pInputs[i].pVirAddr, 0, bufSize);
++    ret = axclrtEngineSetInputBufferByIndex(io, i, devPtr, bufSize);
++    if (ret != 0) {
++      free_io_index(io_data->pInputs, i);
++      fprintf(stderr,
++              "Set input buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
++              bufSize, ret);
++      return -1;
++    }
++  }
++
++  // 2. alloc outputs
++  for (uint32_t i = 0; i < outputNum; i++) {
++    auto bufSize = axclrtEngineGetOutputSizeByIndex(io_info, grpid, i);
++    void *devPtr = NULL;
++    axclError ret = 0;
++    if (AX_ENGINE_ABST_DEFAULT == strategy.first) {
++      ret = axclrtMalloc(&devPtr, bufSize,
++                         axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
++    } else {
++      ret = axclrtMallocCached(
++          &devPtr, bufSize, axclrtMemMallocPolicy::AXCL_MEM_MALLOC_HUGE_FIRST);
++    }
++
++    if (ret != 0) {
++      free_io_index(io_data->pOutputs, i);
++      fprintf(stderr, "Malloc output(index: %d, size: %ld) failed! ret=0x%x\n",
++              i, bufSize, ret);
++      return -1;
++    }
++    std::vector<char> tmp(bufSize, 0);
++    axclrtMemcpy(devPtr, tmp.data(), bufSize,
++                 axclrtMemcpyKind::AXCL_MEMCPY_HOST_TO_DEVICE);
++    axclrtEngineIODims dims;
++    ret = axclrtEngineGetOutputDims(io_info, grpid, i, &dims);
++    if (ret != 0) {
++      free_io_index(io_data->pOutputs, i);
++      fprintf(stderr, "Get output dims(index: %d) failed! ret=0x%x\n", i, ret);
++      return -1;
++    }
++
++    io_data->pOutputs[i].nIndex = i;
++    io_data->pOutputs[i].nSize = bufSize;
++    io_data->pOutputs[i].pBuf = devPtr;
++    io_data->pOutputs[i].dims = dims;
++    io_data->pOutputs[i].Name = axclrtEngineGetOutputNameByIndex(io_info, i);
++    io_data->pOutputs[i].pVirAddr = malloc(bufSize);
++    memset(io_data->pOutputs[i].pVirAddr, 0, bufSize);
++    ret = axclrtEngineSetOutputBufferByIndex(io, i, devPtr, bufSize);
++    if (ret != 0) {
++      free_io_index(io_data->pOutputs, i);
++      fprintf(stderr,
++              "Set output buffer(index: %d, size: %lu) failed! ret=0x%x\n", i,
++              bufSize, ret);
++      return -1;
++    }
++  }
++
++  return 0;
++}
++
++struct ax_joint_runner_axcl_handle_t {
++  uint64_t handle = 0;
++  uint64_t context = 0;
++  axclrtEngineIOInfo io_info = 0;
++  std::vector<axclrtEngineIO> ios;
++  std::vector<AXCL_IO_DATA_T> io_datas;
++
++  // int algo_width, algo_height;
++  // int algo_colorformat;
++};
++
++int ax_runner_axcl::sub_init() {
++  // 4. create context
++  int ret = axclrtEngineCreateContext(m_handle->handle, &m_handle->context);
++  if (0 != ret) {
++    fprintf(stderr, "axclrtEngineCreateContext failed.\n");
++    return ret;
++  }
++  fprintf(stdout, "axclrtEngineCreateContextt is done. \n");
++
++  // 5. set io
++
++  ret = axclrtEngineGetIOInfo(m_handle->handle, &m_handle->io_info);
++  if (0 != ret) {
++    fprintf(stderr, "axclrtEngineGetIOInfo failed.\n");
++    return ret;
++  }
++  fprintf(stdout, "axclrtEngineGetIOInfo is done. \n");
++
++  ret = axclrtEngineGetShapeGroupsCount(m_handle->io_info, &group_count);
++  if (ret != 0) {
++    axclrtEngineUnload(m_handle->handle);
++    return ret;
++  }
++
++  // 6. alloc io
++  if (!_parepare_io) {
++    m_handle->ios.resize(group_count);
++    m_handle->io_datas.resize(group_count);
++    mgroup_input_tensors.resize(group_count);
++    mgroup_output_tensors.resize(group_count);
++
++    memset(&m_handle->io_datas[0], 0, sizeof(AXCL_IO_DATA_T) * group_count);
++
++    auto malloc_strategy =
++        std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_DEFAULT);
++
++    for (int grpid = 0; grpid < group_count; grpid++) {
++      ret = axclrtEngineCreateIO(m_handle->io_info, &m_handle->ios[grpid]);
++      if (ret != 0) {
++        axclrtEngineUnload(m_handle->handle);
++        fprintf(stderr, "Create io failed. ret=0x%x\n", ret);
++        return -1;
++      }
++
++      ret = prepare_io(grpid, m_handle->io_info, m_handle->ios[grpid],
++                       &m_handle->io_datas[grpid], malloc_strategy);
++      if (ret != 0) {
++        free_io(&m_handle->io_datas[grpid]);
++        axclrtEngineDestroyIO(m_handle->ios[grpid]);
++        axclrtEngineUnload(m_handle->handle);
++
++        fprintf(stderr, "prepare_io failed.\n");
++        return ret;
++      }
++    }
++
++    for (int grpid = 0; grpid < group_count; grpid++) {
++      // auto &io_info = m_handle->io_info[grpid];
++      auto &io_data = m_handle->io_datas[grpid];
++      for (uint32_t i = 0; i < io_data.nOutputSize; i++) {
++        ax_runner_tensor_t tensor;
++        tensor.nIdx = i;
++        tensor.sName = std::string(io_data.pOutputs[i].Name);
++        tensor.nSize = io_data.pOutputs[i].nSize;
++        for (int32_t j = 0; j < io_data.pOutputs[i].dims.dimCount; j++) {
++          tensor.vShape.push_back(io_data.pOutputs[i].dims.dims[j]);
++        }
++        tensor.phyAddr = (unsigned long long)io_data.pOutputs[i].pBuf;
++        tensor.pVirAddr = io_data.pOutputs[i].pVirAddr;
++        mgroup_output_tensors[grpid].push_back(tensor);
++      }
++
++      for (size_t i = 0; i < io_data.nInputSize; i++) {
++        ax_runner_tensor_t tensor;
++        tensor.nIdx = i;
++        tensor.sName = std::string(io_data.pInputs[i].Name);
++        tensor.nSize = io_data.pInputs[i].nSize;
++        for (int32_t j = 0; j < io_data.pInputs[i].dims.dimCount; j++) {
++          tensor.vShape.push_back(io_data.pInputs[i].dims.dims[j]);
++        }
++        tensor.phyAddr = (unsigned long long)io_data.pInputs[i].pBuf;
++        tensor.pVirAddr = io_data.pInputs[i].pVirAddr;
++        mgroup_input_tensors[grpid].push_back(tensor);
++      }
++    }
++
++    moutput_tensors = mgroup_output_tensors[0];
++    minput_tensors = mgroup_input_tensors[0];
++    _parepare_io = true;
++  } else {
++  }
++  // for (int grpid = 0; grpid < group_count; grpid++) {
++  //   printf("\ngrpid: %d\n", grpid);
++  //   print_io_info(mgroup_input_tensors[grpid], mgroup_output_tensors[grpid]);
++  //   printf("==================================================\n\n");
++  // }
++
++  return ret;
++}
++
++int ax_runner_axcl::init(const char *model_file) {
++  std::vector<unsigned char> model_buffer;
++  if (!read_file(model_file, model_buffer)) {
++    fprintf(stderr, "read_file failed.\n");
++    return -1;
++  }
++  auto ret = init((char *)model_buffer.data(), model_buffer.size());
++  return ret;
++}
++
++int ax_runner_axcl::init(char *model_buffer, size_t model_size) {
++  if (!m_handle) {
++    m_handle = new ax_joint_runner_axcl_handle_t;
++  }
++  memset((void *)m_handle, 0, sizeof(ax_joint_runner_axcl_handle_t));
++
++  // 3. create handle
++  void *devMem = nullptr;
++  axclrtMalloc(&devMem, model_size, AXCL_MEM_MALLOC_NORMAL_ONLY);
++
++  // 4. copy model to device
++  axclrtMemcpy(devMem, model_buffer, model_size, AXCL_MEMCPY_HOST_TO_DEVICE);
++
++  int ret = axclrtEngineLoadFromMem(devMem, model_size, &m_handle->handle);
++  if (0 != ret) {
++    fprintf(stderr, "AX_ENGINE_CreateHandle");
++    return ret;
++  }
++  axclrtFree(devMem);
++
++  return sub_init();
++}
++
++void ax_runner_axcl::release() {
++  if (m_handle && m_handle->handle) {
++    for (int grpid = 0; grpid < group_count; grpid++) {
++      free_io(&m_handle->io_datas[grpid]);
++      axclrtEngineDestroyIO(m_handle->ios[grpid]);
++    }
++
++    axclrtEngineUnload(m_handle->handle);
++    m_handle->handle = 0;
++  }
++
++  if (m_handle) {
++    delete m_handle;
++    m_handle = nullptr;
++  }
++
++  minput_tensors.clear();
++  moutput_tensors.clear();
++
++  map_input_tensors.clear();
++  map_output_tensors.clear();
++
++  mgroup_input_tensors.clear();
++  mgroup_output_tensors.clear();
++
++  map_group_input_tensors.clear();
++  map_group_output_tensors.clear();
++}
++
++void ax_runner_axcl::deinit() {
++  if (m_handle && m_handle->handle) {
++    axclrtEngineUnload(m_handle->handle);
++    m_handle->handle = 0;
++  }
++}
++
++int ax_runner_axcl::get_algo_width() {
++  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
++    return minput_tensors[0].vShape[2];
++  }
++  return -1;
++}
++int ax_runner_axcl::get_algo_height() {
++  if (minput_tensors.size() == 1 && minput_tensors[0].vShape.size() == 4) {
++    return minput_tensors[0].vShape[1];
++  }
++  return -1;
++}
++
++int ax_runner_axcl::set_input(int grpid, int idx,
++                              unsigned long long int phy_addr,
++                              unsigned long size) {
++  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid], idx,
++                                           (void *)phy_addr, size);
++}
++int ax_runner_axcl::set_output(int grpid, int idx,
++                               unsigned long long int phy_addr,
++                               unsigned long size) {
++  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid], idx,
++                                            (void *)phy_addr, size);
++}
++
++int ax_runner_axcl::set_input(int grpid, std::string name,
++                              unsigned long long int phy_addr,
++                              unsigned long size) {
++  return axclrtEngineSetInputBufferByIndex(m_handle->ios[grpid],
++                                           get_input(grpid, name).nIdx,
++                                           (void *)phy_addr, size);
++}
++
++int ax_runner_axcl::set_output(int grpid, std::string name,
++                               unsigned long long int phy_addr,
++                               unsigned long size) {
++  return axclrtEngineSetOutputBufferByIndex(m_handle->ios[grpid],
++                                            get_output(grpid, name).nIdx,
++                                            (void *)phy_addr, size);
++}
++
++ax_color_space_e ax_runner_axcl::get_color_space() {
++  return ax_color_space_unknown;
++}
++
++int ax_runner_axcl::inference() { return inference(0); }
++
++int ax_runner_axcl::inference(int grpid) {
++  if (_auto_sync_before_inference)
++    for (size_t i = 0; i < mgroup_input_tensors[grpid].size(); i++)
++      axclrtMemcpy((void *)mgroup_input_tensors[grpid][i].phyAddr,
++                   mgroup_input_tensors[grpid][i].pVirAddr,
++                   mgroup_input_tensors[grpid][i].nSize,
++                   AXCL_MEMCPY_HOST_TO_DEVICE);
++
++  auto ret = axclrtEngineExecute(m_handle->handle, m_handle->context, grpid,
++                                 m_handle->ios[grpid]);
++  if (ret != 0) {
++    fprintf(stderr, "axclrtEngineExecute failed. ret=0x%x\n", ret);
++    return ret;
++  }
++
++  if (_auto_sync_after_inference)
++    for (size_t i = 0; i < mgroup_output_tensors[grpid].size(); i++)
++      axclrtMemcpy(mgroup_output_tensors[grpid][i].pVirAddr,
++                   (void *)mgroup_output_tensors[grpid][i].phyAddr,
++                   mgroup_output_tensors[grpid][i].nSize,
++                   AXCL_MEMCPY_DEVICE_TO_HOST);
++  return 0;
++}
+diff --git a/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+new file mode 100644
+index 00000000..4aba11b0
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
+@@ -0,0 +1,40 @@
++// sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#pragma once
++#include "ax_model_runner.hpp"
++
++class ax_runner_axcl : public ax_runner_base {
++ protected:
++  struct ax_joint_runner_axcl_handle_t *m_handle = nullptr;
++  int group_count = 0;
++  bool _parepare_io = false;
++
++  int sub_init();
++
++ public:
++  int init(const char *model_file) override;
++  int init(char *model_buffer, size_t model_size) override;
++
++  void release();
++  void deinit() override;
++
++  int get_algo_width() override;
++  int get_algo_height() override;
++  ax_color_space_e get_color_space() override;
++
++  int set_input(int grpid, int idx, unsigned long long int phy_addr,
++                unsigned long size);
++  int set_output(int grpid, int idx, unsigned long long int phy_addr,
++                 unsigned long size);
++
++  int set_input(int grpid, std::string name, unsigned long long int phy_addr,
++                unsigned long size);
++  int set_output(int grpid, std::string name, unsigned long long int phy_addr,
++                 unsigned long size);
++
++  // int inference(ax_image_t *pstFrame) override;
++  int inference() override;
++  int inference(int grpid) override;
++};
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+new file mode 100644
+index 00000000..59062ac4
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
+@@ -0,0 +1,138 @@
++// sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
++#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../online-recognizer-sense-voice-impl.h
++OfflineRecognitionResult ConvertSenseVoiceResult(
++    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
++    int32_t frame_shift_ms, int32_t subsampling_factor);
++
++class OfflineRecognizerSenseVoiceAxclImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerSenseVoiceAxclImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(
++            std::make_unique<OfflineSenseVoiceModelAxcl>(config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerSenseVoiceAxclImpl(Manager *mgr,
++                                      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineSenseVoiceModelAxcl>(
++            mgr, config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i < n; ++i) {
++      DecodeOneStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  void InitFeatConfig() {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    config_.feat_config.normalize_samples = meta_data.normalize_samples;
++    config_.feat_config.window_type = "hamming";
++    config_.feat_config.high_freq = 0;
++    config_.feat_config.snip_edges = true;
++  }
++
++  void DecodeOneStream(OfflineStream *s) const {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    std::vector<float> f = s->GetFrames();
++
++    int32_t language = 0;
++    if (config_.model_config.sense_voice.language.empty()) {
++      language = 0;
++    } else if (meta_data.lang2id.count(
++                   config_.model_config.sense_voice.language)) {
++      language =
++          meta_data.lang2id.at(config_.model_config.sense_voice.language);
++    } else {
++      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
++                       config_.model_config.sense_voice.language.c_str());
++    }
++
++    int32_t text_norm = config_.model_config.sense_voice.use_itn
++                            ? meta_data.with_itn_id
++                            : meta_data.without_itn_id;
++
++    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
++
++    auto result =
++        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
++
++    int32_t frame_shift_ms = 10;
++    int32_t subsampling_factor = meta_data.window_shift;
++    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
++                                     subsampling_factor);
++
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineSenseVoiceModelAxcl> model_;
++  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_RECOGNIZER_SENSE_VOICE_AXCL_IMPL_H_
+diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+new file mode 100644
+index 00000000..54461590
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
+@@ -0,0 +1,196 @@
++// sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.cc
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#include "sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h"
++
++#include <algorithm>
++#include <array>
++#include <cstring>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelAxcl::Impl {
++ public:
++  ~Impl() { runner_.release(); }
++
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    auto buf = ReadFile(config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    auto buf = ReadFile(mgr, config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++  }
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
++    return meta_data_;
++  }
++
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) {
++    features = ApplyLFR(std::move(features));
++    std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
++
++    // input 0: features
++    auto &in0 = runner_.get_input(0);
++    size_t bytes0 = in0.nSize;
++    if (bytes0 != features.size() * sizeof(float)) {
++      SHERPA_ONNX_LOGE(
++          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
++          in0.nSize, features.size() * sizeof(float));
++      SHERPA_ONNX_EXIT(-1);
++    }
++    std::memcpy(in0.pVirAddr, features.data(), bytes0);
++
++    auto &in1 = runner_.get_input(1);
++    size_t bytes1 = in1.nSize;
++    if (bytes1 != prompt.size() * sizeof(int32_t)) {
++      SHERPA_ONNX_LOGE(
++          "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
++          in1.nSize, prompt.size() * sizeof(int32_t));
++      SHERPA_ONNX_EXIT(-1);
++    }
++    std::memcpy(in1.pVirAddr, prompt.data(), bytes1);
++
++    int ret = runner_.inference();
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("ax_runner_axcl inference failed, ret = %d", ret);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    // output 0
++    auto &out0 = runner_.get_output(0);
++    size_t out_elems = out0.nSize / sizeof(float);
++    std::vector<float> out(out_elems);
++    std::memcpy(out.data(), out0.pVirAddr, out0.nSize);
++    return out;
++  }
++
++ private:
++  void Init(void *model_data, size_t model_data_length) {
++    {
++      if (auto ret = axclInit(0); 0 != ret) {
++        fprintf(stderr, "Init AXCL failed{0x%8x}.\n", ret);
++        return;
++      }
++      axclrtDeviceList lst;
++      if (const auto ret = axclrtGetDeviceList(&lst);
++          0 != ret || 0 == lst.num) {
++        fprintf(stderr,
++                "Get AXCL device failed{0x%8x}, find total %d device.\n", ret,
++                lst.num);
++        return;
++      }
++      if (const auto ret = axclrtSetDevice(lst.devices[0]); 0 != ret) {
++        fprintf(stderr, "Set AXCL device failed{0x%8x}.\n", ret);
++        return;
++      }
++      int ret = axclrtEngineInit(AXCL_VNPU_DISABLE);
++      if (0 != ret) {
++        fprintf(stderr, "axclrtEngineInit %d\n", ret);
++        return;
++      }
++    }
++
++    int ret =
++        runner_.init(reinterpret_cast<char *>(model_data), model_data_length);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("Init ax_runner_axcl failed, ret = %d", ret);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    auto &in0 = runner_.get_input(0);
++    if (in0.vShape.size() < 2) {
++      SHERPA_ONNX_LOGE(
++          "Input tensor rank is too small (rank = %zu). Shape vector is empty "
++          "or has only 1 dim.",
++          in0.vShape.size());
++      SHERPA_ONNX_EXIT(-1);
++    }
++    num_input_frames_ = in0.vShape[1];
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("Axcl SenseVoice model init done with ax_runner_axcl.");
++      SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
++    }
++  }
++
++  std::vector<float> ApplyLFR(std::vector<float> in) const {
++    int32_t lfr_window_size = meta_data_.window_size;
++    int32_t lfr_window_shift = meta_data_.window_shift;
++    int32_t in_feat_dim = 80;
++    int32_t in_num_frames = in.size() / in_feat_dim;
++    int32_t out_num_frames =
++        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
++
++    if (out_num_frames > num_input_frames_) {
++      SHERPA_ONNX_LOGE(
++          "Number of input frames %d is too large. Truncate it to %d frames.",
++          out_num_frames, num_input_frames_);
++      SHERPA_ONNX_LOGE(
++          "Recognition result may be truncated/incomplete. Please select a "
++          "model accepting longer audios.");
++      out_num_frames = num_input_frames_;
++    }
++
++    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
++    std::vector<float> out(num_input_frames_ * out_feat_dim);
++    const float *p_in = in.data();
++    float *p_out = out.data();
++    for (int32_t i = 0; i != out_num_frames; ++i) {
++      std::copy(p_in, p_in + out_feat_dim, p_out);
++      p_out += out_feat_dim;
++      p_in += lfr_window_shift * in_feat_dim;
++    }
++    return out;
++  }
++
++ private:
++  OfflineModelConfig config_;
++  ax_runner_axcl runner_;
++  OfflineSenseVoiceModelMetaData meta_data_;
++  int32_t num_input_frames_ = -1;
++};
++
++OfflineSenseVoiceModelAxcl::~OfflineSenseVoiceModelAxcl() = default;
++
++OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++std::vector<float> OfflineSenseVoiceModelAxcl::Run(std::vector<float> features,
++                                                   int32_t language,
++                                                   int32_t text_norm) const {
++  return impl_->Run(std::move(features), language, text_norm);
++}
++
++const OfflineSenseVoiceModelMetaData &
++OfflineSenseVoiceModelAxcl::GetModelMetadata() const {
++  return impl_->GetModelMetadata();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineSenseVoiceModelAxcl::OfflineSenseVoiceModelAxcl(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+new file mode 100644
+index 00000000..10391ee4
+--- /dev/null
++++ b/sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/axcl/offline-sense-voice-model-axcl.h
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#ifndef SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
++#define SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
++
++#include <memory>
++#include <vector>
++
++#include "axcl.h"
++#include "sherpa-onnx/csrc/axcl/ax_model_runner_axcl.hpp"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelAxcl {
++ public:
++  ~OfflineSenseVoiceModelAxcl();
++
++  explicit OfflineSenseVoiceModelAxcl(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineSenseVoiceModelAxcl(Manager *mgr, const OfflineModelConfig &config);
++
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) const;
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXCL_OFFLINE_SENSE_VOICE_MODEL_AXCL_H_
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axera/io.hpp b/sherpa-onnx/csrc/axera/io.hpp
+new file mode 100644
+index 00000000..3d3f2535
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/io.hpp
+@@ -0,0 +1,255 @@
++// sherpa-onnx/csrc/axera/io.hpp
++//
++// This file is adapted from AXERA's ax-samples project.
++// See the original BSD 3-Clause license below.
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++/*
++ * AXERA is pleased to support the open source community by making ax-samples
++ * available.
++ *
++ * Copyright (c) 2022, AXERA Semiconductor (Shanghai) Co., Ltd. All rights
++ * reserved.
++ *
++ * Licensed under the BSD 3-Clause License (the "License"); you may not use this
++ * file except in compliance with the License. You may obtain a copy of the
++ * License at
++ *
++ * https://opensource.org/licenses/BSD-3-Clause
++ *
++ * Unless required by applicable law or agreed to in writing, software
++ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
++ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
++ * License for the specific language governing permissions and limitations under
++ * the License.
++ */
++/*
++ * Author: AXERA Corporation
++ */
++
++#pragma once
++
++#include <ax_engine_api.h>
++#include <ax_sys_api.h>
++
++#include <cstdio>
++#include <cstring>
++#include <map>
++#include <utility>
++#include <vector>
++
++#define AX_CMM_ALIGN_SIZE 128
++
++inline const char *AX_CMM_SESSION_NAME = "ax-samples-cmm";
++
++typedef enum {
++  AX_ENGINE_ABST_DEFAULT = 0,
++  AX_ENGINE_ABST_CACHED = 1,
++} AX_ENGINE_ALLOC_BUFFER_STRATEGY_T;
++
++typedef std::pair<AX_ENGINE_ALLOC_BUFFER_STRATEGY_T,
++                  AX_ENGINE_ALLOC_BUFFER_STRATEGY_T>
++    INPUT_OUTPUT_ALLOC_STRATEGY;
++
++#define SAMPLE_AX_ENGINE_DEAL_HANDLE        \
++  if (0 != ret) {                           \
++    return AX_ENGINE_DestroyHandle(handle); \
++  }
++
++#define SAMPLE_AX_ENGINE_DEAL_HANDLE_IO     \
++  if (0 != ret) {                           \
++    middleware::free_io(&io_data);          \
++    return AX_ENGINE_DestroyHandle(handle); \
++  }
++
++namespace middleware {
++
++inline void free_io_index(AX_ENGINE_IO_BUFFER_T *io_buf, size_t index) {
++  for (int i = 0; i < (int)index; ++i) {
++    AX_ENGINE_IO_BUFFER_T *pBuf = io_buf + i;
++    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
++  }
++}
++
++inline void free_io(AX_ENGINE_IO_T *io) {
++  for (size_t j = 0; j < io->nInputSize; ++j) {
++    AX_ENGINE_IO_BUFFER_T *pBuf = io->pInputs + j;
++    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
++  }
++  for (size_t j = 0; j < io->nOutputSize; ++j) {
++    AX_ENGINE_IO_BUFFER_T *pBuf = io->pOutputs + j;
++    AX_SYS_MemFree(pBuf->phyAddr, pBuf->pVirAddr);
++  }
++  delete[] io->pInputs;
++  delete[] io->pOutputs;
++}
++
++static inline int prepare_io(AX_ENGINE_IO_INFO_T *info, AX_ENGINE_IO_T *io_data,
++                             INPUT_OUTPUT_ALLOC_STRATEGY strategy) {
++  memset(io_data, 0, sizeof(*io_data));
++  io_data->pInputs = new AX_ENGINE_IO_BUFFER_T[info->nInputSize];
++  memset(io_data->pInputs, 0, sizeof(AX_ENGINE_IO_BUFFER_T) * info->nInputSize);
++  io_data->nInputSize = info->nInputSize;
++
++  auto ret = 0;
++  for (int i = 0; i < (int)info->nInputSize; ++i) {
++    auto meta = info->pInputs[i];
++    auto buffer = &io_data->pInputs[i];
++    if (strategy.first == AX_ENGINE_ABST_CACHED) {
++      ret = AX_SYS_MemAllocCached(
++          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
++          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
++    } else {
++      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
++                            meta.nSize, AX_CMM_ALIGN_SIZE,
++                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
++    }
++
++    if (ret != 0) {
++      free_io_index(io_data->pInputs, i);
++      fprintf(
++          stderr,
++          "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
++          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
++      return ret;
++    }
++    // fprintf(stderr, "Allocate input{%d} { phy: %p, vir: %p, size: %lu Bytes
++    // }. \n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
++  }
++
++  io_data->pOutputs = new AX_ENGINE_IO_BUFFER_T[info->nOutputSize];
++  memset(io_data->pOutputs, 0,
++         sizeof(AX_ENGINE_IO_BUFFER_T) * info->nOutputSize);
++  io_data->nOutputSize = info->nOutputSize;
++  for (int i = 0; i < (int)info->nOutputSize; ++i) {
++    auto meta = info->pOutputs[i];
++    auto buffer = &io_data->pOutputs[i];
++    buffer->nSize = meta.nSize;
++    if (strategy.second == AX_ENGINE_ABST_CACHED) {
++      ret = AX_SYS_MemAllocCached(
++          (AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr, meta.nSize,
++          AX_CMM_ALIGN_SIZE, (const AX_S8 *)(AX_CMM_SESSION_NAME));
++    } else {
++      ret = AX_SYS_MemAlloc((AX_U64 *)(&buffer->phyAddr), &buffer->pVirAddr,
++                            meta.nSize, AX_CMM_ALIGN_SIZE,
++                            (const AX_S8 *)(AX_CMM_SESSION_NAME));
++    }
++    if (ret != 0) {
++      fprintf(
++          stderr,
++          "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes }. fail \n",
++          i, (void *)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
++      free_io_index(io_data->pInputs, io_data->nInputSize);
++      free_io_index(io_data->pOutputs, i);
++      return ret;
++    }
++    // fprintf(stderr, "Allocate output{%d} { phy: %p, vir: %p, size: %lu Bytes
++    // }.\n", i, (void*)buffer->phyAddr, buffer->pVirAddr, (long)meta.nSize);
++  }
++
++  return 0;
++}
++
++static int push_input(const std::vector<uint8_t> &data, AX_ENGINE_IO_T *io_t,
++                      AX_ENGINE_IO_INFO_T *info_t) {
++  if (info_t->nInputSize != 1) {
++    fprintf(stderr, "Only support Input size == 1 current now");
++    return -1;
++  }
++
++  if (data.size() != info_t->pInputs[0].nSize) {
++    fprintf(stderr,
++            "The input data size is not matched with tensor {name: %s, size: "
++            "%d}.\n",
++            info_t->pInputs[0].pName, info_t->pInputs[0].nSize);
++    return -1;
++  }
++
++  memcpy(io_t->pInputs[0].pVirAddr, data.data(), data.size());
++
++  return 0;
++}
++
++static void print_io_info(AX_ENGINE_IO_INFO_T *io_info) {
++  static std::map<AX_ENGINE_DATA_TYPE_T, const char *> data_type = {
++      {AX_ENGINE_DT_UNKNOWN, "UNKNOWN"},
++      {AX_ENGINE_DT_UINT8, "UINT8"},
++      {AX_ENGINE_DT_UINT16, "UINT16"},
++      {AX_ENGINE_DT_FLOAT32, "FLOAT32"},
++      {AX_ENGINE_DT_SINT16, "SINT16"},
++      {AX_ENGINE_DT_SINT8, "SINT8"},
++      {AX_ENGINE_DT_SINT32, "SINT32"},
++      {AX_ENGINE_DT_UINT32, "UINT32"},
++      {AX_ENGINE_DT_FLOAT64, "FLOAT64"},
++      {AX_ENGINE_DT_UINT10_PACKED, "UINT10_PACKED"},
++      {AX_ENGINE_DT_UINT12_PACKED, "UINT12_PACKED"},
++      {AX_ENGINE_DT_UINT14_PACKED, "UINT14_PACKED"},
++      {AX_ENGINE_DT_UINT16_PACKED, "UINT16_PACKED"},
++  };
++
++  static std::map<AX_ENGINE_COLOR_SPACE_T, const char *> color_type = {
++      {AX_ENGINE_CS_FEATUREMAP, "FEATUREMAP"},
++      {AX_ENGINE_CS_RAW8, "RAW8"},
++      {AX_ENGINE_CS_RAW10, "RAW10"},
++      {AX_ENGINE_CS_RAW12, "RAW12"},
++      {AX_ENGINE_CS_RAW14, "RAW14"},
++      {AX_ENGINE_CS_RAW16, "RAW16"},
++      {AX_ENGINE_CS_NV12, "NV12"},
++      {AX_ENGINE_CS_NV21, "NV21"},
++      {AX_ENGINE_CS_RGB, "RGB"},
++      {AX_ENGINE_CS_BGR, "BGR"},
++      {AX_ENGINE_CS_RGBA, "RGBA"},
++      {AX_ENGINE_CS_GRAY, "GRAY"},
++      {AX_ENGINE_CS_YUV444, "YUV444"},
++  };
++  printf("\ninput size: %d\n", io_info->nInputSize);
++  for (uint32_t i = 0; i < io_info->nInputSize; ++i) {
++    // print shape info,like [batchsize x channel x height x width]
++    auto &info = io_info->pInputs[i];
++    printf("    name: \e[1;32m%8s", info.pName);
++
++    std::string dt = "unknown";
++    if (data_type.find(info.eDataType) != data_type.end()) {
++      dt = data_type[info.eDataType];
++      printf(" \e[1;34m[%s] ", dt.c_str());
++    } else {
++      printf(" \e[1;31m[%s] ", dt.c_str());
++    }
++
++    std::string ct = "unknown";
++    if (info.pExtraMeta &&
++        color_type.find(info.pExtraMeta->eColorSpace) != color_type.end()) {
++      ct = color_type[info.pExtraMeta->eColorSpace];
++      printf("\e[1;34m[%s]", ct.c_str());
++    } else {
++      printf("\e[1;31m[%s]", ct.c_str());
++    }
++    printf(" \n        \e[1;31m");
++
++    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
++      printf("%d", info.pShape[s]);
++      if (s != info.nShapeSize - 1) {
++        printf(" x ");
++      }
++    }
++    printf("\e[0m\n\n");
++  }
++
++  printf("\noutput size: %d\n", io_info->nOutputSize);
++  for (uint32_t i = 0; i < io_info->nOutputSize; ++i) {
++    // print shape info,like [batchsize x channel x height x width]
++    auto &info = io_info->pOutputs[i];
++    printf("    name: \e[1;32m%8s \e[1;34m[%s]\e[0m\n        \e[1;31m",
++           info.pName, data_type[info.eDataType]);
++    for (AX_U8 s = 0; s < info.nShapeSize; s++) {
++      printf("%d", info.pShape[s]);
++      if (s != info.nShapeSize - 1) {
++        printf(" x ");
++      }
++    }
++    printf("\e[0m\n\n");
++  }
++}
++
++}  // namespace middleware
+diff --git a/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+new file mode 100644
+index 00000000..039976e1
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h
+@@ -0,0 +1,138 @@
++// sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-axera-impl.h
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
++#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../online-recognizer-sense-voice-impl.h
++OfflineRecognitionResult ConvertSenseVoiceResult(
++    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
++    int32_t frame_shift_ms, int32_t subsampling_factor);
++
++class OfflineRecognizerSenseVoiceAxeraImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerSenseVoiceAxeraImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
++            config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerSenseVoiceAxeraImpl(Manager *mgr,
++                                       const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineSenseVoiceModelAxera>(
++            mgr, config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i < n; ++i) {
++      DecodeOneStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  void InitFeatConfig() {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    config_.feat_config.normalize_samples = meta_data.normalize_samples;
++    config_.feat_config.window_type = "hamming";
++    config_.feat_config.high_freq = 0;
++    config_.feat_config.snip_edges = true;
++  }
++
++  void DecodeOneStream(OfflineStream *s) const {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    std::vector<float> f = s->GetFrames();
++
++    int32_t language = 0;
++    if (config_.model_config.sense_voice.language.empty()) {
++      language = 0;
++    } else if (meta_data.lang2id.count(
++                   config_.model_config.sense_voice.language)) {
++      language =
++          meta_data.lang2id.at(config_.model_config.sense_voice.language);
++    } else {
++      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
++                       config_.model_config.sense_voice.language.c_str());
++    }
++
++    int32_t text_norm = config_.model_config.sense_voice.use_itn
++                            ? meta_data.with_itn_id
++                            : meta_data.without_itn_id;
++
++    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
++
++    auto result =
++        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
++
++    int32_t frame_shift_ms = 10;
++    int32_t subsampling_factor = meta_data.window_shift;
++    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
++                                     subsampling_factor);
++
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineSenseVoiceModelAxera> model_;
++  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_RECOGNIZER_SENSE_VOICE_AXERA_IMPL_H_
+diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+new file mode 100644
+index 00000000..7e964791
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
+@@ -0,0 +1,216 @@
++// sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.cc
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#include "sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h"
++
++#include <algorithm>
++#include <array>
++#include <cstring>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/axera/io.hpp"
++#include "sherpa-onnx/csrc/axera/utils.h"
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelAxera::Impl {
++ public:
++  ~Impl() {
++    middleware::free_io(&io_data_);
++    if (handle_) {
++      AX_ENGINE_DestroyHandle(handle_);
++    }
++  }
++
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    auto buf = ReadFile(config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    auto buf = ReadFile(mgr, config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++  }
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
++    return meta_data_;
++  }
++
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) {
++    features = ApplyLFR(std::move(features));
++
++    std::array<int32_t, 4> prompt{language, 1, 2, text_norm};
++
++    if (!io_info_ || io_info_->nInputSize < 1) {
++      SHERPA_ONNX_LOGE("Axera model expects at least 1 input tensor");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    const auto &in0_meta = io_info_->pInputs[0];
++    size_t bytes0 = in0_meta.nSize;
++
++    if (bytes0 != features.size() * sizeof(float)) {
++      SHERPA_ONNX_LOGE(
++          "Feature size mismatch. model expects %u bytes, but got %zu bytes",
++          in0_meta.nSize, features.size() * sizeof(float));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::memcpy(io_data_.pInputs[0].pVirAddr, features.data(), bytes0);
++
++    //   io_info_->nInputSize >= 2
++    //   io_info_->pInputs[1].nSize == prompt.size() * sizeof(int32_t)
++    if (io_info_->nInputSize >= 2) {
++      const auto &in1_meta = io_info_->pInputs[1];
++      size_t bytes1 = in1_meta.nSize;
++      if (bytes1 != prompt.size() * sizeof(int32_t)) {
++        SHERPA_ONNX_LOGE(
++            "Prompt size mismatch. model expects %u bytes, but got %zu bytes",
++            in1_meta.nSize, prompt.size() * sizeof(int32_t));
++        SHERPA_ONNX_EXIT(-1);
++      }
++      std::memcpy(io_data_.pInputs[1].pVirAddr, prompt.data(), bytes1);
++    }
++
++    auto ret = AX_ENGINE_RunSync(handle_, &io_data_);
++    if (ret != 0) {
++      SHERPA_ONNX_LOGE("AX_ENGINE_RunSync failed, ret = %d", ret);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (io_info_->nOutputSize < 1) {
++      SHERPA_ONNX_LOGE("Axera model has no output tensor");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    const auto &out_meta = io_info_->pOutputs[0];
++    auto &out_buf = io_data_.pOutputs[0];
++
++    size_t out_elems = out_meta.nSize / sizeof(float);
++    std::vector<float> out(out_elems);
++
++    std::memcpy(out.data(), out_buf.pVirAddr, out_meta.nSize);
++
++    return out;
++  }
++
++ private:
++  void Init(void *model_data, size_t model_data_length) {
++    InitEngine(config_.debug);
++
++    InitContext(model_data, model_data_length, config_.debug, &handle_);
++
++    InitInputOutputAttrs(handle_, config_.debug, &io_info_);
++
++    std::memset(&io_data_, 0, sizeof(io_data_));
++
++    PrepareIO(io_info_, &io_data_, config_.debug);
++
++    if (!io_info_ || io_info_->nInputSize == 0 || !io_info_->pInputs) {
++      SHERPA_ONNX_LOGE("No input tensor in Axera model");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    auto &in0 = io_info_->pInputs[0];
++    if (in0.nShapeSize < 2) {
++      SHERPA_ONNX_LOGE("Input tensor rank is too small (nShapeSize = %u)",
++                       in0.nShapeSize);
++      SHERPA_ONNX_EXIT(-1);
++    }
++    num_input_frames_ = in0.pShape[1];
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("Axera SenseVoice model init done.");
++      SHERPA_ONNX_LOGE("  num_input_frames_ = %d", num_input_frames_);
++    }
++  }
++
++  std::vector<float> ApplyLFR(std::vector<float> in) const {
++    int32_t lfr_window_size = meta_data_.window_size;
++    int32_t lfr_window_shift = meta_data_.window_shift;
++    int32_t in_feat_dim = 80;
++    int32_t in_num_frames = in.size() / in_feat_dim;
++    int32_t out_num_frames =
++        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
++
++    if (out_num_frames > num_input_frames_) {
++      SHERPA_ONNX_LOGE(
++          "Number of input frames %d is too large. Truncate it to %d frames.",
++          out_num_frames, num_input_frames_);
++      SHERPA_ONNX_LOGE(
++          "Recognition result may be truncated/incomplete. Please select a "
++          "model accepting longer audios.");
++      out_num_frames = num_input_frames_;
++    }
++
++    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
++    std::vector<float> out(num_input_frames_ * out_feat_dim);
++    const float *p_in = in.data();
++    float *p_out = out.data();
++
++    for (int32_t i = 0; i != out_num_frames; ++i) {
++      std::copy(p_in, p_in + out_feat_dim, p_out);
++      p_out += out_feat_dim;
++      p_in += lfr_window_shift * in_feat_dim;
++    }
++
++    return out;
++  }
++
++ private:
++  OfflineModelConfig config_;
++  AX_ENGINE_HANDLE handle_ = nullptr;
++  AX_ENGINE_IO_INFO_T *io_info_ = nullptr;
++  AX_ENGINE_IO_T io_data_;
++  OfflineSenseVoiceModelMetaData meta_data_;
++  int32_t num_input_frames_ = -1;
++};
++
++OfflineSenseVoiceModelAxera::~OfflineSenseVoiceModelAxera() = default;
++
++OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++std::vector<float> OfflineSenseVoiceModelAxera::Run(std::vector<float> features,
++                                                    int32_t language,
++                                                    int32_t text_norm) const {
++  return impl_->Run(std::move(features), language, text_norm);
++}
++
++const OfflineSenseVoiceModelMetaData &
++OfflineSenseVoiceModelAxera::GetModelMetadata() const {
++  return impl_->GetModelMetadata();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineSenseVoiceModelAxera::OfflineSenseVoiceModelAxera(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+new file mode 100644
+index 00000000..dcbb2381
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/axera/offline-sense-voice-model-axera.h
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#ifndef SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
++#define SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
++
++#include <memory>
++#include <vector>
++
++#include "ax_engine_api.h"
++#include "ax_sys_api.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelAxera {
++ public:
++  ~OfflineSenseVoiceModelAxera();
++
++  explicit OfflineSenseVoiceModelAxera(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineSenseVoiceModelAxera(Manager *mgr, const OfflineModelConfig &config);
++
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) const;
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXERA_OFFLINE_SENSE_VOICE_MODEL_AXERA_H_
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axera/utils.cc b/sherpa-onnx/csrc/axera/utils.cc
+new file mode 100644
+index 00000000..c42f2a6a
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/utils.cc
+@@ -0,0 +1,169 @@
++// sherpa-onnx/csrc/axera/utils.cc
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#include "sherpa-onnx/csrc/axera/utils.h"
++
++#include <string.h>
++
++#include <sstream>
++#include <string>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/axera/io.hpp"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++
++namespace sherpa_onnx {
++
++void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
++                       int32_t height, int32_t width, float *dst) {
++  for (int32_t i = 0; i < n; ++i) {
++    for (int32_t h = 0; h < height; ++h) {
++      for (int32_t w = 0; w < width; ++w) {
++        for (int32_t c = 0; c < channel; ++c) {
++          dst[i * height * width * channel + h * width * channel + w * channel +
++              c] = src[i * height * width * channel + c * height * width +
++                       h * width + w];
++        }
++      }
++    }
++  }
++}
++
++std::string ToString(const AX_ENGINE_IO_INFO_T *io_info) {
++  std::ostringstream os;
++  os << "{";
++  if (!io_info) {
++    os << "null AX_ENGINE_IO_INFO_T}";
++    return os.str();
++  }
++
++  os << "nInputSize: " << io_info->nInputSize;
++  os << ", nOutputSize: " << io_info->nOutputSize;
++  os << ", nMaxBatchSize: " << io_info->nMaxBatchSize;
++  os << ", bDynamicBatchSize: "
++     << (io_info->bDynamicBatchSize ? "true" : "false");
++  os << "}";
++  return os.str();
++}
++
++std::unordered_map<std::string, std::string> Parse(const char *custom_string,
++                                                   bool debug /*= false*/) {
++  std::unordered_map<std::string, std::string> ans;
++  if (!custom_string) {
++    SHERPA_ONNX_LOGE("Parse: custom_string is null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  std::vector<std::string> fields;
++  SplitStringToVector(custom_string, ";", false, &fields);
++  std::vector<std::string> tmp;
++
++  for (const auto &f : fields) {
++    tmp.clear();
++    SplitStringToVector(f, "=", false, &tmp);
++    if (tmp.size() != 2) {
++      SHERPA_ONNX_LOGE("Invalid custom string %s for %s", custom_string,
++                       f.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++    ans[std::move(tmp[0])] = std::move(tmp[1]);
++  }
++
++  if (debug) {
++    for (const auto &p : ans) {
++      SHERPA_ONNX_LOGE("%s: %s", p.first.c_str(), p.second.c_str());
++    }
++  }
++  return ans;
++}
++
++void InitEngine(bool debug) {
++  AX_SYS_Init();
++#ifdef AXERA_TARGET_CHIP_AX620E
++  auto ret = AX_ENGINE_Init();
++#else
++  AX_ENGINE_NPU_ATTR_T npu_attr;
++  memset(&npu_attr, 0, sizeof(npu_attr));
++  npu_attr.eHardMode = AX_ENGINE_VIRTUAL_NPU_DISABLE;
++  auto ret = AX_ENGINE_Init(&npu_attr);
++#endif
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_Init failed, ret = %d", ret);
++    SHERPA_ONNX_EXIT(-1);
++  }
++  if (debug) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_Init done.");
++  }
++}
++
++void InitContext(void *model_data, size_t model_data_length, bool debug,
++                 AX_ENGINE_HANDLE *handle) {
++  if (!handle) {
++    SHERPA_ONNX_LOGE("InitContext: handle is null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  auto ret = AX_ENGINE_CreateHandle(handle, model_data, model_data_length);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle failed, ret = %d", ret);
++    SHERPA_ONNX_EXIT(-1);
++  }
++  if (debug) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_CreateHandle done. handle = %p",
++                     (void *)(*handle));
++  }
++
++  ret = AX_ENGINE_CreateContext(*handle);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext failed, ret = %d", ret);
++    SHERPA_ONNX_EXIT(-1);
++  }
++  if (debug) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_CreateContext done.");
++  }
++}
++
++void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
++                          AX_ENGINE_IO_INFO_T **io_info) {
++  if (!io_info) {
++    SHERPA_ONNX_LOGE("InitInputOutputAttrs: io_info is null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  auto ret = AX_ENGINE_GetIOInfo(handle, io_info);
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo failed, ret = %d", ret);
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  if (debug) {
++    SHERPA_ONNX_LOGE("AX_ENGINE_GetIOInfo done.");
++    SHERPA_ONNX_LOGE("IO_INFO: %s", ToString(*io_info).c_str());
++    middleware::print_io_info(*io_info);
++  }
++}
++
++void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
++               bool debug) {
++  if (!io_info || !io_data) {
++    SHERPA_ONNX_LOGE("PrepareIO: io_info or io_data is null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  auto ret = middleware::prepare_io(
++      io_info, io_data,
++      std::make_pair(AX_ENGINE_ABST_DEFAULT, AX_ENGINE_ABST_CACHED));
++  if (ret != 0) {
++    SHERPA_ONNX_LOGE("middleware::prepare_io failed, ret = %d", ret);
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  if (debug) {
++    SHERPA_ONNX_LOGE("PrepareIO (middleware::prepare_io) done.");
++  }
++}
++
++}  // namespace sherpa_onnx
+\ No newline at end of file
+diff --git a/sherpa-onnx/csrc/axera/utils.h b/sherpa-onnx/csrc/axera/utils.h
+new file mode 100644
+index 00000000..039eefc8
+--- /dev/null
++++ b/sherpa-onnx/csrc/axera/utils.h
+@@ -0,0 +1,37 @@
++// sherpa-onnx/csrc/axera/utils.h
++//
++// Copyright (c)  2025  M5Stack Technology CO LTD
++
++#ifndef SHERPA_ONNX_CSRC_AXERA_UTILS_H_
++#define SHERPA_ONNX_CSRC_AXERA_UTILS_H_
++
++#include <string>
++#include <unordered_map>
++#include <vector>
++
++#include "ax_engine_api.h"
++
++namespace sherpa_onnx {
++
++void ConvertNCHWtoNHWC(const float *src, int32_t n, int32_t channel,
++                       int32_t height, int32_t width, float *dst);
++
++std::string ToString(const AX_ENGINE_IO_INFO_T *io_info);
++
++std::unordered_map<std::string, std::string> Parse(const char *custom_string,
++                                                   bool debug = false);
++
++void InitEngine(bool debug);
++
++void InitContext(void *model_data, size_t model_data_length, bool debug,
++                 AX_ENGINE_HANDLE *handle);
++
++void InitInputOutputAttrs(AX_ENGINE_HANDLE handle, bool debug,
++                          AX_ENGINE_IO_INFO_T **io_info);
++
++void PrepareIO(AX_ENGINE_IO_INFO_T *io_info, AX_ENGINE_IO_T *io_data,
++               bool debug);
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_AXERA_UTILS_H_
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 2ff1a4a7..ae45d304 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -41,6 +41,14 @@
+ #include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
+ #endif
+ 
++#if SHERPA_ONNX_ENABLE_AXERA
++#include "sherpa-onnx/csrc/axera/offline-recognizer-sense-voice-axera-impl.h"
++#endif
++
++#if SHERPA_ONNX_ENABLE_AXCL
++#include "sherpa-onnx/csrc/axcl/offline-recognizer-sense-voice-axcl-impl.h"
++#endif
++
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
+@@ -79,6 +87,48 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #endif
+   }
+ 
++  if (config.model_config.provider == "axera") {
++#if SHERPA_ONNX_ENABLE_AXERA
++    if (config.model_config.sense_voice.model.empty()) {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported "
++          "by axera for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    } else if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxeraImpl>(config);
++    }
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXERA=ON if you "
++        "want to use axera. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/axera/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
++  if (config.model_config.provider == "axcl") {
++#if SHERPA_ONNX_ENABLE_AXCL
++    if (config.model_config.sense_voice.model.empty()) {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported "
++          "by axcl for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    } else if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceAxclImpl>(config);
++    }
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_AXCL=ON if you "
++        "want to use axcl. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/axcl/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
+   if (config.model_config.provider == "ascend") {
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+     if (!config.model_config.sense_voice.model.empty()) {
+
+commit 4c27d9b9b37ee0f2d23f86761f1aabfa1d003190
+Author: Mahmoud ghareeb <mahmoudghareeb11111@gmail.com>
+Date:   Tue Dec 2 15:04:41 2025 +0200
+
+    Fix token log probabilities in offline transducer modified beam search decoder (#2846)
+
+diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+index e97fd636..335b0dec 100644
+--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+@@ -139,7 +139,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
+ 
+           // Store the token log probability (subtract prev log_prob to get original)
+           float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
+-          new_hyp.ys_log_probs.push_back(token_log_prob);
++          new_hyp.ys_probs.push_back(token_log_prob);
+ 
+           if (context_graphs[i] != nullptr) {
+             auto context_res =
+@@ -191,7 +191,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
+     // strip leading blanks
+     r.tokens = {hyp.ys.begin() + context_size, hyp.ys.end()};
+     r.timestamps = std::move(hyp.timestamps);
+-    r.ys_log_probs = std::move(hyp.ys_log_probs);
++    r.ys_log_probs = std::move(hyp.ys_probs);
+   }
+ 
+   return unsorted_ans;
+
+commit 0f369acd518db09f141327780b1acb82410c48ed
+Author: Mahmoud ghareeb <mahmoudghareeb11111@gmail.com>
+Date:   Tue Dec 2 13:00:52 2025 +0200
+
+    Add token-level confidence scores (ys_probs) for offline transducer models (#2843)
+    
+    This PR adds support for retrieving per-token log probabilities (confidence scores) from offline transducer-based ASR models. The changes are comprehensive, touching the C++ core logic to compute the scores, the C-API to expose them, and the Python bindings.
+
+diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
+index 08702f9c..99422a06 100644
+--- a/sherpa-onnx/c-api/c-api.cc
++++ b/sherpa-onnx/c-api/c-api.cc
+@@ -703,12 +703,20 @@ const SherpaOnnxOfflineRecognizerResult *SherpaOnnxGetOfflineStreamResult(
+       r->durations = nullptr;
+     }
+ 
++    if (!result.ys_log_probs.empty() && result.ys_log_probs.size() == r->count) {
++      r->ys_log_probs = new float[r->count];
++      std::copy(result.ys_log_probs.begin(), result.ys_log_probs.end(), r->ys_log_probs);
++    } else {
++      r->ys_log_probs = nullptr;
++    }
++
+     r->tokens = tokens;
+   } else {
+     r->count = 0;
+     r->timestamps = nullptr;
+     r->tokens = nullptr;
+     r->tokens_arr = nullptr;
++    r->ys_log_probs = nullptr;
+   }
+ 
+   return r;
+@@ -720,6 +728,7 @@ void SherpaOnnxDestroyOfflineRecognizerResult(
+     delete[] r->text;
+     delete[] r->timestamps;
+     delete[] r->durations;
++    delete[] r->ys_log_probs;
+     delete[] r->tokens;
+     delete[] r->tokens_arr;
+     delete[] r->json;
+diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
+index 36731e66..e3f7e016 100644
+--- a/sherpa-onnx/c-api/c-api.h
++++ b/sherpa-onnx/c-api/c-api.h
+@@ -666,6 +666,11 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerResult {
+   // Pointer to continuous memory which holds durations (in seconds) for each
+   // token It is NULL if the model does not support durations
+   float *durations;
++
++  // Pointer to continuous memory which holds log probabilities (confidence)
++  // for each token. It is NULL if the model does not support probabilities.
++  // ys_log_probs[i] is the log probability for token i.
++  float *ys_log_probs;
+ } SherpaOnnxOfflineRecognizerResult;
+ 
+ /// Get the result of the offline stream.
+diff --git a/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h b/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
+index fefd3325..b79bac18 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-transducer-impl.h
+@@ -72,6 +72,9 @@ static OfflineRecognitionResult Convert(
+     r.durations.push_back(d * frame_shift_s);
+   }
+ 
++  // Copy token log probabilities (confidence scores)
++  r.ys_log_probs = src.ys_log_probs;
++
+   return r;
+ }
+ 
+diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
+index 8f757e6a..265c8dbe 100644
+--- a/sherpa-onnx/csrc/offline-stream.cc
++++ b/sherpa-onnx/csrc/offline-stream.cc
+@@ -442,6 +442,18 @@ std::string OfflineRecognitionResult::AsJsonString() const {
+   }
+   os << "], ";
+ 
++  os << "\""
++     << "ys_log_probs"
++     << "\""
++     << ": ";
++  os << "[";
++  sep = "";
++  for (auto p : ys_log_probs) {
++    os << sep << std::fixed << std::setprecision(6) << p;
++    sep = ", ";
++  }
++  os << "], ";
++
+   sep = "";
+ 
+   os << "\""
+diff --git a/sherpa-onnx/csrc/offline-stream.h b/sherpa-onnx/csrc/offline-stream.h
+index 5e2a514b..c4838fd6 100644
+--- a/sherpa-onnx/csrc/offline-stream.h
++++ b/sherpa-onnx/csrc/offline-stream.h
+@@ -42,6 +42,9 @@ struct OfflineRecognitionResult {
+   /// only)
+   std::vector<float> durations;
+ 
++  /// ys_log_probs[i] contains the log probability (confidence) for tokens[i].
++  std::vector<float> ys_log_probs;
++
+   std::vector<int32_t> words;
+ 
+   std::string AsJsonString() const;
+diff --git a/sherpa-onnx/csrc/offline-transducer-decoder.h b/sherpa-onnx/csrc/offline-transducer-decoder.h
+index 74d2b9e9..82a4e1bb 100644
+--- a/sherpa-onnx/csrc/offline-transducer-decoder.h
++++ b/sherpa-onnx/csrc/offline-transducer-decoder.h
+@@ -24,6 +24,9 @@ struct OfflineTransducerDecoderResult {
+   /// (post-subsampling). It is converted to seconds by higher layers
+   /// (e.g., Convert() in offline-recognizer-transducer-impl.h).
+   std::vector<float> durations;
++
++  /// ys_log_probs[i] contains the log probability (confidence) for tokens[i].
++  std::vector<float> ys_log_probs;
+ };
+ 
+ class OfflineTransducerDecoder {
+diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+index 98fe78da..2427d412 100644
+--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+@@ -9,6 +9,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "sherpa-onnx/csrc/math.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+ #include "sherpa-onnx/csrc/packed-sequence.h"
+ #include "sherpa-onnx/csrc/slice.h"
+@@ -53,16 +54,22 @@ OfflineTransducerGreedySearchDecoder::Decode(Ort::Value encoder_out,
+       if (blank_penalty_ > 0.0) {
+         p_logit[0] -= blank_penalty_;  // assuming blank id is 0
+       }
++      
++      LogSoftmax(p_logit, vocab_size);
++
+       auto y = static_cast<int32_t>(std::distance(
+-          static_cast<const float *>(p_logit),
+-          std::max_element(static_cast<const float *>(p_logit),
+-                           static_cast<const float *>(p_logit) + vocab_size)));
++          p_logit,
++          std::max_element(p_logit, p_logit + vocab_size)));
++
++      float log_prob = p_logit[y];
++
+       p_logit += vocab_size;
+       // blank id is hardcoded to 0
+       // also, it treats unk as blank
+       if (y != 0 && y != unk_id_) {
+         ans[i].tokens.push_back(y);
+         ans[i].timestamps.push_back(t);
++        ans[i].ys_log_probs.push_back(log_prob);
+         emitted = true;
+       }
+     }
+diff --git a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+index d053a6fd..e97fd636 100644
+--- a/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-modified-beam-search-decoder.cc
+@@ -136,6 +136,11 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
+         if (new_token != 0 && new_token != unk_id_) {
+           new_hyp.ys.push_back(new_token);
+           new_hyp.timestamps.push_back(t);
++
++          // Store the token log probability (subtract prev log_prob to get original)
++          float token_log_prob = p_logprob[k] - prev[hyp_index].log_prob;
++          new_hyp.ys_log_probs.push_back(token_log_prob);
++
+           if (context_graphs[i] != nullptr) {
+             auto context_res =
+                 context_graphs[i]->ForwardOneStep(context_state,
+@@ -186,6 +191,7 @@ OfflineTransducerModifiedBeamSearchDecoder::Decode(
+     // strip leading blanks
+     r.tokens = {hyp.ys.begin() + context_size, hyp.ys.end()};
+     r.timestamps = std::move(hyp.timestamps);
++    r.ys_log_probs = std::move(hyp.ys_log_probs);
+   }
+ 
+   return unsorted_ans;
+diff --git a/sherpa-onnx/python/csrc/offline-stream.cc b/sherpa-onnx/python/csrc/offline-stream.cc
+index fbae871b..203bcf90 100644
+--- a/sherpa-onnx/python/csrc/offline-stream.cc
++++ b/sherpa-onnx/python/csrc/offline-stream.cc
+@@ -45,7 +45,9 @@ static void PybindOfflineRecognitionResult(py::module *m) {  // NOLINT
+       .def_property_readonly("timestamps",
+         [](const PyClass &self) { return self.timestamps; })
+       .def_property_readonly("durations",
+-        [](const PyClass &self) { return self.durations; });
++        [](const PyClass &self) { return self.durations; })
++      .def_property_readonly("ys_log_probs",
++        [](const PyClass &self) { return self.ys_log_probs; });
+ }
+ 
+ void PybindOfflineStream(py::module *m) {
+
+commit 586cd19e225bed3a50c3d0a554736a61a0b33191
+Author: alex-spacemit <jinghui.huang@spacemit.com>
+Date:   Tue Dec 2 14:36:31 2025 +0800
+
+    Add spacemit ort ep for spacemit riscv cpus (#2837)
+    
+    This pull request significantly extends the project's hardware compatibility by integrating a dedicated SpacemiT Execution Provider for ONNX Runtime. The changes enable efficient model inference on SpacemiT RISC-V CPUs, leveraging their RVV1.0 capabilities. This involves updates to the build system, new CMake modules for toolchain and ONNX Runtime package handling, and modifications to the core provider and session management logic to recognize and configure the SpacemiT EP.
+
+diff --git a/.github/workflows/riscv64-spacemit-linux.yaml b/.github/workflows/riscv64-spacemit-linux.yaml
+new file mode 100755
+index 00000000..ccf3b155
+--- /dev/null
++++ b/.github/workflows/riscv64-spacemit-linux.yaml
+@@ -0,0 +1,299 @@
++name: riscv64-spacemit-linux
++
++on:
++  push:
++    branches:
++      - master
++    paths:
++      - '.github/workflows/riscv64-spacemit-linux.yaml'
++      - 'cmake/**'
++      - 'sherpa-onnx/csrc/*'
++      - 'sherpa-onnx/c-api/*'
++      - 'toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake'
++      - 'build-riscv64-linux-gnu-spacemit.sh'
++    tags:
++      - 'v[0-9]+.[0-9]+.[0-9]+*'
++
++  workflow_dispatch:
++
++concurrency:
++  group: riscv64-spacemit-linux-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  riscv64_spacemit_linux:
++    runs-on: ${{ matrix.os }}
++    name: ${{ matrix.os }} ${{ matrix.lib_type }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        lib_type: [shared] #, static]
++
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Update version
++        shell: bash
++        run: |
++          ./new-release.sh
++          git diff .
++
++      - name: ccache
++        uses: hendrikmuhs/ccache-action@v1.2
++        with:
++          key: ${{ matrix.os }}-riscv64-spacemit-${{ matrix.lib_type }}
++
++      - name: cache-qemu
++        id: cache-qemu
++        uses: actions/cache@v4
++        with:
++          path: qemu-install
++          key: qemu-riscv-spacemit-install-20250818
++
++      - name: qemu
++        if: steps.cache-qemu.outputs.cache-hit != 'true'
++        run: |
++          wget -q https://archive.spacemit.com/spacemit-ai/qemu/jdsk-qemu-v10.0.2.tar.gz
++          tar -xf jdsk-qemu-v10.0.2.tar.gz
++          mkdir -p qemu-install/bin
++
++          cp -v ./jdsk-qemu/bin/qemu-riscv64 ./qemu-install/bin
++
++      - name: cache-toolchain
++        id: cache-toolchain
++        uses: actions/cache@v4
++        with:
++          path: toolchain
++          key: https://archive.spacemit.com/toolchain/spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz
++
++      - name: Download toolchain
++        if: steps.cache-toolchain.outputs.cache-hit != 'true'
++        shell: bash
++        run: |
++          wget -q https://archive.spacemit.com/toolchain/spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz
++
++          mkdir $GITHUB_WORKSPACE/toolchain
++
++          tar xvf spacemit-toolchain-linux-glibc-x86_64-v1.1.2.tar.xz --strip-components 1 -C $GITHUB_WORKSPACE/toolchain
++          ls -lh $GITHUB_WORKSPACE/toolchain/bin
++
++      - name: Display toolchain info
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++          riscv64-unknown-linux-gnu-gcc --version
++
++      - name: Display qemu-riscv64 -h
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
++          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
++          qemu-riscv64 -h
++
++      - name: build riscv64-spacemit-linux
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++
++          export CMAKE_CXX_COMPILER_LAUNCHER=ccache
++          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
++
++          cmake --version
++
++          lib_type=${{ matrix.lib_type }}
++
++          if [[ $lib_type == "shared" ]]; then
++            export BUILD_SHARED_LIBS=ON
++          else
++            export BUILD_SHARED_LIBS=OFF
++          fi
++
++          export RISCV_ROOT_PATH=$GITHUB_WORKSPACE/toolchain
++          ./build-riscv64-linux-gnu-spacemit.sh
++
++          ls -lh build-riscv64-linux-gnu-spacemit/bin
++          ls -lh build-riscv64-linux-gnu-spacemit/lib
++
++          echo "---install/lib---"
++          ls -lh build-riscv64-linux-gnu-spacemit/install/lib
++
++          echo "---install/bin---"
++          ls -lh build-riscv64-linux-gnu-spacemit/install/bin
++
++          file build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
++
++          readelf -d build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
++
++      - name: Copy files
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++          riscv64-unknown-linux-gnu-strip --version
++
++          SHERPA_ONNX_VERSION=v$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++          dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-linux-riscv64-spacemit-${{ matrix.lib_type }}
++          mkdir $dst
++
++          cp -v $GITHUB_WORKSPACE/toolchain/sysroot/lib/ld-linux-riscv64-lp64d.so.1 build-riscv64-linux-gnu-spacemit/install/lib/
++
++          ls -lh build-riscv64-linux-gnu-spacemit/install/lib
++
++          cp -a build-riscv64-linux-gnu-spacemit/install/bin $dst/
++          ls -lh $dst/bin/*
++          riscv64-unknown-linux-gnu-strip $dst/bin/*
++          ls -lh $dst
++
++          lib_type=${{ matrix.lib_type }}
++          if [[ $lib_type == "shared" ]]; then
++            cp -a build-riscv64-linux-gnu-spacemit/install/lib $dst/
++            rm -fv $dst/lib/libasound.so
++          fi
++
++          tree $dst
++
++          tar cjvf ${dst}.tar.bz2 $dst
++
++      - uses: actions/upload-artifact@v4
++        if: matrix.lib_type == 'shared'
++        with:
++          name: sherpa-onnx-linux-riscv64-spacemit-shared
++          path: sherpa-onnx-*linux-riscv64-spacemit-shared.tar.bz2
++
++      # https://huggingface.co/docs/hub/spaces-github-actions
++      - name: Publish to huggingface
++        if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            rm -rf huggingface
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++
++            GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/csukuangfj/sherpa-onnx-libs huggingface
++
++            cd huggingface
++            dst=riscv64-spacemit/$SHERPA_ONNX_VERSION
++            mkdir -p $dst
++
++            cp -v ../sherpa-onnx-*-shared.tar.bz2 $dst/
++
++            git status
++            git lfs track "*.bz2"
++
++            git add .
++
++            git commit -m "upload sherpa-onnx-${SHERPA_ONNX_VERSION}-linux-riscv64-spacemit-shared.tar.bz2"
++
++            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-libs main
++
++      - uses: actions/upload-artifact@v4
++        if: matrix.lib_type == 'static'
++        with:
++          name: sherpa-onnx-linux-riscv64-spacemit-static
++          path: sherpa-onnx-*linux-riscv64-spacemit-static.tar.bz2
++
++      - name: Release pre-compiled binaries and libs for riscv64 linux ${{ matrix.lib_type }}
++        if: github.repository_owner == 'csukuangfj' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-riscv64*.tar.bz2
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: v1.12.11
++
++      - name: Release pre-compiled binaries and libs for riscv64 linux ${{ matrix.lib_type }}
++        if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          overwrite: true
++          file: sherpa-onnx-*linux-riscv64*.tar.bz2
++
++      - name: Test sherpa-onnx
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
++          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
++          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
++          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
++
++          ls -lh ./build-riscv64-linux-gnu-spacemit/bin
++
++          echo "----------sherpa-onnx----------"
++          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx --help
++          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx
++
++          echo "----------sherpa-onnx-offline----------"
++          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline --help
++          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline
++
++          echo "----------sherpa-onnx-offline-tts----------"
++          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts --help
++          readelf -d ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts
++
++      - name: Test streaming speech recognition
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
++          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
++          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
++          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
++          echo "Some mistakes in ep graph partition, disable op Gather for spacemit-ep now, will be fixed soon."
++          export SPACEMIT_EP_DISABLE_OP_TYPE_FILTER="Gather"
++
++          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
++          tar xvf sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
++          rm sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2
++
++          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx \
++            --provider=spacemit \
++            --tokens=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/tokens.txt \
++            --encoder=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/encoder-epoch-99-avg-1.onnx \
++            --decoder=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/decoder-epoch-99-avg-1.onnx \
++            --joiner=./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/joiner-epoch-99-avg-1.onnx \
++            ./sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23/test_wavs/0.wav
++
++      - name: Test offline tts
++        shell: bash
++        run: |
++          export PATH=$GITHUB_WORKSPACE/toolchain/bin:$PATH
++          export PATH=$GITHUB_WORKSPACE/qemu-install/bin:$PATH
++          export QEMU_LD_PREFIX=$GITHUB_WORKSPACE/toolchain/sysroot
++          export LD_LIBRARY_PATH=$GITHUB_WORKSPACE/toolchain/sysroot/lib
++          export QEMU_ARGS="-cpu max,vlen=256,elen=64,vext_spec=v1.0"
++          echo "Some mistakes in ep graph partition, disable op Gather;Cast;ConvTranspose for spacemit-ep now, will be fixed soon."
++          export SPACEMIT_EP_DISABLE_OP_TYPE_FILTER="Gather;Cast;ConvTranspose"
++
++          wget -q https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/vits-piper-en_US-lessac-medium.tar.bz2
++          tar xf vits-piper-en_US-lessac-medium.tar.bz2
++          rm vits-piper-en_US-lessac-medium.tar.bz2
++
++          qemu-riscv64 ${QEMU_ARGS} ./build-riscv64-linux-gnu-spacemit/bin/sherpa-onnx-offline-tts \
++            --provider=spacemit \
++            --vits-model=./vits-piper-en_US-lessac-medium/en_US-lessac-medium.onnx \
++            --vits-data-dir=./vits-piper-en_US-lessac-medium/espeak-ng-data \
++            --vits-tokens=./vits-piper-en_US-lessac-medium/tokens.txt \
++            --output-filename=./liliana-piper-en_US-lessac-medium.wav \
++            'liliana, the most beautiful and lovely assistant of our team!'
++
++      - uses: actions/upload-artifact@v4
++        if: matrix.lib_type == 'shared'
++        with:
++          name: wave
++          path: ./*.wav
+diff --git a/.gitignore b/.gitignore
+old mode 100644
+new mode 100755
+index 6ff6eb29..813b50a2
+--- a/.gitignore
++++ b/.gitignore
+@@ -161,3 +161,5 @@ sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-a
+ sherpa-onnx-paraformer-zh-int8-2025-10-07
+ sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+ sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
++build-riscv64-linux-gnu-spacemit/
++spacemit-toolchain*
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 64efc4ba..10467632 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -62,6 +62,7 @@ option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUG
+ option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
+ option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
+ option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
++option(SHERPA_ONNX_ENABLE_SPACEMIT "Whether to build for SpacemiT CPUs " OFF)
+ 
+ set(SHERPA_ONNX_LINUX_ARM64_GPU_ONNXRUNTIME_VERSION "1.11.0" CACHE STRING "Used only for Linux ARM64 GPU. Set to 1.11.0 if you use CUDA 10.2 and cudnn8. Set it to 1.16.0 if you use CUDA 11.4 and cudnn8. Set it to 1.18.0 if you use CUDA 12.2 and cudnn8. Set it to 1.18.1 if you use CUDA 12.6 and cudnn9")
+ 
+@@ -181,6 +182,7 @@ message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAM
+ message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
+ message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
+ message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
++message(STATUS "SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
+ message(STATUS "SHERPA_ONNX_LINK_D3D: ${SHERPA_ONNX_LINK_D3D}")
+ 
+ if(BUILD_SHARED_LIBS OR SHERPA_ONNX_ENABLE_JNI)
+@@ -308,6 +310,10 @@ if(SHERPA_ONNX_ENABLE_QNN)
+   add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_SPACEMIT)
++  add_definitions(-DSHERPA_ONNX_ENABLE_SPACEMIT=1)
++endif()
++
+ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+   set(ASCEND_TOOLKIT_HOME)
+   if(NOT DEFINED ENV{ASCEND_TOOLKIT_HOME})
+diff --git a/build-riscv64-linux-gnu-spacemit.sh b/build-riscv64-linux-gnu-spacemit.sh
+new file mode 100755
+index 00000000..0d50b9f6
+--- /dev/null
++++ b/build-riscv64-linux-gnu-spacemit.sh
+@@ -0,0 +1,71 @@
++#!/usr/bin/env bash
++set -ex
++
++SPACEMIT_TOOLCHAIN=spacemit-toolchain-linux-glibc-x86_64-v1.1.2
++DOWNLOAD_URL="https://archive.spacemit.com/toolchain/${SPACEMIT_TOOLCHAIN}.tar.xz"
++DOWNLOAD_FILE="./riscv-spacemit-toolchain.tar.gz"
++
++if [ -n "$RISCV_ROOT_PATH" ] && [ -d "$RISCV_ROOT_PATH" ]; then
++    echo "LOCAL RISCV_ROOT_PATH: $RISCV_ROOT_PATH"
++else
++    wget -O "$DOWNLOAD_FILE" "$DOWNLOAD_URL" --quiet --show-progress
++    tar -xf "$DOWNLOAD_FILE"
++    export RISCV_ROOT_PATH=$PWD/${SPACEMIT_TOOLCHAIN}
++    echo "DOWNLOAD RISCV_ROOT_PATH: $RISCV_ROOT_PATH"
++fi
++
++
++if [ x$dir = x"" ]; then
++  dir=build-riscv64-linux-gnu-spacemit
++fi
++mkdir -p $dir
++cd $dir
++
++if [ ! -f alsa-lib/src/.libs/libasound.so ]; then
++  echo "Start to cross-compile alsa-lib"
++  if [ ! -d alsa-lib ]; then
++    git clone --depth 1 --branch v1.2.12 https://github.com/alsa-project/alsa-lib
++  fi
++  # If it shows:
++  #  ./gitcompile: line 79: libtoolize: command not found
++  # Please use:
++  #  sudo apt-get install libtool m4 automake
++  #
++  pushd alsa-lib
++  CC=$RISCV_ROOT_PATH/bin/riscv64-unknown-linux-gnu-gcc ./gitcompile --host=riscv64-unknown-linux-gnu
++  popd
++  echo "Finish cross-compiling alsa-lib"
++fi
++
++export CPLUS_INCLUDE_PATH=$PWD/alsa-lib/include:$CPLUS_INCLUDE_PATH
++export SHERPA_ONNX_ALSA_LIB_DIR=$PWD/alsa-lib/src/.libs
++
++if [[ x"$BUILD_SHARED_LIBS" == x"" ]]; then
++  # By default, use shared libraries
++  BUILD_SHARED_LIBS=ON
++fi
++
++cmake \
++  -DBUILD_PIPER_PHONMIZE_EXE=OFF \
++  -DBUILD_PIPER_PHONMIZE_TESTS=OFF \
++  -DBUILD_ESPEAK_NG_EXE=OFF \
++  -DBUILD_ESPEAK_NG_TESTS=OFF \
++  -DCMAKE_INSTALL_PREFIX=./install \
++  -DCMAKE_BUILD_TYPE=Release \
++  -DBUILD_SHARED_LIBS=$BUILD_SHARED_LIBS \
++  -DSHERPA_ONNX_ENABLE_TESTS=OFF \
++  -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
++  -DSHERPA_ONNX_ENABLE_CHECK=OFF \
++  -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
++  -DSHERPA_ONNX_ENABLE_JNI=OFF \
++  -DSHERPA_ONNX_ENABLE_C_API=ON \
++  -DSHERPA_ONNX_ENABLE_WEBSOCKET=ON \
++  -DSHERPA_ONNX_ENABLE_SPACEMIT=ON \
++  -DCMAKE_TOOLCHAIN_FILE=../toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake \
++  ..
++
++make VERBOSE=1 -j4
++make install/strip
++
++# Enable it if only needed
++# cp -v $SHERPA_ONNX_ALSA_LIB_DIR/libasound.so* ./install/lib/
+diff --git a/cmake/onnxruntime-linux-riscv64-spacemit.cmake b/cmake/onnxruntime-linux-riscv64-spacemit.cmake
+new file mode 100755
+index 00000000..bea0ed5d
+--- /dev/null
++++ b/cmake/onnxruntime-linux-riscv64-spacemit.cmake
+@@ -0,0 +1,94 @@
++message(STATUS "CMAKE_SYSTEM_NAME: ${CMAKE_SYSTEM_NAME}")
++message(STATUS "CMAKE_SYSTEM_PROCESSOR: ${CMAKE_SYSTEM_PROCESSOR}")
++
++if(NOT CMAKE_SYSTEM_NAME STREQUAL Linux)
++  message(FATAL_ERROR "This file is for Linux only. Given: ${CMAKE_SYSTEM_NAME}")
++endif()
++
++if(NOT CMAKE_SYSTEM_PROCESSOR STREQUAL riscv64)
++  message(FATAL_ERROR "This file is for riscv64 only. Given: ${CMAKE_SYSTEM_PROCESSOR}")
++endif()
++
++if(NOT BUILD_SHARED_LIBS)
++  message(FATAL_ERROR "This file is for building shared libraries. BUILD_SHARED_LIBS: ${BUILD_SHARED_LIBS}, SHERPA_ONNX_ENABLE_SPACEMIT: ${SHERPA_ONNX_ENABLE_SPACEMIT}")
++endif()
++
++set(onnxruntime_pkg_name "spacemit-ort.riscv64.2.0.1.tar.gz")
++set(onnxruntime_URL  "https://archive.spacemit.com/spacemit-ai/onnxruntime/${onnxruntime_pkg_name}")
++set(onnxruntime_HASH "SHA256=8a15035aca34d5fd95f24444d4c7843265c1a81f49d84ec6fe9c6d0fdf5b55cf")
++
++# If you don't have access to the Internet,
++# please download onnxruntime to one of the following locations.
++# You can add more if you want.
++set(possible_file_locations
++  $ENV{HOME}/Downloads/${onnxruntime_pkg_name}
++  ${CMAKE_SOURCE_DIR}/${onnxruntime_pkg_name}
++  ${CMAKE_BINARY_DIR}/${onnxruntime_pkg_name}
++  /tmp/${onnxruntime_pkg_name}
++  /star-fj/fangjun/download/github/${onnxruntime_pkg_name}
++)
++
++foreach(f IN LISTS possible_file_locations)
++  if(EXISTS ${f})
++    set(onnxruntime_URL  "${f}")
++    file(TO_CMAKE_PATH "${onnxruntime_URL}" onnxruntime_URL)
++    message(STATUS "Found local downloaded onnxruntime: ${onnxruntime_URL}")
++    set(onnxruntime_URL2)
++    break()
++  endif()
++endforeach()
++
++FetchContent_Declare(onnxruntime
++  URL
++    ${onnxruntime_URL}
++    ${onnxruntime_URL2}
++  URL_HASH          ${onnxruntime_HASH}
++)
++
++FetchContent_GetProperties(onnxruntime)
++if(NOT onnxruntime_POPULATED)
++  message(STATUS "Downloading onnxruntime from ${onnxruntime_URL}")
++  FetchContent_Populate(onnxruntime)
++endif()
++message(STATUS "onnxruntime is downloaded to ${onnxruntime_SOURCE_DIR}")
++
++find_library(location_onnxruntime
++  NAMES onnxruntime
++  PATHS "${onnxruntime_SOURCE_DIR}/lib"
++  NO_CMAKE_SYSTEM_PATH
++)
++
++message(STATUS "location_onnxruntime: ${location_onnxruntime}")
++
++find_library(location_spacemit_ep
++  NAMES spacemit_ep
++  PATHS "${onnxruntime_SOURCE_DIR}/lib"
++  NO_CMAKE_SYSTEM_PATH
++)
++
++message(STATUS "location_spacemit_ep: ${location_spacemit_ep}")
++
++add_library(onnxruntime SHARED IMPORTED)
++add_library(spacemit_ep SHARED IMPORTED)
++
++set_target_properties(onnxruntime PROPERTIES
++  IMPORTED_LOCATION ${location_onnxruntime}
++  IMPORTED_LOCATION "${onnxruntime_SOURCE_DIR}/lib/libonnxruntime.so"
++  INTERFACE_INCLUDE_DIRECTORIES "${onnxruntime_SOURCE_DIR}/include/"
++)
++
++set_target_properties(spacemit_ep PROPERTIES
++  IMPORTED_LOCATION ${location_spacemit_ep}
++  IMPORTED_LOCATION "${onnxruntime_SOURCE_DIR}/lib/libspacemit_ep.so"
++  INTERFACE_INCLUDE_DIRECTORIES "${onnxruntime_SOURCE_DIR}/include/"
++)
++
++file(GLOB onnxruntime_lib_files
++  "${onnxruntime_SOURCE_DIR}/lib/libonnxruntime*")
++message(STATUS "onnxruntime lib files: ${onnxruntime_lib_files}")
++install(FILES ${onnxruntime_lib_files} DESTINATION lib)
++
++file(GLOB spacemit_ep_lib_files
++  "${onnxruntime_SOURCE_DIR}/lib/libspacemit_ep*")
++message(STATUS "spacemit_ep lib files: ${spacemit_ep_lib_files}")
++install(FILES ${spacemit_ep_lib_files} DESTINATION lib)
+diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
+old mode 100644
+new mode 100755
+index e96ff9ff..2e709b60
+--- a/cmake/onnxruntime.cmake
++++ b/cmake/onnxruntime.cmake
+@@ -7,7 +7,9 @@ function(download_onnxruntime)
+   if(SHERPA_ONNX_ENABLE_WASM)
+     include(onnxruntime-wasm-simd)
+   elseif(CMAKE_SYSTEM_PROCESSOR STREQUAL riscv64)
+-    if(BUILD_SHARED_LIBS)
++    if(SHERPA_ONNX_ENABLE_SPACEMIT)
++      include(onnxruntime-linux-riscv64-spacemit)
++    elseif(BUILD_SHARED_LIBS)
+       include(onnxruntime-linux-riscv64)
+     else()
+       include(onnxruntime-linux-riscv64-static)
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+old mode 100644
+new mode 100755
+index 2e771d7e..9e6e5647
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -339,6 +339,14 @@ if(SHERPA_ONNX_ENABLE_QNN)
+   target_include_directories(sherpa-onnx-core PRIVATE ${QNN_SDK_ROOT}/include/QNN)
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_SPACEMIT)
++  if(TARGET spacemit_ep)
++    target_link_libraries(sherpa-onnx-core spacemit_ep)
++  else()
++    target_link_libraries(sherpa-onnx-core ${spacemit_ep_lib_files})
++  endif()
++endif()
++
+ if(TARGET onnxruntime)
+   target_link_libraries(sherpa-onnx-core onnxruntime)
+ else()
+diff --git a/sherpa-onnx/csrc/provider.cc b/sherpa-onnx/csrc/provider.cc
+index af3759c7..8f7fe317 100644
+--- a/sherpa-onnx/csrc/provider.cc
++++ b/sherpa-onnx/csrc/provider.cc
+@@ -29,6 +29,8 @@ Provider StringToProvider(std::string s) {
+     return Provider::kTRT;
+   } else if (s == "directml") {
+     return Provider::kDirectML;
++  } else if (s == "spacemit") {
++    return Provider::kSpacemiT;
+   } else {
+     SHERPA_ONNX_LOGE("Unsupported string: %s. Fallback to cpu", s.c_str());
+     return Provider::kCPU;
+diff --git a/sherpa-onnx/csrc/provider.h b/sherpa-onnx/csrc/provider.h
+index 2b85b8a2..8842c797 100644
+--- a/sherpa-onnx/csrc/provider.h
++++ b/sherpa-onnx/csrc/provider.h
+@@ -21,6 +21,7 @@ enum class Provider {
+   kNNAPI = 4,     // NnapiExecutionProvider
+   kTRT = 5,       // TensorRTExecutionProvider
+   kDirectML = 6,  // DmlExecutionProvider
++  kSpacemiT = 7,  // SpacemiTExecutionProvider
+ };
+ 
+ /**
+diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
+old mode 100644
+new mode 100755
+index 24a63bc4..d3d223c3
+--- a/sherpa-onnx/csrc/session.cc
++++ b/sherpa-onnx/csrc/session.cc
+@@ -23,6 +23,10 @@
+ #include "dml_provider_factory.h"  // NOLINT
+ #endif
+ 
++#if defined(SHERPA_ONNX_ENABLE_SPACEMIT)
++#include "spacemit_ort_env.h"  // NOLINT
++#endif
++
+ namespace sherpa_onnx {
+ 
+ static void OrtStatusFailure(OrtStatus *status, const char *s) {
+@@ -239,6 +243,33 @@ Ort::SessionOptions GetSessionOptionsImpl(
+           (int32_t)__ANDROID_API__);
+ #else
+       SHERPA_ONNX_LOGE("NNAPI is for Android only. Fallback to cpu");
++#endif
++      break;
++    }
++    case Provider::kSpacemiT: {
++#if defined(SHERPA_ONNX_ENABLE_SPACEMIT)
++      SHERPA_ONNX_LOGE("Use SpacemiT Execution Provider");
++      // when using SpacemiT Execution Provider, set intra_op_num_threads and
++      // inter_op_num_threads to 1 can improve performance.
++      // all ops run on ep, no need to create multiple threads in onnxruntime.
++      // ep will create SPACEMIT_EP_INTRA_THREAD_NUM threads as intra threads.
++      std::unordered_map<std::string, std::string> provider_options;
++      SHERPA_ONNX_LOGE("Set IntraOpNumThreads to 1");
++      sess_opts.SetIntraOpNumThreads(1);
++      SHERPA_ONNX_LOGE("Set InterOpNumThreads to 1");
++      sess_opts.SetInterOpNumThreads(1);
++      SHERPA_ONNX_LOGE("Set SPACEMIT_EP_INTRA_THREAD_NUM to %d", num_threads);
++      provider_options.insert(
++          std::make_pair("SPACEMIT_EP_INTRA_THREAD_NUM", std::to_string(num_threads)));
++      OrtStatus* sts = Ort::SessionOptionsSpaceMITEnvInit(sess_opts, provider_options);
++      if (sts) {
++        const auto &api = Ort::GetApi();
++        const char *msg = api.GetErrorMessage(sts);
++        SHERPA_ONNX_LOGE("Failed to enable SpacemiT Execution Provider: %s. Fallback to cpu", msg);
++        api.ReleaseStatus(sts);
++      }
++#else
++      SHERPA_ONNX_LOGE("SpacemiT Execution Provider is for SpacemiT AI-CPUs only. Fallback to cpu!");
+ #endif
+       break;
+     }
+diff --git a/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake b/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake
+new file mode 100755
+index 00000000..42ef0fd6
+--- /dev/null
++++ b/toolchains/riscv64-linux-gnu-spacemit.toolchain.cmake
+@@ -0,0 +1,30 @@
++set(CMAKE_SYSTEM_NAME Linux)
++set(CMAKE_SYSTEM_PROCESSOR riscv64)
++set(CMAKE_SYSTEM_VERSION 1)
++
++if(CMAKE_HOST_SYSTEM_PROCESSOR MATCHES "^(riscv)")
++    message(STATUS "HOST SYSTEM ${CMAKE_HOST_SYSTEM_PROCESSOR}")
++else()
++    set(GNU_MACHINE riscv64-unknown-linux-gnu CACHE STRING "GNU compiler triple")
++    if(DEFINED ENV{RISCV_ROOT_PATH})
++        file(TO_CMAKE_PATH $ENV{RISCV_ROOT_PATH} RISCV_ROOT_PATH)
++    else()
++        message(FATAL_ERROR "RISCV_ROOT_PATH env must be defined")
++    endif()
++
++    set(RISCV_ROOT_PATH ${RISCV_ROOT_PATH} CACHE STRING "root path to riscv toolchain")
++    set(CMAKE_C_COMPILER ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-gcc)
++    set(CMAKE_CXX_COMPILER ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-g++)
++    set(CMAKE_STRIP ${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-strip)
++    set(CMAKE_FIND_ROOT_PATH "${RISCV_ROOT_PATH}/sysroot")
++    set(CMAKE_SYSROOT "${RISCV_ROOT_PATH}/sysroot")
++endif()
++
++set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
++set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
++set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)
++set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)
++set(CMAKE_C_FLAGS "-march=rv64gcv_zfh_zvfh_zba_zicbop_zihintpause -mabi=lp64d -ftree-vectorize ${CMAKE_C_FLAGS}")
++set(CMAKE_CXX_FLAGS "-march=rv64gcv_zfh_zvfh_zba_zicbop_zihintpause -mabi=lp64d  -ftree-vectorize ${CXX_FLAGS}")
++set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} -latomic -lrt -lpthread")
++set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} --sysroot=${CMAKE_SYSROOT}")
+
+commit 3fd5a5d55191d3179d72ec5a08c2ab5723e356b0
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Dec 2 13:49:28 2025 +0800
+
+    Avoid NaN in NeMo speaker embedding models. (#2844)
+    
+    Fixes #2818
+    
+    This pull request addresses a critical numerical stability issue within the NeMo speaker embedding models. Due to floating-point precision, the calculated variance could occasionally become negative, leading to NaN values in subsequent computations. The change ensures that the variance is always a positive value, thereby preventing these numerical errors and improving the robustness of the speaker embedding extraction process.
+
+diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
+index ec1c44d6..40253d55 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
++++ b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-impl.h
+@@ -130,7 +130,8 @@ class SpeakerEmbeddingExtractorNeMoImpl : public SpeakerEmbeddingExtractorImpl {
+ 
+     auto EX = m.colwise().mean();
+     auto EX2 = m.array().pow(2).colwise().sum() / num_frames;
+-    auto variance = EX2 - EX.array().pow(2);
++    auto variance = (EX2 - EX.array().pow(2)).max(1e-5);
++
+     auto stddev = variance.array().sqrt();
+ 
+     m = (m.rowwise() - EX).array().rowwise() / (stddev.array() + 1e-5);
+
+commit c66442d219a4f2abfdabf538bbcd70d18b87231f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Dec 1 11:57:23 2025 +0800
+
+    Fix building JNI for Windows (#2840)
+
+diff --git a/.github/workflows/windows-x64-jni.yaml b/.github/workflows/windows-x64-jni.yaml
+index d20fb223..2f9682f5 100644
+--- a/.github/workflows/windows-x64-jni.yaml
++++ b/.github/workflows/windows-x64-jni.yaml
+@@ -52,7 +52,8 @@ jobs:
+             -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF \
+             -DBUILD_ESPEAK_NG_EXE=OFF \
+             -DSHERPA_ONNX_BUILD_C_API_EXAMPLES=OFF  \
+-            -DSHERPA_ONNX_ENABLE_BINARY=ON \
++            -DSHERPA_ONNX_ENABLE_BINARY=OFF \
++            -DSHERPA_ONNX_ENABLE_C_API=OFF \
+             ..
+ 
+       - name: Build sherpa-onnx for windows
+@@ -62,7 +63,6 @@ jobs:
+           cmake --build . --config Release -- -m:2
+           cmake --build . --config Release --target install -- -m:2
+ 
+-          ls -lh ./bin/Release/sherpa-onnx.exe
+           rm -rf install/share
+           rm -rf install/lib/share
+           rm -rf install/lib/pkgconfig
+@@ -81,9 +81,9 @@ jobs:
+           dst=sherpa-onnx-${SHERPA_ONNX_VERSION}-win-x64-jni
+           mkdir -p $dst
+ 
+-          cp -a build/install/bin $dst/
+-          cp -a build/install/lib $dst/
+-          cp -a build/install/include $dst/
++          cp -a build/install/bin $dst/ || true
++          cp -a build/install/lib $dst/ || true
++          cp -a build/install/include $dst/ || true
+ 
+           tar cjvf ${dst}.tar.bz2 $dst
+ 
+@@ -130,4 +130,4 @@ jobs:
+           file: sherpa-onnx-*.tar.bz2
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.12.0
++          # tag: v1.12.18
+diff --git a/sherpa-onnx/jni/common.cc b/sherpa-onnx/jni/common.cc
+index 2de20727..c620016a 100644
+--- a/sherpa-onnx/jni/common.cc
++++ b/sherpa-onnx/jni/common.cc
+@@ -15,6 +15,11 @@ namespace sherpa_onnx {
+ 
+ https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
+  */
++#if defined(_WIN32)
++void PrependAdspLibraryPath(const std::string &new_path) {
++  SHERPA_ONNX_LOGE("This function is not for Windows. Ignore it");
++}
++#else
+ void PrependAdspLibraryPath(const std::string &new_path) {
+   const char *old_path = getenv("ADSP_LIBRARY_PATH");
+   std::string updated_path;
+@@ -43,5 +48,6 @@ Successfully set ADSP_LIBRARY_PATH to
+ 
+    */
+ }
++#endif
+ 
+ }  // namespace sherpa_onnx
+
+commit 1a48531bbd8d7a3ed3c5e6afcf87bc1e673f184c
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Dec 1 11:54:11 2025 +0800
+
+    Add simulate streaming ASR Python example for Paraformer (#2839)
+
+diff --git a/python-api-examples/simulate-streaming-paraformer-microphone.py b/python-api-examples/simulate-streaming-paraformer-microphone.py
+new file mode 100755
+index 00000000..b95e6a21
+--- /dev/null
++++ b/python-api-examples/simulate-streaming-paraformer-microphone.py
+@@ -0,0 +1,243 @@
++#!/usr/bin/env python3
++#
++# Copyright (c)  2025  Xiaomi Corporation
++
++"""
++This file demonstrates how to use sherpa-onnx Python APIs
++with VAD and non-streaming Paraformer for real-time speech recognition
++from a microphone.
++
++Usage:
++
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-paraformer-zh-int8-2025-10-07.tar.bz2
++tar xvf sherpa-onnx-paraformer-zh-int8-2025-10-07.tar.bz2
++
++./python-api-examples/simulate-streaming-paraformer-microphone.py  \
++  --silero-vad-model=./silero_vad.onnx \
++  --paraformer=./sherpa-onnx-paraformer-zh-int8-2025-10-07/model.int8.onnx \
++  --tokens=./sherpa-onnx-paraformer-zh-int8-2025-10-07/tokens.txt
++"""
++import argparse
++import queue
++import sys
++import threading
++import time
++from pathlib import Path
++
++import numpy as np
++
++try:
++    import sounddevice as sd
++except ImportError:
++    print("Please install sounddevice first. You can use")
++    print()
++    print("  pip install sounddevice")
++    print()
++    print("to install it")
++    sys.exit(-1)
++
++import sherpa_onnx
++
++killed = False
++recording_thread = None
++sample_rate = 16000  # Please don't change it
++
++# buffer saves audio samples to be played
++samples_queue = queue.Queue()
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--silero-vad-model",
++        type=str,
++        required=True,
++        help="Path to silero_vad.onnx",
++    )
++
++    parser.add_argument(
++        "--tokens",
++        type=str,
++        help="Path to tokens.txt",
++    )
++
++    parser.add_argument(
++        "--paraformer",
++        default="",
++        type=str,
++        help="Path to the model.onnx from Paraformer",
++    )
++
++    parser.add_argument(
++        "--num-threads",
++        type=int,
++        default=2,
++        help="Number of threads for neural network computation",
++    )
++
++    parser.add_argument(
++        "--hr-lexicon",
++        type=str,
++        default="",
++        help="If not empty, it is the lexicon.txt for homophone replacer",
++    )
++
++    parser.add_argument(
++        "--hr-rule-fsts",
++        type=str,
++        default="",
++        help="If not empty, it is the replace.fst for homophone replacer",
++    )
++
++    return parser.parse_args()
++
++
++def assert_file_exists(filename: str):
++    assert Path(filename).is_file(), (
++        f"{filename} does not exist!\n"
++        "Please refer to "
++        "https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html to download it"
++    )
++
++
++def create_recognizer(args) -> sherpa_onnx.OfflineRecognizer:
++    assert_file_exists(args.paraformer)
++    recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer(
++        paraformer=args.paraformer,
++        tokens=args.tokens,
++        num_threads=args.num_threads,
++        debug=False,
++        hr_rule_fsts=args.hr_rule_fsts,
++        hr_lexicon=args.hr_lexicon,
++    )
++
++    return recognizer
++
++
++def start_recording():
++    # You can use any value you like for samples_per_read
++    samples_per_read = int(0.1 * sample_rate)  # 0.1 second = 100 ms
++
++    with sd.InputStream(channels=1, dtype="float32", samplerate=sample_rate) as s:
++        while not killed:
++            samples, _ = s.read(samples_per_read)  # a blocking read
++            samples = samples.reshape(-1)
++            samples = np.copy(samples)
++            samples_queue.put(samples)
++
++
++def main():
++    devices = sd.query_devices()
++    if len(devices) == 0:
++        print("No microphone devices found")
++        sys.exit(0)
++
++    print(devices)
++
++    # If you want to select a different input device, please use
++    # sd.default.device[0] = xxx
++    # where xxx is the device number
++
++    default_input_device_idx = sd.default.device[0]
++    print(f'Use default device: {devices[default_input_device_idx]["name"]}')
++
++    args = get_args()
++    assert_file_exists(args.tokens)
++    assert_file_exists(args.silero_vad_model)
++
++    assert args.num_threads > 0, args.num_threads
++
++    print("Creating recognizer. Please wait...")
++    recognizer = create_recognizer(args)
++
++    config = sherpa_onnx.VadModelConfig()
++    config.silero_vad.model = args.silero_vad_model
++    config.silero_vad.threshold = 0.5
++    config.silero_vad.min_silence_duration = 0.1  # seconds
++    config.silero_vad.min_speech_duration = 0.25  # seconds
++    # If the current segment is larger than this value, then it increases
++    # the threshold to 0.9 internally. After detecting this segment,
++    # it resets the threshold to its original value.
++    config.silero_vad.max_speech_duration = 8  # seconds
++    config.sample_rate = sample_rate
++
++    window_size = config.silero_vad.window_size
++
++    vad = sherpa_onnx.VoiceActivityDetector(config, buffer_size_in_seconds=100)
++
++    print("Started! Please speak")
++
++    buffer = []
++
++    global recording_thread
++    recording_thread = threading.Thread(target=start_recording)
++    recording_thread.start()
++
++    display = sherpa_onnx.Display()
++
++    started = False
++    started_time = None
++
++    offset = 0
++    while not killed:
++        samples = samples_queue.get()  # a blocking read
++
++        buffer = np.concatenate([buffer, samples])
++        while offset + window_size < len(buffer):
++            vad.accept_waveform(buffer[offset : offset + window_size])
++            if not started and vad.is_speech_detected():
++                started = True
++                started_time = time.time()
++            offset += window_size
++
++        if not started:
++            if len(buffer) > 10 * window_size:
++                offset -= len(buffer) - 10 * window_size
++                buffer = buffer[-10 * window_size :]
++
++        if started and time.time() - started_time > 0.2:
++            stream = recognizer.create_stream()
++            stream.accept_waveform(sample_rate, buffer)
++            recognizer.decode_stream(stream)
++            text = stream.result.text.strip()
++            if text:
++                display.update_text(text)
++                display.display()
++
++            started_time = time.time()
++
++        while not vad.empty():
++            # In general, this while loop is executed only once
++            stream = recognizer.create_stream()
++            stream.accept_waveform(sample_rate, vad.front.samples)
++
++            vad.pop()
++            recognizer.decode_stream(stream)
++
++            text = stream.result.text.strip()
++
++            display.update_text(text)
++
++            buffer = []
++            offset = 0
++            started = False
++            started_time = None
++
++            display.finalize_current_sentence()
++            display.display()
++
++
++if __name__ == "__main__":
++    try:
++        main()
++    except KeyboardInterrupt:
++        killed = True
++        if recording_thread:
++            recording_thread.join()
++        print("\nCaught Ctrl + C. Exiting")
+
+commit 4a49d13938d6fbd7b8aafc032bb8542c8fb890f4
+Author: Wei Kang <wkang.pku@gmail.com>
+Date:   Mon Dec 1 09:51:24 2025 +0800
+
+    [ZipVoice] Fix english tokenization error (#2834)
+
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+index 075b8d0c..26ca18f4 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+@@ -342,7 +342,7 @@ std::vector<TokenIDs> OfflineTtsZipvoiceFrontend::ConvertTextToTokenIds(
+       TokenizeZh(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
+                  &tokens);
+     } else if (pt.second == "en") {
+-      TokenizeEn(pt.first, token2id_, voice, &token_ids, &tokens);
++      TokenizeEn(pt.first, token2id_, "en-us", &token_ids, &tokens);
+     } else if (pt.second == "pinyin") {
+       TokenizePinyin(pt.first, pinyin_encoder_.get(), token2id_, &token_ids,
+                      &tokens);
+
+commit 4b60dc3940d7d9572c64dfa0c4dc576c429c88ff
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Sat Nov 29 10:40:32 2025 +0800
+
+    Fix building without TTS for C API (#2838)
+
+diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
+index 4d612ae1..08702f9c 100644
+--- a/sherpa-onnx/c-api/c-api.cc
++++ b/sherpa-onnx/c-api/c-api.cc
+@@ -1473,6 +1473,13 @@ const SherpaOnnxGeneratedAudio *SherpaOnnxOfflineTtsGenerateWithCallbackWithArg(
+   return nullptr;
+ }
+ 
++const SherpaOnnxGeneratedAudio *SherpaOnnxOfflineTtsGenerateWithZipvoice(
++    const SherpaOnnxOfflineTts *tts, const char *text, const char *prompt_text,
++    const float *prompt_samples, int32_t n_prompt, int32_t prompt_sr,
++    float speed, int32_t num_steps) {
++  return nullptr;
++}
++
+ void SherpaOnnxDestroyOfflineTtsGeneratedAudio(
+     const SherpaOnnxGeneratedAudio *p) {
+   SHERPA_ONNX_LOGE("TTS is not enabled. Please rebuild sherpa-onnx");
+
+commit 624b41c43b63167a0f238db5cb773c6da10a9d39
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 27 18:17:34 2025 +0800
+
+    Release v1.12.18 (#2831)
+
+diff --git a/.github/workflows/pkg-config.yaml b/.github/workflows/pkg-config.yaml
+index 271a8c61..6276ad51 100644
+--- a/.github/workflows/pkg-config.yaml
++++ b/.github/workflows/pkg-config.yaml
+@@ -7,22 +7,6 @@ on:
+       - pkg-config
+     tags:
+       - 'v[0-9]+.[0-9]+.[0-9]+*'
+-    paths:
+-      - '.github/workflows/pkg-config.yaml'
+-      - '.github/scripts/test-offline-tts.sh'
+-      - 'cmake/**'
+-      - 'sherpa-onnx/csrc/*'
+-      - 'sherpa-onnx/c-api/*'
+-      - 'c-api-examples/**'
+-  pull_request:
+-    branches:
+-      - master
+-    paths:
+-      - '.github/workflows/pkg-config.yaml'
+-      - '.github/scripts/test-offline-tts.sh'
+-      - 'cmake/**'
+-      - 'sherpa-onnx/csrc/*'
+-      - 'sherpa-onnx/c-api/*'
+ 
+   workflow_dispatch:
+ 
+diff --git a/CHANGELOG.md b/CHANGELOG.md
+index 8c42ad5e..78bf0cb7 100644
+--- a/CHANGELOG.md
++++ b/CHANGELOG.md
+@@ -1,3 +1,25 @@
++## 1.12.18
++
++* Fix building wheels (#2786)
++* export omniASR_CTC_1B (#2788)
++* Add C++ QNN support for SenseVoice (#2793)
++* Export models for CANN toolkit 7.0 (#2795)
++* Support hotwords with byte level bpe (#2802)
++* Add Android demo with QNN (Qualcomm NPU) for SenseVoice ASR (#2803)
++* Export zipformer ctc models to QNN (#2815)
++* Add spaces between English words for Homophone replacer. (#2817)
++* Add C++ QNN support for Zipformer CTC models. (#2809)
++* Limit symbol visibility in the shared libraries (#2822)
++* Fix warnings for initializing tts lexicon. (#2823)
++* Export zipformer ctc models to Ascend NPU (#2824)
++* Refactor scripts for exporting models to Ascend NPU. (#2825)
++* Add C++ support for Zipformer CTC on Ascend NPU (#2826)
++* Fix segfault when non-wav file is passed to ReadWave (#2821)
++* Avoid calling rknn_dup_context(). (#2828)
++* Add C++ support for Paraformer with RK NPU (#2829)
++* Update README to include NPU support (#2830)
++* Support running whisper large v3 with external data weight (#2807)
++
+ ## 1.12.17
+ 
+ * Fix releasing
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 4bc8cb40..64efc4ba 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -14,7 +14,7 @@ project(sherpa-onnx)
+ # Remember to update
+ # ./CHANGELOG.md
+ # ./new-release.sh
+-set(SHERPA_ONNX_VERSION "1.12.17")
++set(SHERPA_ONNX_VERSION "1.12.18")
+ 
+ # Disable warning about
+ #
+diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
+index 76024de6..214a87cd 100644
+--- a/android/SherpaOnnx/app/build.gradle
++++ b/android/SherpaOnnx/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
+index 76024de6..214a87cd 100644
+--- a/android/SherpaOnnx2Pass/app/build.gradle
++++ b/android/SherpaOnnx2Pass/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
+index c3c5ac3d..7d1505bd 100644
+--- a/android/SherpaOnnxAar/README.md
++++ b/android/SherpaOnnxAar/README.md
+@@ -4,8 +4,8 @@
+ git clone https://github.com/k2-fsa/sherpa-onnx
+ cd sherpa-onnx
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-v1.12.17-android.tar.bz2
+-tar xvf sherpa-onnx-v1.12.17-android.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-v1.12.18-android.tar.bz2
++tar xvf sherpa-onnx-v1.12.18-android.tar.bz2
+ 
+ cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
+ cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
+@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
+ 
+ ./gradlew :sherpa_onnx:assembleRelease
+ ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
+-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.17.aar
++cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.18.aar
+ ```
+diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+index bd6e098c..cf40dd2a 100644
+--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+index 12638bee..27ad7ac2 100644
+--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
+         minSdk = 26
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
+index 93848bc6..f9d0c62b 100644
+--- a/android/SherpaOnnxJavaDemo/app/build.gradle
++++ b/android/SherpaOnnxJavaDemo/app/build.gradle
+@@ -9,8 +9,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 28
+         targetSdk 34
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+@@ -34,5 +34,5 @@ dependencies {
+     implementation 'pub.devrel:easypermissions:3.0.0'
+     implementation 'androidx.core:core-ktx:1.7.0'
+     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
+-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.17'
++    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.18'
+ }
+diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
+index 76024de6..214a87cd 100644
+--- a/android/SherpaOnnxKws/app/build.gradle
++++ b/android/SherpaOnnxKws/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+index 082ea5c8..43b998bf 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+index ef491de9..3a5e95cc 100644
+--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
+         minSdk = 28
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+@@ -58,7 +58,7 @@ dependencies {
+     implementation(libs.compose.foundation)
+     implementation(libs.activity.compose)
+     implementation(libs.core.splashscreen)
+-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.17")
++    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.18")
+     androidTestImplementation(platform(libs.compose.bom))
+     androidTestImplementation(libs.ui.test.junit4)
+     debugImplementation(libs.ui.tooling)
+diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+index a5ce9201..34f1fcd5 100644
+--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+index ddb6f444..1af89d4a 100644
+--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+index a7617b14..41a2bdf9 100644
+--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.slid"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
+index 034325ac..b1151b94 100644
+--- a/android/SherpaOnnxTts/app/build.gradle
++++ b/android/SherpaOnnxTts/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+index 53af0d02..53ad4569 100644
+--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
++++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251113
+-        versionName = "1.12.17"
++        versionCode = 20251127
++        versionName = "1.12.18"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
+index 15784a18..b65311a6 100644
+--- a/android/SherpaOnnxVad/app/build.gradle
++++ b/android/SherpaOnnxVad/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
+index 15784a18..b65311a6 100644
+--- a/android/SherpaOnnxVadAsr/app/build.gradle
++++ b/android/SherpaOnnxVadAsr/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
+index 7f10eb5d..45e27edd 100644
+--- a/android/SherpaOnnxWebSocket/app/build.gradle
++++ b/android/SherpaOnnxWebSocket/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251113
+-        versionName "1.12.17"
++        versionCode 20251127
++        versionName "1.12.18"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/build-ios-shared.sh b/build-ios-shared.sh
+index c1477eb7..a52287f5 100755
+--- a/build-ios-shared.sh
++++ b/build-ios-shared.sh
+@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
+ 	<key>CFBundlePackageType</key>
+ 	<string>FMWK</string>
+ 	<key>CFBundleShortVersionString</key>
+-	<string>1.12.17</string>
++	<string>1.12.18</string>
+ 	<key>CFBundleSupportedPlatforms</key>
+ 	<array>
+ 		<string>iPhoneOS</string>
+diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
+index 3fde5ed0..9eec66cc 100644
+--- a/dart-api-examples/add-punctuations/pubspec.yaml
++++ b/dart-api-examples/add-punctuations/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
+index 909c3751..c48cb85a 100644
+--- a/dart-api-examples/audio-tagging/pubspec.yaml
++++ b/dart-api-examples/audio-tagging/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
+index 762f3493..4b90ab48 100644
+--- a/dart-api-examples/keyword-spotter/pubspec.yaml
++++ b/dart-api-examples/keyword-spotter/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
+index 187661b5..4f89b81f 100644
+--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
+index c2893c4c..1850039f 100644
+--- a/dart-api-examples/speaker-diarization/pubspec.yaml
++++ b/dart-api-examples/speaker-diarization/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
+index 2d68710a..e5122ffa 100644
+--- a/dart-api-examples/speaker-identification/pubspec.yaml
++++ b/dart-api-examples/speaker-identification/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+index 32ec2161..0292fd08 100644
+--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
++++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
+index b9e2166d..90ff0b20 100644
+--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
++++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
+index 577b8efd..f6abcf2f 100644
+--- a/dart-api-examples/streaming-asr/pubspec.yaml
++++ b/dart-api-examples/streaming-asr/pubspec.yaml
+@@ -11,7 +11,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
+index 54c9abf7..d551987f 100644
+--- a/dart-api-examples/tts/pubspec.yaml
++++ b/dart-api-examples/tts/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+index b544cba8..de635d33 100644
+--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
+index 6c124a31..83a6e030 100644
+--- a/dart-api-examples/vad/pubspec.yaml
++++ b/dart-api-examples/vad/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+index 1fe5515f..c3588163 100644
+--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
++++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.17
++version: 1.12.18
+ 
+ topics:
+   - speech-recognition
+@@ -31,7 +31,7 @@ dependencies:
+   record: 6.0.0
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
+index 455b6f1f..0a259b1d 100644
+--- a/flutter-examples/streaming_asr/pubspec.yaml
++++ b/flutter-examples/streaming_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.17
++version: 1.12.18
+ 
+ topics:
+   - speech-recognition
+@@ -30,7 +30,7 @@ dependencies:
+   record: ^6.1.2
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
+index dd4d2433..21d23eb4 100644
+--- a/flutter-examples/tts/pubspec.yaml
++++ b/flutter-examples/tts/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none' # Remove this line if you wish to publish to pub.dev
+ 
+-version: 1.12.17
++version: 1.12.18
+ 
+ environment:
+   sdk: ">=2.17.0 <4.0.0"
+@@ -18,7 +18,7 @@ dependencies:
+   cupertino_icons: ^1.0.6
+   path_provider: ^2.1.3
+   path: ^1.9.0
+-  sherpa_onnx: ^1.12.17
++  sherpa_onnx: ^1.12.18
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   url_launcher: 6.2.6
+diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
+index 825f975f..3491fead 100644
+--- a/flutter/sherpa_onnx/pubspec.yaml
++++ b/flutter/sherpa_onnx/pubspec.yaml
+@@ -17,7 +17,7 @@ topics:
+   - voice-activity-detection
+ 
+ # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+-version: 1.12.17
++version: 1.12.18
+ 
+ homepage: https://github.com/k2-fsa/sherpa-onnx
+ 
+@@ -30,23 +30,23 @@ dependencies:
+   flutter:
+     sdk: flutter
+ 
+-  sherpa_onnx_android: ^1.12.17
++  sherpa_onnx_android: ^1.12.18
+   # sherpa_onnx_android:
+   #   path: ../sherpa_onnx_android
+ 
+-  sherpa_onnx_macos: ^1.12.17
++  sherpa_onnx_macos: ^1.12.18
+   # sherpa_onnx_macos:
+   #   path: ../sherpa_onnx_macos
+ 
+-  sherpa_onnx_linux: ^1.12.17
++  sherpa_onnx_linux: ^1.12.18
+   # sherpa_onnx_linux:
+   #   path: ../sherpa_onnx_linux
+ 
+-  sherpa_onnx_windows: ^1.12.17
++  sherpa_onnx_windows: ^1.12.18
+   # sherpa_onnx_windows:
+   #   path: ../sherpa_onnx_windows
+ 
+-  sherpa_onnx_ios: ^1.12.17
++  sherpa_onnx_ios: ^1.12.18
+   # sherpa_onnx_ios:
+   #   path: ../sherpa_onnx_ios
+ 
+diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+index 3e674158..a4b7b606 100644
+--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
++++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+@@ -7,7 +7,7 @@
+ # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_ios'
+-  s.version          = '1.12.17'
++  s.version          = '1.12.18'
+   s.summary          = 'A new Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ A new Flutter FFI plugin project.
+diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+index dee37a00..dcd8f6b5 100644
+--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
++++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+@@ -4,7 +4,7 @@
+ #
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_macos'
+-  s.version          = '1.12.17'
++  s.version          = '1.12.18'
+   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ sherpa-onnx Flutter FFI plugin project.
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+index dd8845a8..b04e4790 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+@@ -1,7 +1,7 @@
+ /**
+  * Use these variables when you tailor your ArkTS code. They must be of the const type.
+  */
+-export const HAR_VERSION = '1.12.17';
++export const HAR_VERSION = '1.12.18';
+ export const BUILD_MODE_NAME = 'debug';
+ export const DEBUG = true;
+ export const TARGET_NAME = 'default';
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+index 187519e9..9467e26d 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
+ 
+ ```
+   "dependencies": {
+-    "sherpa_onnx": "1.12.17",
++    "sherpa_onnx": "1.12.18",
+   },
+ ```
+ 
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+index 1bfb0d94..7438ad28 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+@@ -1,6 +1,6 @@
+ {
+   "name": "sherpa_onnx",
+-  "version": "1.12.17",
++  "version": "1.12.18",
+   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
+   "main": "Index.ets",
+   "author": "The next-gen Kaldi team",
+diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+index ec30333f..4fc965d4 100644
+--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.17"
++    "sherpa_onnx": "1.12.18"
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+index 992dd9d0..88735c1c 100644
+--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.17",
++    "sherpa_onnx": "1.12.18",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+index 992dd9d0..88735c1c 100644
+--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.17",
++    "sherpa_onnx": "1.12.18",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+index 992dd9d0..88735c1c 100644
+--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.17",
++    "sherpa_onnx": "1.12.18",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+index acc4feb5..dff1eb6b 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
++++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+@@ -1,6 +1,6 @@
+ # Introduction
+ 
+-Please download ./sherpa_onnx-v1.12.17.har
++Please download ./sherpa_onnx-v1.12.18.har
+ from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
+ 
+ Hint: For users who have no access to huggingface, please use
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+index 27182c9b..cf0d6dee 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+@@ -7,7 +7,7 @@
+   "license": "",
+   "dependencies": {
+     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
+-    "sherpa_onnx": "1.12.17",
++    "sherpa_onnx": "1.12.18",
+   }
+ }
+ 
+diff --git a/jitpack.yml b/jitpack.yml
+index 9e83e15c..bd8c3c5d 100644
+--- a/jitpack.yml
++++ b/jitpack.yml
+@@ -2,8 +2,8 @@ jdk:
+   - openjdk17
+ 
+ before_install:
+-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-1.12.17.aar
++  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-1.12.18.aar
+ 
+ install:
+-  - FILE="-Dfile=sherpa-onnx-1.12.17.aar"
+-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.17 -Dpackaging=aar -DgeneratePom=true
++  - FILE="-Dfile=sherpa-onnx-1.12.18.aar"
++  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.18 -Dpackaging=aar -DgeneratePom=true
+diff --git a/mfc-examples/README.md b/mfc-examples/README.md
+index 2c7edb25..683034ac 100644
+--- a/mfc-examples/README.md
++++ b/mfc-examples/README.md
+@@ -5,9 +5,9 @@ for speech recognition.
+ 
+ |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
+ |---------|--------------------|-------------------|------------|
+-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x86-v1.12.17.exe)| Non-streaming speech recognition|
+-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x86-v1.12.17.exe)| Streaming speech recognition|
+-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x86-v1.12.17.exe)| Non-streaming text to speech|
++|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-asr-x86-v1.12.18.exe)| Non-streaming speech recognition|
++|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-streaming-asr-x86-v1.12.18.exe)| Streaming speech recognition|
++|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x64-v1.12.18.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.18/sherpa-onnx-non-streaming-tts-x86-v1.12.18.exe)| Non-streaming text to speech|
+ 
+ Caution: You need to use Windows and install Visual Studio 2022 in order to
+ compile it.
+diff --git a/new-release.sh b/new-release.sh
+index 3758063c..5d4a89c5 100755
+--- a/new-release.sh
++++ b/new-release.sh
+@@ -2,11 +2,11 @@
+ 
+ set -ex
+ 
+-old_version_code=20251022
+-new_version_code=20251113
++old_version_code=20251113
++new_version_code=20251127
+ 
+-old_version="1\.12\.16"
+-new_version="1\.12\.17"
++old_version="1\.12\.17"
++new_version="1\.12\.18"
+ 
+ replace_str="s/$old_version/$new_version/g"
+ 
+diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
+index bf6beed9..3555e6ed 100644
+--- a/nodejs-addon-examples/package.json
++++ b/nodejs-addon-examples/package.json
+@@ -1,5 +1,5 @@
+ {
+   "dependencies": {
+-    "sherpa-onnx-node": "^1.12.17"
++    "sherpa-onnx-node": "^1.12.18"
+   }
+ }
+diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
+index ab2c37d2..db474390 100644
+--- a/nodejs-examples/package.json
++++ b/nodejs-examples/package.json
+@@ -2,7 +2,7 @@
+   "dependencies": {
+     "mic": "^2.1.2",
+     "naudiodon2": "^2.4.0",
+-    "sherpa-onnx": "^1.12.17",
++    "sherpa-onnx": "^1.12.18",
+     "wav": "^1.0.2"
+   }
+ }
+diff --git a/pom.xml b/pom.xml
+index e8191a24..f54d3c84 100644
+--- a/pom.xml
++++ b/pom.xml
+@@ -4,7 +4,7 @@
+     <modelVersion>4.0.0</modelVersion>
+     <groupId>com.k2fsa.sherpa.onnx</groupId>
+     <artifactId>sherpa-onnx-android</artifactId>
+-    <version>1.12.17</version>
++    <version>1.12.18</version>
+     <url>https://github.com/k2-fsa/sherpa-onnx</url>
+     <packaging>pom</packaging>
+     <description>First Android Library</description>
+diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
+index b7a22878..c86f3727 100644
+--- a/scripts/wheel/sherpa-onnx-bin/setup.py
++++ b/scripts/wheel/sherpa-onnx-bin/setup.py
+@@ -13,7 +13,7 @@ print("bin_files", bin_files)
+ 
+ setup(
+     name="sherpa-onnx-bin",
+-    version="1.12.17",
++    version="1.12.18",
+     description="Binary executables for sherpa-onnx",
+     author="The sherpa-onnx development team",
+     url="https://github.com/k2-fsa/sherpa-onnx",
+@@ -23,7 +23,7 @@ setup(
+     packages=[],
+     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
+     install_requires=[
+-        "sherpa-onnx-core==1.12.17",
++        "sherpa-onnx-core==1.12.18",
+     ],
+     classifiers=[
+         "Programming Language :: Python :: 3",
+diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
+index 071319fb..456e2b7f 100644
+--- a/scripts/wheel/sherpa-onnx-core/setup.py
++++ b/scripts/wheel/sherpa-onnx-core/setup.py
+@@ -23,7 +23,7 @@ def get_binaries():
+ 
+ setup(
+     name="sherpa-onnx-core",
+-    version="1.12.17",
++    version="1.12.18",
+     description="Core shared libraries for sherpa-onnx",
+     packages=["sherpa_onnx"],
+     include_package_data=True,
+diff --git a/setup.py b/setup.py
+index 061a2f03..329bddd4 100644
+--- a/setup.py
++++ b/setup.py
+@@ -109,7 +109,7 @@ setuptools.setup(
+         ],
+     },
+     license="Apache licensed, as found in the LICENSE file",
+-    install_requires=["sherpa-onnx-core==1.12.17"] if need_split_package() else None,
++    install_requires=["sherpa-onnx-core==1.12.18"] if need_split_package() else None,
+ )
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
+diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
+index 02c48a40..e118dcc5 100644
+--- a/sherpa-onnx/csrc/version.cc
++++ b/sherpa-onnx/csrc/version.cc
+@@ -7,17 +7,17 @@
+ namespace sherpa_onnx {
+ 
+ const char *GetGitDate() {
+-  static const char *date = "Thu Nov 13 19:08:29 2025";
++  static const char *date = "Thu Nov 27 15:24:39 2025";
+   return date;
+ }
+ 
+ const char *GetGitSha1() {
+-  static const char *sha1 = "6d199b18";
++  static const char *sha1 = "7d1d2270";
+   return sha1;
+ }
+ 
+ const char *GetVersionStr() {
+-  static const char *version = "1.12.17";
++  static const char *version = "1.12.18";
+   return version;
+ }
+ 
+
+commit ec5e631cf70b897e390df673bdd0549bcc9ff334
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 27 18:12:56 2025 +0800
+
+    Support running whisper large v3 with external data weight (#2807)
+
+diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
+index 3921c3b0..22c395b2 100644
+--- a/sherpa-onnx/csrc/offline-whisper-model.cc
++++ b/sherpa-onnx/csrc/offline-whisper-model.cc
+@@ -37,15 +37,13 @@ class OfflineWhisperModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    {
+-      auto buf = ReadFile(config.whisper.encoder);
+-      InitEncoder(buf.data(), buf.size());
+-    }
++    encoder_sess_ = std::make_unique<Ort::Session>(
++        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
++    InitEncoder(nullptr, 0);
+ 
+-    {
+-      auto buf = ReadFile(config.whisper.decoder);
+-      InitDecoder(buf.data(), buf.size());
+-    }
++    decoder_sess_ = std::make_unique<Ort::Session>(
++        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
++    InitDecoder(nullptr, 0);
+   }
+ 
+   explicit Impl(const SpokenLanguageIdentificationConfig &config)
+@@ -53,15 +51,13 @@ class OfflineWhisperModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    {
+-      auto buf = ReadFile(config.whisper.encoder);
+-      InitEncoder(buf.data(), buf.size());
+-    }
++    encoder_sess_ = std::make_unique<Ort::Session>(
++        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.encoder), sess_opts_);
++    InitEncoder(nullptr, 0);
+ 
+-    {
+-      auto buf = ReadFile(config.whisper.decoder);
+-      InitDecoder(buf.data(), buf.size());
+-    }
++    decoder_sess_ = std::make_unique<Ort::Session>(
++        env_, SHERPA_ONNX_TO_ORT_PATH(config.whisper.decoder), sess_opts_);
++    InitDecoder(nullptr, 0);
+   }
+ 
+   template <typename Manager>
+@@ -234,8 +230,16 @@ class OfflineWhisperModel::Impl {
+ 
+  private:
+   void InitEncoder(void *model_data, size_t model_data_length) {
+-    encoder_sess_ = std::make_unique<Ort::Session>(
+-        env_, model_data, model_data_length, sess_opts_);
++    if (model_data) {
++      encoder_sess_ = std::make_unique<Ort::Session>(
++          env_, model_data, model_data_length, sess_opts_);
++    } else if (!encoder_sess_) {
++      SHERPA_ONNX_LOGE(
++          "Please pass buffer data or initialize encoder session outside of "
++          "this "
++          "function");
++      SHERPA_ONNX_EXIT(-1);
++    }
+ 
+     GetInputNames(encoder_sess_.get(), &encoder_input_names_,
+                   &encoder_input_names_ptr_);
+@@ -293,8 +297,15 @@ class OfflineWhisperModel::Impl {
+   }
+ 
+   void InitDecoder(void *model_data, size_t model_data_length) {
+-    decoder_sess_ = std::make_unique<Ort::Session>(
+-        env_, model_data, model_data_length, sess_opts_);
++    if (model_data) {
++      decoder_sess_ = std::make_unique<Ort::Session>(
++          env_, model_data, model_data_length, sess_opts_);
++    } else if (!decoder_sess_) {
++      SHERPA_ONNX_LOGE(
++          "Please pass buffer data or initialize decoder session outside of "
++          "this function");
++      SHERPA_ONNX_EXIT(-1);
++    }
+ 
+     GetInputNames(decoder_sess_.get(), &decoder_input_names_,
+                   &decoder_input_names_ptr_);
+
+commit 7d1d2270a1fddeed4851abc1f1985189679e6a32
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 27 15:24:39 2025 +0800
+
+    Update README to include NPU support (#2830)
+
+diff --git a/README.md b/README.md
+index ed9ffd1f..6eb4cd4e 100644
+--- a/README.md
++++ b/README.md
+@@ -45,6 +45,12 @@ For Rust support, please see [sherpa-rs][sherpa-rs]
+ 
+ It also supports WebAssembly.
+ 
++### Supported NPUs
++
++| [1. Rockchip NPU (RKNN)][rknpu-doc] | [2. Qualcomm NPU (QNN)][qnn-doc]  | [3. Ascend NPU][ascend-doc] |
++|-------------------------------------|-----------------------------------|-----------------------------|
++|                                   |                                 |                           |
++
+ [Join our discord](https://discord.gg/fJdxzg2VbG)
+ 
+ 
+@@ -573,3 +579,6 @@ a multimodal chatbot based on go with sherpa-onnx's speech lib api.
+ [kws-url]: https://k2-fsa.github.io/sherpa/onnx/kws/index.html
+ [punct-url]: https://k2-fsa.github.io/sherpa/onnx/punctuation/index.html
+ [se-url]: https://k2-fsa.github.io/sherpa/onnx/speech-enhancement/index.html
++[rknpu-doc]: https://k2-fsa.github.io/sherpa/onnx/rknn/index.html
++[qnn-doc]: https://k2-fsa.github.io/sherpa/onnx/qnn/index.html
++[ascend-doc]: https://k2-fsa.github.io/sherpa/onnx/ascend/index.html
+
+commit 6873c3bf609f64486e40b3c7f71e1d8d7f3a175a
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 27 15:03:08 2025 +0800
+
+    Add C++ support for Paraformer with RK NPU (#2829)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 6ad708f8..2e771d7e 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -25,6 +25,7 @@ set(sources
+   keyword-spotter-impl.cc
+   keyword-spotter.cc
+   lodr-fst.cc
++  math.cc
+   offline-canary-model-config.cc
+   offline-canary-model.cc
+   offline-ctc-fst-decoder-config.cc
+@@ -182,6 +183,7 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+   list(APPEND sources
+     ./rknn/context-blocking-queue-rknn.cc
+     ./rknn/offline-sense-voice-model-rknn.cc
++    ./rknn/offline-paraformer-model-rknn.cc
+     ./rknn/online-stream-rknn.cc
+     ./rknn/online-transducer-greedy-search-decoder-rknn.cc
+     ./rknn/online-transducer-modified-beam-search-decoder-rknn.cc
+diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+index 739b752b..8acd020d 100644
+--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+@@ -27,57 +27,11 @@
+ #include "sherpa-onnx/csrc/ascend/utils.h"
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/math.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+ 
+ namespace sherpa_onnx {
+ 
+-static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
+-  for (int32_t i = 0; i < n; ++i) {
+-    in_out[i] += scale * src[i];
+-  }
+-}
+-
+-static void Scale(const float *src, float scale, int32_t n, float *out) {
+-  for (int32_t i = 0; i < n; ++i) {
+-    out[i] = scale * src[i];
+-  }
+-}
+-
+-static std::vector<float> ComputeAcousticEmbedding(
+-    std::vector<float> encoder_out, std::vector<float> alphas,
+-    int32_t encoder_dim) {
+-  std::vector<float> ans;
+-  ans.reserve(encoder_out.size());
+-
+-  float acc = 0;
+-  std::vector<float> cur_emb(encoder_dim);
+-  for (int32_t i = 0; i < static_cast<int32_t>(alphas.size()); ++i) {
+-    float w = alphas[i];
+-
+-    acc += w;
+-    if (acc >= 1) {
+-      float overflow = acc - 1;
+-      float remain = w - overflow;
+-
+-      ScaleAdd(encoder_out.data() + i * encoder_dim, remain, encoder_dim,
+-               cur_emb.data());
+-
+-      ans.insert(ans.end(), cur_emb.begin(), cur_emb.end());
+-
+-      Scale(encoder_out.data() + i * encoder_dim, overflow, encoder_dim,
+-            cur_emb.data());
+-
+-      acc = overflow;
+-    } else {
+-      ScaleAdd(encoder_out.data() + i * encoder_dim, w, encoder_dim,
+-               cur_emb.data());
+-    }
+-  }
+-  // TODO(fangjun): The last cur_emb is not used
+-
+-  return ans;
+-}
+-
+ class OfflineParaformerModelAscend::Impl {
+  public:
+   explicit Impl(const OfflineModelConfig &config) : config_(config) {
+@@ -133,6 +87,9 @@ class OfflineParaformerModelAscend::Impl {
+     std::lock_guard<std::mutex> lock(mutex_);
+ 
+     features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
+ 
+     int32_t num_frames = features.size() / 560;
+ 
+@@ -154,13 +111,16 @@ class OfflineParaformerModelAscend::Impl {
+                     num_frames * sizeof(float), ACL_MEMCPY_DEVICE_TO_HOST);
+     SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
+ 
+-    std::vector<float> acoustic_embedding = ComputeAcousticEmbedding(
+-        std::move(encoder_out_cpu), std::move(alphas_cpu), encoder_dim_);
++    std::vector<float> acoustic_embedding =
++        ComputeAcousticEmbedding(encoder_out_cpu, alphas_cpu, encoder_dim_);
+     if (acoustic_embedding.empty()) {
+       // no speech in the audio file
+       return {};
+     }
+ 
++    encoder_out_cpu.clear();
++    alphas_cpu.clear();
++
+     int32_t num_tokens = acoustic_embedding.size() / encoder_dim_;
+ 
+     RunDecoder(num_frames, std::move(acoustic_embedding));
+@@ -366,6 +326,10 @@ class OfflineParaformerModelAscend::Impl {
+     int32_t in_feat_dim = 80;
+ 
+     int32_t in_num_frames = in.size() / in_feat_dim;
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
+     int32_t out_num_frames =
+         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+ 
+diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+index 97e1ae86..53da0300 100644
+--- a/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
++++ b/sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h
+@@ -111,6 +111,10 @@ class OfflineRecognizerSenseVoiceAscendImpl : public OfflineRecognizerImpl {
+                             : meta_data.without_itn_id;
+ 
+     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    if (logits.empty()) {
++      return;
++    }
++
+     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+ 
+     auto result =
+diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+index a7409206..542d0e4f 100644
+--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+@@ -57,6 +57,9 @@ class OfflineSenseVoiceModelAscend::Impl {
+     std::lock_guard<std::mutex> lock(mutex_);
+ 
+     features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
+ 
+     int32_t num_frames = features.size() / 560;
+ 
+@@ -161,6 +164,10 @@ class OfflineSenseVoiceModelAscend::Impl {
+     int32_t in_feat_dim = 80;
+ 
+     int32_t in_num_frames = in.size() / in_feat_dim;
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
+     int32_t out_num_frames =
+         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+ 
+diff --git a/sherpa-onnx/csrc/math.cc b/sherpa-onnx/csrc/math.cc
+new file mode 100644
+index 00000000..d3628191
+--- /dev/null
++++ b/sherpa-onnx/csrc/math.cc
+@@ -0,0 +1,57 @@
++// sherpa-onnx/csrc/math.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#include "sherpa-onnx/csrc/math.h"
++
++#include <vector>
++namespace sherpa_onnx {
++
++static void ScaleAdd(const float *src, float scale, int32_t n, float *in_out) {
++  for (int32_t i = 0; i < n; ++i) {
++    in_out[i] += scale * src[i];
++  }
++}
++
++static void Scale(const float *src, float scale, int32_t n, float *out) {
++  for (int32_t i = 0; i < n; ++i) {
++    out[i] = scale * src[i];
++  }
++}
++
++// this if for Paraformer
++std::vector<float> ComputeAcousticEmbedding(
++    const std::vector<float> &encoder_out, const std::vector<float> &alphas,
++    int32_t encoder_dim) {
++  std::vector<float> ans;
++  ans.reserve(encoder_out.size());
++
++  float acc = 0;
++  std::vector<float> cur_emb(encoder_dim);
++  for (int32_t i = 0; i < static_cast<int32_t>(alphas.size()); ++i) {
++    float w = alphas[i];
++
++    acc += w;
++    if (acc >= 1) {
++      float overflow = acc - 1;
++      float remain = w - overflow;
++
++      ScaleAdd(encoder_out.data() + i * encoder_dim, remain, encoder_dim,
++               cur_emb.data());
++
++      ans.insert(ans.end(), cur_emb.begin(), cur_emb.end());
++
++      Scale(encoder_out.data() + i * encoder_dim, overflow, encoder_dim,
++            cur_emb.data());
++
++      acc = overflow;
++    } else {
++      ScaleAdd(encoder_out.data() + i * encoder_dim, w, encoder_dim,
++               cur_emb.data());
++    }
++  }
++  // TODO(fangjun): The last cur_emb is not used
++
++  return ans;
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/math.h b/sherpa-onnx/csrc/math.h
+index 21fd3803..1edc065d 100644
+--- a/sherpa-onnx/csrc/math.h
++++ b/sherpa-onnx/csrc/math.h
+@@ -131,5 +131,10 @@ std::vector<int32_t> TopkIndex(const std::vector<std::vector<T>> &vec,
+   return TopkIndex(flatten.data(), flatten.size(), topk);
+ }
+ 
++// For Paraformer
++std::vector<float> ComputeAcousticEmbedding(
++    const std::vector<float> &encoder_out, const std::vector<float> &alphas,
++    int32_t encoder_dim);
++
+ }  // namespace sherpa_onnx
+ #endif  // SHERPA_ONNX_CSRC_MATH_H_
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+index e71b77bb..e7075bc4 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
++++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+@@ -17,8 +17,10 @@ namespace sherpa_onnx {
+ void OfflineParaformerModelConfig::Register(ParseOptions *po) {
+   po->Register(
+       "paraformer", &model,
+-      "Path to model.onnx of Paraformer or if you use Ascend NPU, it is "
+-      "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om");
++      "Path to model.onnx of Paraformer. If you use Ascend NPU, it is "
++      "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
++      "If you use RK NPU, it is "
++      "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn");
+ }
+ 
+ bool OfflineParaformerModelConfig::Validate() const {
+@@ -47,7 +49,24 @@ bool OfflineParaformerModelConfig::Validate() const {
+     return true;
+   }
+ 
+-  SHERPA_ONNX_LOGE("Please pass *.onnx or *.om models. Given '%s'",
++  if (EndsWith(model, ".rknn")) {
++    std::vector<std::string> filenames;
++    SplitStringToVector(model, ",", false, &filenames);
++    if (filenames.size() != 3 || !EndsWith(filenames[0], "encoder.rknn") ||
++        !EndsWith(filenames[1], "predictor.rknn") ||
++        !EndsWith(filenames[2], "decoder.rknn")) {
++      SHERPA_ONNX_LOGE(
++          "For RKNN, you should pass "
++          "/path/encoder.rknn,/path/predictor.rknn,/path/decoder.rknn. "
++          "Given '%s'",
++          model.c_str());
++      return false;
++    }
++
++    return true;
++  }
++
++  SHERPA_ONNX_LOGE("Please pass *.onnx, *.om, or *.rknn models. Given '%s'",
+                    model.c_str());
+   return false;
+ }
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.h b/sherpa-onnx/csrc/offline-paraformer-model-config.h
+index 1bb822b8..e4fa94d8 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model-config.h
++++ b/sherpa-onnx/csrc/offline-paraformer-model-config.h
+@@ -13,6 +13,10 @@ namespace sherpa_onnx {
+ struct OfflineParaformerModelConfig {
+   // for ascend npu,
+   // model is "/path/to/encoder.om,/path/to/predictor.om,/path/to/decoder.om"
++  //
++  // for rknn,
++  // model is
++  // "/path/to/encoder.rknn,/path/to/predictor.rknn,/path/to/decoder.rknn"
+   std::string model;
+ 
+   OfflineParaformerModelConfig() = default;
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 318be0e0..2ff1a4a7 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -37,6 +37,7 @@
+ #include "sherpa-onnx/csrc/text-utils.h"
+ 
+ #if SHERPA_ONNX_ENABLE_RKNN
++#include "sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h"
+ #include "sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h"
+ #endif
+ 
+@@ -57,14 +58,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     const OfflineRecognizerConfig &config) {
+   if (config.model_config.provider == "rknn") {
+ #if SHERPA_ONNX_ENABLE_RKNN
+-    if (config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
++    } else if (!config.model_config.paraformer.model.empty()) {
++      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(config);
++    } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
++          "Only SenseVoice and Paraformer models are currently supported "
+           "by rknn for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+-    } else if (!config.model_config.sense_voice.model.empty()) {
+-      return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -317,12 +320,16 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     Manager *mgr, const OfflineRecognizerConfig &config) {
+   if (config.model_config.provider == "rknn") {
+ #if SHERPA_ONNX_ENABLE_RKNN
+-    if (config.model_config.sense_voice.model.empty()) {
+-      SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
+-          "by rknn for non-streaming ASR. Fallback to CPU");
+-    } else if (!config.model_config.sense_voice.model.empty()) {
++    if (!config.model_config.sense_voice.model.empty()) {
+       return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(mgr, config);
++    } else if (!config.model_config.paraformer.model.empty()) {
++      return std::make_unique<OfflineRecognizerParaformerRknnImpl>(mgr, config);
++    } else {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice and Paraformer models are currently supported "
++          "by rknn for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -348,7 +355,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
+-          "supported by Ascend NPU for non-streaming ASR. Fallback to CPU");
++          "supported by Ascend NPU for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -666,8 +675,8 @@ OfflineRecognizerImpl::OfflineRecognizerImpl(
+         itn_list_.push_back(
+             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
+       }  // for (; !reader->Done(); reader->Next())
+-    }  // for (const auto &f : files)
+-  }  // if (!config.rule_fars.empty())
++    }    // for (const auto &f : files)
++  }      // if (!config.rule_fars.empty())
+ 
+   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
+     auto hr_config = config.hr;
+diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+index 22e0a6ed..a8185cd4 100644
+--- a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+@@ -76,7 +76,7 @@ class ContextBlockingQueueRknn::Impl {
+ 
+ ContextBlockingQueueRknn::ContextBlockingQueueRknn(rknn_context context,
+                                                    int32_t num_threads,
+-                                                   int32_t capacity /*= 20*/)
++                                                   int32_t capacity /*= 10*/)
+     : impl_(std::make_unique<Impl>(context, num_threads, capacity)) {}
+ 
+ ContextBlockingQueueRknn::~ContextBlockingQueueRknn() = default;
+diff --git a/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
+new file mode 100644
+index 00000000..c23460e1
+--- /dev/null
++++ b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
+@@ -0,0 +1,406 @@
++// sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
++
++#include <algorithm>
++#include <array>
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/math.h"
++#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
++#include "sherpa-onnx/csrc/rknn/macros.h"
++#include "sherpa-onnx/csrc/rknn/utils.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++
++namespace sherpa_onnx {
++
++class OfflineParaformerModelRknn::Impl {
++ public:
++  ~Impl() {
++    auto ret = rknn_destroy(encoder_ctx_);
++    if (ret != RKNN_SUCC) {
++      SHERPA_ONNX_LOGE("Failed to destroy the encoder context");
++    }
++
++    ret = rknn_destroy(predictor_ctx_);
++    if (ret != RKNN_SUCC) {
++      SHERPA_ONNX_LOGE("Failed to destroy the predictor context");
++    }
++
++    ret = rknn_destroy(decoder_ctx_);
++    if (ret != RKNN_SUCC) {
++      SHERPA_ONNX_LOGE("Failed to destroy the decoder context");
++    }
++  }
++
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    std::vector<std::string> filenames;
++    SplitStringToVector(config_.paraformer.model, ",", false, &filenames);
++    if (filenames.size() != 3) {
++      SHERPA_ONNX_LOGE("Invalid Paraformer RK NPU model '%s'",
++                       config_.paraformer.model.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    {
++      auto buf = ReadFile(filenames[0]);
++      InitEncoder(buf.data(), buf.size());
++    }
++
++    {
++      auto buf = ReadFile(filenames[1]);
++      InitPredictor(buf.data(), buf.size());
++    }
++
++    {
++      auto buf = ReadFile(filenames[2]);
++      InitDecoder(buf.data(), buf.size());
++    }
++
++    PostInit();
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    std::vector<std::string> filenames;
++    SplitStringToVector(config_.paraformer.model, ",", false, &filenames);
++    if (filenames.size() != 3) {
++      SHERPA_ONNX_LOGE("Invalid Paraformer RK NPU model '%s'",
++                       config_.paraformer.model.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    {
++      auto buf = ReadFile(mgr, filenames[0]);
++      InitEncoder(buf.data(), buf.size());
++    }
++
++    {
++      auto buf = ReadFile(mgr, filenames[1]);
++      InitPredictor(buf.data(), buf.size());
++    }
++
++    {
++      auto buf = ReadFile(mgr, filenames[2]);
++      InitDecoder(buf.data(), buf.size());
++    }
++
++    PostInit();
++  }
++
++  std::vector<float> Run(std::vector<float> features) {
++    std::vector<float> encoder_out = RunEncoder(features);
++    if (encoder_out.empty()) {
++      return {};
++    }
++
++    std::vector<float> alphas = RunPredictor(encoder_out);
++
++    std::vector<float> acoustic_embedding =
++        ComputeAcousticEmbedding(encoder_out, alphas, encoder_out_dim_);
++    if (acoustic_embedding.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("No speech found in the input audio");
++      }
++
++      return {};
++    }
++
++    int32_t num_tokens = acoustic_embedding.size() / encoder_out_dim_;
++
++    acoustic_embedding.resize(encoder_out.size());
++
++    return RunDecoder(std::move(encoder_out), std::move(acoustic_embedding),
++                      num_tokens);
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++
++ private:
++  std::vector<float> RunEncoder(std::vector<float> features) {
++    features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
++
++    std::vector<rknn_input> inputs(encoder_input_attrs_.size());
++
++    inputs[0].index = encoder_input_attrs_[0].index;
++    inputs[0].type = RKNN_TENSOR_FLOAT32;
++    inputs[0].fmt = encoder_input_attrs_[0].fmt;
++    inputs[0].buf = reinterpret_cast<void *>(features.data());
++    inputs[0].size = features.size() * sizeof(float);
++
++    std::vector<float> out(encoder_output_attrs_[0].n_elems);
++
++    std::vector<rknn_output> outputs(encoder_output_attrs_.size());
++    outputs[0].index = encoder_output_attrs_[0].index;
++    outputs[0].is_prealloc = 1;
++    outputs[0].want_float = 1;
++    outputs[0].size = out.size() * sizeof(float);
++    outputs[0].buf = reinterpret_cast<void *>(out.data());
++
++    rknn_context ctx = encoder_ctx_queue_->Take();
++
++    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set encoder inputs");
++
++    ret = rknn_run(ctx, nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the encoder model");
++
++    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get encoder output");
++
++    encoder_ctx_queue_->Put(ctx);
++
++    return out;
++  }
++
++  std::vector<float> RunPredictor(const std::vector<float> &encoder_out) {
++    std::vector<rknn_input> inputs(predictor_input_attrs_.size());
++
++    inputs[0].index = predictor_input_attrs_[0].index;
++    inputs[0].type = RKNN_TENSOR_FLOAT32;
++    inputs[0].fmt = predictor_input_attrs_[0].fmt;
++    inputs[0].buf =
++        reinterpret_cast<void *>(const_cast<float *>(encoder_out.data()));
++    inputs[0].size = encoder_out.size() * sizeof(float);
++
++    std::vector<float> out(predictor_output_attrs_[0].n_elems);
++
++    std::vector<rknn_output> outputs(predictor_output_attrs_.size());
++    outputs[0].index = predictor_output_attrs_[0].index;
++    outputs[0].is_prealloc = 1;
++    outputs[0].want_float = 1;
++    outputs[0].size = out.size() * sizeof(float);
++    outputs[0].buf = reinterpret_cast<void *>(out.data());
++
++    rknn_context ctx = predictor_ctx_queue_->Take();
++
++    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set predictor inputs");
++
++    ret = rknn_run(ctx, nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the predictor model");
++
++    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get predictor output");
++
++    predictor_ctx_queue_->Put(ctx);
++
++    return out;
++  }
++
++  std::vector<float> RunDecoder(std::vector<float> encoder_out,
++                                std::vector<float> acoustic_embedding,
++                                int32_t num_tokens) {
++    int32_t num_frames = encoder_out.size() / encoder_out_dim_;
++
++    std::vector<rknn_input> inputs(decoder_input_attrs_.size());
++
++    inputs[0].index = decoder_input_attrs_[0].index;
++    inputs[0].type = RKNN_TENSOR_FLOAT32;
++    inputs[0].fmt = decoder_input_attrs_[0].fmt;
++    inputs[0].buf = reinterpret_cast<void *>(encoder_out.data());
++    inputs[0].size = encoder_out.size() * sizeof(float);
++
++    inputs[1].index = decoder_input_attrs_[1].index;
++    inputs[1].type = RKNN_TENSOR_FLOAT32;
++    inputs[1].fmt = decoder_input_attrs_[1].fmt;
++    inputs[1].buf = reinterpret_cast<void *>(acoustic_embedding.data());
++    inputs[1].size = acoustic_embedding.size() * sizeof(float);
++
++    std::vector<float> mask(num_frames, 1);
++    std::fill(mask.begin() + num_tokens, mask.end(), 0);
++
++    inputs[2].index = decoder_input_attrs_[2].index;
++    inputs[2].type = RKNN_TENSOR_FLOAT32;
++    inputs[2].fmt = decoder_input_attrs_[2].fmt;
++    inputs[2].buf = reinterpret_cast<void *>(mask.data());
++    inputs[2].size = mask.size() * sizeof(float);
++
++    std::vector<float> out(decoder_output_attrs_[0].n_elems);
++
++    std::vector<rknn_output> outputs(decoder_output_attrs_.size());
++    outputs[0].index = decoder_output_attrs_[0].index;
++    outputs[0].is_prealloc = 1;
++    outputs[0].want_float = 1;
++    outputs[0].size = out.size() * sizeof(float);
++    outputs[0].buf = reinterpret_cast<void *>(out.data());
++
++    rknn_context ctx = decoder_ctx_queue_->Take();
++
++    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set decoder inputs");
++
++    ret = rknn_run(ctx, nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run the decoder model");
++
++    ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
++    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get decoder output");
++
++    decoder_ctx_queue_->Put(ctx);
++
++    return out;
++  }
++
++  void InitEncoder(void *model_data, size_t model_data_length) {
++    InitContext(model_data, model_data_length, config_.debug, &encoder_ctx_);
++
++    InitInputOutputAttrs(encoder_ctx_, config_.debug, &encoder_input_attrs_,
++                         &encoder_output_attrs_);
++
++    num_input_frames_ = encoder_input_attrs_[0].dims[1];
++    encoder_out_dim_ = encoder_output_attrs_[0].dims[2];
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("num_input_frames_: %d", num_input_frames_);
++      SHERPA_ONNX_LOGE("encoder_out_dim:: %d", encoder_out_dim_);
++    }
++  }
++
++  void InitPredictor(void *model_data, size_t model_data_length) {
++    InitContext(model_data, model_data_length, config_.debug, &predictor_ctx_);
++
++    InitInputOutputAttrs(predictor_ctx_, config_.debug, &predictor_input_attrs_,
++                         &predictor_output_attrs_);
++  }
++
++  void InitDecoder(void *model_data, size_t model_data_length) {
++    InitContext(model_data, model_data_length, config_.debug, &decoder_ctx_);
++
++    InitInputOutputAttrs(decoder_ctx_, config_.debug, &decoder_input_attrs_,
++                         &decoder_output_attrs_);
++    vocab_size_ = decoder_output_attrs_[0].dims[2];
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
++    }
++  }
++
++  std::vector<float> ApplyLFR(std::vector<float> in) const {
++    int32_t lfr_window_size = 7;
++    int32_t lfr_window_shift = 6;
++    int32_t in_feat_dim = 80;
++
++    int32_t in_num_frames = in.size() / in_feat_dim;
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
++    int32_t out_num_frames =
++        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
++
++    if (out_num_frames > num_input_frames_) {
++      SHERPA_ONNX_LOGE(
++          "Number of input frames %d is too large. Truncate it to %d frames.",
++          out_num_frames, num_input_frames_);
++
++      SHERPA_ONNX_LOGE(
++          "Recognition result may be truncated/incomplete. Please select a "
++          "model accepting longer audios.");
++
++      out_num_frames = num_input_frames_;
++    }
++
++    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
++
++    std::vector<float> out(num_input_frames_ * out_feat_dim);
++
++    const float *p_in = in.data();
++    float *p_out = out.data();
++
++    for (int32_t i = 0; i != out_num_frames; ++i) {
++      std::copy(p_in, p_in + out_feat_dim, p_out);
++
++      p_out += out_feat_dim;
++      p_in += lfr_window_shift * in_feat_dim;
++    }
++
++    return out;
++  }
++
++  void PostInit() {
++    if (config_.num_threads > 1) {
++      config_.num_threads = 1;
++    }
++
++    encoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        encoder_ctx_, config_.num_threads);
++
++    predictor_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        predictor_ctx_, config_.num_threads);
++
++    decoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        decoder_ctx_, config_.num_threads);
++  }
++
++ private:
++  OfflineModelConfig config_;
++
++  rknn_context encoder_ctx_ = 0;
++  rknn_context predictor_ctx_ = 0;
++  rknn_context decoder_ctx_ = 0;
++
++  std::unique_ptr<ContextBlockingQueueRknn> encoder_ctx_queue_;
++  std::unique_ptr<ContextBlockingQueueRknn> predictor_ctx_queue_;
++  std::unique_ptr<ContextBlockingQueueRknn> decoder_ctx_queue_;
++
++  std::vector<rknn_tensor_attr> encoder_input_attrs_;
++  std::vector<rknn_tensor_attr> encoder_output_attrs_;
++
++  std::vector<rknn_tensor_attr> predictor_input_attrs_;
++  std::vector<rknn_tensor_attr> predictor_output_attrs_;
++
++  std::vector<rknn_tensor_attr> decoder_input_attrs_;
++  std::vector<rknn_tensor_attr> decoder_output_attrs_;
++
++  int32_t vocab_size_ = 0;
++  int32_t num_input_frames_ = -1;
++  int32_t encoder_out_dim_ = -1;
++};
++
++OfflineParaformerModelRknn::~OfflineParaformerModelRknn() = default;
++
++OfflineParaformerModelRknn::OfflineParaformerModelRknn(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineParaformerModelRknn::OfflineParaformerModelRknn(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++std::vector<float> OfflineParaformerModelRknn::Run(
++    std::vector<float> features) const {
++  return impl_->Run(std::move(features));
++}
++
++int32_t OfflineParaformerModelRknn::VocabSize() const {
++  return impl_->VocabSize();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineParaformerModelRknn::OfflineParaformerModelRknn(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineParaformerModelRknn::OfflineParaformerModelRknn(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
+new file mode 100644
+index 00000000..17eeb584
+--- /dev/null
++++ b/sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
+@@ -0,0 +1,40 @@
++// sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
++#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++class OfflineParaformerModelRknn {
++ public:
++  ~OfflineParaformerModelRknn();
++
++  explicit OfflineParaformerModelRknn(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineParaformerModelRknn(Manager *mgr, const OfflineModelConfig &config);
++
++  /**
++   * @param features A tensor of shape (num_frames, feature_dim)
++   *                 before applying LFR.
++   * @returns Return a tensor of shape (num_output_frames, vocab_size)
++   */
++  std::vector<float> Run(std::vector<float> features) const;
++
++  int32_t VocabSize() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_PARAFORMER_MODEL_RKNN_H_
+diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+new file mode 100644
+index 00000000..28fc17a6
+--- /dev/null
++++ b/sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
+@@ -0,0 +1,121 @@
++// sherpa-onnx/csrc/rknn/offline-recognizer-paraformer-rknn-impl.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
++#define SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/rknn/offline-paraformer-model-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../offline-recognizer-paraformer-impl.h
++OfflineRecognitionResult Convert(const OfflineParaformerDecoderResult &src,
++                                 const SymbolTable &sym_table);
++
++class OfflineRecognizerParaformerRknnImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerParaformerRknnImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(
++            std::make_unique<OfflineParaformerModelRknn>(config.model_config)) {
++    if (config.decoding_method != "greedy_search") {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerParaformerRknnImpl(Manager *mgr,
++                                      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineParaformerModelRknn>(
++            mgr, config.model_config)) {
++    if (config.decoding_method != "greedy_search") {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i < n; ++i) {
++      DecodeOneStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  void InitFeatConfig() {
++    config_.feat_config.normalize_samples = false;
++    config_.feat_config.window_type = "hamming";
++    config_.feat_config.high_freq = 0;
++    config_.feat_config.snip_edges = true;
++  }
++
++  void DecodeOneStream(OfflineStream *s) const {
++    std::vector<float> f = s->GetFrames();
++
++    std::vector<float> logits = model_->Run(std::move(f));
++    if (logits.empty()) {
++      SHERPA_ONNX_LOGE("No speech detected");
++      return;
++    }
++
++    int32_t vocab_size = model_->VocabSize();
++    int32_t num_tokens = logits.size() / vocab_size;
++
++    int32_t eos_id = symbol_table_["</s>"];
++
++    OfflineParaformerDecoderResult r;
++    const float *p = logits.data();
++    for (int32_t i = 0; i < num_tokens; ++i) {
++      auto max_idx = static_cast<int64_t>(
++          std::distance(p, std::max_element(p, p + vocab_size)));
++
++      if (max_idx == eos_id) {
++        break;
++      }
++      r.tokens.push_back(max_idx);
++      p += vocab_size;
++    }
++
++    auto result = Convert(r, symbol_table_);
++    result.text = ApplyInverseTextNormalization(std::move(result.text));
++    result.text = ApplyHomophoneReplacer(std::move(result.text));
++    s->SetResult(result);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineParaformerModelRknn> model_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_RKNN_OFFLINE_RECOGNIZER_PARAFORMER_RKNN_IMPL_H_
+diff --git a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+index 7236b6b8..8daccec0 100644
+--- a/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
++++ b/sherpa-onnx/csrc/rknn/offline-recognizer-sense-voice-rknn-impl.h
+@@ -111,6 +111,10 @@ class OfflineRecognizerSenseVoiceRknnImpl : public OfflineRecognizerImpl {
+                             : meta_data.without_itn_id;
+ 
+     std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    if (logits.empty()) {
++      return;
++    }
++
+     int32_t num_out_frames = logits.size() / meta_data.vocab_size;
+ 
+     auto result =
+diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+index 0c485c69..805ba7ab 100644
+--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <algorithm>
+ #include <array>
++#include <memory>
+ #include <utility>
+ #include <vector>
+ 
+@@ -56,6 +57,9 @@ class OfflineSenseVoiceModelRknn::Impl {
+   std::vector<float> Run(std::vector<float> features, int32_t language,
+                          int32_t text_norm) {
+     features = ApplyLFR(std::move(features));
++    if (features.empty()) {
++      return {};
++    }
+ 
+     std::vector<rknn_input> inputs(input_attrs_.size());
+ 
+@@ -160,6 +164,11 @@ class OfflineSenseVoiceModelRknn::Impl {
+     int32_t in_feat_dim = 80;
+ 
+     int32_t in_num_frames = in.size() / in_feat_dim;
++
++    if (in_num_frames < lfr_window_size) {
++      return {};
++    }
++
+     int32_t out_num_frames =
+         (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
+ 
+diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
+index ddbc86b1..7b8a86b1 100644
+--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
++++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.h
+@@ -8,7 +8,6 @@
+ #include <utility>
+ #include <vector>
+ 
+-#include "rknn_api.h"  // NOLINT
+ #include "sherpa-onnx/csrc/offline-model-config.h"
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
+ 
+
+commit ae9f8700767e257e9f730c8679b1617d05296981
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Nov 26 12:05:32 2025 +0800
+
+    Avoid calling rknn_dup_context(). (#2828)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index c7873d76..6ad708f8 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -130,6 +130,7 @@ set(sources
+   ten-vad-model-config.cc
+   ten-vad-model.cc
+   text-utils.cc
++  timer.cc
+   transducer-keyword-decoder.cc
+   transpose.cc
+   unbind.cc
+@@ -179,6 +180,7 @@ list(APPEND sources
+ )
+ if(SHERPA_ONNX_ENABLE_RKNN)
+   list(APPEND sources
++    ./rknn/context-blocking-queue-rknn.cc
+     ./rknn/offline-sense-voice-model-rknn.cc
+     ./rknn/online-stream-rknn.cc
+     ./rknn/online-transducer-greedy-search-decoder-rknn.cc
+diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+new file mode 100644
+index 00000000..22e0a6ed
+--- /dev/null
++++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
+@@ -0,0 +1,90 @@
++// sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.cc
++//
++// Copyright      2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
++
++#include <condition_variable>
++#include <mutex>
++#include <queue>
++
++#include "sherpa-onnx/csrc/rknn/macros.h"
++#include "sherpa-onnx/csrc/rknn/utils.h"
++
++namespace sherpa_onnx {
++
++class ContextBlockingQueueRknn::Impl {
++ public:
++  Impl(rknn_context context, int32_t num_threads, int32_t capacity) {
++    for (int32_t i = 0; i < capacity; ++i) {
++      rknn_context bak = 0;
++      auto ret = rknn_dup_context(&context, &bak);
++      SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate context");
++
++      SetCoreMask(bak, num_threads);
++      queue_.push(bak);
++    }
++  }
++  rknn_context Take() {
++    std::unique_lock<std::mutex> lock(mutex_);
++
++    cv_.wait(lock, [&] { return stopped_ || !queue_.empty(); });
++
++    if (stopped_ && queue_.empty()) {
++      return 0;
++    }
++
++    rknn_context ctx = queue_.front();
++    queue_.pop();
++    return ctx;
++  }
++
++  void Put(rknn_context ctx) {
++    {
++      std::lock_guard<std::mutex> lock(mutex_);
++      if (stopped_) {
++        rknn_destroy(ctx);
++        return;
++      }
++      queue_.push(ctx);
++    }
++    cv_.notify_one();
++  }
++
++  ~Impl() {
++    {
++      std::lock_guard<std::mutex> lock(mutex_);
++      stopped_ = true;
++    }
++    cv_.notify_all();
++    Cleanup();
++  }
++
++ private:
++  void Cleanup() {
++    while (!queue_.empty()) {
++      rknn_destroy(queue_.front());
++      queue_.pop();
++    }
++  }
++
++  std::queue<rknn_context> queue_;
++  std::mutex mutex_;
++  std::condition_variable cv_;
++  bool stopped_ = false;
++};
++
++ContextBlockingQueueRknn::ContextBlockingQueueRknn(rknn_context context,
++                                                   int32_t num_threads,
++                                                   int32_t capacity /*= 20*/)
++    : impl_(std::make_unique<Impl>(context, num_threads, capacity)) {}
++
++ContextBlockingQueueRknn::~ContextBlockingQueueRknn() = default;
++
++rknn_context ContextBlockingQueueRknn::Take() { return impl_->Take(); }
++
++void ContextBlockingQueueRknn::Put(rknn_context context) {
++  impl_->Put(context);
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
+new file mode 100644
+index 00000000..c5651e7b
+--- /dev/null
++++ b/sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
+@@ -0,0 +1,29 @@
++// sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h
++//
++// Copyright      2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
++#define SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
++
++#include <memory>
++
++#include "rknn_api.h"  // NOLINT
++
++namespace sherpa_onnx {
++
++class ContextBlockingQueueRknn {
++ public:
++  ContextBlockingQueueRknn(rknn_context context, int32_t num_threads,
++                           int32_t capacity = 10);
++  ~ContextBlockingQueueRknn();
++
++  rknn_context Take();
++  void Put(rknn_context context);
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_RKNN_CONTEXT_BLOCKING_QUEUE_RKNN_H_
+diff --git a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+index f2cbfcaa..0c485c69 100644
+--- a/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/offline-sense-voice-model-rknn.cc
+@@ -19,6 +19,7 @@
+ #endif
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
+ #include "sherpa-onnx/csrc/rknn/macros.h"
+ #include "sherpa-onnx/csrc/rknn/utils.h"
+ 
+@@ -34,18 +35,18 @@ class OfflineSenseVoiceModelRknn::Impl {
+   }
+ 
+   explicit Impl(const OfflineModelConfig &config) : config_(config) {
+-    {
+-      auto buf = ReadFile(config_.sense_voice.model);
+-      Init(buf.data(), buf.size());
+-    }
++    auto buf = ReadFile(config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++
++    PostInit();
+   }
+ 
+   template <typename Manager>
+   Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
+-    {
+-      auto buf = ReadFile(mgr, config_.sense_voice.model);
+-      Init(buf.data(), buf.size());
+-    }
++    auto buf = ReadFile(mgr, config_.sense_voice.model);
++    Init(buf.data(), buf.size());
++
++    PostInit();
+   }
+ 
+   const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
+@@ -81,13 +82,9 @@ class OfflineSenseVoiceModelRknn::Impl {
+     outputs[0].size = out.size() * sizeof(float);
+     outputs[0].buf = reinterpret_cast<void *>(out.data());
+ 
+-    rknn_context ctx = 0;
+-    auto ret = rknn_dup_context(&ctx_, &ctx);
+-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the ctx");
++    rknn_context ctx = ctx_queue_->Take();
+ 
+-    SetCoreMask(ctx, config_.num_threads);
+-
+-    ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
++    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set inputs");
+ 
+     ret = rknn_run(ctx, nullptr);
+@@ -96,7 +93,7 @@ class OfflineSenseVoiceModelRknn::Impl {
+     ret = rknn_outputs_get(ctx, outputs.size(), outputs.data(), nullptr);
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get model output");
+ 
+-    rknn_destroy(ctx);
++    ctx_queue_->Put(ctx);
+ 
+     return out;
+   }
+@@ -195,10 +192,16 @@ class OfflineSenseVoiceModelRknn::Impl {
+     return out;
+   }
+ 
++  void PostInit() {
++    ctx_queue_ =
++        std::make_unique<ContextBlockingQueueRknn>(ctx_, config_.num_threads);
++  }
++
+  private:
+   OfflineModelConfig config_;
+ 
+   rknn_context ctx_ = 0;
++  std::unique_ptr<ContextBlockingQueueRknn> ctx_queue_;
+ 
+   std::vector<rknn_tensor_attr> input_attrs_;
+   std::vector<rknn_tensor_attr> output_attrs_;
+diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
+index cc83a3ef..f488f907 100644
+--- a/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/online-zipformer-ctc-model-rknn.cc
+@@ -21,6 +21,7 @@
+ #endif
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
+ #include "sherpa-onnx/csrc/rknn/macros.h"
+ #include "sherpa-onnx/csrc/rknn/utils.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+@@ -37,18 +38,18 @@ class OnlineZipformerCtcModelRknn::Impl {
+   }
+ 
+   explicit Impl(const OnlineModelConfig &config) : config_(config) {
+-    {
+-      auto buf = ReadFile(config.zipformer2_ctc.model);
+-      Init(buf.data(), buf.size());
+-    }
++    auto buf = ReadFile(config.zipformer2_ctc.model);
++    Init(buf.data(), buf.size());
++
++    PostInit();
+   }
+ 
+   template <typename Manager>
+   Impl(Manager *mgr, const OnlineModelConfig &config) : config_(config) {
+-    {
+-      auto buf = ReadFile(mgr, config.zipformer2_ctc.model);
+-      Init(buf.data(), buf.size());
+-    }
++    auto buf = ReadFile(mgr, config.zipformer2_ctc.model);
++    Init(buf.data(), buf.size());
++
++    PostInit();
+   }
+ 
+   std::vector<std::vector<uint8_t>> GetInitStates() const {
+@@ -140,13 +141,9 @@ class OnlineZipformerCtcModelRknn::Impl {
+       }
+     }
+ 
+-    rknn_context ctx = 0;
+-    auto ret = rknn_dup_context(&ctx_, &ctx);
+-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the ctx");
++    rknn_context ctx = ctx_queue_->Take();
+ 
+-    SetCoreMask(ctx, config_.num_threads);
+-
+-    ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
++    auto ret = rknn_inputs_set(ctx, inputs.size(), inputs.data());
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set inputs");
+ 
+     ret = rknn_run(ctx, nullptr);
+@@ -173,7 +170,7 @@ class OnlineZipformerCtcModelRknn::Impl {
+       }
+     }
+ 
+-    rknn_destroy(ctx);
++    ctx_queue_->Put(ctx);
+ 
+     return {std::move(out), std::move(next_states)};
+   }
+@@ -232,9 +229,15 @@ class OnlineZipformerCtcModelRknn::Impl {
+     }
+   }
+ 
++  void PostInit() {
++    ctx_queue_ =
++        std::make_unique<ContextBlockingQueueRknn>(ctx_, config_.num_threads);
++  }
++
+  private:
+   OnlineModelConfig config_;
+   rknn_context ctx_ = 0;
++  std::unique_ptr<ContextBlockingQueueRknn> ctx_queue_;
+ 
+   std::vector<rknn_tensor_attr> input_attrs_;
+   std::vector<rknn_tensor_attr> output_attrs_;
+diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+index 2a13deb8..e70a3c54 100644
+--- a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+@@ -21,6 +21,7 @@
+ #endif
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/rknn/context-blocking-queue-rknn.h"
+ #include "sherpa-onnx/csrc/rknn/macros.h"
+ #include "sherpa-onnx/csrc/rknn/utils.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+@@ -61,6 +62,8 @@ class OnlineZipformerTransducerModelRknn::Impl {
+       auto buf = ReadFile(config.transducer.joiner);
+       InitJoiner(buf.data(), buf.size());
+     }
++
++    PostInit();
+   }
+ 
+   template <typename Manager>
+@@ -79,6 +82,8 @@ class OnlineZipformerTransducerModelRknn::Impl {
+       auto buf = ReadFile(mgr, config.transducer.joiner);
+       InitJoiner(buf.data(), buf.size());
+     }
++
++    PostInit();
+   }
+ 
+   std::vector<std::vector<uint8_t>> GetEncoderInitStates() const {
+@@ -170,19 +175,13 @@ class OnlineZipformerTransducerModelRknn::Impl {
+       }
+     }
+ 
+-    rknn_context encoder_ctx = 0;
+-
+-    // https://github.com/rockchip-linux/rknpu2/blob/master/runtime/RK3588/Linux/librknn_api/include/rknn_api.h#L444C1-L444C75
+-    // rknn_dup_context(rknn_context* context_in, rknn_context* context_out);
+-    auto ret = rknn_dup_context(&encoder_ctx_, &encoder_ctx);
+-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the encoder ctx");
++    rknn_context encoder_ctx = encoder_ctx_queue_->Take();
+ 
+-    SetCoreMask(encoder_ctx, config_.num_threads);
+-
+-    ret = rknn_inputs_set(encoder_ctx, inputs.size(), inputs.data());
++    auto ret = rknn_inputs_set(encoder_ctx, inputs.size(), inputs.data());
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set encoder inputs");
+ 
+     ret = rknn_run(encoder_ctx, nullptr);
++
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to run encoder");
+ 
+     ret =
+@@ -207,7 +206,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
+       }
+     }
+ 
+-    rknn_destroy(encoder_ctx);
++    encoder_ctx_queue_->Put(encoder_ctx);
+ 
+     return {std::move(encoder_out), std::move(next_states)};
+   }
+@@ -230,13 +229,9 @@ class OnlineZipformerTransducerModelRknn::Impl {
+     output.size = decoder_out.size() * sizeof(float);
+     output.buf = decoder_out.data();
+ 
+-    rknn_context decoder_ctx = 0;
+-    auto ret = rknn_dup_context(&decoder_ctx_, &decoder_ctx);
+-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the decoder ctx");
++    rknn_context decoder_ctx = decoder_ctx_queue_->Take();
+ 
+-    SetCoreMask(decoder_ctx, config_.num_threads);
+-
+-    ret = rknn_inputs_set(decoder_ctx, 1, &input);
++    auto ret = rknn_inputs_set(decoder_ctx, 1, &input);
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set decoder inputs");
+ 
+     ret = rknn_run(decoder_ctx, nullptr);
+@@ -245,7 +240,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
+     ret = rknn_outputs_get(decoder_ctx, 1, &output, nullptr);
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get decoder output");
+ 
+-    rknn_destroy(decoder_ctx);
++    decoder_ctx_queue_->Put(decoder_ctx);
+ 
+     return decoder_out;
+   }
+@@ -273,13 +268,9 @@ class OnlineZipformerTransducerModelRknn::Impl {
+     output.size = joiner_out.size() * sizeof(float);
+     output.buf = joiner_out.data();
+ 
+-    rknn_context joiner_ctx = 0;
+-    auto ret = rknn_dup_context(&joiner_ctx_, &joiner_ctx);
+-    SHERPA_ONNX_RKNN_CHECK(ret, "Failed to duplicate the joiner ctx");
+-
+-    SetCoreMask(joiner_ctx, config_.num_threads);
++    rknn_context joiner_ctx = joiner_ctx_queue_->Take();
+ 
+-    ret = rknn_inputs_set(joiner_ctx, inputs.size(), inputs.data());
++    auto ret = rknn_inputs_set(joiner_ctx, inputs.size(), inputs.data());
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to set joiner inputs");
+ 
+     ret = rknn_run(joiner_ctx, nullptr);
+@@ -288,7 +279,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
+     ret = rknn_outputs_get(joiner_ctx, 1, &output, nullptr);
+     SHERPA_ONNX_RKNN_CHECK(ret, "Failed to get joiner output");
+ 
+-    rknn_destroy(joiner_ctx);
++    joiner_ctx_queue_->Put(joiner_ctx);
+ 
+     return joiner_out;
+   }
+@@ -413,12 +404,25 @@ class OnlineZipformerTransducerModelRknn::Impl {
+     }
+   }
+ 
++  void PostInit() {
++    encoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        encoder_ctx_, config_.num_threads);
++    decoder_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        decoder_ctx_, config_.num_threads);
++    joiner_ctx_queue_ = std::make_unique<ContextBlockingQueueRknn>(
++        joiner_ctx_, config_.num_threads);
++  }
++
+  private:
+   OnlineModelConfig config_;
+   rknn_context encoder_ctx_ = 0;
+   rknn_context decoder_ctx_ = 0;
+   rknn_context joiner_ctx_ = 0;
+ 
++  std::unique_ptr<ContextBlockingQueueRknn> encoder_ctx_queue_;
++  std::unique_ptr<ContextBlockingQueueRknn> decoder_ctx_queue_;
++  std::unique_ptr<ContextBlockingQueueRknn> joiner_ctx_queue_;
++
+   std::vector<rknn_tensor_attr> encoder_input_attrs_;
+   std::vector<rknn_tensor_attr> encoder_output_attrs_;
+ 
+diff --git a/sherpa-onnx/csrc/sherpa-onnx.cc b/sherpa-onnx/csrc/sherpa-onnx.cc
+index 14783fd8..1712e833 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx.cc
+@@ -16,6 +16,7 @@
+ #include "sherpa-onnx/csrc/online-stream.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+ #include "sherpa-onnx/csrc/symbol-table.h"
++#include "sherpa-onnx/csrc/timer.h"
+ #include "sherpa-onnx/csrc/wave-reader.h"
+ 
+ typedef struct {
+@@ -96,7 +97,10 @@ for a list of pre-trained models to download.
+     return -1;
+   }
+ 
++  printf("Start to create recognizer\n");
++  sherpa_onnx::Timer timer;
+   sherpa_onnx::OnlineRecognizer recognizer(config);
++  printf("Recognizer created in %.5f s\n", timer.Elapsed());
+ 
+   std::vector<Stream> ss;
+ 
+diff --git a/sherpa-onnx/csrc/timer.cc b/sherpa-onnx/csrc/timer.cc
+new file mode 100644
+index 00000000..e24a492c
+--- /dev/null
++++ b/sherpa-onnx/csrc/timer.cc
+@@ -0,0 +1,42 @@
++// sherpa-onnx/csrc/timer.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/timer.h"
++
++#include <chrono>
++#include <memory>
++
++namespace sherpa_onnx {
++
++// modified from https://github.com/kaldi-asr/kaldi/blob/master/src/base/timer.h
++class Timer::Impl {
++ public:
++  Impl() { Reset(); }
++
++  using high_resolution_clock = std::chrono::high_resolution_clock;
++
++  void Reset() { begin_ = high_resolution_clock::now(); }
++
++  // Return time in seconds
++  double Elapsed() {
++    auto end = high_resolution_clock::now();
++    auto diff =
++        std::chrono::duration_cast<std::chrono::microseconds>(end - begin_);
++    return diff.count() / 1000000.0;
++  }
++
++ private:
++  high_resolution_clock::time_point begin_;
++};
++
++Timer::Timer() : impl_(std::make_unique<Impl>()) {}
++
++Timer::~Timer() = default;
++
++void Timer::Reset() const { impl_->Reset(); }
++
++// Return time in seconds
++double Timer::Elapsed() const { return impl_->Elapsed(); }
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/timer.h b/sherpa-onnx/csrc/timer.h
+new file mode 100644
+index 00000000..60d87883
+--- /dev/null
++++ b/sherpa-onnx/csrc/timer.h
+@@ -0,0 +1,29 @@
++// sherpa-onnx/csrc/timer.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_TIMER_H_
++#define SHERPA_ONNX_CSRC_TIMER_H_
++
++#include <memory>
++
++namespace sherpa_onnx {
++
++class Timer {
++ public:
++  Timer();
++  ~Timer();
++
++  void Reset() const;
++
++  // Return time in seconds
++  double Elapsed() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_TIMER_H_
+
+commit 0a7ec6edca222c93bbceb31d6ca3e20a262488a8
+Author: Joe Cheng <joe@posit.co>
+Date:   Tue Nov 25 17:21:13 2025 -0800
+
+    Fix segfault when non-wav file is passed to ReadWave (#2821)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index c76d6859..c7873d76 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -717,6 +717,7 @@ if(SHERPA_ONNX_ENABLE_TESTS)
+     transpose-test.cc
+     unbind-test.cc
+     utfcpp-test.cc
++    wave-reader-test.cc
+   )
+   if(SHERPA_ONNX_ENABLE_TTS)
+     list(APPEND sherpa_onnx_test_srcs
+diff --git a/sherpa-onnx/csrc/wave-reader-test.cc b/sherpa-onnx/csrc/wave-reader-test.cc
+new file mode 100644
+index 00000000..286aab06
+--- /dev/null
++++ b/sherpa-onnx/csrc/wave-reader-test.cc
+@@ -0,0 +1,134 @@
++// sherpa-onnx/csrc/wave-reader-test.cc
++//
++// Copyright (c)  2025  Posit Software, PBC
++
++#include "sherpa-onnx/csrc/wave-reader.h"
++
++#include <cstdio>
++#include <fstream>
++#include <string>
++
++#if defined(_WIN32)
++#include <windows.h>
++#else
++#include <unistd.h>
++#endif
++
++#include "gtest/gtest.h"
++
++namespace sherpa_onnx {
++
++// RAII helper class for managing temporary test files
++class TempFile {
++ public:
++  TempFile() : TempFile("") {}
++
++  explicit TempFile(const std::string& suffix) {
++#if defined(_WIN32)
++    char temp_path[MAX_PATH];
++    char temp_file[MAX_PATH];
++    GetTempPathA(MAX_PATH, temp_path);
++    GetTempFileNameA(temp_path, "sot", 0, temp_file);
++    path_ = temp_file;
++    if (!suffix.empty()) {
++      path_ += suffix;
++      std::remove(temp_file);  // Remove the file without suffix
++    }
++#else
++    char temp_template[] = "/tmp/sherpa_onnx_test_XXXXXX";
++    int fd = mkstemp(temp_template);
++    if (fd != -1) {
++      close(fd);
++      path_ = temp_template;
++      if (!suffix.empty()) {
++        path_ += suffix;
++        std::remove(temp_template);  // Remove the file without suffix
++      }
++    }
++#endif
++  }
++
++  ~TempFile() {
++    if (!path_.empty()) {
++      std::remove(path_.c_str());
++    }
++  }
++
++  const char* path() const { return path_.c_str(); }
++
++ private:
++  std::string path_;
++};
++
++TEST(WaveReader, TestNonWavFile) {
++  // Create a temporary file with non-WAV content (e.g., webm-like header)
++  TempFile temp_file(".webm");
++
++  {
++    std::ofstream out(temp_file.path(), std::ios::binary);
++    // Write some content that doesn't start with RIFF
++    // (webm files typically start with EBML header: 0x1a45dfa3)
++    const unsigned char webm_header[] = {
++        0x1a, 0x45, 0xdf, 0xa3,  // EBML header signature (NOT RIFF)
++        0x01, 0x00, 0x00, 0x00,
++        0x00, 0x00, 0x00, 0x1f,
++        0x42, 0x86, 0x81, 0x01,
++        // Add some more bytes to make it look like a real file
++        0x42, 0xf7, 0x81, 0x01,
++        0x42, 0xf2, 0x81, 0x04,
++        'w', 'e', 'b', 'm'
++    };
++    out.write(reinterpret_cast<const char*>(webm_header), sizeof(webm_header));
++  }
++
++  // Test C++ API - should not segfault
++  int32_t sample_rate = -1;
++  bool is_ok = false;
++  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
++
++  EXPECT_FALSE(is_ok);
++  EXPECT_TRUE(samples.empty());
++  EXPECT_EQ(sample_rate, -1);
++}
++
++TEST(WaveReader, TestNonExistentFile) {
++  // Generate a unique path but don't create the file
++  TempFile temp_file(".wav");
++
++  // Test C++ API - should not segfault
++  int32_t sample_rate = -1;
++  bool is_ok = false;
++  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
++
++  EXPECT_FALSE(is_ok);
++  EXPECT_TRUE(samples.empty());
++  EXPECT_EQ(sample_rate, -1);
++}
++
++TEST(WaveReader, TestTruncatedWaveFile) {
++  // Create a temporary file with truncated WAV header
++  TempFile temp_file(".wav");
++
++  {
++    std::ofstream out(temp_file.path(), std::ios::binary);
++    // Write only partial WAV header (less than 44 bytes required)
++    const unsigned char partial_wav[] = {
++        'R', 'I', 'F', 'F',  // chunk_id
++        0x00, 0x00, 0x00, 0x00,  // chunk_size
++        'W', 'A', 'V', 'E'  // format
++        // Missing the rest of the header
++    };
++    out.write(reinterpret_cast<const char*>(partial_wav), sizeof(partial_wav));
++  }
++
++  // Test C++ API - should not segfault
++  int32_t sample_rate = -1;
++  bool is_ok = false;
++  std::vector<float> samples = ReadWave(temp_file.path(), &sample_rate, &is_ok);
++
++  EXPECT_FALSE(is_ok);
++  EXPECT_TRUE(samples.empty());
++  EXPECT_EQ(sample_rate, -1);
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/wave-reader.cc b/sherpa-onnx/csrc/wave-reader.cc
+index 56fa2718..4f2932a0 100644
+--- a/sherpa-onnx/csrc/wave-reader.cc
++++ b/sherpa-onnx/csrc/wave-reader.cc
+@@ -344,6 +344,10 @@ std::vector<float> ReadWave(std::istream &is, int32_t *sampling_rate,
+                             bool *is_ok) {
+   auto samples = ReadWaveImpl(is, sampling_rate, is_ok);
+ 
++  if (!*is_ok || samples.empty()) {
++    return {};
++  }
++
+   if (samples.size() > 1) {
+     SHERPA_ONNX_LOGE(
+         "Warning: %d channels are found. We only use the first channel.\n",
+
+commit ca160f14d6ad7171a43586a8a0644e4e98cba0f8
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 25 18:29:56 2025 +0800
+
+    Add C++ support for Zipformer CTC on Ascend NPU (#2826)
+    
+    This pull request significantly enhances the sherpa-onnx library by integrating C++ support for Zipformer CTC models on Huawei Ascend NPUs. This allows for efficient, hardware-accelerated inference of Zipformer CTC-based automatic speech recognition tasks, providing users with more options for deploying high-performance ASR solutions on Ascend platforms. The changes involve adding new model and recognizer implementations, updating the build system, and extending the recognizer factory to handle the new model type.
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index ef480bdd..c76d6859 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -202,6 +202,7 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+   list(APPEND sources
+     ./ascend/offline-paraformer-model-ascend.cc
+     ./ascend/offline-sense-voice-model-ascend.cc
++    ./ascend/offline-zipformer-ctc-model-ascend.cc
+     ./ascend/utils.cc
+   )
+ endif()
+diff --git a/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h b/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
+new file mode 100644
+index 00000000..b289d0e9
+--- /dev/null
++++ b/sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
+@@ -0,0 +1,130 @@
++// sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
++#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
++
++#include <ios>
++#include <memory>
++#include <sstream>
++#include <string>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../offline-recognizer-ctc-impl.h
++OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
++                                 const SymbolTable &sym_table,
++                                 int32_t frame_shift_ms,
++                                 int32_t subsampling_factor);
++
++class OfflineRecognizerZipformerCtcAscendImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerZipformerCtcAscendImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(std::make_unique<OfflineZipformerCtcModelAscend>(
++            config.model_config)) {
++    Init();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerZipformerCtcAscendImpl(Manager *mgr,
++                                          const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineZipformerCtcModelAscend>(
++            mgr, config.model_config)) {
++    Init();
++  }
++
++  void Init() {
++    if (config_.decoding_method == "greedy_search") {
++      if (!symbol_table_.Contains("<blk>") &&
++          !symbol_table_.Contains("<eps>") &&
++          !symbol_table_.Contains("<blank>") &&
++          config_.model_config.omnilingual.model.empty()) {
++        // for omnilingual asr, its blank id is 0
++        SHERPA_ONNX_LOGE(
++            "We expect that tokens.txt contains "
++            "the symbol <blk> or <eps> or <blank> and its ID.");
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      int32_t blank_id = 0;
++      if (symbol_table_.Contains("<blk>")) {
++        blank_id = symbol_table_["<blk>"];
++      } else if (symbol_table_.Contains("<eps>")) {
++        // for tdnn models of the yesno recipe from icefall
++        blank_id = symbol_table_["<eps>"];
++      } else if (symbol_table_.Contains("<blank>")) {
++        // for Wenet CTC models
++        blank_id = symbol_table_["<blank>"];
++      }
++
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config_.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i != n; ++i) {
++      DecodeStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  // Decode a single stream.
++  // Some models do not support batch size > 1, e.g., WeNet CTC models.
++  void DecodeStream(OfflineStream *s) const {
++    std::vector<float> f = s->GetFrames();
++
++    int32_t vocab_size = model_->VocabSize();
++
++    std::vector<float> log_probs = model_->Run(std::move(f));
++    int32_t num_out_frames = log_probs.size() / vocab_size;
++
++    auto result =
++        decoder_->Decode(log_probs.data(), num_out_frames, vocab_size);
++
++    int32_t frame_shift_ms = 10;
++
++    auto r = Convert(result, symbol_table_, frame_shift_ms,
++                     model_->SubsamplingFactor());
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineZipformerCtcModelAscend> model_;
++  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_ASCEND_IMPL_H_
+diff --git a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
+new file mode 100644
+index 00000000..44547d3b
+--- /dev/null
++++ b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
+@@ -0,0 +1,216 @@
++// sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++// References:
++// https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/83RC1alpha003/API/appdevgapi/aclcppdevg_03_0298.html
++#include "sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h"
++
++#include <algorithm>
++#include <array>
++#include <memory>
++#include <mutex>  // NOLINT
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/ascend/macros.h"
++#include "sherpa-onnx/csrc/ascend/utils.h"
++#include "sherpa-onnx/csrc/file-utils.h"
++
++namespace sherpa_onnx {
++
++class OfflineZipformerCtcModelAscend::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    PreInit();
++    InitModel(config_.zipformer_ctc.model);
++    PostInit();
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    PreInit();
++    {
++      auto buf = ReadFile(mgr, config_.zipformer_ctc.model);
++      InitModel(buf.data(), buf.size());
++    }
++    PostInit();
++  }
++
++  std::vector<float> Run(std::vector<float> features) {
++    // TODO(fangjun): Support multi clients
++    std::lock_guard<std::mutex> lock(mutex_);
++
++    int32_t num_frames = features.size() / feat_dim_;
++
++    if (num_frames != max_num_frames_) {
++      if (num_frames > max_num_frames_) {
++        SHERPA_ONNX_LOGE(
++            "Number of input frames %d is too large. Truncate it to %d frames.",
++            num_frames, max_num_frames_);
++
++        SHERPA_ONNX_LOGE(
++            "Recognition result may be truncated/incomplete. Please select a "
++            "model accepting longer audios.");
++      }
++
++      features.resize(max_num_frames_ * feat_dim_);
++
++      num_frames = max_num_frames_;
++    }
++
++    aclError ret =
++        aclrtMemcpy(*x_ptr_, features.size() * sizeof(float), features.data(),
++                    features.size() * sizeof(float), ACL_MEMCPY_HOST_TO_DEVICE);
++    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
++
++    AclMdlDataset input_dataset;
++    AclDataBuffer x_buf(*x_ptr_, features.size() * sizeof(float));
++    input_dataset.AddBuffer(x_buf);
++
++    AclMdlDataset output_dataset;
++
++    AclDataBuffer logits_buf(*log_probs_ptr_,
++                             num_output_frames_ * vocab_size_ * sizeof(float));
++    output_dataset.AddBuffer(logits_buf);
++
++    ret = aclmdlExecute(*model_, input_dataset, output_dataset);
++    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclmdlExecute");
++
++    std::vector<float> log_probs(num_output_frames_ * vocab_size_);
++    ret = aclrtMemcpy(
++        log_probs.data(), num_output_frames_ * vocab_size_ * sizeof(float),
++        *log_probs_ptr_, num_output_frames_ * vocab_size_ * sizeof(float),
++        ACL_MEMCPY_DEVICE_TO_HOST);
++    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtMemcpy");
++
++    return log_probs;
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++
++  int32_t SubsamplingFactor() const { return subsampling_factor_; }
++
++ private:
++  void InitModel(const std::string &filename) {
++    model_ = std::make_unique<AclModel>(filename);
++    if (config_.debug) {
++      auto s = model_->GetInfo();
++      SHERPA_ONNX_LOGE("%s", s.c_str());
++    }
++  }
++
++  void InitModel(void *data, size_t size) {
++    model_ = std::make_unique<AclModel>(data, size);
++    if (config_.debug) {
++      auto s = model_->GetInfo();
++      SHERPA_ONNX_LOGE("%s", s.c_str());
++    }
++  }
++
++  void PreInit() {
++    int32_t device_id = 0;
++    aclError ret = aclrtSetDevice(device_id);
++    SHERPA_ONNX_ASCEND_CHECK(
++        ret, "Failed to call aclrtSetDevice with device id: %d", device_id);
++
++    context_ = std::make_unique<AclContext>(device_id);
++
++    ret = aclrtSetCurrentContext(*context_);
++    SHERPA_ONNX_ASCEND_CHECK(ret, "Failed to call aclrtSetCurrentContext");
++  }
++
++  void PostInit() {
++    auto in_shape = model_->GetInputShapes()[0];
++
++    max_num_frames_ = in_shape[1];
++    feat_dim_ = in_shape[2];
++
++    auto out_shape = model_->GetOutputShapes()[0];
++
++    num_output_frames_ = out_shape[1];
++    vocab_size_ = out_shape[2];
++
++    subsampling_factor_ = max_num_frames_ / out_shape[1];
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("max_num_frames: %d", max_num_frames_);
++      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
++      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
++      SHERPA_ONNX_LOGE("subsampling_factor: %d", subsampling_factor_);
++    }
++
++    Preallocate();
++  }
++
++  void Preallocate() {
++    x_ptr_ = std::make_unique<AclDevicePtr>(max_num_frames_ * feat_dim_ *
++                                            sizeof(float));
++
++    log_probs_ptr_ = std::make_unique<AclDevicePtr>(
++        num_output_frames_ * vocab_size_ * sizeof(float));
++  }
++
++ private:
++  std::mutex mutex_;
++  Acl acl_;
++
++  std::unique_ptr<AclContext> context_;
++
++  OfflineModelConfig config_;
++
++  std::unique_ptr<AclModel> model_;
++  int32_t vocab_size_ = 0;
++  int32_t max_num_frames_ = 0;
++  int32_t num_output_frames_ = 0;
++  int32_t feat_dim_ = 0;
++  int32_t subsampling_factor_ = 0;
++
++  std::unique_ptr<AclDevicePtr> x_ptr_;
++  std::unique_ptr<AclDevicePtr> log_probs_ptr_;
++};
++
++OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++OfflineZipformerCtcModelAscend::~OfflineZipformerCtcModelAscend() = default;
++
++std::vector<float> OfflineZipformerCtcModelAscend::Run(
++    std::vector<float> features) const {
++  return impl_->Run(std::move(features));
++}
++
++int32_t OfflineZipformerCtcModelAscend::VocabSize() const {
++  return impl_->VocabSize();
++}
++
++int32_t OfflineZipformerCtcModelAscend::SubsamplingFactor() const {
++  return impl_->SubsamplingFactor();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineZipformerCtcModelAscend::OfflineZipformerCtcModelAscend(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
+new file mode 100644
+index 00000000..0c643f38
+--- /dev/null
++++ b/sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
+@@ -0,0 +1,40 @@
++// sherpa-onnx/csrc/ascend/offline-zipformer-ctc-model-ascend.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
++#define SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
++
++#include <memory>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++class OfflineZipformerCtcModelAscend {
++ public:
++  ~OfflineZipformerCtcModelAscend();
++
++  explicit OfflineZipformerCtcModelAscend(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineZipformerCtcModelAscend(Manager *mgr,
++                                 const OfflineModelConfig &config);
++
++  /**
++   * @param features A tensor of shape (num_frames, feature_dim)
++   * @returns Return a tensor of shape (num_output_frames, vocab_size)
++   */
++  std::vector<float> Run(std::vector<float> features) const;
++
++  int32_t VocabSize() const;
++  int32_t SubsamplingFactor() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_ASCEND_OFFLINE_ZIPFORMER_CTC_MODEL_ASCEND_H_
+diff --git a/sherpa-onnx/csrc/ascend/utils.cc b/sherpa-onnx/csrc/ascend/utils.cc
+index 2977c769..f424d1f2 100644
+--- a/sherpa-onnx/csrc/ascend/utils.cc
++++ b/sherpa-onnx/csrc/ascend/utils.cc
+@@ -50,8 +50,10 @@ static const char *AclDataTypeToString(aclDataType data_type) {
+       return "ACL_COMPLEX128";
+     case ACL_BF16:
+       return "ACL_BF16";
++#if defined(ACL_INT4)
+     case ACL_INT4:
+       return "ACL_INT4";
++#endif
+     case ACL_UINT1:
+       return "ACL_UINT1";
+     case ACL_COMPLEX32:
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 1046f9e1..318be0e0 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -43,6 +43,7 @@
+ #if SHERPA_ONNX_ENABLE_ASCEND_NPU
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-paraformer-ascend-impl.h"
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
++#include "sherpa-onnx/csrc/ascend/offline-recognizer-zipformer-ctc-ascend-impl.h"
+ #endif
+ 
+ #if SHERPA_ONNX_ENABLE_QNN
+@@ -81,10 +82,12 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       return std::make_unique<OfflineRecognizerSenseVoiceAscendImpl>(config);
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(config);
++    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++      return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice and Paraformer models are currently supported "
+-          "by Ascend NPU for non-streaming ASR.");
++          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
++          "supported by Ascend NPU for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+     }
+@@ -115,7 +118,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     SHERPA_ONNX_LOGE(
+         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
+         "you want to use qnn. See also "
+-        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
++        "https://k2-fsa.github.io/sherpa/onnx/qnn/build.html");
+     SHERPA_ONNX_EXIT(-1);
+     return nullptr;
+ #endif
+@@ -339,10 +342,13 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     } else if (!config.model_config.paraformer.model.empty()) {
+       return std::make_unique<OfflineRecognizerParaformerAscendImpl>(mgr,
+                                                                      config);
++    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++      return std::make_unique<OfflineRecognizerZipformerCtcAscendImpl>(mgr,
++                                                                       config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice and Paraformer models are currently supported "
+-          "by Ascend NPU for non-streaming ASR. Fallback to CPU");
++          "Only SenseVoice, Paraformer, and Zipformer CTC models are currently "
++          "supported by Ascend NPU for non-streaming ASR. Fallback to CPU");
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -372,7 +378,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     SHERPA_ONNX_LOGE(
+         "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
+         "you want to use qnn. See also "
+-        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
++        "https://k2-fsa.github.io/sherpa/onnx/qnn/build.html");
+     SHERPA_ONNX_EXIT(-1);
+     return nullptr;
+ #endif
+
+commit 589877c95ff224a31011577a6d59781c5aca5820
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 25 15:12:43 2025 +0800
+
+    Refactor scripts for exporting models to Ascend NPU. (#2825)
+    
+    This pull request refactors and expands the model export infrastructure for Ascend NPUs by introducing new configuration generation capabilities for Paraformer and SenseVoice models. It also enhances existing scripts with improved documentation for Docker image sources, streamlining the process of preparing diverse models for deployment on Ascend hardware across various configurations.
+
+diff --git a/.github/scripts/export-ascend/__init__.py b/.github/scripts/export-ascend/__init__.py
+new file mode 100644
+index 00000000..e69de29b
+diff --git a/.github/scripts/export-ascend/generate_paraformer.py b/.github/scripts/export-ascend/generate_paraformer.py
+new file mode 100755
+index 00000000..fa22d64b
+--- /dev/null
++++ b/.github/scripts/export-ascend/generate_paraformer.py
+@@ -0,0 +1,46 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import itertools
++import json
++from dataclasses import asdict, dataclass
++
++from generate_zipformer_ctc_20250703 import get_image
++
++
++@dataclass
++class Config:
++    # 7.0, 8.0, 8.2
++    cann: str
++
++    # 910B, 910B2, 910B3, 310P3
++    soc_version: str
++
++    # FunASR, WSChuan-ASR
++    framework: str
++
++    image: str = ""
++
++    def __post_init__(self):
++        self.image = get_image(self.cann, soc_version=self.soc_version)
++
++
++def main():
++    cann_version = ["7.0", "8.0", "8.2"]
++    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    framework_list = ["FunASR", "WSChuan-ASR"]
++
++    configs = [
++        Config(cann=cann, soc_version=soc, framework=framework)
++        for cann, soc, framework in itertools.product(
++            cann_version, soc_version, framework_list
++        )
++    ]
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/scripts/export-ascend/generate_sense_voice.py b/.github/scripts/export-ascend/generate_sense_voice.py
+new file mode 100755
+index 00000000..f1ecbf39
+--- /dev/null
++++ b/.github/scripts/export-ascend/generate_sense_voice.py
+@@ -0,0 +1,46 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import itertools
++import json
++from dataclasses import asdict, dataclass
++
++from generate_zipformer_ctc_20250703 import get_image
++
++
++@dataclass
++class Config:
++    # 7.0, 8.0, 8.2
++    cann: str
++
++    # 910B, 910B2, 910B3, 310P3
++    soc_version: str
++
++    # FunASR, WSYue-ASR
++    framework: str
++
++    image: str = ""
++
++    def __post_init__(self):
++        self.image = get_image(self.cann, soc_version=self.soc_version)
++
++
++def main():
++    cann_version = ["7.0", "8.0", "8.2"]
++    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    framework_list = ["FunASR", "WSYue-ASR"]
++
++    configs = [
++        Config(cann=cann, soc_version=soc, framework=framework)
++        for cann, soc, framework in itertools.product(
++            cann_version, soc_version, framework_list
++        )
++    ]
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+index bea3127c..511bfee1 100755
+--- a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
++++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+@@ -6,6 +6,12 @@ import json
+ from dataclasses import asdict, dataclass
+ 
+ 
++# image: ascendai/cann:latest
++# image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
++# see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
++# see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
++# and
++# https://quay.io/repository/ascend/cann?tab=tags
+ def get_image(cann: str, soc_version: str):
+     cann2image_910 = {
+         "7.0": "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8",
+diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+index 2fbd89f1..63587194 100644
+--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
++++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - ascend-910b3
++      - refactor-ascend-export-script
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -11,7 +11,30 @@ concurrency:
+   cancel-in-progress: true
+ 
+ jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-ascend/generate_sense_voice.py
++          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
+   export-paraformer-to-rknn:
++    needs: generate_build_matrix
+     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+     name: ${{ matrix.framework }} ${{ matrix.soc_version }} ${{ matrix.cann }}
+     runs-on: ubuntu-latest
+@@ -19,99 +42,9 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        include:
+-          # ===== Ascend 910B =====
+-          - soc_version: "910B"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSChuan-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSChuan-ASR"
+-            cann: "8.2"
+-
+-          # ===== Ascend 910B2 =====
+-          - soc_version: "910B2"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSChuan-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSChuan-ASR"
+-            cann: "8.2"
+-
+-          # ===== Ascend 910B3 =====
+-          - soc_version: "910B3"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSChuan-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSChuan-ASR"
+-            cann: "8.2"
+-
+-          # ===== Ascend 310 =====
+-          - soc_version: "310P3"
+-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "310P3"
+-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
+-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+-            framework: "WSChuan-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "310P3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "310P3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
+-            framework: "WSChuan-ASR"
+-            cann: "8.2"
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+ 
+     container:
+-      # image: ascendai/cann:latest
+-      # image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
+-      # see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
+-      # see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
+-      # and
+-      # https://quay.io/repository/ascend/cann?tab=tags
+       image: ${{ matrix.image }}
+ 
+     steps:
+@@ -139,7 +72,6 @@ jobs:
+ 
+           find /usr/local/Ascend -name "libascend*.so" 2>/dev/null
+ 
+-
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
+diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+index f71752b0..0be4e277 100644
+--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
++++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - cann-7.0
++      - refactor-ascend-export-script
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -11,149 +11,39 @@ concurrency:
+   cancel-in-progress: true
+ 
+ jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-ascend/generate_sense_voice.py
++          MATRIX=$(python3 .github/scripts/export-ascend/generate_sense_voice.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
+   export-sense-voice-to-ascend-npu:
++    needs: generate_build_matrix
+     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+     name: ${{ matrix.framework }} ${{ matrix.soc_version }} ${{ matrix.cann }}
+     runs-on: ubuntu-latest
+-
+     strategy:
+       fail-fast: false
+       matrix:
+-        include:
+-          # ===== Ascend 910B =====
+-          - soc_version: "910B"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "FunASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "WSYue-ASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSYue-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSYue-ASR"
+-            cann: "8.2"
+-
+-          # ===== Ascend 910B2 =====
+-          - soc_version: "910B2"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "FunASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B2"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "WSYue-ASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSYue-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B2"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSYue-ASR"
+-            cann: "8.2"
+-
+-          # ===== Ascend 910B3 =====
+-          - soc_version: "910B3"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "FunASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B3"
+-            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
+-            framework: "WSYue-ASR"
+-            cann: "7.0"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+-            framework: "WSYue-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "910B3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
+-            framework: "WSYue-ASR"
+-            cann: "8.2"
+-
+-
+-          # ===== Ascend 310 =====
+-          - soc_version: "310P3"
+-            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
+-            framework: "FunASR"
+-            cann: "7.0"
+-
+-          - soc_version: "310P3"
+-            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
+-            framework: "WSYue-ASR"
+-            cann: "7.0"
+-
+-          - soc_version: "310P3"
+-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
+-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.0"
+-
+-          - soc_version: "310P3"
+-            # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
+-            image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+-            framework: "WSYue-ASR"
+-            cann: "8.0"
+-
+-          - soc_version: "310P3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
+-            framework: "FunASR"
+-            cann: "8.2"
+-
+-          - soc_version: "310P3"
+-            image: "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2"
+-            framework: "WSYue-ASR"
+-            cann: "8.2"
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
+ 
+     container:
+-      # image: ascendai/cann:latest
+-      # image: ascendai/cann:8.1.rc1-910b-ubuntu22.04-py3.10
+-      # see https://hub.docker.com/r/gpustack/ascendai-cann/tags?name=8.0
+-      # see https://hub.docker.com/r/gpustack/devel-ascendai-cann/tags?name=310p
+-      # and
+-      # https://quay.io/repository/ascend/cann?tab=tags
+       image: ${{ matrix.image }}
+ 
+     steps:
+
+commit 10c6bd421640e254622cccc215ae9cff33daa35e
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 25 14:56:35 2025 +0800
+
+    Export zipformer ctc models to Ascend NPU (#2824)
+    
+    This pull request significantly enhances the support for Zipformer CTC models on Ascend NPU. It introduces automated configuration generation for model exports and provides robust testing utilities for both ONNX and Ascend-specific .om model formats. These additions streamline the deployment and validation process of speech recognition models on Ascend hardware, ensuring compatibility and performance across different NPU environments.
+
+diff --git a/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+new file mode 100755
+index 00000000..bea3127c
+--- /dev/null
++++ b/.github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
+@@ -0,0 +1,63 @@
++#!/usr/bin/env python3
++# Copyright    2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import itertools
++import json
++from dataclasses import asdict, dataclass
++
++
++def get_image(cann: str, soc_version: str):
++    cann2image_910 = {
++        "7.0": "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8",
++        "8.0": "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9",
++        "8.2": "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2",
++    }
++
++    cann2image_310 = {
++        "7.0": "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9",
++        "8.0": "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2",
++        "8.2": "gpustack/devel-ascendai-cann:8.2.rc1-310p-ubuntu20.04-v2",
++    }
++    if "910" in soc_version:
++        return cann2image_910[cann]
++    elif "310" in soc_version:
++        return cann2image_310[cann]
++    else:
++        raise ValueError(f"Unsupported soc_version {soc_version}")
++
++
++@dataclass
++class Config:
++    # 7.0, 8.0, 8.2
++    cann: str
++
++    # 910B, 910B2, 910B3, 310P3
++    soc_version: str
++
++    num_seconds: str
++
++    image: str = ""
++
++    def __post_init__(self):
++        self.image = get_image(self.cann, soc_version=self.soc_version)
++
++
++def main():
++    cann_version = ["7.0", "8.0", "8.2"]
++    soc_version = ["910B", "910B2", "910B3", "310P3"]
++    input_in_seconds = ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++
++    configs = [
++        Config(cann=cann, soc_version=soc, num_seconds=sec)
++        for cann, soc, sec in itertools.product(
++            cann_version, soc_version, input_in_seconds
++        )
++    ]
++
++    ans = [asdict(c) for c in configs]
++
++    print(json.dumps({"include": ans}))
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+new file mode 100644
+index 00000000..4169f7c0
+--- /dev/null
++++ b/.github/workflows/export-zipformer-ctc-to-ascend-20250703.yaml
+@@ -0,0 +1,177 @@
++name: export-zipformer-ctc-to-ascend-npu-20250703
++
++on:
++  push:
++    branches:
++      - export-zipformer-ctc-ascend
++  workflow_dispatch:
++
++concurrency:
++  group: export-zipformer-ctc-to-ascend-npu-20250703-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  generate_build_matrix:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    # see https://github.com/pytorch/pytorch/pull/50633
++    runs-on: ubuntu-latest
++    outputs:
++      matrix: ${{ steps.set-matrix.outputs.matrix }}
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Generating build matrix
++        id: set-matrix
++        run: |
++          # outputting for debugging purposes
++          python3 .github/scripts/export-ascend/generate_zipformer_ctc_20250703.py
++          MATRIX=$(python3 .github/scripts/export-ascend/generate_zipformer_ctc_20250703.py)
++
++          # deprecated
++          # echo "::set-output name=matrix::${MATRIX}"
++          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
++
++  export-zipformer-ctc-to-ascend-npu-20250703:
++    needs: generate_build_matrix
++    name: ${{ matrix.soc_version }} ${{ matrix.cann }} ${{ matrix.num_seconds }}
++    runs-on: ubuntu-latest
++    strategy:
++      fail-fast: false
++      matrix:
++        ${{ fromJson(needs.generate_build_matrix.outputs.matrix) }}
++
++    container:
++      image: ${{ matrix.image }}
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python 3.8
++        uses: actions/setup-python@v5
++        with:
++          python-version: "3.8"
++
++      - name: Show Python
++        shell: bash
++        run: |
++          python3 --version
++          which python3
++
++      - name: Install curl
++        shell: bash
++        run: apt-get update && apt-get install -y curl bzip2
++
++      - name: Verify environment
++        shell: bash
++        run: |
++          ls -lh /usr/local/Ascend/ascend-toolkit/set_env.sh
++
++          find /usr/local/Ascend -name "libascend*.so" 2>/dev/null
++
++
++          source /usr/local/Ascend/ascend-toolkit/set_env.sh
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
++
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
++          echo "CANN environment:"
++          which atc || echo "atc not found"
++          atc --help
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          python3 -m pip install "numpy<2" \
++                  onnx==1.17.0 \
++                  torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
++                  attrs psutil scipy decorator cloudpickle ml-dtypes tornado \
++                  sentencepiece \
++                  pyyaml
++
++      - name: Export ${{ matrix.num_seconds }}
++        shell: bash
++        run: |
++          mkdir tmp
++          cd tmp
++
++          t=${{ matrix.num_seconds }}
++          num_frames=$(($t*100))
++
++          echo "num_frames: $num_frames"
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
++          chmod +x generate_test_data.py
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
++
++
++          source /usr/local/Ascend/ascend-toolkit/set_env.sh
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
++
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
++          soc_version=${{ matrix.soc_version }}
++          cann=${{ matrix.cann }}
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/model-$t-seconds.onnx
++          mv model-$t-seconds.onnx model.onnx
++
++          atc --model=./model.onnx \
++            --framework=5 \
++            --host_env_os=linux \
++            --host_env_cpu=aarch64 \
++            --output=model \
++            --input_format=ND \
++            --input_shape="x:1,${num_frames},80" \
++            --soc_version="Ascend${soc_version}"
++
++          ls -lh *.om
++
++          echo "collect results"
++          d=sherpa-onnx-ascend-${soc_version}-cann-${cann}-$t-seconds-zipformer-ctc-zh-2025-07-03
++
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          cp -v model_linux_aarch64.om $d/model.om || cp -v model.om $d/model.om
++          cp -v tokens.txt $d
++          cp -v ../scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py $d
++          cp -v ../scripts/zipformer-ctc/ascend/2025-07-03/test_om.py $d
++          cp -v *.wav $d/test_wavs
++          ls -lh $d
++          tar cjfv $d.tar.bz2 $d
++          ls -lh *.tar.bz2
++          rm -rf $d
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-ascend
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          tag: asr-models-ascend
+diff --git a/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py b/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py
+new file mode 100755
+index 00000000..8cf52247
+--- /dev/null
++++ b/scripts/zipformer-ctc/ascend/2025-07-03/onnx_test.py
+@@ -0,0 +1,173 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++from typing import Tuple
++
++import kaldi_native_fbank as knf
++import numpy as np
++import onnxruntime as ort
++import soundfile as sf
++
++BPE_UNK = chr(8263)
++PRINTABLE_BASE_CHARS = (
++    list(range(256, 287 + 1))
++    + list(range(32, 126 + 1))
++    + list(range(288, 305 + 1))
++    + list(range(308, 318 + 1))
++    + list(range(321, 328 + 1))
++    + list(range(330, 382 + 1))
++    + list(range(384, 422 + 1))
++)
++
++
++BYTE_TO_BCHAR = {b: chr(PRINTABLE_BASE_CHARS[b]) for b in range(256)}
++BCHAR_TO_BYTE = {bc: b for b, bc in BYTE_TO_BCHAR.items()}
++BCHAR_TO_BYTE[BPE_UNK] = 32  # map unk to space
++
++
++def load_tokens(filename):
++    ans = dict()
++    i = 0
++    with open(filename, encoding="utf-8") as f:
++        for line in f:
++            ans[i] = line.strip().split()[0]
++            i += 1
++    return ans
++
++
++def load_audio(filename: str) -> Tuple[np.ndarray, int]:
++    data, sample_rate = sf.read(
++        filename,
++        always_2d=True,
++        dtype="float32",
++    )
++    data = data[:, 0]  # use only the first channel
++
++    if sample_rate != 16000:
++        import librosa
++
++        data = librosa.resample(data, orig_sr=sample_rate, target_sr=16000)
++        sample_rate = 16000
++
++    samples = np.ascontiguousarray(data)
++    return samples, sample_rate
++
++
++def compute_feat(
++    samples: np.ndarray,
++    sample_rate: int,
++    max_len: int,
++):
++    opts = knf.FbankOptions()
++    opts.frame_opts.dither = 0
++    opts.frame_opts.snip_edges = False
++    opts.frame_opts.window_type = "povey"
++    opts.frame_opts.samp_freq = sample_rate
++    opts.mel_opts.num_bins = 80
++
++    online_fbank = knf.OnlineFbank(opts)
++    online_fbank.accept_waveform(sample_rate, samples.tolist())
++    online_fbank.input_finished()
++
++    features = np.stack(
++        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
++    )
++
++    if features.shape[0] > max_len:
++        features = features[:max_len]
++    elif features.shape[0] < max_len:
++        features = np.pad(
++            features,
++            ((0, max_len - features.shape[0]), (0, 0)),
++            mode="constant",
++            constant_values=0,
++        )
++
++    features = np.ascontiguousarray(features)
++
++    assert features.data.contiguous is True
++    assert features.dtype == np.float32, features.dtype
++
++    return features
++
++
++class OnnxModel:
++    def __init__(self, filename):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = 1
++        session_opts.intra_op_num_threads = 1
++
++        self.session_opts = session_opts
++
++        self.model = ort.InferenceSession(
++            filename,
++            sess_options=self.session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++        shape = self.model.get_inputs()[0].shape
++        self.max_len = shape[1]
++
++        for i in self.model.get_inputs():
++            print(i)
++
++        print("-----")
++
++        for i in self.model.get_outputs():
++            print(i)
++
++    def __call__(self, x):
++        log_probs = self.model.run(
++            [
++                self.model.get_outputs()[0].name,
++            ],
++            {self.model.get_inputs()[0].name: x[None]},
++        )[0]
++
++        return log_probs
++
++
++def main():
++    wave = "./0.wav"
++    wave = "./1.wav"
++    samples, sample_rate = load_audio(wave)
++
++    model = OnnxModel("./model.onnx")
++
++    features = compute_feat(
++        samples=samples,
++        sample_rate=sample_rate,
++        max_len=model.max_len,
++    )
++    print("features", features.shape)
++
++    log_probs = model(features)
++
++    idx = log_probs[0].argmax(axis=-1)
++    print("idx", idx)
++    print(len(idx))
++    prev = -1
++    ids = []
++    for i in idx:
++        if i != prev:
++            ids.append(i)
++        prev = i
++    ids = [i for i in ids if i != 0]
++    print(ids)
++
++    tokens = load_tokens("./tokens.txt")
++    text = "".join([tokens[i] for i in ids])
++
++    s = b""
++    for t in text:
++        if t == "":
++            continue
++        elif t in BCHAR_TO_BYTE:
++            s += bytes([BCHAR_TO_BYTE[t]])
++        else:
++            print("skip OOV", t)
++
++    print(s.decode())
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py b/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py
+new file mode 100755
+index 00000000..1d4ebee2
+--- /dev/null
++++ b/scripts/zipformer-ctc/ascend/2025-07-03/test_om.py
+@@ -0,0 +1,73 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++from ais_bench.infer.interface import InferSession
++
++from onnx_test import BCHAR_TO_BYTE, compute_feat, load_audio, load_tokens
++
++
++class OmModel:
++    def __init__(self):
++        self.model = InferSession(device_id=0, model_path="./model.om", debug=False)
++
++        self.max_len = self.model.get_inputs()[0].shape[1]
++        print("---model---")
++        for i in self.model.get_inputs():
++            print(i.name, i.datatype, i.shape)
++
++        print("-----")
++
++        for i in self.model.get_outputs():
++            print(i.name, i.datatype, i.shape)
++
++    def __call__(self, x):
++        """
++        Args:
++          x: (N, T, C)
++        Returns:
++          log_probs: (N, T, vocab_size)
++        """
++        return self.model.infer([x], mode="static", custom_sizes=10000000)[0]
++
++
++def main():
++    samples, sample_rate = load_audio("./test_wavs/0.wav")
++    model = OmModel()
++
++    features = compute_feat(
++        samples=samples, sample_rate=sample_rate, max_len=model.max_len
++    )
++    print("features.shape", features.shape)
++
++    log_probs = model(x=features[None])
++    print("log_probs.shape", log_probs.shape, type(log_probs))
++
++    idx = log_probs[0].argmax(axis=-1)
++    print("idx", idx)
++    print(len(idx))
++    prev = -1
++    ids = []
++    for i in idx:
++        if i != prev:
++            ids.append(i)
++        prev = i
++    ids = [i for i in ids if i != 0]
++    print(ids)
++
++    tokens = load_tokens("./tokens.txt")
++    text = "".join([tokens[i] for i in ids])
++
++    s = b""
++    for t in text:
++        if t == "":
++            continue
++        elif t in BCHAR_TO_BYTE:
++            s += bytes([BCHAR_TO_BYTE[t]])
++        else:
++            print("skip OOV", t)
++
++    print(s.decode())
++
++
++if __name__ == "__main__":
++    main()
+
+commit 3c79bda15625e5756eac648e3fde641522e004c1
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 25 14:50:51 2025 +0800
+
+    Fix warnings for initializing tts lexicon. (#2823)
+    
+    This pull request focuses on improving the robustness of the TTS lexicon initialization process by eliminating a recurring warning. The change ensures that the lexicon parser correctly handles and ignores empty or whitespace-only lines, leading to cleaner log output and more reliable lexicon loading.
+
+diff --git a/sherpa-onnx/csrc/character-lexicon.cc b/sherpa-onnx/csrc/character-lexicon.cc
+index 60d48b55..483828bb 100644
+--- a/sherpa-onnx/csrc/character-lexicon.cc
++++ b/sherpa-onnx/csrc/character-lexicon.cc
+@@ -266,6 +266,10 @@ class CharacterLexicon::Impl {
+ 
+     while (std::getline(is, line)) {
+       ++line_num;
++      if (line.find_first_not_of(" \t\n\v\f\r") == std::string::npos) {
++        // Line is empty or only spaces/tabs, skip it
++        continue;
++      }
+ 
+       std::istringstream iss(line);
+ 
+
+commit 98713062411d3921482232c3cf363ad46ad5048b
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 25 14:34:09 2025 +0800
+
+    Limit symbol visibility in the shared libraries (#2822)
+    
+    This pull request refines the build process for the sherpa-onnx shared libraries by introducing explicit symbol visibility control. This change aims to improve library encapsulation, prevent symbol conflicts with other loaded libraries, and potentially optimize library size and load performance by only exposing necessary public API symbols.
+
+diff --git a/sherpa-onnx/c-api/CMakeLists.txt b/sherpa-onnx/c-api/CMakeLists.txt
+index 03909373..cbb81dfb 100644
+--- a/sherpa-onnx/c-api/CMakeLists.txt
++++ b/sherpa-onnx/c-api/CMakeLists.txt
+@@ -12,6 +12,16 @@ add_library(sherpa-onnx-cxx-api cxx-api.cc)
+ target_link_libraries(sherpa-onnx-cxx-api sherpa-onnx-c-api)
+ target_include_directories(sherpa-onnx-cxx-api PUBLIC ${PROJECT_SOURCE_DIR})
+ 
++if(ANDROID OR (UNIX AND NOT APPLE))
++  set_target_properties(sherpa-onnx-c-api PROPERTIES
++    LINK_FLAGS "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols-c.lds"
++  )
++elseif(APPLE)
++  set_target_properties(sherpa-onnx-c-api PROPERTIES
++    LINK_FLAGS "-Wl,-exported_symbols_list,${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols-c.exp"
++  )
++endif()
++
+ install(
+   TARGETS
+     sherpa-onnx-c-api
+diff --git a/sherpa-onnx/c-api/generate.sh b/sherpa-onnx/c-api/generate.sh
+new file mode 100755
+index 00000000..6ba93bbd
+--- /dev/null
++++ b/sherpa-onnx/c-api/generate.sh
+@@ -0,0 +1,5 @@
++#!/usr/bin/env bash
++set -ex
++
++nm -g ../../build/lib/libsherpa-onnx-c-api.dylib | awk '$2=="T" && $3 ~ /^_Sherpa/ {print $3}' | sort  > ./sherpa-onnx-symbols-c.exp
++
+diff --git a/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp
+new file mode 100644
+index 00000000..aedffcde
+--- /dev/null
++++ b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.exp
+@@ -0,0 +1,149 @@
++_SherpaOfflinePunctuationAddPunct
++_SherpaOfflinePunctuationFreeText
++_SherpaOnnxAcceptWaveformOffline
++_SherpaOnnxAudioTaggingCompute
++_SherpaOnnxAudioTaggingCreateOfflineStream
++_SherpaOnnxAudioTaggingFreeResults
++_SherpaOnnxCircularBufferFree
++_SherpaOnnxCircularBufferGet
++_SherpaOnnxCircularBufferHead
++_SherpaOnnxCircularBufferPop
++_SherpaOnnxCircularBufferPush
++_SherpaOnnxCircularBufferReset
++_SherpaOnnxCircularBufferSize
++_SherpaOnnxCreateAudioTagging
++_SherpaOnnxCreateCircularBuffer
++_SherpaOnnxCreateDisplay
++_SherpaOnnxCreateKeywordSpotter
++_SherpaOnnxCreateKeywordStream
++_SherpaOnnxCreateKeywordStreamWithKeywords
++_SherpaOnnxCreateLinearResampler
++_SherpaOnnxCreateOfflinePunctuation
++_SherpaOnnxCreateOfflineRecognizer
++_SherpaOnnxCreateOfflineSpeakerDiarization
++_SherpaOnnxCreateOfflineSpeechDenoiser
++_SherpaOnnxCreateOfflineStream
++_SherpaOnnxCreateOfflineStreamWithHotwords
++_SherpaOnnxCreateOfflineTts
++_SherpaOnnxCreateOnlinePunctuation
++_SherpaOnnxCreateOnlineRecognizer
++_SherpaOnnxCreateOnlineStream
++_SherpaOnnxCreateOnlineStreamWithHotwords
++_SherpaOnnxCreateSpeakerEmbeddingExtractor
++_SherpaOnnxCreateSpeakerEmbeddingManager
++_SherpaOnnxCreateSpokenLanguageIdentification
++_SherpaOnnxCreateVoiceActivityDetector
++_SherpaOnnxDecodeKeywordStream
++_SherpaOnnxDecodeMultipleKeywordStreams
++_SherpaOnnxDecodeMultipleOfflineStreams
++_SherpaOnnxDecodeMultipleOnlineStreams
++_SherpaOnnxDecodeOfflineStream
++_SherpaOnnxDecodeOnlineStream
++_SherpaOnnxDestroyAudioTagging
++_SherpaOnnxDestroyCircularBuffer
++_SherpaOnnxDestroyDenoisedAudio
++_SherpaOnnxDestroyDisplay
++_SherpaOnnxDestroyKeywordResult
++_SherpaOnnxDestroyKeywordSpotter
++_SherpaOnnxDestroyLinearResampler
++_SherpaOnnxDestroyOfflinePunctuation
++_SherpaOnnxDestroyOfflineRecognizer
++_SherpaOnnxDestroyOfflineRecognizerResult
++_SherpaOnnxDestroyOfflineSpeakerDiarization
++_SherpaOnnxDestroyOfflineSpeechDenoiser
++_SherpaOnnxDestroyOfflineStream
++_SherpaOnnxDestroyOfflineStreamResultJson
++_SherpaOnnxDestroyOfflineTts
++_SherpaOnnxDestroyOfflineTtsGeneratedAudio
++_SherpaOnnxDestroyOnlinePunctuation
++_SherpaOnnxDestroyOnlineRecognizer
++_SherpaOnnxDestroyOnlineRecognizerResult
++_SherpaOnnxDestroyOnlineStream
++_SherpaOnnxDestroyOnlineStreamResultJson
++_SherpaOnnxDestroySpeakerEmbeddingExtractor
++_SherpaOnnxDestroySpeakerEmbeddingManager
++_SherpaOnnxDestroySpeechSegment
++_SherpaOnnxDestroySpokenLanguageIdentification
++_SherpaOnnxDestroySpokenLanguageIdentificationResult
++_SherpaOnnxDestroyVoiceActivityDetector
++_SherpaOnnxFileExists
++_SherpaOnnxFreeKeywordResultJson
++_SherpaOnnxFreeWave
++_SherpaOnnxGetGitDate
++_SherpaOnnxGetGitSha1
++_SherpaOnnxGetKeywordResult
++_SherpaOnnxGetKeywordResultAsJson
++_SherpaOnnxGetOfflineStreamResult
++_SherpaOnnxGetOfflineStreamResultAsJson
++_SherpaOnnxGetOnlineStreamResult
++_SherpaOnnxGetOnlineStreamResultAsJson
++_SherpaOnnxGetVersionStr
++_SherpaOnnxIsKeywordStreamReady
++_SherpaOnnxIsOnlineStreamReady
++_SherpaOnnxLinearResamplerResample
++_SherpaOnnxLinearResamplerResampleFree
++_SherpaOnnxLinearResamplerResampleGetInputSampleRate
++_SherpaOnnxLinearResamplerResampleGetOutputSampleRate
++_SherpaOnnxLinearResamplerReset
++_SherpaOnnxOfflineRecognizerSetConfig
++_SherpaOnnxOfflineSpeakerDiarizationDestroyResult
++_SherpaOnnxOfflineSpeakerDiarizationDestroySegment
++_SherpaOnnxOfflineSpeakerDiarizationGetSampleRate
++_SherpaOnnxOfflineSpeakerDiarizationProcess
++_SherpaOnnxOfflineSpeakerDiarizationProcessWithCallback
++_SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg
++_SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments
++_SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers
++_SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime
++_SherpaOnnxOfflineSpeakerDiarizationSetConfig
++_SherpaOnnxOfflineSpeechDenoiserGetSampleRate
++_SherpaOnnxOfflineSpeechDenoiserRun
++_SherpaOnnxOfflineTtsGenerate
++_SherpaOnnxOfflineTtsGenerateWithCallback
++_SherpaOnnxOfflineTtsGenerateWithCallbackWithArg
++_SherpaOnnxOfflineTtsGenerateWithProgressCallback
++_SherpaOnnxOfflineTtsGenerateWithProgressCallbackWithArg
++_SherpaOnnxOfflineTtsGenerateWithZipvoice
++_SherpaOnnxOfflineTtsNumSpeakers
++_SherpaOnnxOfflineTtsSampleRate
++_SherpaOnnxOnlinePunctuationAddPunct
++_SherpaOnnxOnlinePunctuationFreeText
++_SherpaOnnxOnlineStreamAcceptWaveform
++_SherpaOnnxOnlineStreamInputFinished
++_SherpaOnnxOnlineStreamIsEndpoint
++_SherpaOnnxOnlineStreamReset
++_SherpaOnnxPrint
++_SherpaOnnxReadWave
++_SherpaOnnxReadWaveFromBinaryData
++_SherpaOnnxResetKeywordStream
++_SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding
++_SherpaOnnxSpeakerEmbeddingExtractorCreateStream
++_SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding
++_SherpaOnnxSpeakerEmbeddingExtractorDim
++_SherpaOnnxSpeakerEmbeddingExtractorIsReady
++_SherpaOnnxSpeakerEmbeddingManagerAdd
++_SherpaOnnxSpeakerEmbeddingManagerAddList
++_SherpaOnnxSpeakerEmbeddingManagerAddListFlattened
++_SherpaOnnxSpeakerEmbeddingManagerContains
++_SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers
++_SherpaOnnxSpeakerEmbeddingManagerFreeBestMatches
++_SherpaOnnxSpeakerEmbeddingManagerFreeSearch
++_SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers
++_SherpaOnnxSpeakerEmbeddingManagerGetBestMatches
++_SherpaOnnxSpeakerEmbeddingManagerNumSpeakers
++_SherpaOnnxSpeakerEmbeddingManagerRemove
++_SherpaOnnxSpeakerEmbeddingManagerSearch
++_SherpaOnnxSpeakerEmbeddingManagerVerify
++_SherpaOnnxSpokenLanguageIdentificationCompute
++_SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream
++_SherpaOnnxVoiceActivityDetectorAcceptWaveform
++_SherpaOnnxVoiceActivityDetectorClear
++_SherpaOnnxVoiceActivityDetectorDetected
++_SherpaOnnxVoiceActivityDetectorEmpty
++_SherpaOnnxVoiceActivityDetectorFlush
++_SherpaOnnxVoiceActivityDetectorFront
++_SherpaOnnxVoiceActivityDetectorPop
++_SherpaOnnxVoiceActivityDetectorReset
++_SherpaOnnxWaveFileSize
++_SherpaOnnxWriteWave
++_SherpaOnnxWriteWaveToBuffer
+diff --git a/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds
+new file mode 100644
+index 00000000..5805cafc
+--- /dev/null
++++ b/sherpa-onnx/c-api/sherpa-onnx-symbols-c.lds
+@@ -0,0 +1,8 @@
++{
++  global:
++    SherpaOnnx*;
++    # For offline punctuation.
++    SherpaOffline*;
++  local:
++    *;
++};
+diff --git a/sherpa-onnx/jni/CMakeLists.txt b/sherpa-onnx/jni/CMakeLists.txt
+index 56cf7bd9..6ba5fcda 100644
+--- a/sherpa-onnx/jni/CMakeLists.txt
++++ b/sherpa-onnx/jni/CMakeLists.txt
+@@ -48,5 +48,15 @@ add_library(sherpa-onnx-jni SHARED ${sources})
+ target_compile_definitions(sherpa-onnx-jni PRIVATE SHERPA_ONNX_BUILD_SHARED_LIBS=1)
+ target_compile_definitions(sherpa-onnx-jni PRIVATE SHERPA_ONNX_BUILD_MAIN_LIB=1)
+ 
++if(ANDROID OR (UNIX AND NOT APPLE))
++  set_target_properties(sherpa-onnx-jni PROPERTIES
++    LINK_FLAGS "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols.lds"
++  )
++elseif(APPLE)
++  set_target_properties(sherpa-onnx-jni PROPERTIES
++    LINK_FLAGS "-Wl,-exported_symbols_list,${CMAKE_CURRENT_SOURCE_DIR}/sherpa-onnx-symbols.exp"
++  )
++endif()
++
+ target_link_libraries(sherpa-onnx-jni sherpa-onnx-core)
+ install(TARGETS sherpa-onnx-jni DESTINATION lib)
+diff --git a/sherpa-onnx/jni/generate.sh b/sherpa-onnx/jni/generate.sh
+new file mode 100755
+index 00000000..aa1781ba
+--- /dev/null
++++ b/sherpa-onnx/jni/generate.sh
+@@ -0,0 +1,5 @@
++#!/usr/bin/env bash
++set -ex
++
++nm -g ../../build/lib/libsherpa-onnx-jni.dylib | awk '$2=="T" && $3 ~ /^_Java_com_k2fsa/ {print $3}' | sort  > ./sherpa-onnx-symbols.exp
++
+diff --git a/sherpa-onnx/jni/sherpa-onnx-symbols.exp b/sherpa-onnx/jni/sherpa-onnx-symbols.exp
+new file mode 100644
+index 00000000..af18d292
+--- /dev/null
++++ b/sherpa-onnx/jni/sherpa-onnx-symbols.exp
+@@ -0,0 +1,110 @@
++_Java_com_k2fsa_sherpa_onnx_AudioTagging_compute
++_Java_com_k2fsa_sherpa_onnx_AudioTagging_createStream
++_Java_com_k2fsa_sherpa_onnx_AudioTagging_delete
++_Java_com_k2fsa_sherpa_onnx_AudioTagging_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_AudioTagging_newFromFile
++_Java_com_k2fsa_sherpa_onnx_DenoisedAudio_saveImpl
++_Java_com_k2fsa_sherpa_onnx_GeneratedAudio_saveImpl
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_createStream
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_decode
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_delete
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_getResult
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_isReady
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_newFromFile
++_Java_com_k2fsa_sherpa_onnx_KeywordSpotter_reset
++_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_addPunctuation
++_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_delete
++_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OfflinePunctuation_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_createStream
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_decode
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_decodeStreams
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_delete
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_getResult
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_prependAdspLibraryPath
++_Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_setConfig
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_delete
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_getSampleRate
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_process
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_processWithCallback
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeakerDiarization_setConfig
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_delete
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_getSampleRate
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OfflineSpeechDenoiser_run
++_Java_com_k2fsa_sherpa_onnx_OfflineStream_acceptWaveform
++_Java_com_k2fsa_sherpa_onnx_OfflineStream_delete
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_delete
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_generateImpl
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_generateWithCallbackImpl
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_getNumSpeakers
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_getSampleRate
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OfflineTts_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_addPunctuation
++_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_delete
++_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OnlinePunctuation_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_createStream
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_decode
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_decodeStreams
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_delete
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_getResult
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_isEndpoint
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_isReady
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_newFromFile
++_Java_com_k2fsa_sherpa_onnx_OnlineRecognizer_reset
++_Java_com_k2fsa_sherpa_onnx_OnlineStream_acceptWaveform
++_Java_com_k2fsa_sherpa_onnx_OnlineStream_delete
++_Java_com_k2fsa_sherpa_onnx_OnlineStream_inputFinished
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_compute
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_createStream
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_delete
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_dim
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_isReady
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingExtractor_newFromFile
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_add
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_addList
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_allSpeakerNames
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_contains
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_create
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_delete
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_numSpeakers
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_remove
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_search
++_Java_com_k2fsa_sherpa_onnx_SpeakerEmbeddingManager_verify
++_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_compute
++_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_createStream
++_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_delete
++_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_SpokenLanguageIdentification_newFromFile
++_Java_com_k2fsa_sherpa_onnx_Vad_acceptWaveform
++_Java_com_k2fsa_sherpa_onnx_Vad_clear
++_Java_com_k2fsa_sherpa_onnx_Vad_compute
++_Java_com_k2fsa_sherpa_onnx_Vad_delete
++_Java_com_k2fsa_sherpa_onnx_Vad_empty
++_Java_com_k2fsa_sherpa_onnx_Vad_flush
++_Java_com_k2fsa_sherpa_onnx_Vad_front
++_Java_com_k2fsa_sherpa_onnx_Vad_isSpeechDetected
++_Java_com_k2fsa_sherpa_onnx_Vad_newFromAsset
++_Java_com_k2fsa_sherpa_onnx_Vad_newFromFile
++_Java_com_k2fsa_sherpa_onnx_Vad_pop
++_Java_com_k2fsa_sherpa_onnx_Vad_reset
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getGitDate2
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getGitSha12
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_00024Companion_getVersionStr2
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_getGitDate2
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_getGitSha12
++_Java_com_k2fsa_sherpa_onnx_VersionInfo_getVersionStr2
++_Java_com_k2fsa_sherpa_onnx_WaveReader_00024Companion_readWaveFromAsset
++_Java_com_k2fsa_sherpa_onnx_WaveReader_00024Companion_readWaveFromFile
++_Java_com_k2fsa_sherpa_onnx_WaveReader_readWaveFromFile
++_Java_com_k2fsa_sherpa_onnx_WaveWriter_writeWaveToFile
+diff --git a/sherpa-onnx/jni/sherpa-onnx-symbols.lds b/sherpa-onnx/jni/sherpa-onnx-symbols.lds
+new file mode 100644
+index 00000000..4931fa00
+--- /dev/null
++++ b/sherpa-onnx/jni/sherpa-onnx-symbols.lds
+@@ -0,0 +1,6 @@
++{
++  global:
++    Java_com_k2fsa_sherpa_onnx*;
++  local:
++    *;
++};
+
+commit d1c458b95d92c74ed7339f9ac24c27dae93bcdd4
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 24 18:14:22 2025 +0800
+
+    Add C++ QNN support for Zipformer CTC models. (#2809)
+
+diff --git a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
+index 60191019..8289439d 100644
+--- a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
++++ b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
+@@ -4,7 +4,7 @@ on:
+   push:
+     branches:
+       - apk
+-      - android-qnn-2
++      - zipformer-ctc-qnn-2
+ 
+   workflow_dispatch:
+ 
+@@ -24,8 +24,8 @@ jobs:
+       fail-fast: false
+       matrix:
+         os: [ubuntu-latest]
+-        total: ["5"]
+-        index: ["0", "1", "2", "3", "4"]
++        total: ["10"]
++        index: ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -47,7 +47,7 @@ jobs:
+       - name: ccache
+         uses: hendrikmuhs/ccache-action@v1.2
+         with:
+-          key: ${{ matrix.os }}-android
++          key: ${{ matrix.os }}-android-qnn
+ 
+       - name: Display NDK HOME
+         shell: bash
+diff --git a/.gitignore b/.gitignore
+index 060ce16b..6ff6eb29 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -159,3 +159,5 @@ configuration.json
+ sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+ sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
+ sherpa-onnx-paraformer-zh-int8-2025-10-07
++sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
++sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+index 1afcf2a9..470850c9 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+@@ -125,20 +125,32 @@ object SimulateStreamingAsr {
+                 OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
+ 
+                 // for qnn, we need to copy *.so files from assets folder to sd card
+-                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()) {
++                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty() && config.modelConfig.zipformerCtc.qnnConfig.backendLib.isEmpty()) {
+                     Log.e(TAG, "You should provide libQnnHtp.so for qnn")
+                     throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
+                 }
+                 config.modelConfig.tokens =
+                     copyAssetToInternalStorage(config.modelConfig.tokens, context)
+ 
+-                config.modelConfig.senseVoice.model =
+-                    copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
+-
+-                config.modelConfig.senseVoice.qnnConfig.contextBinary = copyAssetToInternalStorage(
+-                    config.modelConfig.senseVoice.qnnConfig.contextBinary,
+-                    context
+-                )
++                if (config.modelConfig.senseVoice.model.isNotEmpty()) {
++                    config.modelConfig.senseVoice.model =
++                        copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
++
++                    config.modelConfig.senseVoice.qnnConfig.contextBinary =
++                        copyAssetToInternalStorage(
++                            config.modelConfig.senseVoice.qnnConfig.contextBinary,
++                            context
++                        )
++                } else if (config.modelConfig.zipformerCtc.model.isNotEmpty()) {
++                    config.modelConfig.zipformerCtc.model =
++                        copyAssetToInternalStorage(config.modelConfig.zipformerCtc.model, context)
++
++                    config.modelConfig.zipformerCtc.qnnConfig.contextBinary =
++                        copyAssetToInternalStorage(
++                            config.modelConfig.zipformerCtc.qnnConfig.contextBinary,
++                            context
++                        )
++                }
+ 
+                 if (config.hr.lexicon.isNotEmpty()) {
+                     config.hr.lexicon = copyAssetToInternalStorage(config.hr.lexicon, context)
+diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+index 28f31b1a..d70bff23 100755
+--- a/scripts/apk/generate-qnn-vad-asr-apk-script.py
++++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+@@ -35,7 +35,6 @@ class Model:
+ 
+     # e.g., zh, en, zh_en
+     lang: str
+-    lang2: str
+ 
+     # e.g., whisper, paraformer, zipformer
+     short_name: str = ""
+@@ -55,7 +54,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9000,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="5-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -72,7 +70,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9001,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="8-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -89,7 +86,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9002,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="10-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -106,7 +102,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9003,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="13-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -123,7 +118,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9004,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="15-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -140,7 +134,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9005,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="18-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -157,7 +150,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9006,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="20-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -174,7 +166,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9007,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="23-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -191,7 +182,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9008,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="25-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -208,7 +198,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9009,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="28-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -225,7 +214,6 @@ def get_models():
+             model_name="sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
+             idx=9010,
+             lang="zh_en_ko_ja_yue",
+-            lang2="",
+             short_name="30-seconds-sense_voice_2024_07_17_int8",
+             use_hr=True,
+             cmd="""
+@@ -235,6 +223,182 @@ def get_models():
+ 
+             ls -lh
+ 
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9011,
++            lang="zh",
++            short_name="5-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9012,
++            lang="zh",
++            short_name="8-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9013,
++            lang="zh",
++            short_name="10-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9014,
++            lang="zh",
++            short_name="13-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9015,
++            lang="zh",
++            short_name="15-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9016,
++            lang="zh",
++            short_name="18-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9017,
++            lang="zh",
++            short_name="20-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9018,
++            lang="zh",
++            short_name="23-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9019,
++            lang="zh",
++            short_name="25-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9020,
++            lang="zh",
++            short_name="28-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8",
++            idx=9021,
++            lang="zh",
++            short_name="30-seconds-zipformer_ctc_2025_07_03_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
+             popd
+             """,
+         ),
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 58bafbab..ef480bdd 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -208,10 +208,11 @@ endif()
+ 
+ if(SHERPA_ONNX_ENABLE_QNN)
+   list(APPEND sources
++    ./qnn/offline-sense-voice-model-qnn.cc
++    ./qnn/offline-zipformer-ctc-model-qnn.cc
+     ./qnn/qnn-backend.cc
+     ./qnn/qnn-model.cc
+     ./qnn/utils.cc
+-    ./qnn/offline-sense-voice-model-qnn.cc
+   )
+ endif()
+ 
+diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+index 78a72fc1..11f7b81d 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+@@ -23,10 +23,10 @@
+ 
+ namespace sherpa_onnx {
+ 
+-static OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
+-                                        const SymbolTable &sym_table,
+-                                        int32_t frame_shift_ms,
+-                                        int32_t subsampling_factor) {
++OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
++                                 const SymbolTable &sym_table,
++                                 int32_t frame_shift_ms,
++                                 int32_t subsampling_factor) {
+   OfflineRecognitionResult r;
+   r.tokens.reserve(src.tokens.size());
+   r.timestamps.reserve(src.timestamps.size());
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index ff4e055d..1046f9e1 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -47,6 +47,7 @@
+ 
+ #if SHERPA_ONNX_ENABLE_QNN
+ #include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
++#include "sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h"
+ #endif
+ 
+ namespace sherpa_onnx {
+@@ -101,10 +102,12 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #if SHERPA_ONNX_ENABLE_QNN
+     if (!config.model_config.sense_voice.model.empty()) {
+       return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
++    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++      return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
+-          "by qnn for non-streaming ASR.");
++          "Only SenseVoice models and Zipformer CTC models are currently "
++          "supported by qnn for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+     }
+@@ -355,10 +358,13 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #if SHERPA_ONNX_ENABLE_QNN
+     if (!config.model_config.sense_voice.model.empty()) {
+       return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
++    } else if (!config.model_config.zipformer_ctc.model.empty()) {
++      return std::make_unique<OfflineRecognizerZipformerCtcQnnImpl>(mgr,
++                                                                    config);
+     } else {
+       SHERPA_ONNX_LOGE(
+-          "Only SenseVoice models are currently supported "
+-          "by qnn for non-streaming ASR.");
++          "Only SenseVoice models and Zipformer CTC models are currently "
++          "supported by qnn for non-streaming ASR.");
+       SHERPA_ONNX_EXIT(-1);
+       return nullptr;
+     }
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+index e38dfcbb..ddadbfb9 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+@@ -22,7 +22,10 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
+       "sense-voice-use-itn", &use_itn,
+       "True to enable inverse text normalization. False to disable it.");
+ 
+-  qnn_config.Register(po);
++  std::string prefix = "sense-voice";
++  ParseOptions p(prefix, po);
++
++  qnn_config.Register(&p);
+ }
+ 
+ bool OfflineSenseVoiceModelConfig::Validate() const {
+diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+index fd5e0321..0aaaacbb 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+@@ -8,11 +8,17 @@
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/text-utils.h"
+ 
+ namespace sherpa_onnx {
+ 
+ void OfflineZipformerCtcModelConfig::Register(ParseOptions *po) {
+   po->Register("zipformer-ctc-model", &model, "Path to zipformer CTC model");
++
++  std::string prefix = "zipformer-ctc";
++  ParseOptions p(prefix, po);
++
++  qnn_config.Register(&p);
+ }
+ 
+ bool OfflineZipformerCtcModelConfig::Validate() const {
+@@ -22,6 +28,10 @@ bool OfflineZipformerCtcModelConfig::Validate() const {
+     return false;
+   }
+ 
++  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
++    return qnn_config.Validate();
++  }
++
+   return true;
+ }
+ 
+@@ -29,7 +39,13 @@ std::string OfflineZipformerCtcModelConfig::ToString() const {
+   std::ostringstream os;
+ 
+   os << "OfflineZipformerCtcModelConfig(";
+-  os << "model=\"" << model << "\")";
++  os << "model=\"" << model << "\"";
++
++  if (!qnn_config.backend_lib.empty()) {
++    os << ", qnn_config=" << qnn_config.ToString() << ", ";
++  }
++
++  os << ")";
+ 
+   return os.str();
+ }
+diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
+index 702575e7..44ff0fff 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
++++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h
+@@ -7,6 +7,7 @@
+ #include <string>
+ 
+ #include "sherpa-onnx/csrc/parse-options.h"
++#include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -14,6 +15,7 @@ namespace sherpa_onnx {
+ // https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/zipformer/export-onnx-ctc.py
+ struct OfflineZipformerCtcModelConfig {
+   std::string model;
++  QnnConfig qnn_config;
+ 
+   OfflineZipformerCtcModelConfig() = default;
+ 
+diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
+index 2276cd79..4db59535 100644
+--- a/sherpa-onnx/csrc/qnn-config.cc
++++ b/sherpa-onnx/csrc/qnn-config.cc
+@@ -37,11 +37,8 @@ bool QnnConfig::Validate() const {
+     return false;
+   }
+ 
+-  if (!FileExists(backend_lib)) {
+-    SHERPA_ONNX_LOGE("--qnn-backend-lib: '%s' does not exist",
+-                     backend_lib.c_str());
+-    return false;
+-  }
++  // we don't check whether backend_lib and system_lib exist or not since
++  // dlopen() will find them by searching predefined paths
+ 
+   if (!context_binary.empty() && FileExists(context_binary)) {
+     if (system_lib.empty()) {
+@@ -50,12 +47,6 @@ bool QnnConfig::Validate() const {
+           "--qnn-context-binary");
+       return false;
+     }
+-
+-    if (!FileExists(system_lib)) {
+-      SHERPA_ONNX_LOGE("--qnn-system-lib: '%s' does not exist",
+-                       system_lib.c_str());
+-      return false;
+-    }
+   }
+ 
+   return true;
+diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h b/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
+new file mode 100644
+index 00000000..a7877c8b
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
+@@ -0,0 +1,130 @@
++// sherpa-onnx/csrc/qnn/offline-recognizer-zipformer-ctc-qnn-impl.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
++#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
++
++#include <ios>
++#include <memory>
++#include <sstream>
++#include <string>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h"
++#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../offline-recognizer-ctc-impl.h
++OfflineRecognitionResult Convert(const OfflineCtcDecoderResult &src,
++                                 const SymbolTable &sym_table,
++                                 int32_t frame_shift_ms,
++                                 int32_t subsampling_factor);
++
++class OfflineRecognizerZipformerCtcQnnImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerZipformerCtcQnnImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(std::make_unique<OfflineZipformerCtcModelQnn>(
++            config.model_config)) {
++    Init();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerZipformerCtcQnnImpl(Manager *mgr,
++                                       const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineZipformerCtcModelQnn>(
++            mgr, config.model_config)) {
++    Init();
++  }
++
++  void Init() {
++    if (config_.decoding_method == "greedy_search") {
++      if (!symbol_table_.Contains("<blk>") &&
++          !symbol_table_.Contains("<eps>") &&
++          !symbol_table_.Contains("<blank>") &&
++          config_.model_config.omnilingual.model.empty()) {
++        // for omnilingual asr, its blank id is 0
++        SHERPA_ONNX_LOGE(
++            "We expect that tokens.txt contains "
++            "the symbol <blk> or <eps> or <blank> and its ID.");
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      int32_t blank_id = 0;
++      if (symbol_table_.Contains("<blk>")) {
++        blank_id = symbol_table_["<blk>"];
++      } else if (symbol_table_.Contains("<eps>")) {
++        // for tdnn models of the yesno recipe from icefall
++        blank_id = symbol_table_["<eps>"];
++      } else if (symbol_table_.Contains("<blank>")) {
++        // for Wenet CTC models
++        blank_id = symbol_table_["<blank>"];
++      }
++
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config_.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i != n; ++i) {
++      DecodeStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  // Decode a single stream.
++  // Some models do not support batch size > 1, e.g., WeNet CTC models.
++  void DecodeStream(OfflineStream *s) const {
++    std::vector<float> f = s->GetFrames();
++
++    int32_t vocab_size = model_->VocabSize();
++
++    std::vector<float> log_probs = model_->Run(std::move(f));
++    int32_t num_out_frames = log_probs.size() / vocab_size;
++
++    auto result =
++        decoder_->Decode(log_probs.data(), num_out_frames, vocab_size);
++
++    int32_t frame_shift_ms = 10;
++
++    auto r = Convert(result, symbol_table_, frame_shift_ms,
++                     model_->SubsamplingFactor());
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineZipformerCtcModelQnn> model_;
++  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_ZIPFORMER_CTC_QNN_IMPL_H_
+diff --git a/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
+new file mode 100644
+index 00000000..6af8a49e
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
+@@ -0,0 +1,247 @@
++// sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h"
++
++#include <algorithm>
++#include <array>
++#include <memory>
++#include <mutex>  // NOLINT
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/qnn/macros.h"
++#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
++#include "sherpa-onnx/csrc/qnn/qnn-model.h"
++
++namespace sherpa_onnx {
++
++class OfflineZipformerCtcModelQnn::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    backend_ = std::make_unique<QnnBackend>(
++        config.zipformer_ctc.qnn_config.backend_lib, config_.debug);
++
++    const auto &context_binary =
++        config_.zipformer_ctc.qnn_config.context_binary;
++
++    if (context_binary.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from model lib since context binary is not given");
++      }
++
++      InitFromModelLib();
++
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Skip generating context binary since you don't provide a path to "
++            "save it");
++      }
++    } else if (!FileExists(context_binary)) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from model lib since context binary '%s' does not exist",
++            context_binary.c_str());
++      }
++
++      InitFromModelLib();
++
++      CreateContextBinary();
++    } else {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("Init from context binary '%s'",
++                         context_binary.c_str());
++      }
++      InitFromContextBinary();
++    }
++
++    PostInit();
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    SHERPA_ONNX_LOGE(
++        "Please copy all files from assets to SD card and set assetManager to "
++        "null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  std::vector<float> Run(std::vector<float> features) {
++    int32_t num_frames = features.size() / feat_dim_;
++
++    if (num_frames != max_num_frames_) {
++      if (num_frames > max_num_frames_) {
++        SHERPA_ONNX_LOGE(
++            "Number of input frames %d is too large. Truncate it to %d frames.",
++            num_frames, max_num_frames_);
++
++        SHERPA_ONNX_LOGE(
++            "Recognition result may be truncated/incomplete. Please select a "
++            "model accepting longer audios.");
++      }
++
++      features.resize(max_num_frames_ * feat_dim_);
++
++      num_frames = max_num_frames_;
++    }
++
++    std::lock_guard<std::mutex> lock(mutex_);
++
++    model_->SetInputTensorData("x", features.data(), features.size());
++
++    model_->Run();
++
++    return model_->GetOutputTensorData("log_probs");
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++  int32_t SubsamplingFactor() const { return subsampling_factor_; }
++
++ private:
++  void InitFromModelLib() {
++    backend_->InitContext();
++
++    model_ = std::make_unique<QnnModel>(config_.zipformer_ctc.model,
++                                        backend_.get(), config_.debug);
++  }
++
++  void InitFromContextBinary() {
++    model_ = std::make_unique<QnnModel>(
++        config_.zipformer_ctc.qnn_config.context_binary,
++        config_.zipformer_ctc.qnn_config.system_lib, backend_.get(),
++        BinaryContextTag{}, config_.debug);
++  }
++
++  void CreateContextBinary() {
++    const auto &context_binary =
++        config_.zipformer_ctc.qnn_config.context_binary;
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
++    }
++
++    bool ok = model_->SaveBinaryContext(context_binary);
++
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
++                       context_binary.c_str());
++    }
++
++    if (config_.debug && ok) {
++      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
++      SHERPA_ONNX_LOGE(
++          "It should be super fast the next time you init the system.");
++      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
++    }
++  }
++
++  void PostInit() { CheckModel(); }
++
++  void CheckModel() {
++    const auto &input_tensor_names = model_->InputTensorNames();
++    if (input_tensor_names.size() != 1) {
++      SHERPA_ONNX_LOGE("Expect 1 input tensor. Actual %d",
++                       static_cast<int32_t>(input_tensor_names.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[0] != "x") {
++      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
++                       input_tensor_names[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> x_shape = model_->TensorShape(input_tensor_names[0]);
++    if (x_shape.size() != 3) {
++      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
++                       static_cast<int32_t>(x_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    max_num_frames_ = x_shape[1];
++    feat_dim_ = x_shape[2];
++
++    if (!model_->HasTensor("log_probs")) {
++      SHERPA_ONNX_LOGE("Model does not have output node 'log_probs'");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    auto out_shape = model_->TensorShape("log_probs");
++    vocab_size_ = out_shape[2];
++
++    subsampling_factor_ = max_num_frames_ / out_shape[1];
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("max_num_frames: %d", max_num_frames_);
++      SHERPA_ONNX_LOGE("feat_dim: %d", feat_dim_);
++      SHERPA_ONNX_LOGE("vocab_size: %d", vocab_size_);
++      SHERPA_ONNX_LOGE("subsampling_factor: %d", subsampling_factor_);
++    }
++  }
++
++ private:
++  std::mutex mutex_;
++
++  OfflineModelConfig config_;
++
++  std::unique_ptr<QnnBackend> backend_;
++  std::unique_ptr<QnnModel> model_;
++
++  int32_t max_num_frames_ = 0;
++  int32_t feat_dim_ = 0;
++  int32_t vocab_size_ = 0;
++  int32_t subsampling_factor_ = 1;
++};
++
++OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++OfflineZipformerCtcModelQnn::~OfflineZipformerCtcModelQnn() = default;
++
++std::vector<float> OfflineZipformerCtcModelQnn::Run(
++    std::vector<float> features) const {
++  return impl_->Run(std::move(features));
++}
++
++int32_t OfflineZipformerCtcModelQnn::VocabSize() const {
++  return impl_->VocabSize();
++}
++
++int32_t OfflineZipformerCtcModelQnn::SubsamplingFactor() const {
++  return impl_->SubsamplingFactor();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineZipformerCtcModelQnn::OfflineZipformerCtcModelQnn(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
+new file mode 100644
+index 00000000..f11cd2d5
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
+@@ -0,0 +1,39 @@
++// sherpa-onnx/csrc/qnn/offline-zipformer-ctc-model-qnn.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
++#define SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
++
++#include <memory>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++class OfflineZipformerCtcModelQnn {
++ public:
++  ~OfflineZipformerCtcModelQnn();
++
++  explicit OfflineZipformerCtcModelQnn(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineZipformerCtcModelQnn(Manager *mgr, const OfflineModelConfig &config);
++
++  /**
++   * @param features A tensor of shape (num_frames, feature_dim)
++   * @returns Return a tensor of shape (num_output_frames, vocab_size)
++   */
++  std::vector<float> Run(std::vector<float> features) const;
++
++  int32_t VocabSize() const;
++  int32_t SubsamplingFactor() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_ZIPFORMER_CTC_MODEL_QNN_H_
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
+index 441820da..4e90950d 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
+@@ -134,7 +134,7 @@ class QnnModel::Impl {
+     if (ret != QNN_SUCCESS) {
+       SHERPA_ONNX_LOGE(
+           "Failed to get context binary info from '%s'. ret code is %d",
+-          binary_context_file.c_str(), ret);
++          binary_context_file.c_str(), static_cast<int32_t>(ret));
+ 
+       qnn_system_interface_.systemContextFree(sys_ctx_handle);
+       return false;
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
+index 115f0c2d..a1624da8 100644
+--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineZipformerCtcModelConfig.java
+@@ -4,9 +4,11 @@ package com.k2fsa.sherpa.onnx;
+ 
+ public class OfflineZipformerCtcModelConfig {
+     private final String model;
++    private final QnnConfig qnnConfig;
+ 
+     private OfflineZipformerCtcModelConfig(Builder builder) {
+         this.model = builder.model;
++        this.qnnConfig = builder.qnnConfig;
+     }
+ 
+     public static Builder builder() {
+@@ -17,8 +19,13 @@ public class OfflineZipformerCtcModelConfig {
+         return model;
+     }
+ 
++    public QnnConfig getQnnConfig() {
++        return qnnConfig;
++    }
++
+     public static class Builder {
+         private String model = "";
++        private QnnConfig qnnConfig = QnnConfig.builder().build();
+ 
+         public OfflineZipformerCtcModelConfig build() {
+             return new OfflineZipformerCtcModelConfig(this);
+@@ -28,5 +35,10 @@ public class OfflineZipformerCtcModelConfig {
+             this.model = model;
+             return this;
+         }
++
++        public Builder setQnnConfig(QnnConfig qnnConfig) {
++            this.qnnConfig = qnnConfig;
++            return this;
++        }
+     }
+ }
+diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
+index ed940cbf..3995d40a 100644
+--- a/sherpa-onnx/jni/offline-recognizer.cc
++++ b/sherpa-onnx/jni/offline-recognizer.cc
+@@ -203,6 +203,24 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
+   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.zipformer_ctc.model, model,
+                               zipformer_ctc_config_cls, zipformer_ctc_config);
+ 
++  fid = env->GetFieldID(zipformer_ctc_config_cls, "qnnConfig",
++                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
++
++  qnn_config = env->GetObjectField(zipformer_ctc_config, fid);
++  qnn_config_cls = env->GetObjectClass(qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.zipformer_ctc.qnn_config.backend_lib, backendLib,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.zipformer_ctc.qnn_config.context_binary, contextBinary,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.zipformer_ctc.qnn_config.system_lib, systemLib,
++      qnn_config_cls, qnn_config);
++
+   // wenet ctc
+   fid = env->GetFieldID(model_config_cls, "wenetCtc",
+                         "Lcom/k2fsa/sherpa/onnx/OfflineWenetCtcModelConfig;");
+@@ -332,7 +350,7 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile(JNIEnv *env,
+     }
+   }
+ 
+-  if (config.model_config.provider != "qnn" && !config.Validate()) {
++  if (!config.Validate()) {
+     SHERPA_ONNX_LOGE("Errors found in config!");
+     return 0;
+   }
+diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+index d564f9d3..b1b4bd8e 100644
+--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
++++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+@@ -34,6 +34,7 @@ data class OfflineDolphinModelConfig(
+ 
+ data class OfflineZipformerCtcModelConfig(
+     var model: String = "",
++    var qnnConfig: QnnConfig = QnnConfig(),
+ )
+ 
+ data class OfflineWenetCtcModelConfig(
+@@ -768,6 +769,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                     ),
+                 ),
+                 tokens = "$modelDir/tokens.txt",
++                debug = true,
+             )
+         }
+ 
+@@ -784,6 +786,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                     ),
+                 ),
+                 tokens = "$modelDir/tokens.txt",
++                debug = true,
+             )
+         }
+ 
+@@ -800,6 +803,7 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                     ),
+                 ),
+                 tokens = "$modelDir/tokens.txt",
++                debug = true,
+             )
+         }
+ 
+@@ -930,6 +934,185 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                 tokens = "$modelDir/tokens.txt",
+             )
+         }
++
++        9011 -> {
++            val modelDir = "sherpa-onnx-qnn-5-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
++
++        9012 -> {
++            val modelDir = "sherpa-onnx-qnn-8-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
++
++        9013 -> {
++            val modelDir = "sherpa-onnx-qnn-10-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++                debug = true,
++            )
++        }
++
++        9014 -> {
++            val modelDir = "sherpa-onnx-qnn-13-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9015 -> {
++            val modelDir = "sherpa-onnx-qnn-15-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9016 -> {
++            val modelDir = "sherpa-onnx-qnn-18-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9017 -> {
++            val modelDir = "sherpa-onnx-qnn-20-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9018 -> {
++            val modelDir = "sherpa-onnx-qnn-23-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9019 -> {
++            val modelDir = "sherpa-onnx-qnn-25-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9020 -> {
++            val modelDir = "sherpa-onnx-qnn-28-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9021 -> {
++            val modelDir = "sherpa-onnx-qnn-30-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                zipformerCtc = OfflineZipformerCtcModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
+     }
+     return null
+ }
+
+commit b4ab72e51bde1cb5bdf2fa0996a2aa21dbf9c634
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 24 16:00:44 2025 +0800
+
+    Add spaces between English words for Homophone replacer. (#2817)
+    
+    This pull request refines the output formatting of the Homophone Replacer by implementing more robust spacing logic. It ensures that English words are properly separated and that the final output does not contain any unnecessary trailing spaces, leading to a more natural and correct text presentation.
+
+diff --git a/sherpa-onnx/csrc/homophone-replacer.cc b/sherpa-onnx/csrc/homophone-replacer.cc
+index 9dd223d5..db3325eb 100644
+--- a/sherpa-onnx/csrc/homophone-replacer.cc
++++ b/sherpa-onnx/csrc/homophone-replacer.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/homophone-replacer.h"
+ 
++#include <cctype>
+ #include <fstream>
+ #include <memory>
+ #include <sstream>
+@@ -176,6 +177,9 @@ class HomophoneReplacer::Impl {
+           current_pronunciations.clear();
+         }
+         ans += w;
++        if (isalpha(w[0])) {
++          ans.push_back(' ');
++        }
+         continue;
+       }
+ 
+@@ -196,6 +200,10 @@ class HomophoneReplacer::Impl {
+       SHERPA_ONNX_LOGE("Output text: '%s'", ans.c_str());
+     }
+ 
++    if (!ans.empty() && ans.back() == ' ') {
++      ans.pop_back();
++    }
++
+     return ans;
+   }
+ 
+
+commit 28c27bd4275f2d0e4231663b25babf6b1bd908e6
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 24 15:57:48 2025 +0800
+
+    Export zipformer ctc models to QNN (#2815)
+
+diff --git a/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
+new file mode 100644
+index 00000000..4b411384
+--- /dev/null
++++ b/.github/workflows/export-zipformer-ctc-to-qnn-20250703.yaml
+@@ -0,0 +1,303 @@
++name: export-zipformer-ctc-to-qnn-20250703
++
++on:
++  push:
++    branches:
++      - qnn-zipformer-ctc-models
++  workflow_dispatch:
++
++concurrency:
++  group: export-zipformer-ctc-to-qnn-20250703-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  export-zipformer-ctc-to-qnn-20250703:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: ${{ matrix.input_in_seconds }}
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-22.04]
++        python-version: ["3.10"]
++        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python ${{ matrix.python-version }}
++        uses: actions/setup-python@v5
++        with:
++          python-version: ${{ matrix.python-version }}
++
++      - name: Display NDK HOME
++        shell: bash
++        run: |
++          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
++          ls -lh ${ANDROID_NDK_LATEST_HOME}
++
++      - name: Create Python virtual environment
++        shell: bash
++        run: |
++          python3 -m venv py310
++          which python3
++          source py310/bin/activate
++          which python3
++
++      - name: Show ndk-build help
++        shell: bash
++        run: |
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          ndk-build --help
++
++      - name: Download toolkit
++        shell: bash
++        run: |
++          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
++          ls -lh v2.33.0.250327.zip
++
++      - name: Unzip toolkit
++        shell: bash
++        run: |
++          unzip v2.33.0.250327.zip
++
++      - name: Show
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---ls -lh qairt---"
++
++          ls -lh qairt
++
++          echo "---"
++
++      - name: Install linux dependencies
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---"
++
++          ls -lh qairt
++
++          cd qairt/2.33.0.250327/bin
++          source envsetup.sh
++
++          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          cd qairt/2.33.0.250327/bin
++          source envsetup.sh
++
++          python3 -m pip install \
++            mock \
++            numpy \
++            opencv-python \
++            optuna \
++            packaging \
++            pandas \
++            paramiko \
++            pathlib2 \
++            pillow \
++            plotly \
++            protobuf \
++            psutil \
++            pydantic \
++            pytest \
++            pyyaml \
++            rich \
++            scikit-optimize \
++            scipy \
++            six \
++            tabulate \
++            typing-extensions \
++            xlsxwriter
++
++          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
++
++          which python3
++
++      - name: Install onnx dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++          python3 -m pip install --upgrade \
++            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
++            kaldi_native_fbank \
++            pip \
++            "numpy<2" \
++            onnx==1.17.0 \
++            onnxruntime==1.17.1 \
++            soundfile \
++            librosa \
++            onnxsim \
++            sentencepiece \
++            pyyaml
++
++          which python3
++
++      - name: Show qnn-onnx-converter help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-onnx-converter --help
++
++      - name: Show qnn-model-lib-generator help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-model-lib-generator --help
++
++      - name: Show qnn-net-run help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-net-run --help
++
++      - name: Run ${{ matrix.input_in_seconds }}
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++
++          mkdir tmp
++
++          cd tmp
++
++          t=${{ matrix.input_in_seconds }}
++          num_frames=$(($t*100))
++
++          echo "num_frames: $num_frames"
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/generate_test_data.py
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/test.py
++          chmod +x generate_test_data.py
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/0.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/1.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/8k.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/tokens.txt
++
++          ./generate_test_data.py --num-frames $num_frames --wav 0.wav
++          ./generate_test_data.py --num-frames $num_frames --wav 1.wav
++          ./generate_test_data.py --num-frames $num_frames --wav 8k.wav
++
++          echo -e "0.raw\n1.raw\n8k.raw" > input_list.txt
++
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-zipformer-ctc-zh-2025-07-03-source-models/resolve/main/model-$t-seconds.onnx
++
++          python3 ../scripts/pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
++
++
++          echo "export to qnn"
++          echo "----------$t----------"
++
++          qnn-onnx-converter \
++            --input_network model-$t-seconds.onnx \
++            --output_path ./model-$t-seconds-quantized \
++            --out_node log_probs \
++            --input_list ./input_list.txt \
++            --use_native_input_files  \
++            --input_dtype x float32 \
++            --act_bitwidth 16 \
++            --bias_bitwidth 32 \
++            --input_layout x NTF
++
++          ls -lh
++          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
++          echo "----"
++          ls -lh
++
++          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++            -c "model-$t-seconds-quantized.cpp" \
++            -b "model-$t-seconds-quantized.bin" \
++            -o model_libs > /dev/null 2>&1
++
++          ls -lh model_libs/*/
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "collect results"
++
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-zipformer-ctc-zh-2025-07-03-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v model_libs/$p/lib*.so $d/libmodel.so
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++
++            echo "num_frames=$num_frames" > $d/info.txt
++            echo "target=$p" >> $d/info.txt
++
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: ${{ matrix.input_in_seconds }}-seconds
++          path: ./tmp/*.json
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          tag: asr-models-qnn
+
+commit 16d62b6a08f617c2bd6d21d411911c6462607f08
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 20 17:22:07 2025 +0800
+
+    Add Android demo with QNN (Qualcomm NPU) for SenseVoice ASR (#2803)
+
+diff --git a/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
+new file mode 100644
+index 00000000..60191019
+--- /dev/null
++++ b/.github/workflows/apk-qnn-vad-asr-simulated-streaming.yaml
+@@ -0,0 +1,192 @@
++name: apk-qnn-vad-asr-simulated-streaming
++
++on:
++  push:
++    branches:
++      - apk
++      - android-qnn-2
++
++  workflow_dispatch:
++
++concurrency:
++  group: apk-qnn-vad-asr-simulated-streaming-${{ github.ref }}
++  cancel-in-progress: true
++
++permissions:
++  contents: write
++
++jobs:
++  simulated_streaming_asr:
++    if: github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa'
++    runs-on: ${{ matrix.os }}
++    name: ${{ matrix.index }}/${{ matrix.total }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        total: ["5"]
++        index: ["0", "1", "2", "3", "4"]
++
++    steps:
++      - uses: actions/checkout@v4
++        with:
++          fetch-depth: 0
++
++      - name: Update version
++        shell: bash
++        run: |
++          ./new-release.sh
++          git diff .
++
++      # https://github.com/actions/setup-java
++      - uses: actions/setup-java@v4
++        with:
++          distribution: 'temurin' # See 'Supported distributions' for available options
++          java-version: '21'
++
++      - name: ccache
++        uses: hendrikmuhs/ccache-action@v1.2
++        with:
++          key: ${{ matrix.os }}-android
++
++      - name: Display NDK HOME
++        shell: bash
++        run: |
++          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
++          ls -lh ${ANDROID_NDK_LATEST_HOME}
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          python3 -m pip install --upgrade pip jinja2
++
++      - name: Setup build tool version variable
++        shell: bash
++        run: |
++          echo "---"
++          ls -lh /usr/local/lib/android/
++          echo "---"
++
++          ls -lh /usr/local/lib/android/sdk
++          echo "---"
++
++          ls -lh /usr/local/lib/android/sdk/build-tools
++          echo "---"
++
++          BUILD_TOOL_VERSION=$(ls /usr/local/lib/android/sdk/build-tools/ | tail -n 1)
++          echo "BUILD_TOOL_VERSION=$BUILD_TOOL_VERSION" >> $GITHUB_ENV
++          echo "Last build tool version is: $BUILD_TOOL_VERSION"
++
++      - name: Generate build script
++        shell: bash
++        run: |
++          cd scripts/apk
++
++          total=${{ matrix.total }}
++          index=${{ matrix.index }}
++
++          ./generate-qnn-vad-asr-apk-script.py --total $total --index $index
++
++          chmod +x build-apk-qnn-vad-asr-simulate-streaming.sh
++          mv -v ./build-apk-qnn-vad-asr-simulate-streaming.sh ../..
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: build-script-${{ matrix.total }}-${{ matrix.index }}
++          path: ./build-apk-qnn-vad-asr-simulate-streaming.sh
++
++      - name: build APK
++        shell: bash
++        run: |
++          export CMAKE_CXX_COMPILER_LAUNCHER=ccache
++          export PATH="/usr/lib/ccache:/usr/local/opt/ccache/libexec:$PATH"
++          cmake --version
++
++          export ANDROID_NDK=$ANDROID_NDK_LATEST_HOME
++          ./build-apk-qnn-vad-asr-simulate-streaming.sh
++
++      - name: Display APK
++        shell: bash
++        run: |
++          ls -lh ./apks/
++          du -h -d1 .
++
++      # https://github.com/marketplace/actions/sign-android-release
++      - uses: r0adkll/sign-android-release@v1
++        name: Sign app APK
++        with:
++          releaseDirectory: ./apks
++          signingKeyBase64: ${{ secrets.ANDROID_SIGNING_KEY }}
++          alias: ${{ secrets.ANDROID_SIGNING_KEY_ALIAS }}
++          keyStorePassword: ${{ secrets.ANDROID_SIGNING_KEY_STORE_PASSWORD }}
++        env:
++          BUILD_TOOLS_VERSION: ${{ env.BUILD_TOOL_VERSION }}
++
++      - name: Display APK after signing
++        shell: bash
++        run: |
++          ls -lh ./apks/
++          du -h -d1 .
++
++      - name: Rename APK after signing
++        shell: bash
++        run: |
++          cd apks
++          rm -fv signingKey.jks
++          rm -fv *.apk.idsig
++          rm -fv *-aligned.apk
++
++          all_apks=$(ls -1 *-signed.apk)
++          echo "----"
++          echo $all_apks
++          echo "----"
++          for apk in ${all_apks[@]}; do
++            n=$(echo $apk | sed -e s/-signed//)
++            mv -v $apk $n
++          done
++
++          cd ..
++
++          ls -lh ./apks/
++          du -h -d1 .
++
++      - name: Display APK after rename
++        shell: bash
++        run: |
++          ls -lh ./apks/
++          du -h -d1 .
++
++      - name: Publish to huggingface
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            rm -rf huggingface
++            export GIT_LFS_SKIP_SMUDGE=1
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++
++            SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++            echo "SHERPA_ONNX_VERSION $SHERPA_ONNX_VERSION"
++
++            git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-apk huggingface
++            cd huggingface
++            du -h -d1 .
++            git fetch
++            git pull
++            git merge -m "merge remote" --ff origin main
++
++            d=qnn-vad-asr-simulated-streaming/$SHERPA_ONNX_VERSION
++            mkdir -p $d
++            cp -v ../apks/*.apk $d/
++            git status
++            git lfs track "*.apk"
++            git add .
++            git commit -m "add more apks for qnn"
++            git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/sherpa-onnx-apk main
+diff --git a/.gitignore b/.gitignore
+index 768fc466..060ce16b 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -157,4 +157,5 @@ am.mvn
+ config.yaml
+ configuration.json
+ sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
+ sherpa-onnx-paraformer-zh-int8-2025-10-07
+diff --git a/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+new file mode 120000
+index 00000000..cfd1fc18
+--- /dev/null
++++ b/android/SherpaOnnx2Pass/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+@@ -0,0 +1 @@
++../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
+\ No newline at end of file
+diff --git a/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+new file mode 120000
+index 00000000..cfd1fc18
+--- /dev/null
++++ b/android/SherpaOnnxAar/sherpa_onnx/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+@@ -0,0 +1 @@
++../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
+\ No newline at end of file
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
+index c97f7eb2..0c69c725 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/AndroidManifest.xml
+@@ -14,6 +14,19 @@
+         android:supportsRtl="true"
+         android:theme="@style/Theme.SimulateStreamingAsr"
+         tools:targetApi="31">
++
++        <!--
++        required by qnn
++
++        If you don't add it, you would get an error from the deviceCreate() API
++        and the error code is 14001
++
++        It is located at /vendor/lib64/libcdsprpc.so on your Phone
++        -->
++        <uses-native-library
++            android:name="libcdsprpc.so"
++            android:required="false"/>
++
+         <activity
+             android:name=".MainActivity"
+             android:exported="true"
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+new file mode 120000
+index 00000000..cfd1fc18
+--- /dev/null
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+@@ -0,0 +1 @@
++../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
+\ No newline at end of file
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
+index 76282453..8663e4ac 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/MainActivity.kt
+@@ -57,9 +57,6 @@ class MainActivity : ComponentActivity() {
+             }
+         }
+         ActivityCompat.requestPermissions(this, permissions, REQUEST_RECORD_AUDIO_PERMISSION)
+-
+-        SimulateStreamingAsr.initOfflineRecognizer(this.assets, this.application)
+-        SimulateStreamingAsr.initVad(this.assets)
+     }
+ 
+     @Deprecated("Deprecated in Java")
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+index 3dd22b47..1afcf2a9 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/SimulateStreamingAsr.kt
+@@ -1,6 +1,6 @@
+ package com.k2fsa.sherpa.onnx.simulate.streaming.asr
+ 
+-import android.app.Application
++import android.content.Context
+ import android.content.res.AssetManager
+ import android.util.Log
+ import com.k2fsa.sherpa.onnx.HomophoneReplacerConfig
+@@ -11,7 +11,51 @@ import com.k2fsa.sherpa.onnx.getOfflineModelConfig
+ import com.k2fsa.sherpa.onnx.getVadModelConfig
+ import java.io.File
+ import java.io.FileOutputStream
+-import java.io.IOException
++import java.io.InputStream
++import java.io.OutputStream
++
++
++fun assetExists(assetManager: AssetManager, path: String): Boolean {
++    val dir = path.substringBeforeLast('/', "")
++    val fileName = path.substringAfterLast('/')
++
++    val files = assetManager.list(dir) ?: return false
++    return files.contains(fileName)
++}
++
++fun copyAssetToInternalStorage(path: String, context: Context): String {
++    val targetRoot = context.filesDir
++    val outFile = File(targetRoot, path)
++
++    if (!assetExists(context.assets, path = path)) {
++        // for context binary, if it is does not exist, we return a path
++        // that can be written to
++        outFile.parentFile?.mkdirs()
++        Log.i(TAG, "$path does not exist, return ${outFile.absolutePath}")
++        return outFile.absolutePath
++    }
++
++    if (outFile.exists()) {
++        val assetSize = context.assets.open(path).use { it.available() }
++        if (outFile.length() == assetSize.toLong()) {
++            Log.i(TAG, "$targetRoot/$path already exists, skip copying, return $targetRoot/$path")
++
++            return "$targetRoot/$path"
++        }
++    }
++
++    outFile.parentFile?.mkdirs()
++
++    context.assets.open(path).use { input: InputStream ->
++        FileOutputStream(outFile).use { output: OutputStream ->
++            input.copyTo(output)
++        }
++    }
++    Log.i(TAG, "Copied $path to $targetRoot/$path")
++
++    return outFile.absolutePath
++}
++
+ 
+ object SimulateStreamingAsr {
+     private var _recognizer: OfflineRecognizer? = null
+@@ -26,7 +70,7 @@ object SimulateStreamingAsr {
+             return _vad!!
+         }
+ 
+-    fun initOfflineRecognizer(assetManager: AssetManager? = null, application: Application) {
++    fun initOfflineRecognizer(context: Context, asrModelType: Int) {
+         synchronized(this) {
+             if (_recognizer != null) {
+                 return
+@@ -35,13 +79,12 @@ object SimulateStreamingAsr {
+             // Please change getOfflineModelConfig() to add new models
+             // See https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html
+             // for a list of available models
+-            val asrModelType = 15
+             val asrRuleFsts: String?
+             asrRuleFsts = null
+             Log.i(TAG, "Select model type $asrModelType for ASR")
+ 
+             val useHr = false
+-            val hr =  HomophoneReplacerConfig(
++            val hr = HomophoneReplacerConfig(
+                 // Used only when useHr is true
+                 // Please download the following 2 files from
+                 // https://github.com/k2-fsa/sherpa-onnx/releases/tag/hr-files
+@@ -69,6 +112,46 @@ object SimulateStreamingAsr {
+                 config.hr = hr
+             }
+ 
++            var assetManager: AssetManager? = context.assets
++
++            if (config.modelConfig.provider == "qnn") {
++                // We assume you have copied files like libQnnHtpV81Skel.so to jniLibs/arm64-v8a
++                Log.i(TAG, "nativelibdir: ${context.applicationInfo.nativeLibraryDir}")
++
++                // If we don't set the environment variable for ADSP_LIBRARY_PATH, we will see
++                // the error code 1008 from qnn_interface.deviceCreate()
++                // See also
++                // https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
++                OfflineRecognizer.prependAdspLibraryPath(context.applicationInfo.nativeLibraryDir)
++
++                // for qnn, we need to copy *.so files from assets folder to sd card
++                if (config.modelConfig.senseVoice.qnnConfig.backendLib.isEmpty()) {
++                    Log.e(TAG, "You should provide libQnnHtp.so for qnn")
++                    throw IllegalArgumentException("You should provide libQnnHtp.so for qnn")
++                }
++                config.modelConfig.tokens =
++                    copyAssetToInternalStorage(config.modelConfig.tokens, context)
++
++                config.modelConfig.senseVoice.model =
++                    copyAssetToInternalStorage(config.modelConfig.senseVoice.model, context)
++
++                config.modelConfig.senseVoice.qnnConfig.contextBinary = copyAssetToInternalStorage(
++                    config.modelConfig.senseVoice.qnnConfig.contextBinary,
++                    context
++                )
++
++                if (config.hr.lexicon.isNotEmpty()) {
++                    config.hr.lexicon = copyAssetToInternalStorage(config.hr.lexicon, context)
++                }
++
++                if (config.hr.ruleFsts.isNotEmpty()) {
++                    // it assumes there is only one fst. otherwise, you need to copy each fst separately
++                    config.hr.ruleFsts = copyAssetToInternalStorage(config.hr.ruleFsts, context)
++                }
++
++                assetManager = null
++            }
++
+             _recognizer = OfflineRecognizer(
+                 assetManager = assetManager,
+                 config = config,
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+index 36c6a3be..e6d5cb3d 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens/Home.kt
+@@ -25,6 +25,7 @@ import androidx.compose.foundation.lazy.rememberLazyListState
+ import androidx.compose.material3.Button
+ import androidx.compose.material3.Text
+ import androidx.compose.runtime.Composable
++import androidx.compose.runtime.LaunchedEffect
+ import androidx.compose.runtime.getValue
+ import androidx.compose.runtime.mutableStateListOf
+ import androidx.compose.runtime.mutableStateOf
+@@ -46,6 +47,7 @@ import kotlinx.coroutines.CoroutineScope
+ import kotlinx.coroutines.Dispatchers
+ import kotlinx.coroutines.channels.Channel
+ import kotlinx.coroutines.launch
++import kotlinx.coroutines.withContext
+ 
+ private var audioRecord: AudioRecord? = null
+ 
+@@ -63,6 +65,29 @@ fun HomeScreen() {
+     val lazyColumnListState = rememberLazyListState()
+     val coroutineScope = rememberCoroutineScope()
+ 
++    var isInitialized by remember { mutableStateOf(false) }
++
++    // we change asrModelType in github actions
++    val asrModelType = 15
++
++    LaunchedEffect(Unit) {
++        if (asrModelType >= 9000) {
++            resultList.add("Using QNN for Qualcomm NPU (HTP backend)")
++            resultList.add("It takes about 10s for the first run to start")
++            resultList.add("Later runs require less than 1 second")
++        }
++
++        withContext(Dispatchers.Default) {
++            // Call your heavy initialization off the main thread
++            SimulateStreamingAsr.initOfflineRecognizer(activity, asrModelType)
++            SimulateStreamingAsr.initVad(activity.assets)
++        }
++
++        // Back on the Main thread: update UI state
++        isInitialized = true
++        resultList.clear()
++    }
++
+     val onRecordingButtonClick: () -> Unit = {
+         isStarted = !isStarted
+         if (isStarted) {
+@@ -211,13 +236,32 @@ fun HomeScreen() {
+             audioRecord = null
+         }
+     }
++
+     Box(
+         modifier = Modifier.fillMaxSize(),
+         contentAlignment = Alignment.TopCenter,
+     ) {
+         Column(modifier = Modifier) {
++            if (!isInitialized) {
++                Row(
++                    modifier = Modifier.fillMaxWidth(),
++                    horizontalArrangement = Arrangement.Center,
++                ) {
++                    Text(text = "Initializing... Please wait")
++                }
++            }
++            if (asrModelType >= 9000) {
++                Row(
++                    modifier = Modifier.fillMaxWidth(),
++                    horizontalArrangement = Arrangement.Center,
++                ) {
++                    Text(text = "Qualcomm NPU (HTP backend with QNN)")
++                }
++            }
++
+             HomeButtonRow(
+                 isStarted = isStarted,
++                isInitialized = isInitialized,
+                 onRecordingButtonClick = onRecordingButtonClick,
+                 onCopyButtonClick = {
+                     if (resultList.isNotEmpty()) {
+@@ -255,7 +299,7 @@ fun HomeScreen() {
+                     state = lazyColumnListState
+                 ) {
+                     itemsIndexed(resultList) { index, line ->
+-                        Text(text = "${index+1}: $line")
++                        Text(text = "${index + 1}: $line")
+                     }
+                 }
+             }
+@@ -269,6 +313,7 @@ fun HomeScreen() {
+ private fun HomeButtonRow(
+     modifier: Modifier = Modifier,
+     isStarted: Boolean,
++    isInitialized: Boolean,
+     onRecordingButtonClick: () -> Unit,
+     onCopyButtonClick: () -> Unit,
+     onClearButtonClick: () -> Unit,
+@@ -278,20 +323,27 @@ private fun HomeButtonRow(
+         horizontalArrangement = Arrangement.Center,
+     ) {
+         Button(
+-            onClick = onRecordingButtonClick
++            onClick = onRecordingButtonClick,
++            enabled = isInitialized,
+         ) {
+             Text(text = stringResource(if (isStarted) R.string.stop else R.string.start))
+         }
+ 
+         Spacer(modifier = Modifier.width(24.dp))
+ 
+-        Button(onClick = onCopyButtonClick) {
++        Button(
++            onClick = onCopyButtonClick,
++            enabled = isInitialized,
++        ) {
+             Text(text = stringResource(id = R.string.copy))
+         }
+ 
+         Spacer(modifier = Modifier.width(24.dp))
+ 
+-        Button(onClick = onClearButtonClick) {
++        Button(
++            onClick = onClearButtonClick,
++            enabled = isInitialized,
++        ) {
+             Text(text = stringResource(id = R.string.clear))
+         }
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt b/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+new file mode 120000
+index 00000000..cfd1fc18
+--- /dev/null
++++ b/android/SherpaOnnxVadAsr/app/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.kt
+@@ -0,0 +1 @@
++../../../../../../../../../../sherpa-onnx/kotlin-api/QnnConfig.kt
+\ No newline at end of file
+diff --git a/kotlin-api-examples/QnnConfig.kt b/kotlin-api-examples/QnnConfig.kt
+new file mode 120000
+index 00000000..951cd308
+--- /dev/null
++++ b/kotlin-api-examples/QnnConfig.kt
+@@ -0,0 +1 @@
++../sherpa-onnx/kotlin-api/QnnConfig.kt
+\ No newline at end of file
+diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
+index 6dc1d164..427d19fb 100755
+--- a/kotlin-api-examples/run.sh
++++ b/kotlin-api-examples/run.sh
+@@ -275,6 +275,7 @@ function testOfflineAsr() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_offline_asr.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+@@ -304,6 +305,7 @@ function testInverseTextNormalizationOfflineAsr() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_itn_offline_asr.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+@@ -457,6 +459,7 @@ function testOfflineSenseVoiceWithHr() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_offline_sense_voice_with_hr.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+@@ -478,6 +481,7 @@ function testOfflineNeMoCanary() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_offline_nemo_canary.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+@@ -500,6 +504,7 @@ function testOfflineOmnilingualAsrCtc() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_offline_omnilingual_asr_ctc.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+@@ -521,6 +526,7 @@ function testOfflineWenetCtc() {
+   kotlinc-jvm -include-runtime -d $out_filename \
+     test_offline_wenet_ctc.kt \
+     FeatureConfig.kt \
++    QnnConfig.kt \
+     HomophoneReplacerConfig.kt \
+     OfflineRecognizer.kt \
+     OfflineStream.kt \
+diff --git a/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in b/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in
+new file mode 100644
+index 00000000..6a9398f4
+--- /dev/null
++++ b/scripts/apk/build-apk-qnn-vad-asr-simulate-streaming.sh.in
+@@ -0,0 +1,132 @@
++#!/usr/bin/env bash
++#
++# Auto generated! Please DO NOT EDIT!
++
++# Please set the environment variable ANDROID_NDK
++# before running this script
++
++# Inside the $ANDROID_NDK directory, you can find a binary ndk-build
++# and some other files like the file "build/cmake/android.toolchain.cmake"
++
++set -ex
++
++log() {
++  # This function is from espnet
++  local fname=${BASH_SOURCE[1]##*/}
++  echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
++}
++
++SHERPA_ONNX_VERSION=$(grep "SHERPA_ONNX_VERSION" ./CMakeLists.txt  | cut -d " " -f 2  | cut -d '"' -f 2)
++
++log "Building simulated-streaming VAD + ASR APK + QNN for sherpa-onnx v${SHERPA_ONNX_VERSION}"
++
++export SHERPA_ONNX_ENABLE_TTS=OFF
++
++export SHERPA_ONNX_ENABLE_QNN=ON
++
++log "Download qnn header files"
++
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/qnn-include-2.40.0.251030.tar.bz2
++tar xf qnn-include-2.40.0.251030.tar.bz2
++rm qnn-include-2.40.0.251030.tar.bz2
++ls -lh qnn-include-2.40.0.251030
++
++export QNN_SDK_ROOT=$PWD/qnn-include-2.40.0.251030
++
++log "====================arm64-v8a================="
++./build-android-arm64-v8a.sh
++
++cp -v ./build-android-arm64-v8a/install/lib/*.so ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
++
++log "=======Download qnn libs============"
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/qnn-libs-2.40.0.251030.tar.bz2
++tar xvf qnn-libs-2.40.0.251030.tar.bz2
++rm qnn-libs-2.40.0.251030.tar.bz2
++cp -v qnn-libs-2.40.0.251030/*.so ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
++
++rm -rf qnn-libs-2.40.0.251030
++
++ls -lh ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/jniLibs/arm64-v8a/
++
++mkdir -p apks
++
++{% for model in model_list %}
++pushd ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/
++model_name={{ model.model_name }}-android-aarch64
++type={{ model.idx }}
++lang={{ model.lang }}
++short_name={{ model.short_name }}
++
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models-qnn/${model_name}.tar.bz2
++tar xvf ${model_name}.tar.bz2
++
++{% if model.use_hr %}
++  if [ ! -f lexicon.txt ]; then
++    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lexicon.txt
++  fi
++
++  if [ ! -f replace.fst ]; then
++    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/replace.fst
++  fi
++{% endif %}
++
++{{ model.cmd }}
++
++rm -rf  *.tar.bz2
++ls -lh $model_name
++
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx
++
++popd
++# Now we are at the project root directory
++
++git checkout .
++
++pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens
++sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./Home.kt
++popd
++
++pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr
++
++{% if model.use_hr %}
++  sed -i.bak s/"useHr = false/useHr = true/" ./SimulateStreamingAsr.kt
++{% endif %}
++
++{% if model.rule_fsts %}
++  rule_fsts={{ model.rule_fsts }}
++  sed -i.bak s%"asrRuleFsts = null"%"asrRuleFsts = \"$rule_fsts\""% ./MainActivity.kt
++{% endif %}
++
++git diff
++popd
++
++for arch in arm64-v8a; do
++  log "------------------------------------------------------------"
++  log "build simulated-streaming ASR apk for $arch"
++  log "------------------------------------------------------------"
++  src_arch=$arch
++  if [ $arch == "armeabi-v7a" ]; then
++    src_arch=armv7-eabi
++  elif [ $arch == "x86_64" ]; then
++    src_arch=x86-64
++  fi
++
++  pushd ./android/SherpaOnnxSimulateStreamingAsr
++  sed -i.bak s/2048/9012/g ./gradle.properties
++  git diff ./gradle.properties
++  ./gradlew assembleRelease
++  popd
++
++  mv android/SherpaOnnxSimulateStreamingAsr/app/build/outputs/apk/release/app-release-unsigned.apk ./apks/sherpa-onnx-${SHERPA_ONNX_VERSION}-qnn-$arch-simulated_streaming_asr-$lang-$short_name.apk
++  ls -lh apks
++done
++
++rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/$model_name
++rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/lexicon.txt
++rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/replace.fst
++
++{% endfor %}
++
++git checkout .
++
++ls -lh apks/
+diff --git a/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in b/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
+index d315c973..7051883d 100644
+--- a/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
++++ b/scripts/apk/build-apk-vad-asr-simulate-streaming.sh.in
+@@ -44,12 +44,6 @@ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/$
+ tar xvf ${model_name}.tar.bz2
+ 
+ {% if model.use_hr %}
+-  if [ ! -d dict ]; then
+-    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/dict.tar.bz2
+-    tar xvf dict.tar.bz2
+-    rm dict.tar.bz2
+-  fi
+-
+   if [ ! -f lexicon.txt ]; then
+     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/lexicon.txt
+   fi
+@@ -70,8 +64,12 @@ popd
+ # Now we are at the project root directory
+ 
+ git checkout .
++
++pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr/screens
++sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./Home.kt
++popd
++
+ pushd android/SherpaOnnxSimulateStreamingAsr/app/src/main/java/com/k2fsa/sherpa/onnx/simulate/streaming/asr
+-sed -i.bak s/"asrModelType = 15/asrModelType = $type/" ./SimulateStreamingAsr.kt
+ 
+ {% if model.use_hr %}
+   sed -i.bak s/"useHr = false/useHr = true/" ./SimulateStreamingAsr.kt
+@@ -112,7 +110,6 @@ for arch in arm64-v8a armeabi-v7a x86_64 x86; do
+ done
+ 
+ rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/$model_name
+-rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/dict
+ rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/lexicon.txt
+ rm -rf ./android/SherpaOnnxSimulateStreamingAsr/app/src/main/assets/replace.fst
+ 
+diff --git a/scripts/apk/generate-qnn-vad-asr-apk-script.py b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+new file mode 100755
+index 00000000..28f31b1a
+--- /dev/null
++++ b/scripts/apk/generate-qnn-vad-asr-apk-script.py
+@@ -0,0 +1,292 @@
++#!/usr/bin/env python3
++
++import argparse
++from dataclasses import dataclass
++from pathlib import Path
++
++import jinja2
++
++
++def get_args():
++    parser = argparse.ArgumentParser()
++    parser.add_argument(
++        "--total",
++        type=int,
++        default=1,
++        help="Number of runners",
++    )
++    parser.add_argument(
++        "--index",
++        type=int,
++        default=0,
++        help="Index of the current runner",
++    )
++    return parser.parse_args()
++
++
++@dataclass
++class Model:
++    # We will download
++    # https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/{model_name}.tar.bz2
++    model_name: str
++
++    # The type of the model, e..g, 0, 1, 2. It is hardcoded in the kotlin code
++    idx: int
++
++    # e.g., zh, en, zh_en
++    lang: str
++    lang2: str
++
++    # e.g., whisper, paraformer, zipformer
++    short_name: str = ""
++
++    # cmd is used to remove extra file from the model directory
++    cmd: str = ""
++
++    rule_fsts: str = ""
++
++    use_hr: bool = False
++
++
++# See get_2nd_models() in ./generate-asr-2pass-apk-script.py
++def get_models():
++    models = [
++        Model(
++            model_name="sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9000,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="5-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9001,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="8-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9002,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="10-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9003,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="13-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9004,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="15-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9005,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="18-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9006,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="20-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9007,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="23-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9008,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="25-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9009,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="28-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8",
++            idx=9010,
++            lang="zh_en_ko_ja_yue",
++            lang2="",
++            short_name="30-seconds-sense_voice_2024_07_17_int8",
++            use_hr=True,
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
++            popd
++            """,
++        ),
++    ]
++    return models
++
++
++def main():
++    args = get_args()
++    index = args.index
++    total = args.total
++    assert 0 <= index < total, (index, total)
++
++    all_model_list = get_models()
++
++    num_models = len(all_model_list)
++
++    num_per_runner = num_models // total
++    if num_per_runner <= 0:
++        raise ValueError(f"num_models: {num_models}, num_runners: {total}")
++
++    start = index * num_per_runner
++    end = start + num_per_runner
++
++    remaining = num_models - args.total * num_per_runner
++
++    print(f"{index}/{total}: {start}-{end}/{num_models}")
++
++    d = dict()
++    d["model_list"] = all_model_list[start:end]
++    if index < remaining:
++        s = args.total * num_per_runner + index
++        d["model_list"].append(all_model_list[s])
++        print(f"{s}/{num_models}")
++
++    filename_list = [
++        "./build-apk-qnn-vad-asr-simulate-streaming.sh",
++    ]
++    for filename in filename_list:
++        environment = jinja2.Environment()
++        if not Path(f"{filename}.in").is_file():
++            print(f"skip {filename}")
++            continue
++
++        with open(f"{filename}.in") as f:
++            s = f.read()
++        template = environment.from_string(s)
++
++        s = template.render(**d)
++        with open(filename, "w") as f:
++            print(s, file=f)
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/sherpa-onnx/csrc/macros.h b/sherpa-onnx/csrc/macros.h
+index 2788292d..0a0e1b2a 100644
+--- a/sherpa-onnx/csrc/macros.h
++++ b/sherpa-onnx/csrc/macros.h
+@@ -21,13 +21,15 @@
+ 
+ #if __ANDROID_API__ >= 8
+ #include "android/log.h"
+-#define SHERPA_ONNX_LOGE(...)                                            \
+-  do {                                                                   \
+-    fprintf(stderr, "%s:%s:%d ", __FILE__, __func__,                     \
+-            static_cast<int>(__LINE__));                                 \
+-    fprintf(stderr, ##__VA_ARGS__);                                      \
+-    fprintf(stderr, "\n");                                               \
+-    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", ##__VA_ARGS__); \
++#define SHERPA_ONNX_LOGE(...)                                                  \
++  do {                                                                         \
++    fprintf(stderr, "%s:%s:%d ", __FILE__, __func__,                           \
++            static_cast<int32_t>(__LINE__));                                   \
++    fprintf(stderr, ##__VA_ARGS__);                                            \
++    fprintf(stderr, "\n");                                                     \
++    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", "%s:%s:%d", __FILE__, \
++                        __func__, static_cast<int32_t>(__LINE__));             \
++    __android_log_print(ANDROID_LOG_WARN, "sherpa-onnx", ##__VA_ARGS__);       \
+   } while (0)
+ #elif defined(__OHOS__)
+ #define SHERPA_ONNX_LOGE(...) OH_LOG_INFO(LOG_APP, ##__VA_ARGS__)
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
+index ec1c9838..441820da 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
+@@ -132,8 +132,9 @@ class QnnModel::Impl {
+         sys_ctx_handle, static_cast<void *>(buffer.data()), buffer.size(),
+         &binary_info, &binary_info_size);
+     if (ret != QNN_SUCCESS) {
+-      SHERPA_ONNX_LOGE("Failed to get context binary info from '%s'",
+-                       binary_context_file.c_str());
++      SHERPA_ONNX_LOGE(
++          "Failed to get context binary info from '%s'. ret code is %d",
++          binary_context_file.c_str(), ret);
+ 
+       qnn_system_interface_.systemContextFree(sys_ctx_handle);
+       return false;
+diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
+index 6bc3c054..3206d7b9 100644
+--- a/sherpa-onnx/java-api/Makefile
++++ b/sherpa-onnx/java-api/Makefile
+@@ -15,6 +15,7 @@ java_files += WaveWriter.java
+ java_files += EndpointRule.java
+ java_files += EndpointConfig.java
+ java_files += FeatureConfig.java
++java_files += QnnConfig.java
+ java_files += HomophoneReplacerConfig.java
+ java_files += OnlineLMConfig.java
+ java_files += OnlineParaformerModelConfig.java
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
+index b14de19b..79529401 100644
+--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineSenseVoiceModelConfig.java
+@@ -6,11 +6,13 @@ public class OfflineSenseVoiceModelConfig {
+     private final String model;
+     private final String language;
+     private final boolean useInverseTextNormalization;
++    private final QnnConfig qnnConfig;
+ 
+     private OfflineSenseVoiceModelConfig(Builder builder) {
+         this.model = builder.model;
+         this.language = builder.language;
+         this.useInverseTextNormalization = builder.useInverseTextNormalization;
++        this.qnnConfig = builder.qnnConfig;
+     }
+ 
+     public static Builder builder() {
+@@ -29,10 +31,15 @@ public class OfflineSenseVoiceModelConfig {
+         return useInverseTextNormalization;
+     }
+ 
++    public QnnConfig getQnnConfig() {
++        return qnnConfig;
++    }
++
+     public static class Builder {
+         private String model = "";
+         private String language = "";
+         private boolean useInverseTextNormalization = true;
++        private QnnConfig qnnConfig = QnnConfig.builder().build();
+ 
+         public OfflineSenseVoiceModelConfig build() {
+             return new OfflineSenseVoiceModelConfig(this);
+@@ -52,5 +59,10 @@ public class OfflineSenseVoiceModelConfig {
+             this.useInverseTextNormalization = useInverseTextNormalization;
+             return this;
+         }
++
++        public Builder setQnnConfig(QnnConfig qnnConfig) {
++            this.qnnConfig = qnnConfig;
++            return this;
++        }
+     }
+ }
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java
+new file mode 100644
+index 00000000..c46958fe
+--- /dev/null
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/QnnConfig.java
+@@ -0,0 +1,56 @@
++// Copyright 2025 Xiaomi Corporation
++
++package com.k2fsa.sherpa.onnx;
++
++public class QnnConfig {
++    private final String backendLib;
++    private final String contextBinary;
++    private final String systemLib;
++
++    private QnnConfig(Builder builder) {
++        this.backendLib = builder.backendLib;
++        this.contextBinary = builder.contextBinary;
++        this.systemLib = builder.systemLib;
++    }
++
++    public static Builder builder() {
++        return new Builder();
++    }
++
++    public String getBackendLib() {
++        return backendLib;
++    }
++
++    public String getContextBinary() {
++        return contextBinary;
++    }
++
++    public String getSystemLib() {
++        return systemLib;
++    }
++
++    public static class Builder {
++        private String backendLib = "";
++        private String contextBinary = "";
++        private String systemLib = "";
++
++        public QnnConfig build() {
++            return new QnnConfig(this);
++        }
++
++        public Builder setBackendLib(String backendLib) {
++            this.backendLib = backendLib;
++            return this;
++        }
++
++        public Builder setContextBinary(String contextBinary) {
++            this.contextBinary = contextBinary;
++            return this;
++        }
++
++        public Builder setSystemLib(String systemLib) {
++            this.systemLib = systemLib;
++            return this;
++        }
++    }
++}
+diff --git a/sherpa-onnx/jni/CMakeLists.txt b/sherpa-onnx/jni/CMakeLists.txt
+index cff5ea6e..56cf7bd9 100644
+--- a/sherpa-onnx/jni/CMakeLists.txt
++++ b/sherpa-onnx/jni/CMakeLists.txt
+@@ -12,6 +12,7 @@ endif()
+ 
+ set(sources
+   audio-tagging.cc
++  common.cc
+   jni.cc
+   keyword-spotter.cc
+   offline-punctuation.cc
+diff --git a/sherpa-onnx/jni/common.cc b/sherpa-onnx/jni/common.cc
+new file mode 100644
+index 00000000..2de20727
+--- /dev/null
++++ b/sherpa-onnx/jni/common.cc
+@@ -0,0 +1,47 @@
++// sherpa-onnx/jni/common.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#include "sherpa-onnx/jni/common.h"
++
++#include <stdlib.h>
++
++#include <string>
++
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++/* For qnn to load libQnnHtpVxxSkel.so, e.g., libQnnHtpV81Skel.so file
++
++https://workbench.aihub.qualcomm.com/docs/hub/faq.html#why-am-i-seeing-error-1008-when-trying-to-use-htp
++ */
++void PrependAdspLibraryPath(const std::string &new_path) {
++  const char *old_path = getenv("ADSP_LIBRARY_PATH");
++  std::string updated_path;
++
++  if (old_path && !std::string(old_path).empty()) {
++    // Caution(fangjun):
++    // 1. Must use ; here, not :
++    // 2. Must use prepend, not append
++    updated_path = new_path + ";" + std::string(old_path);
++  } else {
++    updated_path = new_path;  // no old path
++  }
++
++  if (setenv("ADSP_LIBRARY_PATH", updated_path.c_str(), 1) != 0) {
++    SHERPA_ONNX_LOGE("Failed to set ADSP_LIBRARY_PATH to '%s'",
++                     updated_path.c_str());
++  } else {
++    SHERPA_ONNX_LOGE("Successfully set ADSP_LIBRARY_PATH to '%s'",
++                     updated_path.c_str());
++  }
++  /*
++You will see something like the following:
++
++Successfully set ADSP_LIBRARY_PATH to
++'/data/app/~~pHS2-9SwVjl9ma3cIKtj-g==/com.k2fsa.sherpa.onnx.simulate.streaming.asr-ejCDb8LodsnyK5cr3SvGjA==/lib/arm64;/odm/lib/rfsa/adsp;/vendor/lib/rfsa/adsp/;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp'
++
++   */
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/jni/common.h b/sherpa-onnx/jni/common.h
+index f9ae2e52..4c478515 100644
+--- a/sherpa-onnx/jni/common.h
++++ b/sherpa-onnx/jni/common.h
+@@ -193,4 +193,8 @@ inline bool ValidatePointer(JNIEnv *env, jlong ptr, const char *functionName,
+   return true;
+ }
+ 
++namespace sherpa_onnx {
++void PrependAdspLibraryPath(const std::string &new_path);
++}
++
+ #endif  // SHERPA_ONNX_JNI_COMMON_H_
+diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
+index b091670e..ed940cbf 100644
+--- a/sherpa-onnx/jni/offline-recognizer.cc
++++ b/sherpa-onnx/jni/offline-recognizer.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+ 
++#include <stdlib.h>
++
+ #include <memory>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+@@ -164,6 +166,23 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
+                             useInverseTextNormalization, sense_voice_config_cls,
+                             sense_voice_config);
+ 
++  fid = env->GetFieldID(sense_voice_config_cls, "qnnConfig",
++                        "Lcom/k2fsa/sherpa/onnx/QnnConfig;");
++  jobject qnn_config = env->GetObjectField(sense_voice_config, fid);
++  jclass qnn_config_cls = env->GetObjectClass(qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.sense_voice.qnn_config.backend_lib, backendLib,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.sense_voice.qnn_config.context_binary, contextBinary,
++      qnn_config_cls, qnn_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(
++      ans.model_config.sense_voice.qnn_config.system_lib, systemLib,
++      qnn_config_cls, qnn_config);
++
+   // nemo
+   fid = env->GetFieldID(
+       model_config_cls, "nemo",
+@@ -313,7 +332,7 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_newFromFile(JNIEnv *env,
+     }
+   }
+ 
+-  if (!config.Validate()) {
++  if (config.model_config.provider != "qnn" && !config.Validate()) {
+     SHERPA_ONNX_LOGE("Errors found in config!");
+     return 0;
+   }
+@@ -461,3 +480,13 @@ Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_getResult(JNIEnv *env,
+ 
+   return obj_arr;
+ }
++
++SHERPA_ONNX_EXTERN_C
++JNIEXPORT void JNICALL
++Java_com_k2fsa_sherpa_onnx_OfflineRecognizer_prependAdspLibraryPath(
++    JNIEnv *env, jclass /*cls*/, jstring new_path) {
++  const char *p = env->GetStringUTFChars(new_path, nullptr);
++  sherpa_onnx::PrependAdspLibraryPath(p);
++
++  env->ReleaseStringUTFChars(new_path, p);
++}
+diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+index 1e94f4f9..d564f9d3 100644
+--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
++++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+@@ -76,6 +76,7 @@ data class OfflineSenseVoiceModelConfig(
+     var model: String = "",
+     var language: String = "",
+     var useInverseTextNormalization: Boolean = true,
++    var qnnConfig: QnnConfig = QnnConfig(),
+ )
+ 
+ data class OfflineModelConfig(
+@@ -191,6 +192,9 @@ class OfflineRecognizer(
+         init {
+             System.loadLibrary("sherpa-onnx-jni")
+         }
++
++        @JvmStatic
++        external fun prependAdspLibraryPath(newPath: String) // for qnn
+     }
+ }
+ 
+@@ -746,6 +750,186 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                 tokens = "$modelDir/tokens.txt",
+             )
+         }
++
++        9000 -> {
++            val modelDir = "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                      // Please copy libQnnHtp.so and libQnnSystem.so to jniLibs/arm64-v8a by yourself
++                      //
++                      // model.bin is created in the first run and is used from the second run
++                      // to speed up the initialization
++                      backendLib = "libQnnHtp.so",
++                      systemLib = "libQnnSystem.so",
++                      contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9001 -> {
++            val modelDir = "sherpa-onnx-qnn-8-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9002 -> {
++            val modelDir = "sherpa-onnx-qnn-10-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9003 -> {
++            val modelDir = "sherpa-onnx-qnn-13-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9004 -> {
++            val modelDir = "sherpa-onnx-qnn-15-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9005 -> {
++            val modelDir = "sherpa-onnx-qnn-18-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9006 -> {
++            val modelDir = "sherpa-onnx-qnn-20-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9007 -> {
++            val modelDir = "sherpa-onnx-qnn-23-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9008 -> {
++            val modelDir = "sherpa-onnx-qnn-25-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9009 -> {
++            val modelDir = "sherpa-onnx-qnn-28-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
++
++        9010 -> {
++            val modelDir = "sherpa-onnx-qnn-30-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"
++            return OfflineModelConfig(
++                provider = "qnn",
++                senseVoice = OfflineSenseVoiceModelConfig(
++                    model = "$modelDir/libmodel.so",
++                    qnnConfig = QnnConfig(
++                        backendLib = "libQnnHtp.so",
++                        systemLib = "libQnnSystem.so",
++                        contextBinary = "$modelDir/model.bin",
++                    ),
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
+     }
+     return null
+ }
+diff --git a/sherpa-onnx/kotlin-api/QnnConfig.kt b/sherpa-onnx/kotlin-api/QnnConfig.kt
+new file mode 100644
+index 00000000..ea7aa511
+--- /dev/null
++++ b/sherpa-onnx/kotlin-api/QnnConfig.kt
+@@ -0,0 +1,7 @@
++package com.k2fsa.sherpa.onnx
++
++data class QnnConfig(
++    var backendLib: String = "",
++    var contextBinary: String = "",
++    var systemLib: String = "",
++)
+
+commit 2fcde7d3c63f1df2d8b59d70cd5dd268c14f7a97
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Nov 19 18:21:16 2025 +0800
+
+    Support hotwords with byte level bpe (#2802)
+
+diff --git a/.gitignore b/.gitignore
+index 37337ac3..768fc466 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -157,3 +157,4 @@ am.mvn
+ config.yaml
+ configuration.json
+ sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++sherpa-onnx-paraformer-zh-int8-2025-10-07
+diff --git a/scripts/bbpe/generate_bbpe_table.py b/scripts/bbpe/generate_bbpe_table.py
+index 0b520424..e164c420 100755
+--- a/scripts/bbpe/generate_bbpe_table.py
++++ b/scripts/bbpe/generate_bbpe_table.py
+@@ -46,8 +46,10 @@ def main():
+     s += "      "
+     for i, (k, v) in enumerate(BCHAR_TO_BYTE.items()):
+         s += "{"
+-        if k in ["\\", '"']:
+-            s += f'"\{k}", {v}'
++        if k == "\\":
++            s += f'"\\\\", {v}'
++        elif k == '"':
++            s += f'"\\"", {v}'
+         else:
+             s += f'"{k}", {v}'
+         s += "}, "
+@@ -59,6 +61,29 @@ def main():
+     s += "  return table\n;"
+     s += "}\n"
+ 
++    s += "\n"
++    s += "const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token() {\n"
++    s += "  static const std::unordered_map<uint8_t, std::string> table = {\n"
++
++    s += "      "
++    for i, (k, v) in enumerate(BCHAR_TO_BYTE.items()):
++        s += "{"
++        if k == "\\":
++            s += f'{v}, "\\\\"'
++        elif k == '"':
++            s += f'{v}, "\\""'
++        else:
++            s += f'{v}, "{k}"'
++
++        s += "}, "
++        if i > 0 and i % 7 == 0:
++            s += "\n"
++            s += "      "
++    s += "};\n"
++    s += "\n"
++    s += "  return table\n;"
++    s += "}\n"
++
+     with open("bbpe.cc", "w", encoding="utf-8") as f:
+         f.write(s)
+ 
+diff --git a/sherpa-onnx/csrc/bbpe.cc b/sherpa-onnx/csrc/bbpe.cc
+index 1aa67ffa..de523bea 100644
+--- a/sherpa-onnx/csrc/bbpe.cc
++++ b/sherpa-onnx/csrc/bbpe.cc
+@@ -1,6 +1,6 @@
+ // sherpa-onnx/csrc/bbpe.cc
+ //
+-// Copyright (c)  2024  Xiaomi Corporation
++// Copyright (c)  2024 Xiaomi Corporation
+ 
+ // Auto-generated! DO NOT EDIT
+ 
+@@ -59,3 +59,53 @@ const std::unordered_map<std::string, uint8_t> &GetByteBpeTable() {
+ 
+   return table;
+ }
++
++const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token() {
++  static const std::unordered_map<uint8_t, std::string> table = {
++      {0, ""},   {1, ""},   {2, ""},   {3, ""},   {4, ""},   {5, ""},
++      {6, ""},   {7, ""},   {8, ""},   {9, ""},   {10, ""},  {11, ""},
++      {12, ""},  {13, ""},  {14, ""},  {15, ""},  {16, ""},  {17, ""},
++      {18, ""},  {19, ""},  {20, ""},  {21, ""},  {22, ""},  {23, ""},
++      {24, ""},  {25, ""},  {26, ""},  {27, ""},  {28, ""},  {29, ""},
++      {30, ""},  {31, ""},  {32, " "},  {33, "!"},  {34, "\""}, {35, "#"},
++      {36, "$"},  {37, "%"},  {38, "&"},  {39, "'"},  {40, "("},  {41, ")"},
++      {42, "*"},  {43, "+"},  {44, ","},  {45, "-"},  {46, "."},  {47, "/"},
++      {48, "0"},  {49, "1"},  {50, "2"},  {51, "3"},  {52, "4"},  {53, "5"},
++      {54, "6"},  {55, "7"},  {56, "8"},  {57, "9"},  {58, ":"},  {59, ";"},
++      {60, "<"},  {61, "="},  {62, ">"},  {63, "?"},  {64, "@"},  {65, "A"},
++      {66, "B"},  {67, "C"},  {68, "D"},  {69, "E"},  {70, "F"},  {71, "G"},
++      {72, "H"},  {73, "I"},  {74, "J"},  {75, "K"},  {76, "L"},  {77, "M"},
++      {78, "N"},  {79, "O"},  {80, "P"},  {81, "Q"},  {82, "R"},  {83, "S"},
++      {84, "T"},  {85, "U"},  {86, "V"},  {87, "W"},  {88, "X"},  {89, "Y"},
++      {90, "Z"},  {91, "["},  {92, "\\"}, {93, "]"},  {94, "^"},  {95, "_"},
++      {96, "`"},  {97, "a"},  {98, "b"},  {99, "c"},  {100, "d"}, {101, "e"},
++      {102, "f"}, {103, "g"}, {104, "h"}, {105, "i"}, {106, "j"}, {107, "k"},
++      {108, "l"}, {109, "m"}, {110, "n"}, {111, "o"}, {112, "p"}, {113, "q"},
++      {114, "r"}, {115, "s"}, {116, "t"}, {117, "u"}, {118, "v"}, {119, "w"},
++      {120, "x"}, {121, "y"}, {122, "z"}, {123, "{"}, {124, "|"}, {125, "}"},
++      {126, "~"}, {127, ""}, {128, ""}, {129, ""}, {130, ""}, {131, ""},
++      {132, ""}, {133, ""}, {134, ""}, {135, ""}, {136, ""}, {137, ""},
++      {138, ""}, {139, ""}, {140, ""}, {141, ""}, {142, ""}, {143, ""},
++      {144, ""}, {145, ""}, {146, ""}, {147, ""}, {148, ""}, {149, ""},
++      {150, ""}, {151, ""}, {152, ""}, {153, ""}, {154, ""}, {155, ""},
++      {156, ""}, {157, ""}, {158, ""}, {159, ""}, {160, ""}, {161, ""},
++      {162, ""}, {163, ""}, {164, ""}, {165, ""}, {166, ""}, {167, ""},
++      {168, ""}, {169, ""}, {170, ""}, {171, ""}, {172, ""}, {173, ""},
++      {174, ""}, {175, ""}, {176, ""}, {177, ""}, {178, ""}, {179, ""},
++      {180, ""}, {181, ""}, {182, ""}, {183, ""}, {184, ""}, {185, ""},
++      {186, ""}, {187, ""}, {188, ""}, {189, ""}, {190, ""}, {191, ""},
++      {192, ""}, {193, ""}, {194, ""}, {195, ""}, {196, ""}, {197, ""},
++      {198, ""}, {199, ""}, {200, ""}, {201, ""}, {202, ""}, {203, ""},
++      {204, ""}, {205, ""}, {206, ""}, {207, ""}, {208, ""}, {209, ""},
++      {210, ""}, {211, ""}, {212, ""}, {213, ""}, {214, ""}, {215, ""},
++      {216, ""}, {217, ""}, {218, ""}, {219, ""}, {220, ""}, {221, ""},
++      {222, ""}, {223, ""}, {224, ""}, {225, ""}, {226, ""}, {227, ""},
++      {228, ""}, {229, ""}, {230, ""}, {231, ""}, {232, ""}, {233, ""},
++      {234, ""}, {235, ""}, {236, ""}, {237, ""}, {238, ""}, {239, ""},
++      {240, ""}, {241, ""}, {242, ""}, {243, ""}, {244, ""}, {245, ""},
++      {246, ""}, {247, ""}, {248, ""}, {249, ""}, {250, ""}, {251, ""},
++      {252, ""}, {253, ""}, {254, ""}, {255, ""},
++  };
++
++  return table;
++}
+diff --git a/sherpa-onnx/csrc/bbpe.h b/sherpa-onnx/csrc/bbpe.h
+index 0b6a4ecf..7325077c 100644
+--- a/sherpa-onnx/csrc/bbpe.h
++++ b/sherpa-onnx/csrc/bbpe.h
+@@ -13,4 +13,6 @@
+ // https://github.com/k2-fsa/icefall/blob/master/icefall/byte_utils.py#L280
+ const std::unordered_map<std::string, uint8_t> &GetByteBpeTable();
+ 
++const std::unordered_map<uint8_t, std::string> &GetByteBpeTableId2Token();
++
+ #endif  // SHERPA_ONNX_CSRC_BBPE_H_
+diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
+index 0171c379..7d536750 100644
+--- a/sherpa-onnx/csrc/offline-model-config.cc
++++ b/sherpa-onnx/csrc/offline-model-config.cc
+@@ -45,17 +45,18 @@ void OfflineModelConfig::Register(ParseOptions *po) {
+                "Valid values are: transducer, paraformer, nemo_ctc, whisper, "
+                "tdnn, zipformer2_ctc, telespeech_ctc, fire_red_asr."
+                "All other values lead to loading the model twice.");
+-  po->Register("modeling-unit", &modeling_unit,
+-               "The modeling unit of the model, commonly used units are bpe, "
+-               "cjkchar, cjkchar+bpe, etc. Currently, it is needed only when "
+-               "hotwords are provided, we need it to encode the hotwords into "
+-               "token sequence.");
++  po->Register(
++      "modeling-unit", &modeling_unit,
++      "The modeling unit of the model, commonly used units are bpe, "
++      "bbpe, cjkchar, cjkchar+bpe, etc. Currently, it is needed only when "
++      "hotwords are provided, we need it to encode the hotwords into "
++      "token sequence.");
+   po->Register("bpe-vocab", &bpe_vocab,
+                "The vocabulary generated by google's sentencepiece program. "
+                "It is a file has two columns, one is the token, the other is "
+                "the log probability, you can get it from the directory where "
+                "your bpe model is generated. Only used when hotwords provided "
+-               "and the modeling unit is bpe or cjkchar+bpe");
++               "and the modeling unit is bpe, bbpe, or cjkchar+bpe");
+ }
+ 
+ bool OfflineModelConfig::Validate() const {
+@@ -98,7 +99,8 @@ bool OfflineModelConfig::Validate() const {
+   }
+ 
+   if (!modeling_unit.empty() &&
+-      (modeling_unit == "bpe" || modeling_unit == "cjkchar+bpe")) {
++      (modeling_unit == "bpe" || modeling_unit == "cjkchar+bpe" ||
++       modeling_unit == "bbpe")) {
+     if (!FileExists(bpe_vocab)) {
+       SHERPA_ONNX_LOGE("bpe_vocab: '%s' does not exist", bpe_vocab.c_str());
+       return false;
+diff --git a/sherpa-onnx/csrc/utils.cc b/sherpa-onnx/csrc/utils.cc
+index f40b6769..63190697 100644
+--- a/sherpa-onnx/csrc/utils.cc
++++ b/sherpa-onnx/csrc/utils.cc
+@@ -11,6 +11,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "sherpa-onnx/csrc/bbpe.h"
+ #include "sherpa-onnx/csrc/log.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+@@ -152,6 +153,23 @@ bool EncodeHotwords(std::istream &is, const std::string &modeling_unit,
+         for (const auto &bpe : bpes) {
+           oss << " " << bpe;
+         }
++      } else if (modeling_unit == "bbpe") {
++        std::vector<std::string> bpes;
++
++        const auto &id2token = GetByteBpeTableId2Token();
++        std::string tokens;
++        for (size_t i = 0; i < word.length(); ++i) {
++          uint8_t byte = static_cast<uint8_t>(word[i]);
++          tokens += id2token.at(byte);
++          if ((i + 1) % 3 == 0 && (i + 1) < word.length()) {
++            tokens += " ";
++          }
++        }
++
++        bpe_encoder->Encode(tokens, &bpes);
++        for (const auto &bpe : bpes) {
++          oss << " " << bpe;
++        }
+       } else {
+         if (modeling_unit != "cjkchar+bpe") {
+           SHERPA_ONNX_LOGE(
+diff --git a/sherpa-onnx/csrc/utils.h b/sherpa-onnx/csrc/utils.h
+index a9d59e8a..a2d4dbdd 100644
+--- a/sherpa-onnx/csrc/utils.h
++++ b/sherpa-onnx/csrc/utils.h
+@@ -28,7 +28,7 @@ namespace sherpa_onnx {
+  */
+ bool EncodeHotwords(std::istream &is, const std::string &modeling_unit,
+                     const SymbolTable &symbol_table,
+-                    const ssentencepiece::Ssentencepiece *bpe_encoder_,
++                    const ssentencepiece::Ssentencepiece *bpe_encoder,
+                     std::vector<std::vector<int32_t>> *hotwords_id,
+                     std::vector<float> *boost_scores);
+ 
+
+commit 3f13f7a3c00719a75c50593d5d3942ffba6e6552
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 18 14:06:50 2025 +0800
+
+    Export models for CANN toolkit 7.0 (#2795)
+
+diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+index 4f10df79..f71752b0 100644
+--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
++++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - ascend-910b3
++      - cann-7.0
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -21,6 +21,16 @@ jobs:
+       matrix:
+         include:
+           # ===== Ascend 910B =====
++          - soc_version: "910B"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "FunASR"
++            cann: "7.0"
++
++          - soc_version: "910B"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "WSYue-ASR"
++            cann: "7.0"
++
+           - soc_version: "910B"
+             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+             framework: "FunASR"
+@@ -42,6 +52,16 @@ jobs:
+             cann: "8.2"
+ 
+           # ===== Ascend 910B2 =====
++          - soc_version: "910B2"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "FunASR"
++            cann: "7.0"
++
++          - soc_version: "910B2"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "WSYue-ASR"
++            cann: "7.0"
++
+           - soc_version: "910B2"
+             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+             framework: "FunASR"
+@@ -63,6 +83,16 @@ jobs:
+             cann: "8.2"
+ 
+           # ===== Ascend 910B3 =====
++          - soc_version: "910B3"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "FunASR"
++            cann: "7.0"
++
++          - soc_version: "910B3"
++            image: "quay.io/ascend/cann:7.0.1.beta1-910b-ubuntu22.04-py3.8"
++            framework: "WSYue-ASR"
++            cann: "7.0"
++
+           - soc_version: "910B3"
+             image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
+             framework: "FunASR"
+@@ -85,6 +115,16 @@ jobs:
+ 
+ 
+           # ===== Ascend 310 =====
++          - soc_version: "310P3"
++            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
++            framework: "FunASR"
++            cann: "7.0"
++
++          - soc_version: "310P3"
++            image: "quay.io/ascend/cann:7.0.1-310p-ubuntu22.04-py3.9"
++            framework: "WSYue-ASR"
++            cann: "7.0"
++
+           - soc_version: "310P3"
+             # image: "gpustack/ascendai-cann:8.0.RC2.alpha003-310p-ubuntu20.04-py3.9"
+             image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+@@ -145,6 +185,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           echo "CANN environment:"
+           which atc || echo "atc not found"
+           atc --help
+@@ -189,6 +232,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           soc_version=${{ matrix.soc_version }}
+           cann=${{ matrix.cann }}
+ 
+@@ -256,6 +302,9 @@ jobs:
+           source /usr/local/Ascend/ascend-toolkit/set_env.sh
+           export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/linux/x86_64:$LD_LIBRARY_PATH
+ 
++          # for cann 7.0.0
++          export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/x86_64:$LD_LIBRARY_PATH
++
+           soc_version=${{ matrix.soc_version }}
+           cann=${{ matrix.cann }}
+ 
+
+commit 6659d47d05ccb246ba1ea340360c096664403cde
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 17 18:56:13 2025 +0800
+
+    Add C++ QNN support for SenseVoice (#2793)
+
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 20190dc7..4bc8cb40 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -304,6 +304,10 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+   add_definitions(-DSHERPA_ONNX_ENABLE_RKNN=1)
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_QNN)
++  add_definitions(-DSHERPA_ONNX_ENABLE_QNN=1)
++endif()
++
+ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+   set(ASCEND_TOOLKIT_HOME)
+   if(NOT DEFINED ENV{ASCEND_TOOLKIT_HOME})
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
+index 7cd62e59..0d80f483 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/utils.cc
+@@ -3,6 +3,7 @@
+ #include <memory>
+ #include <sstream>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "macros.h"  // NOLINT
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index a9a09286..58bafbab 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -192,7 +192,7 @@ if(SHERPA_ONNX_ENABLE_RKNN)
+ 
+ endif()
+ 
+-if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU)
++if(SHERPA_ONNX_ENABLE_RKNN OR SHERPA_ONNX_ENABLE_ASCEND_NPU OR SHERPA_ONNX_ENABLE_QNN)
+   list(APPEND sources
+     ./rknn/offline-ctc-greedy-search-decoder-rknn.cc
+   )
+@@ -211,6 +211,7 @@ if(SHERPA_ONNX_ENABLE_QNN)
+     ./qnn/qnn-backend.cc
+     ./qnn/qnn-model.cc
+     ./qnn/utils.cc
++    ./qnn/offline-sense-voice-model-qnn.cc
+   )
+ endif()
+ 
+diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+index 54da97b3..739b752b 100644
+--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+@@ -426,6 +426,11 @@ OfflineParaformerModelAscend::OfflineParaformerModelAscend(
+     const OfflineModelConfig &config)
+     : impl_(std::make_unique<Impl>(config)) {}
+ 
++template <typename Manager>
++OfflineParaformerModelAscend::OfflineParaformerModelAscend(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
+ OfflineParaformerModelAscend::~OfflineParaformerModelAscend() = default;
+ 
+ std::vector<float> OfflineParaformerModelAscend::Run(
+diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+index 9192f0ae..a7409206 100644
+--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+@@ -216,6 +216,11 @@ OfflineSenseVoiceModelAscend::OfflineSenseVoiceModelAscend(
+     const OfflineModelConfig &config)
+     : impl_(std::make_unique<Impl>(config)) {}
+ 
++template <typename Manager>
++OfflineSenseVoiceModelAscend::OfflineSenseVoiceModelAscend(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
+ OfflineSenseVoiceModelAscend::~OfflineSenseVoiceModelAscend() = default;
+ 
+ std::vector<float> OfflineSenseVoiceModelAscend::Run(
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 3a675fc1..ff4e055d 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -45,6 +45,10 @@
+ #include "sherpa-onnx/csrc/ascend/offline-recognizer-sense-voice-ascend-impl.h"
+ #endif
+ 
++#if SHERPA_ONNX_ENABLE_QNN
++#include "sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h"
++#endif
++
+ namespace sherpa_onnx {
+ 
+ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+@@ -54,7 +58,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     if (config.model_config.sense_voice.model.empty()) {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice models are currently supported "
+-          "by rknn for non-streaming ASR. Fallback to CPU");
++          "by rknn for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
+     } else if (!config.model_config.sense_voice.model.empty()) {
+       return std::make_unique<OfflineRecognizerSenseVoiceRknnImpl>(config);
+     }
+@@ -77,7 +83,9 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     } else {
+       SHERPA_ONNX_LOGE(
+           "Only SenseVoice and Paraformer models are currently supported "
+-          "by Ascend NPU for non-streaming ASR. Fallback to CPU");
++          "by Ascend NPU for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
+     }
+ #else
+     SHERPA_ONNX_LOGE(
+@@ -89,6 +97,27 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #endif
+   }
+ 
++  if (config.model_config.provider == "qnn") {
++#if SHERPA_ONNX_ENABLE_QNN
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(config);
++    } else {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported "
++          "by qnn for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    }
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
++        "you want to use qnn. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
+   if (!config.model_config.sense_voice.model.empty()) {
+     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(config);
+   }
+@@ -175,7 +204,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     model_filename = config.model_config.whisper.encoder;
+   } else {
+     SHERPA_ONNX_LOGE("Please provide a model");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+ 
+   auto buf = ReadFile(model_filename);
+@@ -228,7 +257,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+         "https://github.com/Tele-AI/TeleSpeech-ASR"
+         "\n"
+         "\n");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+ 
+   if (model_type == "conformer" || model_type == "zipformer" ||
+@@ -274,7 +303,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       " - TeleSpeech CTC models\n",
+       model_type.c_str());
+ 
+-  exit(-1);
++  SHERPA_ONNX_EXIT(-1);
+ }
+ 
+ template <typename Manager>
+@@ -322,6 +351,27 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+ #endif
+   }
+ 
++  if (config.model_config.provider == "qnn") {
++#if SHERPA_ONNX_ENABLE_QNN
++    if (!config.model_config.sense_voice.model.empty()) {
++      return std::make_unique<OfflineRecognizerSenseVoiceQnnImpl>(mgr, config);
++    } else {
++      SHERPA_ONNX_LOGE(
++          "Only SenseVoice models are currently supported "
++          "by qnn for non-streaming ASR.");
++      SHERPA_ONNX_EXIT(-1);
++      return nullptr;
++    }
++#else
++    SHERPA_ONNX_LOGE(
++        "Please rebuild sherpa-onnx with -DSHERPA_ONNX_ENABLE_QNN=ON if "
++        "you want to use qnn. See also "
++        "https://k2-fsa.github.io/sherpa/onnx/qnn/install.html");
++    SHERPA_ONNX_EXIT(-1);
++    return nullptr;
++#endif
++  }
++
+   if (!config.model_config.sense_voice.model.empty()) {
+     return std::make_unique<OfflineRecognizerSenseVoiceImpl>(mgr, config);
+   }
+@@ -406,7 +456,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+     model_filename = config.model_config.whisper.encoder;
+   } else {
+     SHERPA_ONNX_LOGE("Please provide a model");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+ 
+   auto buf = ReadFile(mgr, model_filename);
+@@ -459,7 +509,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+         "https://github.com/Tele-AI/TeleSpeech-ASR"
+         "\n"
+         "\n");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+ 
+   if (model_type == "conformer" || model_type == "zipformer" ||
+@@ -505,7 +555,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       " - TeleSpeech CTC models\n",
+       model_type.c_str());
+ 
+-  exit(-1);
++  SHERPA_ONNX_EXIT(-1);
+ }
+ 
+ OfflineRecognizerImpl::OfflineRecognizerImpl(
+diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
+index 8a02fc16..cc819bf4 100644
+--- a/sherpa-onnx/csrc/offline-recognizer.cc
++++ b/sherpa-onnx/csrc/offline-recognizer.cc
+@@ -139,10 +139,6 @@ std::string OfflineRecognizerConfig::ToString() const {
+   os << "lm_config=" << lm_config.ToString() << ", ";
+   os << "ctc_fst_decoder_config=" << ctc_fst_decoder_config.ToString() << ", ";
+ 
+-  if (!qnn_config.backend_lib.empty()) {
+-    os << "qnn_config=" << qnn_config.ToString() << ", ";
+-  }
+-
+   os << "decoding_method=\"" << decoding_method << "\", ";
+   os << "max_active_paths=" << max_active_paths << ", ";
+   os << "hotwords_file=\"" << hotwords_file << "\", ";
+diff --git a/sherpa-onnx/csrc/offline-recognizer.h b/sherpa-onnx/csrc/offline-recognizer.h
+index ae2c23a4..1fcc1016 100644
+--- a/sherpa-onnx/csrc/offline-recognizer.h
++++ b/sherpa-onnx/csrc/offline-recognizer.h
+@@ -17,7 +17,6 @@
+ #include "sherpa-onnx/csrc/offline-stream.h"
+ #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+-#include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -28,7 +27,6 @@ struct OfflineRecognizerConfig {
+   OfflineModelConfig model_config;
+   OfflineLMConfig lm_config;
+   OfflineCtcFstDecoderConfig ctc_fst_decoder_config;
+-  QnnConfig qnn_config;
+ 
+   std::string decoding_method = "greedy_search";
+   int32_t max_active_paths = 4;
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+index cc18a11a..e38dfcbb 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+@@ -8,6 +8,7 @@
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/text-utils.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -20,6 +21,8 @@ void OfflineSenseVoiceModelConfig::Register(ParseOptions *po) {
+   po->Register(
+       "sense-voice-use-itn", &use_itn,
+       "True to enable inverse text normalization. False to disable it.");
++
++  qnn_config.Register(po);
+ }
+ 
+ bool OfflineSenseVoiceModelConfig::Validate() const {
+@@ -40,6 +43,10 @@ bool OfflineSenseVoiceModelConfig::Validate() const {
+     }
+   }
+ 
++  if (EndsWith(model, ".so") || EndsWith(model, ".bin")) {
++    return qnn_config.Validate();
++  }
++
+   return true;
+ }
+ 
+@@ -48,6 +55,11 @@ std::string OfflineSenseVoiceModelConfig::ToString() const {
+ 
+   os << "OfflineSenseVoiceModelConfig(";
+   os << "model=\"" << model << "\", ";
++
++  if (!qnn_config.backend_lib.empty()) {
++    os << "qnn_config=" << qnn_config.ToString() << ", ";
++  }
++
+   os << "language=\"" << language << "\", ";
+   os << "use_itn=" << (use_itn ? "True" : "False") << ")";
+ 
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.h b/sherpa-onnx/csrc/offline-sense-voice-model-config.h
+index f19e959e..6f5fcf2f 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.h
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.h
+@@ -7,6 +7,7 @@
+ #include <string>
+ 
+ #include "sherpa-onnx/csrc/parse-options.h"
++#include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -22,6 +23,8 @@ struct OfflineSenseVoiceModelConfig {
+   // false to not use inverse text normalization
+   bool use_itn = false;
+ 
++  QnnConfig qnn_config;
++
+   OfflineSenseVoiceModelConfig() = default;
+   OfflineSenseVoiceModelConfig(const std::string &model,
+                                const std::string &language, bool use_itn)
+diff --git a/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h b/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+new file mode 100644
+index 00000000..7c793ed9
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
+@@ -0,0 +1,138 @@
++// sherpa-onnx/csrc/qnn/offline-recognizer-sense-voice-qnn-impl.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
++#define SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
++
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-recognizer-impl.h"
++#include "sherpa-onnx/csrc/offline-recognizer.h"
++#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
++#include "sherpa-onnx/csrc/rknn/offline-ctc-greedy-search-decoder-rknn.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++
++namespace sherpa_onnx {
++
++// defined in ../offline-recognizer-sense-voice-impl.h
++OfflineRecognitionResult ConvertSenseVoiceResult(
++    const OfflineCtcDecoderResult &src, const SymbolTable &sym_table,
++    int32_t frame_shift_ms, int32_t subsampling_factor);
++
++class OfflineRecognizerSenseVoiceQnnImpl : public OfflineRecognizerImpl {
++ public:
++  explicit OfflineRecognizerSenseVoiceQnnImpl(
++      const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(config),
++        config_(config),
++        symbol_table_(config_.model_config.tokens),
++        model_(
++            std::make_unique<OfflineSenseVoiceModelQnn>(config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  template <typename Manager>
++  OfflineRecognizerSenseVoiceQnnImpl(Manager *mgr,
++                                     const OfflineRecognizerConfig &config)
++      : OfflineRecognizerImpl(mgr, config),
++        config_(config),
++        symbol_table_(mgr, config_.model_config.tokens),
++        model_(std::make_unique<OfflineSenseVoiceModelQnn>(
++            mgr, config.model_config)) {
++    const auto &meta_data = model_->GetModelMetadata();
++    if (config.decoding_method == "greedy_search") {
++      decoder_ = std::make_unique<OfflineCtcGreedySearchDecoderRknn>(
++          meta_data.blank_id);
++    } else {
++      SHERPA_ONNX_LOGE("Only greedy_search is supported at present. Given %s",
++                       config.decoding_method.c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitFeatConfig();
++  }
++
++  std::unique_ptr<OfflineStream> CreateStream() const override {
++    return std::make_unique<OfflineStream>(config_.feat_config);
++  }
++
++  void DecodeStreams(OfflineStream **ss, int32_t n) const override {
++    for (int32_t i = 0; i < n; ++i) {
++      DecodeOneStream(ss[i]);
++    }
++  }
++
++  OfflineRecognizerConfig GetConfig() const override { return config_; }
++
++ private:
++  void InitFeatConfig() {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    config_.feat_config.normalize_samples = meta_data.normalize_samples;
++    config_.feat_config.window_type = "hamming";
++    config_.feat_config.high_freq = 0;
++    config_.feat_config.snip_edges = true;
++  }
++
++  void DecodeOneStream(OfflineStream *s) const {
++    const auto &meta_data = model_->GetModelMetadata();
++
++    std::vector<float> f = s->GetFrames();
++
++    int32_t language = 0;
++    if (config_.model_config.sense_voice.language.empty()) {
++      language = 0;
++    } else if (meta_data.lang2id.count(
++                   config_.model_config.sense_voice.language)) {
++      language =
++          meta_data.lang2id.at(config_.model_config.sense_voice.language);
++    } else {
++      SHERPA_ONNX_LOGE("Unknown language: %s. Use 0 instead.",
++                       config_.model_config.sense_voice.language.c_str());
++    }
++
++    int32_t text_norm = config_.model_config.sense_voice.use_itn
++                            ? meta_data.with_itn_id
++                            : meta_data.without_itn_id;
++
++    std::vector<float> logits = model_->Run(std::move(f), language, text_norm);
++    int32_t num_out_frames = logits.size() / meta_data.vocab_size;
++
++    auto result =
++        decoder_->Decode(logits.data(), num_out_frames, meta_data.vocab_size);
++
++    int32_t frame_shift_ms = 10;
++    int32_t subsampling_factor = meta_data.window_shift;
++    auto r = ConvertSenseVoiceResult(result, symbol_table_, frame_shift_ms,
++                                     subsampling_factor);
++
++    r.text = ApplyInverseTextNormalization(std::move(r.text));
++    r.text = ApplyHomophoneReplacer(std::move(r.text));
++    s->SetResult(r);
++  }
++
++ private:
++  OfflineRecognizerConfig config_;
++  SymbolTable symbol_table_;
++  std::unique_ptr<OfflineSenseVoiceModelQnn> model_;
++  std::unique_ptr<OfflineCtcGreedySearchDecoderRknn> decoder_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_RECOGNIZER_SENSE_VOICE_QNN_IMPL_H_
+diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+new file mode 100644
+index 00000000..b628dc20
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
+@@ -0,0 +1,288 @@
++// sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h"
++
++#include <algorithm>
++#include <array>
++#include <memory>
++#include <mutex>  // NOLINT
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/qnn/macros.h"
++#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
++#include "sherpa-onnx/csrc/qnn/qnn-model.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelQnn::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config) : config_(config) {
++    backend_ = std::make_unique<QnnBackend>(
++        config.sense_voice.qnn_config.backend_lib, config_.debug);
++
++    const auto &context_binary = config_.sense_voice.qnn_config.context_binary;
++
++    if (context_binary.empty()) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from model lib since context binary is not given");
++      }
++
++      InitFromModelLib();
++
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Skip generating context binary since you don't provide a path to "
++            "save it");
++      }
++
++    } else if (!FileExists(context_binary)) {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE(
++            "Init from model lib since context binary '%s' does not exist",
++            context_binary.c_str());
++      }
++
++      InitFromModelLib();
++
++      CreateContextBinary();
++    } else {
++      if (config_.debug) {
++        SHERPA_ONNX_LOGE("Init from context binary '%s'",
++                         context_binary.c_str());
++      }
++      InitFromContextBinary();
++    }
++
++    PostInit();
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config) : config_(config) {
++    SHERPA_ONNX_LOGE(
++        "Please copy all files from assets to SD card and set assetManager to "
++        "null");
++    SHERPA_ONNX_EXIT(-1);
++  }
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const {
++    return meta_data_;
++  }
++
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) {
++    std::lock_guard<std::mutex> lock(mutex_);
++
++    features = ApplyLFR(std::move(features));
++
++    int32_t num_frames = features.size() / feat_dim_;
++
++    model_->SetInputTensorData("x", features.data(), features.size());
++
++    std::array<int32_t, 4> prompt = {language, 1, 2, text_norm};
++    model_->SetInputTensorData("prompt", prompt.data(), prompt.size());
++
++    model_->Run();
++
++    return model_->GetOutputTensorData("logits");
++  }
++
++ private:
++  void InitFromModelLib() {
++    backend_->InitContext();
++
++    model_ = std::make_unique<QnnModel>(config_.sense_voice.model,
++                                        backend_.get(), config_.debug);
++  }
++
++  void InitFromContextBinary() {
++    model_ = std::make_unique<QnnModel>(
++        config_.sense_voice.qnn_config.context_binary,
++        config_.sense_voice.qnn_config.system_lib, backend_.get(),
++        BinaryContextTag{}, config_.debug);
++  }
++
++  void CreateContextBinary() {
++    const auto &context_binary = config_.sense_voice.qnn_config.context_binary;
++
++    if (config_.debug) {
++      SHERPA_ONNX_LOGE("Creating context binary '%s'.", context_binary.c_str());
++    }
++
++    bool ok = model_->SaveBinaryContext(context_binary);
++
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to save context binary to '%s'",
++                       context_binary.c_str());
++    }
++
++    if (config_.debug && ok) {
++      SHERPA_ONNX_LOGE("Saved context binary to '%s'.", context_binary.c_str());
++      SHERPA_ONNX_LOGE(
++          "It should be super fast the next time you init the system.");
++      SHERPA_ONNX_LOGE("Remember to also provide libQnnSystem.so.");
++    }
++  }
++
++  void PostInit() { CheckModel(); }
++
++  void CheckModel() {
++    const auto &input_tensor_names = model_->InputTensorNames();
++    if (input_tensor_names.size() != 2) {
++      SHERPA_ONNX_LOGE("Expect two input tensors. Actual %d",
++                       static_cast<int32_t>(input_tensor_names.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[0] != "x") {
++      SHERPA_ONNX_LOGE("The 1st input should be x, actual '%s'",
++                       input_tensor_names[0].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (input_tensor_names[1] != "prompt") {
++      SHERPA_ONNX_LOGE("The 2nd input should be prompt, actual '%s'",
++                       input_tensor_names[1].c_str());
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> x_shape = model_->TensorShape(input_tensor_names[0]);
++    if (x_shape.size() != 3) {
++      SHERPA_ONNX_LOGE("The 1st input should be 3-d, actual '%d'",
++                       static_cast<int32_t>(x_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[0] != 1) {
++      SHERPA_ONNX_LOGE("The x.shape[0] should be 1, actual '%d'", x_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (x_shape[2] != feat_dim_) {
++      SHERPA_ONNX_LOGE("The x.shape[2] should be %d, actual '%d'", feat_dim_,
++                       x_shape[2]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    std::vector<int32_t> prompt_shape =
++        model_->TensorShape(input_tensor_names[1]);
++
++    if (prompt_shape.size() != 1) {
++      SHERPA_ONNX_LOGE("The 2nd input should be 1-d, actual '%d'",
++                       static_cast<int32_t>(prompt_shape.size()));
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (prompt_shape[0] != 4) {
++      SHERPA_ONNX_LOGE("The prompt.shape[0] should be 4, actual '%d'",
++                       prompt_shape[0]);
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    if (!model_->HasTensor("logits")) {
++      SHERPA_ONNX_LOGE("Model does not have output node 'logits'");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    expected_num_frames_ = x_shape[1];
++  }
++
++  std::vector<float> ApplyLFR(std::vector<float> in) const {
++    int32_t lfr_window_size = meta_data_.window_size;
++    int32_t lfr_window_shift = meta_data_.window_shift;
++    int32_t in_feat_dim = 80;
++
++    int32_t in_num_frames = in.size() / in_feat_dim;
++    int32_t out_num_frames =
++        (in_num_frames - lfr_window_size) / lfr_window_shift + 1;
++
++    if (out_num_frames > expected_num_frames_) {
++      SHERPA_ONNX_LOGE(
++          "Number of input frames %d is too large. Truncate it to %d frames.",
++          out_num_frames, expected_num_frames_);
++
++      SHERPA_ONNX_LOGE(
++          "Recognition result may be truncated/incomplete. Please select a "
++          "model accepting longer audios.");
++
++      out_num_frames = expected_num_frames_;
++    }
++
++    int32_t out_feat_dim = in_feat_dim * lfr_window_size;
++
++    // if out_num_frames < expected_num_frames_, it uses 0 padding
++    std::vector<float> out(expected_num_frames_ * out_feat_dim, 0);
++
++    const float *p_in = in.data();
++    float *p_out = out.data();
++
++    for (int32_t i = 0; i != out_num_frames; ++i) {
++      std::copy(p_in, p_in + out_feat_dim, p_out);
++
++      p_out += out_feat_dim;
++      p_in += lfr_window_shift * in_feat_dim;
++    }
++
++    return out;
++  }
++
++ private:
++  std::mutex mutex_;
++
++  OfflineModelConfig config_;
++  OfflineSenseVoiceModelMetaData meta_data_;
++
++  std::unique_ptr<QnnBackend> backend_;
++  std::unique_ptr<QnnModel> model_;
++
++  int32_t expected_num_frames_ = 0;
++  int32_t feat_dim_ = 560;
++};
++
++OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++OfflineSenseVoiceModelQnn::~OfflineSenseVoiceModelQnn() = default;
++
++std::vector<float> OfflineSenseVoiceModelQnn::Run(std::vector<float> features,
++                                                  int32_t language,
++                                                  int32_t text_norm) const {
++  return impl_->Run(std::move(features), language, text_norm);
++}
++
++const OfflineSenseVoiceModelMetaData &
++OfflineSenseVoiceModelQnn::GetModelMetadata() const {
++  return impl_->GetModelMetadata();
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineSenseVoiceModelQnn::OfflineSenseVoiceModelQnn(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
+new file mode 100644
+index 00000000..0a9342e0
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
+@@ -0,0 +1,43 @@
++// sherpa-onnx/csrc/qnn/offline-sense-voice-model-qnn.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
++#define SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
++
++#include <memory>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-model-config.h"
++#include "sherpa-onnx/csrc/offline-sense-voice-model-meta-data.h"
++
++namespace sherpa_onnx {
++
++class OfflineSenseVoiceModelQnn {
++ public:
++  ~OfflineSenseVoiceModelQnn();
++
++  explicit OfflineSenseVoiceModelQnn(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineSenseVoiceModelQnn(Manager *mgr, const OfflineModelConfig &config);
++
++  /**
++   * @param features A tensor of shape (num_frames, feature_dim)
++   *                 before applying LFR.
++   * @param language
++   * @param text_norm
++   * @returns Return a tensor of shape (num_output_frames, vocab_size)
++   */
++  std::vector<float> Run(std::vector<float> features, int32_t language,
++                         int32_t text_norm) const;
++
++  const OfflineSenseVoiceModelMetaData &GetModelMetadata() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_OFFLINE_SENSE_VOICE_MODEL_QNN_H_
+diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+index df4299ac..b482acf4 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-backend.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+@@ -22,7 +22,7 @@ namespace sherpa_onnx {
+ 
+ class QnnBackend::Impl {
+  public:
+-  explicit Impl(const std::string &backend_lib) {
++  explicit Impl(const std::string &backend_lib, bool debug) : debug_(debug) {
+     bool ok = InitQnnInterface(backend_lib);
+     if (!ok) {
+       SHERPA_ONNX_LOGE("Failed to init qnn interface from '%s'",
+@@ -95,7 +95,10 @@ class QnnBackend::Impl {
+                        backend_lib.c_str(), dlerror());
+       return false;
+     }
+-    SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
++    }
+ 
+     const char *symbol = "QnnInterface_getProviders";
+     auto get_interface_providers =
+@@ -106,7 +109,10 @@ class QnnBackend::Impl {
+                        dlerror());
+       return false;
+     }
+-    SHERPA_ONNX_LOGE("Got %s", symbol);
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("Got %s", symbol);
++    }
+ 
+     const QnnInterface_t **interface_providers = nullptr;
+     uint32_t num_providers = 0;
+@@ -218,8 +224,8 @@ class QnnBackend::Impl {
+ 
+ QnnBackend::~QnnBackend() = default;
+ 
+-QnnBackend::QnnBackend(const std::string &backend_lib)
+-    : impl_(std::make_unique<Impl>(backend_lib)) {}
++QnnBackend::QnnBackend(const std::string &backend_lib, bool debug)
++    : impl_(std::make_unique<Impl>(backend_lib, debug)) {}
+ 
+ void QnnBackend::InitContext() const { impl_->InitContext(); }
+ 
+diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.h b/sherpa-onnx/csrc/qnn/qnn-backend.h
+index 62049a69..07a236a8 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-backend.h
++++ b/sherpa-onnx/csrc/qnn/qnn-backend.h
+@@ -13,7 +13,7 @@ namespace sherpa_onnx {
+ 
+ class QnnBackend {
+  public:
+-  explicit QnnBackend(const std::string &backend_lib);
++  explicit QnnBackend(const std::string &backend_lib, bool debug);
+   ~QnnBackend();
+ 
+   void InitContext() const;
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
+index 735995ea..ec1c9838 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
+@@ -21,8 +21,8 @@ namespace sherpa_onnx {
+ 
+ class QnnModel::Impl {
+  public:
+-  Impl(const std::string &model_so, const QnnBackend *backend)
+-      : backend_(backend) {
++  Impl(const std::string &model_so, const QnnBackend *backend, bool debug)
++      : debug_(debug), backend_(backend) {
+     bool ok = InitModel(model_so);
+     if (!ok) {
+       SHERPA_ONNX_LOGE("Failed to load '%s'", model_so.c_str());
+@@ -42,8 +42,8 @@ class QnnModel::Impl {
+   }
+ 
+   Impl(const std::string &binary_context_file, const std::string &system_lib,
+-       const QnnBackend *backend, BinaryContextTag)
+-      : backend_(backend) {
++       const QnnBackend *backend, BinaryContextTag, bool debug)
++      : debug_(debug), backend_(backend) {
+     bool ok = LoadSystemLib(binary_context_file, system_lib);
+     if (!ok) {
+       return;
+@@ -61,7 +61,9 @@ class QnnModel::Impl {
+                        system_lib.c_str(), dlerror());
+       return false;
+     }
+-    SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
++    if (debug_) {
++      SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
++    }
+ 
+     auto get_system_interface_providers =
+         reinterpret_cast<QnnSystemInterfaceGetProvidersFnType>(
+@@ -247,6 +249,11 @@ class QnnModel::Impl {
+       return false;
+     }
+     std::ofstream ofs(filename, std::ios::binary | std::ios::trunc);
++    if (!ofs) {
++      SHERPA_ONNX_LOGE("Failed to create '%s'", filename.c_str());
++      return false;
++    }
++
+     ofs.write(reinterpret_cast<const char *>(saveBuffer.data()),
+               saveBuffer.size());
+ 
+@@ -329,7 +336,6 @@ class QnnModel::Impl {
+     }
+ 
+     FillData(t, p, n);
+-    SHERPA_ONNX_LOGE("set %s", name.c_str());
+ 
+     return true;
+   }
+@@ -359,7 +365,6 @@ class QnnModel::Impl {
+     }
+ 
+     FillData(t, p, n);
+-    SHERPA_ONNX_LOGE("set %s", name.c_str());
+ 
+     return true;
+   }
+@@ -440,7 +445,10 @@ class QnnModel::Impl {
+                        model_so.c_str(), dlerror());
+       return false;
+     }
+-    SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
++    }
+ 
+     return true;
+   }
+@@ -480,7 +488,9 @@ class QnnModel::Impl {
+         &graphs_info, &graphs_count, debug_, LogCallback, backend_->LogLevel());
+     SHERPA_ONNX_QNN_CHECK(ret, "Failed to call compose_graphs_fn_handle_");
+ 
+-    SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
++    if (debug_) {
++      SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
++    }
+ 
+     for (uint32_t i = 0; i < graphs_count; ++i) {
+       if (debug_) {
+@@ -509,11 +519,14 @@ class QnnModel::Impl {
+     input_tensor_names_.reserve(graph.num_input_tensors);
+ 
+     for (uint32_t i = 0; i < graph.num_input_tensors; ++i) {
+-      SHERPA_ONNX_LOGE("input %d", (int)i);
+       auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
+ 
+       CopyTensorInfo(graph.input_tensors[i], *p);
+-      PrintTensor(p->v2);
++
++      if (debug_) {
++        SHERPA_ONNX_LOGE("input %d", (int)i);
++        PrintTensor(p->v2);
++      }
+ 
+       std::string name = p->v1.name;
+       name2tensor_[name] = p.get();
+@@ -531,8 +544,11 @@ class QnnModel::Impl {
+ 
+       CopyTensorInfo(graph.output_tensors[i], *p);
+ 
+-      SHERPA_ONNX_LOGE("output %d", (int)i);
+-      PrintTensor(p->v2);
++      if (debug_ && (i + 3 > graph.num_output_tensors)) {
++        SHERPA_ONNX_LOGE("output %d", (int)i);
++
++        PrintTensor(p->v2);
++      }
+ 
+       std::string name = p->v1.name;
+       name2tensor_[name] = p.get();
+@@ -610,14 +626,15 @@ class QnnModel::Impl {
+ 
+ QnnModel::~QnnModel() = default;
+ 
+-QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend)
+-    : impl_(std::make_unique<Impl>(model_so, backend)) {}
++QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend,
++                   bool debug)
++    : impl_(std::make_unique<Impl>(model_so, backend, debug)) {}
+ 
+ QnnModel::QnnModel(const std::string &binary_context_file,
+                    const std::string &system_lib, const QnnBackend *backend,
+-                   BinaryContextTag tag)
++                   BinaryContextTag tag, bool debug)
+     : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
+-                                   tag)) {}  // NOLINT
++                                   tag, debug)) {}  // NOLINT
+ 
+ bool QnnModel::SaveBinaryContext(const std::string &filename) const {
+   return impl_->SaveBinaryContext(filename);
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.h b/sherpa-onnx/csrc/qnn/qnn-model.h
+index d4cbfa3d..a1be4df4 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-model.h
++++ b/sherpa-onnx/csrc/qnn/qnn-model.h
+@@ -18,10 +18,10 @@ struct BinaryContextTag {};
+ 
+ class QnnModel {
+  public:
+-  QnnModel(const std::string &model_so, const QnnBackend *backend);
++  QnnModel(const std::string &model_so, const QnnBackend *backend, bool debug);
+   QnnModel(const std::string &binary_context_file,
+            const std::string &system_lib, const QnnBackend *backend,
+-           BinaryContextTag tag);
++           BinaryContextTag tag, bool debug);
+   ~QnnModel();
+ 
+   bool SaveBinaryContext(const std::string &filename) const;
+
+commit e7ef78be11fed077be12eb2dbab0dd79ce3da351
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Fri Nov 14 15:47:41 2025 +0800
+
+    export omniASR_CTC_1B (#2788)
+
+diff --git a/.github/workflows/android-rknn.yaml b/.github/workflows/android-rknn.yaml
+index cd333e5a..265fd318 100644
+--- a/.github/workflows/android-rknn.yaml
++++ b/.github/workflows/android-rknn.yaml
+@@ -139,7 +139,7 @@ jobs:
+           file: sherpa-onnx-*-android-rknn.tar.bz2
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.12.13
++          # tag: v1.12.17
+ 
+   build-android-aar-rknn:
+     needs: [build-android-rknn-libs]
+@@ -275,7 +275,7 @@ jobs:
+           file: ./*.aar
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.12.13
++          # tag: v1.12.17
+ 
+       - name: Release android aar
+         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+diff --git a/.github/workflows/android-static.yaml b/.github/workflows/android-static.yaml
+index e6ed40ab..90fa5360 100644
+--- a/.github/workflows/android-static.yaml
++++ b/.github/workflows/android-static.yaml
+@@ -171,7 +171,7 @@ jobs:
+           file: sherpa-onnx-*-android*.tar.bz2
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.11.3
++          # tag: v1.12.17
+ 
+   build-android-aar-static:
+     needs: [build-android-static-libs]
+@@ -306,4 +306,4 @@ jobs:
+           file: ./*.aar
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.11.3
++          # tag: v1.12.17
+diff --git a/.github/workflows/android.yaml b/.github/workflows/android.yaml
+index 46147bad..20eed0c7 100644
+--- a/.github/workflows/android.yaml
++++ b/.github/workflows/android.yaml
+@@ -175,9 +175,9 @@ jobs:
+           file_glob: true
+           overwrite: true
+           file: sherpa-onnx-*-android.tar.bz2
+-          repo_name: k2-fsa/sherpa-onnx
+-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: v1.12.14
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.17
+ 
+       - name: Release android libs
+         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+@@ -318,9 +318,9 @@ jobs:
+           file_glob: true
+           overwrite: true
+           file: ./*.aar
+-          repo_name: k2-fsa/sherpa-onnx
+-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: v1.12.14
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.17
+ 
+       - name: Release android aar
+         if: github.repository_owner == 'k2-fsa' && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+index 51a6f280..c11d7091 100644
+--- a/.github/workflows/export-omnilingual-asr-to-onnx.yaml
++++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+@@ -3,7 +3,7 @@ name: export-omnilingual-asr-to-onnx
+ on:
+   push:
+     branches:
+-      - export-omnilingual-asr
++      - omnilingual-1b
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -13,13 +13,14 @@ concurrency:
+ jobs:
+   export-omnilingual-asr-to-onnx:
+     if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
+-    name: export omnilingual-asr
++    name: ${{ matrix.model_card }}
+     runs-on: ${{ matrix.os }}
+     strategy:
+       fail-fast: false
+       matrix:
+         os: [ubuntu-latest]
+         python-version: ["3.10"]
++        model_card: ["omniASR_CTC_300M", "omniASR_CTC_1B"]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -58,9 +59,11 @@ jobs:
+         shell: bash
+         run: |
+           cd scripts/omnilingual-asr
+-          python3 ./export-onnx.py
++          model_card=${{ matrix.model_card }}
++          python3 ./export-onnx.py --model-card $model_card
+ 
+           ls -lh *.onnx
++          ls -lh *.weights || true
+ 
+           rm README.md
+ 
+@@ -78,11 +81,15 @@ jobs:
+           echo "---collect files----"
+ 
+           d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
++          if [[ $model_card == omniASR_CTC_1B ]]; then
++            d=sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-2025-11-12
++          fi
+ 
+           mkdir -p $d
+           mkdir -p $d/test_wavs
+ 
+           mv -v model.onnx $d
++          mv -v model.weights $d || true
+           cp -v tokens.txt $d
+           cp -v README.md $d
+           cp -v LICENSE* $d
+@@ -94,6 +101,9 @@ jobs:
+           mv $d ../..
+ 
+           d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++          if [[ $model_card == omniASR_CTC_1B ]]; then
++            d=sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-int8-2025-11-12
++          fi
+ 
+           mkdir -p $d
+           mkdir -p $d/test_wavs
+@@ -115,6 +125,16 @@ jobs:
+ 
+           ls -lh *.tar.bz2
+ 
++          df -h
++          rm -fv onnx_* model.encoder* model.final*
++
++          ls -lh ~/.cache/fairseq2/assets/*
++
++          rm -rf ~/.cache/fairseq2/assets/
++          rm -rf ~/.cache
++
++          df -h
++
+       - name: Publish to huggingface
+         env:
+           HF_TOKEN: ${{ secrets.HF_TOKEN }}
+@@ -130,10 +150,18 @@ jobs:
+             export GIT_LFS_SKIP_SMUDGE=1
+             export GIT_CLONE_PROTECTION_ACTIVE=false
+ 
++            model_card=${{ matrix.model_card }}
++
+             dirs=(
+               sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
+               sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+             )
++            if [[ $model_card == omniASR_CTC_1B ]]; then
++              dirs=(
++                sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-2025-11-12
++                sherpa-onnx-omnilingual-asr-1600-languages-1B-ctc-int8-2025-11-12
++              )
++            fi
+ 
+             for d in ${dirs[@]}; do
+               rm -rf huggingface
+@@ -143,9 +171,11 @@ jobs:
+               git fetch
+               git pull
+               echo "pwd: $PWD"
+-              cp -a ../$d/* .
++              rm -fv ./*.weights
++              mv -v ../$d/* .
+ 
+               git lfs track "*.onnx"
++              git lfs track "*.weights"
+               git lfs track "*.wav"
+               ls -lh
+               git add .
+@@ -159,6 +189,20 @@ jobs:
+               popd
+             done
+ 
++      # List large files first (safe)
++      - name: List .tar.bz2 files larger than 2GB
++        run: |
++          ls -lh *.tar.bz2
++          echo "----"
++          find . -type f -name "*.tar.bz2" -size +2G -print
++
++      # Delete large files
++      - name: Delete .tar.bz2 files larger than 2GB
++        run: |
++          find . -type f -name "*.tar.bz2" -size +2G -delete
++
++          ls -lh *.tar.bz2
++
+       - name: Release
+         uses: svenstaro/upload-release-action@v2
+         with:
+diff --git a/scripts/omnilingual-asr/export-onnx.py b/scripts/omnilingual-asr/export-onnx.py
+index deaf556d..e8f765bb 100755
+--- a/scripts/omnilingual-asr/export-onnx.py
++++ b/scripts/omnilingual-asr/export-onnx.py
+@@ -1,6 +1,7 @@
+ #!/usr/bin/env python3
+ # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+ 
++import argparse
+ from typing import Dict
+ 
+ import onnx
+@@ -10,7 +11,20 @@ from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline
+ from onnxruntime.quantization import QuantType, quantize_dynamic
+ 
+ 
+-def add_meta_data(filename: str, meta_data: Dict[str, str]):
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
++    )
++    parser.add_argument(
++        "--model-card",
++        type=str,
++        required=True,
++        help="omniASR_CTC_300M, or omniASR_CTC_1B",
++    )
++    return parser.parse_args()
++
++
++def add_meta_data(filename: str, meta_data: Dict[str, str], model_card: str):
+     """Add meta data to an ONNX model. It is changed in-place.
+ 
+     Args:
+@@ -28,6 +42,18 @@ def add_meta_data(filename: str, meta_data: Dict[str, str]):
+         meta.key = key
+         meta.value = str(value)
+ 
++    if "300M" in model_card:
++        onnx.save(model, filename)
++    else:
++        external_filename = filename.split(".onnx")[0]
++        onnx.save(
++            model,
++            filename,
++            save_as_external_data=True,
++            all_tensors_to_one_file=True,
++            location=external_filename + ".weights",
++        )
++
+ 
+ class ModelWrapper(torch.nn.Module):
+     def __init__(self, model):
+@@ -46,8 +72,10 @@ class ModelWrapper(torch.nn.Module):
+ 
+ @torch.no_grad()
+ def main():
++    args = get_args()
++    print(vars(args))
+     pipeline = ASRInferencePipeline(
+-        model_card="omniASR_CTC_300M",
++        model_card=args.model_card,
+         device="cpu",
+         dtype=torch.float32,
+     )
+@@ -87,7 +115,7 @@ def main():
+         "comment": "300M-CTC",
+     }
+ 
+-    add_meta_data("model.onnx", meta_data)
++    add_meta_data("model.onnx", meta_data, args.model_card)
+     print("saved to model.onnx")
+ 
+     quantize_dynamic(
+diff --git a/scripts/omnilingual-asr/test.py b/scripts/omnilingual-asr/test.py
+index 9415902a..7257cd18 100755
+--- a/scripts/omnilingual-asr/test.py
++++ b/scripts/omnilingual-asr/test.py
+@@ -76,7 +76,7 @@ def load_audio(filename):
+     return (samples - mean) / np.sqrt(var + eps)
+ 
+ 
+-def test(filename, wav_file_list, num_iter=10):
++def test(filename, wav_file_list, num_iter=1):
+     id2token = load_tokens()
+     model = OnnxModel(filename)
+ 
+diff --git a/sherpa-onnx/csrc/file-utils.cc b/sherpa-onnx/csrc/file-utils.cc
+index b8361445..6ad16166 100644
+--- a/sherpa-onnx/csrc/file-utils.cc
++++ b/sherpa-onnx/csrc/file-utils.cc
+@@ -26,8 +26,19 @@ void AssertFileExists(const std::string &filename) {
+ }
+ 
+ std::vector<char> ReadFile(const std::string &filename) {
+-  std::ifstream input(filename, std::ios::binary);
+-  std::vector<char> buffer(std::istreambuf_iterator<char>(input), {});
++  std::ifstream file(filename, std::ios::binary | std::ios::ate);
++  if (!file.is_open()) {
++    return {};
++  }
++
++  std::streamsize size = file.tellg();
++  file.seekg(0, std::ios::beg);
++
++  std::vector<char> buffer(size);
++  if (!file.read(buffer.data(), size)) {
++    return {};
++  }
++
+   return buffer;
+ }
+ 
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+index 7df94402..d5bbc176 100644
+--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+@@ -36,8 +36,9 @@ class OfflineOmnilingualAsrCtcModel::Impl {
+         env_(ORT_LOGGING_LEVEL_ERROR),
+         sess_opts_(GetSessionOptions(config)),
+         allocator_{} {
+-    auto buf = ReadFile(config_.omnilingual.model);
+-    Init(buf.data(), buf.size());
++    sess_ = std::make_unique<Ort::Session>(
++        env_, SHERPA_ONNX_TO_ORT_PATH(config_.omnilingual.model), sess_opts_);
++    Init(nullptr, 0);
+   }
+ 
+   template <typename Manager>
+@@ -101,8 +102,18 @@ class OfflineOmnilingualAsrCtcModel::Impl {
+ 
+  private:
+   void Init(void *model_data, size_t model_data_length) {
+-    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
+-                                           sess_opts_);
++    // For models with 1B parameters, weights are saved externally
++    // in model.weights
++    // We cannot create session from buffer in this case.
++    if (model_data) {
++      sess_ = std::make_unique<Ort::Session>(env_, model_data,
++                                             model_data_length, sess_opts_);
++    } else if (!sess_) {
++      SHERPA_ONNX_LOGE(
++          "Please pass buffer data or initialize session outside of this "
++          "function");
++      SHERPA_ONNX_EXIT(-1);
++    }
+ 
+     GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
+ 
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index 37279c37..3a675fc1 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -334,6 +334,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       !config.model_config.zipformer_ctc.model.empty() ||
+       !config.model_config.tdnn.model.empty() ||
+       !config.model_config.wenet_ctc.model.empty() ||
++      !config.model_config.omnilingual.model.empty() ||
+       !config.model_config.dolphin.model.empty()) {
+     return std::make_unique<OfflineRecognizerCtcImpl>(mgr, config);
+   }
+diff --git a/sherpa-onnx/csrc/session.cc b/sherpa-onnx/csrc/session.cc
+index a33594f0..24a63bc4 100644
+--- a/sherpa-onnx/csrc/session.cc
++++ b/sherpa-onnx/csrc/session.cc
+@@ -56,6 +56,9 @@ Ort::SessionOptions GetSessionOptionsImpl(
+   // sess_opts.SetLogSeverityLevel(ORT_LOGGING_LEVEL_VERBOSE);
+   // sess_opts.EnableProfiling("profile");
+ 
++  // If you want to speed up initialization, please uncomment the following line
++  // sess_opts.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_DISABLE_ALL);
++
+   switch (p) {
+     case Provider::kCPU:
+       break;  // nothing to do for the CPU provider
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+index 41fb3a4f..8f4d1934 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+@@ -123,8 +123,18 @@ for a list of pre-trained models to download.
+   }
+ 
+   fprintf(stderr, "Creating recognizer ...\n");
++  const auto begin_init = std::chrono::steady_clock::now();
++
+   sherpa_onnx::OfflineRecognizer recognizer(config);
+ 
++  const auto end_init = std::chrono::steady_clock::now();
++  float elapsed_seconds_init =
++      std::chrono::duration_cast<std::chrono::milliseconds>(end_init -
++                                                            begin_init)
++          .count() /
++      1000.;
++  fprintf(stderr, "recognizer created in %.3f s\n", elapsed_seconds_init);
++
+   fprintf(stderr, "Started\n");
+   const auto begin = std::chrono::steady_clock::now();
+ 
+diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
+index 5a3fff09..3f0f80d5 100644
+--- a/sherpa-onnx/csrc/text-utils.h
++++ b/sherpa-onnx/csrc/text-utils.h
+@@ -182,6 +182,12 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
+ 
+ bool IsPunct(const std::string &s);
+ 
++#if defined(_WIN32)
++#define SHERPA_ONNX_TO_ORT_PATH(s) (ToWideString(s).c_str())
++#else
++#define SHERPA_ONNX_TO_ORT_PATH(s) ((s).c_str())
++#endif
++
+ }  // namespace sherpa_onnx
+ 
+ #endif  // SHERPA_ONNX_CSRC_TEXT_UTILS_H_
+
+commit bb96ea34bbf7f97ffd076bee69814b4c68b67558
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Fri Nov 14 07:30:24 2025 +0800
+
+    Fix building wheels (#2786)
+
+diff --git a/.github/workflows/aarch64-linux-gnu-shared.yaml b/.github/workflows/aarch64-linux-gnu-shared.yaml
+index d5f2b9c8..1c6fdf3f 100644
+--- a/.github/workflows/aarch64-linux-gnu-shared.yaml
++++ b/.github/workflows/aarch64-linux-gnu-shared.yaml
+@@ -255,7 +255,7 @@ jobs:
+           file: sherpa-onnx-*linux-aarch64*.tar.bz2
+           # repo_name: k2-fsa/sherpa-onnx
+           # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          # tag: v1.12.13
++          # tag: v1.12.17
+ 
+       - name: Test offline Moonshine
+         if: matrix.build_type != 'Debug'
+diff --git a/.github/workflows/ascend.yaml b/.github/workflows/ascend.yaml
+index 05543985..280ece7a 100644
+--- a/.github/workflows/ascend.yaml
++++ b/.github/workflows/ascend.yaml
+@@ -2,10 +2,6 @@ name: ascend
+ 
+ on:
+   push:
+-    branches:
+-      - master
+-      - ci-ascend
+-  pull_request:
+     branches:
+       - master
+ 
+diff --git a/.github/workflows/build-wheels-armv7l.yaml b/.github/workflows/build-wheels-armv7l.yaml
+index 5f861283..accd3784 100644
+--- a/.github/workflows/build-wheels-armv7l.yaml
++++ b/.github/workflows/build-wheels-armv7l.yaml
+@@ -21,7 +21,7 @@ jobs:
+       fail-fast: false
+       matrix:
+         os: [ubuntu-latest]
+-        python-version: ["3.7", "3.8", "3.9", "3.10", "3.11"]
++        python-version: ["3.8", "3.9", "3.10", "3.11"]
+ 
+     steps:
+       - uses: actions/checkout@v4
+diff --git a/.github/workflows/build-wheels-linux.yaml b/.github/workflows/build-wheels-linux.yaml
+index 0f71b6c7..d9757a6a 100644
+--- a/.github/workflows/build-wheels-linux.yaml
++++ b/.github/workflows/build-wheels-linux.yaml
+@@ -427,4 +427,4 @@ jobs:
+           TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}
+         shell: bash
+         run: |
+-          twine upload dist/sherpa-onnx-*.tar.gz
++          twine upload dist/sherpa*.tar.gz
+diff --git a/.github/workflows/build-wheels-win32.yaml b/.github/workflows/build-wheels-win32.yaml
+index 3dca70c1..d76d9d8f 100644
+--- a/.github/workflows/build-wheels-win32.yaml
++++ b/.github/workflows/build-wheels-win32.yaml
+@@ -26,7 +26,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [windows-latest]
++        os: [windows-2022]
+ 
+     steps:
+       - uses: actions/checkout@v4
+@@ -263,7 +263,7 @@ jobs:
+     strategy:
+       fail-fast: false
+       matrix:
+-        os: [windows-latest]
++        os: [windows-2022]
+         python-version: ["cp38", "cp39", "cp310", "cp311", "cp312", "cp313", "cp314"]
+ 
+     steps:
+diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+index 332d8d96..51a6f280 100644
+--- a/.github/workflows/export-omnilingual-asr-to-onnx.yaml
++++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+@@ -66,7 +66,6 @@ jobs:
+ 
+           curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/README.md
+           curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE
+-          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE-CC-BY-4.0.md
+ 
+           curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en.wav
+           curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/es.wav
+diff --git a/.github/workflows/mfc.yaml b/.github/workflows/mfc.yaml
+index dd18942a..5911f7a0 100644
+--- a/.github/workflows/mfc.yaml
++++ b/.github/workflows/mfc.yaml
+@@ -125,9 +125,9 @@ jobs:
+           file_glob: true
+           overwrite: true
+           file: ./mfc-examples/${{ matrix.arch }}/Release/sherpa-onnx-streaming-*.exe
+-          repo_name: k2-fsa/sherpa-onnx
+-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: v1.12.15
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.17
+ 
+       - name: Release pre-compiled binaries and libs for Windows ${{ matrix.arch }}
+         if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+@@ -136,9 +136,9 @@ jobs:
+           file_glob: true
+           overwrite: true
+           file: ./mfc-examples/${{ matrix.arch }}/Release/sherpa-onnx-non-streaming-*.exe
+-          repo_name: k2-fsa/sherpa-onnx
+-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: v1.12.15
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.17
+ 
+       - name: Release pre-compiled binaries and libs for Windows ${{ matrix.arch }}
+         if: (github.repository_owner == 'csukuangfj' || github.repository_owner == 'k2-fsa') && github.event_name == 'push' && contains(github.ref, 'refs/tags/')
+@@ -147,6 +147,6 @@ jobs:
+           file_glob: true
+           overwrite: true
+           file: ./mfc-examples/${{ matrix.arch }}/sherpa-onnx-non-streaming-*.exe
+-          repo_name: k2-fsa/sherpa-onnx
+-          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: v1.12.15
++          # repo_name: k2-fsa/sherpa-onnx
++          # repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          # tag: v1.12.17
+diff --git a/scripts/go/release.sh b/scripts/go/release.sh
+index c9334e56..c0dbeb83 100755
+--- a/scripts/go/release.sh
++++ b/scripts/go/release.sh
+@@ -31,7 +31,8 @@ function linux() {
+   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_x86_64.whl
+   unzip sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_x86_64.whl
+ 
+-  cp -v sherpa_onnx/lib/*.so* $dst
++  rm -fv $dst/_sherpa*.so
++  cp -v sherpa_onnx/lib/lib*.so* $dst
+ 
+   cd ..
+   rm -rf t
+@@ -43,7 +44,8 @@ function linux() {
+   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_aarch64.whl
+   unzip ./sherpa_onnx_core-${SHERPA_ONNX_VERSION}-py3-none-manylinux2014_aarch64.whl
+ 
+-  cp -v sherpa_onnx/lib/*.so* $dst
++  rm -fv $dst/_sherpa*.so
++  cp -v sherpa_onnx/lib/lib*.so* $dst
+ 
+   cd ..
+   rm -rf t
+@@ -55,7 +57,8 @@ function linux() {
+   wget -q https://huggingface.co/csukuangfj/sherpa-onnx-wheels/resolve/main/cpu/$SHERPA_ONNX_VERSION/sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
+   unzip ./sherpa_onnx-${SHERPA_ONNX_VERSION}-cp38-cp38-linux_armv7l.whl
+ 
+-  cp -v sherpa_onnx/lib/*.so* $dst
++  rm -fv $dst/_sherpa*.so
++  cp -v sherpa_onnx/lib/lib*.so* $dst
+ 
+   cd ..
+   rm -rf t
+diff --git a/setup.py b/setup.py
+index 648d118c..061a2f03 100644
+--- a/setup.py
++++ b/setup.py
+@@ -43,7 +43,7 @@ def get_package_version():
+     return latest_version
+ 
+ 
+-package_name = "sherpa-onnx"
++package_name = "sherpa_onnx"
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "a") as f:
+     f.write(f"__version__ = '{get_package_version()}'\n")
+@@ -81,13 +81,17 @@ setuptools.setup(
+         "sherpa_onnx": "sherpa-onnx/python/sherpa_onnx",
+     },
+     packages=["sherpa_onnx"],
+-    data_files=[
+-        ("Scripts", get_binaries_to_install())
+-        if is_windows()
+-        else ("bin", get_binaries_to_install())
+-    ]
+-    if get_binaries_to_install()
+-    else None,
++    data_files=(
++        [
++            (
++                ("Scripts", get_binaries_to_install())
++                if is_windows()
++                else ("bin", get_binaries_to_install())
++            )
++        ]
++        if get_binaries_to_install()
++        else None
++    ),
+     url="https://github.com/k2-fsa/sherpa-onnx",
+     long_description=read_long_description(),
+     long_description_content_type="text/markdown",
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+index 0ed69a58..7df94402 100644
+--- a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
+ 
+ #include <algorithm>
++#include <cmath>
+ #include <memory>
+ #include <string>
+ #include <utility>
+
+commit fb2b2f0d7b13a1e9d5516016635667c5e254b8ad
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 19:24:04 2025 +0800
+
+    Release v1.12.17 (#2785)
+
+diff --git a/CHANGELOG.md b/CHANGELOG.md
+index 7efbb6b3..8c42ad5e 100644
+--- a/CHANGELOG.md
++++ b/CHANGELOG.md
+@@ -1,3 +1,7 @@
++## 1.12.17
++
++* Fix releasing
++
+ ## 1.12.16
+ 
+ * Support exporting SenseVoice and Paraformer to Ascend 310P3 NPU. (#2716)
+@@ -33,7 +37,7 @@
+ * Add Dart API for Omnilingual ASR CTC models (#2779)
+ * Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)
+ * Add Pascal API for Omnilingual ASR CTC models (#2782)
+-* Add Koltin and Java API for Omnilingual ASR CTC models (#2783)
++* Add Kotlin and Java API for Omnilingual ASR CTC models (#2783)
+ 
+ ## 1.12.15
+ 
+@@ -504,7 +508,7 @@
+ * Fix: Prepend 0 to tokenization to prevent word skipping for Kokoro. (#1787)
+ * Export Kokoro 1.0 to sherpa-onnx (#1788)
+ * Add C++ and Python API for Kokoro 1.0 multilingual TTS model (#1795)
+-* Add Java and Koltin API for Kokoro TTS 1.0 (#1798)
++* Add Java and Kotlin API for Kokoro TTS 1.0 (#1798)
+ * Add Android demo for Kokoro TTS 1.0 (#1799)
+ * Add C API for Kokoro TTS 1.0 (#1801)
+ * Add CXX API for Kokoro TTS 1.0 (#1802)
+@@ -544,7 +548,7 @@
+ * Add Pascal API for Kokoro TTS models (#1724)
+ * Add JavaScript API (node-addon) for Kokoro TTS models (#1725)
+ * Add JavaScript (WebAssembly) API for Kokoro TTS models. (#1726)
+-* Add Koltin and Java API for Kokoro TTS models (#1728)
++* Add Kotlin and Java API for Kokoro TTS models (#1728)
+ * Update README.md for KWS to not use git lfs. (#1729)
+ 
+ 
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index ac0be33b..20190dc7 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -14,7 +14,7 @@ project(sherpa-onnx)
+ # Remember to update
+ # ./CHANGELOG.md
+ # ./new-release.sh
+-set(SHERPA_ONNX_VERSION "1.12.16")
++set(SHERPA_ONNX_VERSION "1.12.17")
+ 
+ # Disable warning about
+ #
+diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
+index 1d59d8cb..76024de6 100644
+--- a/android/SherpaOnnx/app/build.gradle
++++ b/android/SherpaOnnx/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 32
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
+index 1d59d8cb..76024de6 100644
+--- a/android/SherpaOnnx2Pass/app/build.gradle
++++ b/android/SherpaOnnx2Pass/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 32
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
+index 3b1a5392..c3c5ac3d 100644
+--- a/android/SherpaOnnxAar/README.md
++++ b/android/SherpaOnnxAar/README.md
+@@ -4,8 +4,8 @@
+ git clone https://github.com/k2-fsa/sherpa-onnx
+ cd sherpa-onnx
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-v1.12.16-android.tar.bz2
+-tar xvf sherpa-onnx-v1.12.16-android.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-v1.12.17-android.tar.bz2
++tar xvf sherpa-onnx-v1.12.17-android.tar.bz2
+ 
+ cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
+ cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
+@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
+ 
+ ./gradlew :sherpa_onnx:assembleRelease
+ ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
+-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.16.aar
++cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.17.aar
+ ```
+diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+index b4407bbe..bd6e098c 100644
+--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+index 2b95c60a..12638bee 100644
+--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 26
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
+index 039efe01..93848bc6 100644
+--- a/android/SherpaOnnxJavaDemo/app/build.gradle
++++ b/android/SherpaOnnxJavaDemo/app/build.gradle
+@@ -10,7 +10,7 @@ android {
+         minSdk 28
+         targetSdk 34
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+@@ -34,5 +34,5 @@ dependencies {
+     implementation 'pub.devrel:easypermissions:3.0.0'
+     implementation 'androidx.core:core-ktx:1.7.0'
+     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
+-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.16'
++    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.17'
+ }
+diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
+index 1d59d8cb..76024de6 100644
+--- a/android/SherpaOnnxKws/app/build.gradle
++++ b/android/SherpaOnnxKws/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 32
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+index 5cd52b3c..082ea5c8 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+index 39ef31e9..ef491de9 100644
+--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 28
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+@@ -58,7 +58,7 @@ dependencies {
+     implementation(libs.compose.foundation)
+     implementation(libs.activity.compose)
+     implementation(libs.core.splashscreen)
+-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.16")
++    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.17")
+     androidTestImplementation(platform(libs.compose.bom))
+     androidTestImplementation(libs.ui.test.junit4)
+     debugImplementation(libs.ui.tooling)
+diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+index e538009a..a5ce9201 100644
+--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+index e49b49c2..ddb6f444 100644
+--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+index 98f11773..a7617b14 100644
+--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
+index 2636a9b6..034325ac 100644
+--- a/android/SherpaOnnxTts/app/build.gradle
++++ b/android/SherpaOnnxTts/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 32
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+index 1b1d0bc1..53af0d02 100644
+--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
++++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+@@ -12,7 +12,7 @@ android {
+         minSdk = 21
+         targetSdk = 34
+         versionCode = 20251113
+-        versionName = "1.12.16"
++        versionName = "1.12.17"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
+index 566e93d2..15784a18 100644
+--- a/android/SherpaOnnxVad/app/build.gradle
++++ b/android/SherpaOnnxVad/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 33
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
+index 566e93d2..15784a18 100644
+--- a/android/SherpaOnnxVadAsr/app/build.gradle
++++ b/android/SherpaOnnxVadAsr/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 33
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
+index 23547587..7f10eb5d 100644
+--- a/android/SherpaOnnxWebSocket/app/build.gradle
++++ b/android/SherpaOnnxWebSocket/app/build.gradle
+@@ -12,7 +12,7 @@ android {
+         minSdk 21
+         targetSdk 32
+         versionCode 20251113
+-        versionName "1.12.16"
++        versionName "1.12.17"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/build-ios-shared.sh b/build-ios-shared.sh
+index 80ca7b1f..c1477eb7 100755
+--- a/build-ios-shared.sh
++++ b/build-ios-shared.sh
+@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
+ 	<key>CFBundlePackageType</key>
+ 	<string>FMWK</string>
+ 	<key>CFBundleShortVersionString</key>
+-	<string>1.12.16</string>
++	<string>1.12.17</string>
+ 	<key>CFBundleSupportedPlatforms</key>
+ 	<array>
+ 		<string>iPhoneOS</string>
+diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
+index 4b1994a0..3fde5ed0 100644
+--- a/dart-api-examples/add-punctuations/pubspec.yaml
++++ b/dart-api-examples/add-punctuations/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
+index 752eec75..909c3751 100644
+--- a/dart-api-examples/audio-tagging/pubspec.yaml
++++ b/dart-api-examples/audio-tagging/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
+index 727da49f..762f3493 100644
+--- a/dart-api-examples/keyword-spotter/pubspec.yaml
++++ b/dart-api-examples/keyword-spotter/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
+index 42b1c2b0..187661b5 100644
+--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
+index f52042bb..c2893c4c 100644
+--- a/dart-api-examples/speaker-diarization/pubspec.yaml
++++ b/dart-api-examples/speaker-diarization/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
+index bbe878f8..2d68710a 100644
+--- a/dart-api-examples/speaker-identification/pubspec.yaml
++++ b/dart-api-examples/speaker-identification/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+index 0161e730..32ec2161 100644
+--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
++++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
+index 3248a361..b9e2166d 100644
+--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
++++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
+index 630ddd72..577b8efd 100644
+--- a/dart-api-examples/streaming-asr/pubspec.yaml
++++ b/dart-api-examples/streaming-asr/pubspec.yaml
+@@ -11,7 +11,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
+index ee263496..54c9abf7 100644
+--- a/dart-api-examples/tts/pubspec.yaml
++++ b/dart-api-examples/tts/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+index 005c49e1..b544cba8 100644
+--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
+index 7206336a..6c124a31 100644
+--- a/dart-api-examples/vad/pubspec.yaml
++++ b/dart-api-examples/vad/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+index 103aa7ee..1fe5515f 100644
+--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
++++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.16
++version: 1.12.17
+ 
+ topics:
+   - speech-recognition
+@@ -31,7 +31,7 @@ dependencies:
+   record: 6.0.0
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
+index 23aab88d..455b6f1f 100644
+--- a/flutter-examples/streaming_asr/pubspec.yaml
++++ b/flutter-examples/streaming_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.16
++version: 1.12.17
+ 
+ topics:
+   - speech-recognition
+@@ -30,7 +30,7 @@ dependencies:
+   record: ^6.1.2
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
+index 2c393901..dd4d2433 100644
+--- a/flutter-examples/tts/pubspec.yaml
++++ b/flutter-examples/tts/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none' # Remove this line if you wish to publish to pub.dev
+ 
+-version: 1.12.16
++version: 1.12.17
+ 
+ environment:
+   sdk: ">=2.17.0 <4.0.0"
+@@ -18,7 +18,7 @@ dependencies:
+   cupertino_icons: ^1.0.6
+   path_provider: ^2.1.3
+   path: ^1.9.0
+-  sherpa_onnx: ^1.12.16
++  sherpa_onnx: ^1.12.17
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   url_launcher: 6.2.6
+diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
+index a04d6a71..825f975f 100644
+--- a/flutter/sherpa_onnx/pubspec.yaml
++++ b/flutter/sherpa_onnx/pubspec.yaml
+@@ -17,7 +17,7 @@ topics:
+   - voice-activity-detection
+ 
+ # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+-version: 1.12.16
++version: 1.12.17
+ 
+ homepage: https://github.com/k2-fsa/sherpa-onnx
+ 
+@@ -30,23 +30,23 @@ dependencies:
+   flutter:
+     sdk: flutter
+ 
+-  sherpa_onnx_android: ^1.12.16
++  sherpa_onnx_android: ^1.12.17
+   # sherpa_onnx_android:
+   #   path: ../sherpa_onnx_android
+ 
+-  sherpa_onnx_macos: ^1.12.16
++  sherpa_onnx_macos: ^1.12.17
+   # sherpa_onnx_macos:
+   #   path: ../sherpa_onnx_macos
+ 
+-  sherpa_onnx_linux: ^1.12.16
++  sherpa_onnx_linux: ^1.12.17
+   # sherpa_onnx_linux:
+   #   path: ../sherpa_onnx_linux
+ 
+-  sherpa_onnx_windows: ^1.12.16
++  sherpa_onnx_windows: ^1.12.17
+   # sherpa_onnx_windows:
+   #   path: ../sherpa_onnx_windows
+ 
+-  sherpa_onnx_ios: ^1.12.16
++  sherpa_onnx_ios: ^1.12.17
+   # sherpa_onnx_ios:
+   #   path: ../sherpa_onnx_ios
+ 
+diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+index 3119aff1..3e674158 100644
+--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
++++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+@@ -7,7 +7,7 @@
+ # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_ios'
+-  s.version          = '1.12.16'
++  s.version          = '1.12.17'
+   s.summary          = 'A new Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ A new Flutter FFI plugin project.
+diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+index 17def84d..dee37a00 100644
+--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
++++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+@@ -4,7 +4,7 @@
+ #
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_macos'
+-  s.version          = '1.12.16'
++  s.version          = '1.12.17'
+   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ sherpa-onnx Flutter FFI plugin project.
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+index 0228e2c5..dd8845a8 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+@@ -1,7 +1,7 @@
+ /**
+  * Use these variables when you tailor your ArkTS code. They must be of the const type.
+  */
+-export const HAR_VERSION = '1.12.16';
++export const HAR_VERSION = '1.12.17';
+ export const BUILD_MODE_NAME = 'debug';
+ export const DEBUG = true;
+ export const TARGET_NAME = 'default';
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+index d1b4325d..187519e9 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
+ 
+ ```
+   "dependencies": {
+-    "sherpa_onnx": "1.12.16",
++    "sherpa_onnx": "1.12.17",
+   },
+ ```
+ 
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+index b76d8527..1bfb0d94 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+@@ -1,6 +1,6 @@
+ {
+   "name": "sherpa_onnx",
+-  "version": "1.12.16",
++  "version": "1.12.17",
+   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
+   "main": "Index.ets",
+   "author": "The next-gen Kaldi team",
+diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+index 19efd920..ec30333f 100644
+--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.16"
++    "sherpa_onnx": "1.12.17"
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+index 913e059e..992dd9d0 100644
+--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.16",
++    "sherpa_onnx": "1.12.17",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+index 913e059e..992dd9d0 100644
+--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.16",
++    "sherpa_onnx": "1.12.17",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+index 913e059e..992dd9d0 100644
+--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.16",
++    "sherpa_onnx": "1.12.17",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+index 3d6da438..acc4feb5 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
++++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+@@ -1,6 +1,6 @@
+ # Introduction
+ 
+-Please download ./sherpa_onnx-v1.12.16.har
++Please download ./sherpa_onnx-v1.12.17.har
+ from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
+ 
+ Hint: For users who have no access to huggingface, please use
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+index a1215a29..27182c9b 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+@@ -7,7 +7,7 @@
+   "license": "",
+   "dependencies": {
+     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
+-    "sherpa_onnx": "1.12.16",
++    "sherpa_onnx": "1.12.17",
+   }
+ }
+ 
+diff --git a/jitpack.yml b/jitpack.yml
+index aefbd313..9e83e15c 100644
+--- a/jitpack.yml
++++ b/jitpack.yml
+@@ -2,8 +2,8 @@ jdk:
+   - openjdk17
+ 
+ before_install:
+-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-1.12.16.aar
++  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-1.12.17.aar
+ 
+ install:
+-  - FILE="-Dfile=sherpa-onnx-1.12.16.aar"
+-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.16 -Dpackaging=aar -DgeneratePom=true
++  - FILE="-Dfile=sherpa-onnx-1.12.17.aar"
++  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.17 -Dpackaging=aar -DgeneratePom=true
+diff --git a/mfc-examples/README.md b/mfc-examples/README.md
+index 89f9292b..2c7edb25 100644
+--- a/mfc-examples/README.md
++++ b/mfc-examples/README.md
+@@ -5,9 +5,9 @@ for speech recognition.
+ 
+ |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
+ |---------|--------------------|-------------------|------------|
+-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x86-v1.12.16.exe)| Non-streaming speech recognition|
+-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x86-v1.12.16.exe)| Streaming speech recognition|
+-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x86-v1.12.16.exe)| Non-streaming text to speech|
++|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-asr-x86-v1.12.17.exe)| Non-streaming speech recognition|
++|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-streaming-asr-x86-v1.12.17.exe)| Streaming speech recognition|
++|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x64-v1.12.17.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.17/sherpa-onnx-non-streaming-tts-x86-v1.12.17.exe)| Non-streaming text to speech|
+ 
+ Caution: You need to use Windows and install Visual Studio 2022 in order to
+ compile it.
+diff --git a/new-release.sh b/new-release.sh
+index d4784850..3758063c 100755
+--- a/new-release.sh
++++ b/new-release.sh
+@@ -5,8 +5,8 @@ set -ex
+ old_version_code=20251022
+ new_version_code=20251113
+ 
+-old_version="1\.12\.15"
+-new_version="1\.12\.16"
++old_version="1\.12\.16"
++new_version="1\.12\.17"
+ 
+ replace_str="s/$old_version/$new_version/g"
+ 
+diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
+index c4f6a333..bf6beed9 100644
+--- a/nodejs-addon-examples/package.json
++++ b/nodejs-addon-examples/package.json
+@@ -1,5 +1,5 @@
+ {
+   "dependencies": {
+-    "sherpa-onnx-node": "^1.12.16"
++    "sherpa-onnx-node": "^1.12.17"
+   }
+ }
+diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
+index f10c5745..ab2c37d2 100644
+--- a/nodejs-examples/package.json
++++ b/nodejs-examples/package.json
+@@ -2,7 +2,7 @@
+   "dependencies": {
+     "mic": "^2.1.2",
+     "naudiodon2": "^2.4.0",
+-    "sherpa-onnx": "^1.12.16",
++    "sherpa-onnx": "^1.12.17",
+     "wav": "^1.0.2"
+   }
+ }
+diff --git a/pom.xml b/pom.xml
+index 51eb50c8..e8191a24 100644
+--- a/pom.xml
++++ b/pom.xml
+@@ -4,7 +4,7 @@
+     <modelVersion>4.0.0</modelVersion>
+     <groupId>com.k2fsa.sherpa.onnx</groupId>
+     <artifactId>sherpa-onnx-android</artifactId>
+-    <version>1.12.16</version>
++    <version>1.12.17</version>
+     <url>https://github.com/k2-fsa/sherpa-onnx</url>
+     <packaging>pom</packaging>
+     <description>First Android Library</description>
+diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
+index 02dc7925..b7a22878 100644
+--- a/scripts/wheel/sherpa-onnx-bin/setup.py
++++ b/scripts/wheel/sherpa-onnx-bin/setup.py
+@@ -13,7 +13,7 @@ print("bin_files", bin_files)
+ 
+ setup(
+     name="sherpa-onnx-bin",
+-    version="1.12.16",
++    version="1.12.17",
+     description="Binary executables for sherpa-onnx",
+     author="The sherpa-onnx development team",
+     url="https://github.com/k2-fsa/sherpa-onnx",
+@@ -23,7 +23,7 @@ setup(
+     packages=[],
+     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
+     install_requires=[
+-        "sherpa-onnx-core==1.12.16",
++        "sherpa-onnx-core==1.12.17",
+     ],
+     classifiers=[
+         "Programming Language :: Python :: 3",
+diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
+index a71b3815..071319fb 100644
+--- a/scripts/wheel/sherpa-onnx-core/setup.py
++++ b/scripts/wheel/sherpa-onnx-core/setup.py
+@@ -23,7 +23,7 @@ def get_binaries():
+ 
+ setup(
+     name="sherpa-onnx-core",
+-    version="1.12.16",
++    version="1.12.17",
+     description="Core shared libraries for sherpa-onnx",
+     packages=["sherpa_onnx"],
+     include_package_data=True,
+diff --git a/setup.py b/setup.py
+index 6ffc01dc..648d118c 100644
+--- a/setup.py
++++ b/setup.py
+@@ -105,7 +105,7 @@ setuptools.setup(
+         ],
+     },
+     license="Apache licensed, as found in the LICENSE file",
+-    install_requires=["sherpa-onnx-core==1.12.16"] if need_split_package() else None,
++    install_requires=["sherpa-onnx-core==1.12.17"] if need_split_package() else None,
+ )
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
+diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
+index a7fc063a..02c48a40 100644
+--- a/sherpa-onnx/csrc/version.cc
++++ b/sherpa-onnx/csrc/version.cc
+@@ -7,17 +7,17 @@
+ namespace sherpa_onnx {
+ 
+ const char *GetGitDate() {
+-  static const char *date = "Thu Nov 13 19:03:01 2025";
++  static const char *date = "Thu Nov 13 19:08:29 2025";
+   return date;
+ }
+ 
+ const char *GetGitSha1() {
+-  static const char *sha1 = "0f647d32";
++  static const char *sha1 = "6d199b18";
+   return sha1;
+ }
+ 
+ const char *GetVersionStr() {
+-  static const char *version = "1.12.16";
++  static const char *version = "1.12.17";
+   return version;
+ }
+ 
+
+commit 6d199b18558b0f0ce383fcef83b3105d0439f85d
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 19:08:29 2025 +0800
+
+    Release v1.12.16 (#2784)
+
+diff --git a/CHANGELOG.md b/CHANGELOG.md
+index 6e28b458..7efbb6b3 100644
+--- a/CHANGELOG.md
++++ b/CHANGELOG.md
+@@ -1,3 +1,40 @@
++## 1.12.16
++
++* Support exporting SenseVoice and Paraformer to Ascend 310P3 NPU. (#2716)
++* Demo for no stream vad asr with flutter (#2705)
++* Fix crashing in Android KWS demo (#2719)
++* Add C++ API with ACL C API for SenseVoice ASR on Ascend NPU (#2728)
++* Allow up to 30 seconds ASR for sense-voice on Ascend NPU (#2729)
++* Fix compilation error for Ascend NPU (#2731)
++* docs: fix Flutter TTS macOS mirror link targets; fix speech-enhancement link typo (#2723)
++* Export models for Ascend910B2 (#2740)
++* Add C++ runtime for Paraformer on Ascend NPU. (#2741)
++* Expose ys probs to JNI, Kotlin and Java API (#2736)
++* Add CI for Ascend NPU (#2743)
++* Export models for CANN 8.2 (#2745)
++* Fix validating model config for Paraformer. (#2749)
++* Add cxx API for online punctuation models (#2759)
++* Export sense voice to qnn (#2760)
++* Export models to Ascend 910B3 (#2761)
++* Support MatchaTTS models for Chinese+English. (#2763)
++* Fix zipvoice. (#2764)
++* Support passing multiple lexicon files for matcha tts models. (#2765)
++* Begin to add qnn C API (#2766)
++* Add QnnConfig. (#2768)
++* Fix missing includes. (#2769)
++* Begin to export omnilingual-asr to sherpa-onnx (#2770)
++* Add C++ and Python API for Omnilingual ASR models. (#2772)
++* Add C API for Omnilingual ASR CTC models (#2773)
++* Add CXX API for Omnilingual ASR CTC models (#2774)
++* Add C# API for Omnilingual ASR CTC models (#2775)
++* Add Swift API for Omnilingual ASR CTC models (#2776)
++* Add Go API for Omnilingual ASR CTC models (#2778)
++* Add JavaScript (node-addon) API for Omnilingual ASR CTC models (#2780)
++* Add Dart API for Omnilingual ASR CTC models (#2779)
++* Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)
++* Add Pascal API for Omnilingual ASR CTC models (#2782)
++* Add Koltin and Java API for Omnilingual ASR CTC models (#2783)
++
+ ## 1.12.15
+ 
+ * Exposing online punctuation model support in node-addon-api (#2609)
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 92a3ff01..ac0be33b 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -14,7 +14,7 @@ project(sherpa-onnx)
+ # Remember to update
+ # ./CHANGELOG.md
+ # ./new-release.sh
+-set(SHERPA_ONNX_VERSION "1.12.15")
++set(SHERPA_ONNX_VERSION "1.12.16")
+ 
+ # Disable warning about
+ #
+diff --git a/android/SherpaOnnx/app/build.gradle b/android/SherpaOnnx/app/build.gradle
+index 1e20e730..1d59d8cb 100644
+--- a/android/SherpaOnnx/app/build.gradle
++++ b/android/SherpaOnnx/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnx2Pass/app/build.gradle b/android/SherpaOnnx2Pass/app/build.gradle
+index 1e20e730..1d59d8cb 100644
+--- a/android/SherpaOnnx2Pass/app/build.gradle
++++ b/android/SherpaOnnx2Pass/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxAar/README.md b/android/SherpaOnnxAar/README.md
+index d8a67dc6..3b1a5392 100644
+--- a/android/SherpaOnnxAar/README.md
++++ b/android/SherpaOnnxAar/README.md
+@@ -4,8 +4,8 @@
+ git clone https://github.com/k2-fsa/sherpa-onnx
+ cd sherpa-onnx
+ 
+-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-v1.12.15-android.tar.bz2
+-tar xvf sherpa-onnx-v1.12.15-android.tar.bz2
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-v1.12.16-android.tar.bz2
++tar xvf sherpa-onnx-v1.12.16-android.tar.bz2
+ 
+ cp -v jniLibs/arm64-v8a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/arm64-v8a/
+ cp -v jniLibs/armeabi-v7a/* android/SherpaOnnxAar/sherpa_onnx/src/main/jniLibs/armeabi-v7a/
+@@ -16,5 +16,5 @@ cd android/SherpaOnnxAar
+ 
+ ./gradlew :sherpa_onnx:assembleRelease
+ ls -lh ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar
+-cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.15.aar
++cp ./sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar ../../sherpa-onnx-1.12.16.aar
+ ```
+diff --git a/android/SherpaOnnxAudioTagging/app/build.gradle.kts b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+index 66925f8b..b4407bbe 100644
+--- a/android/SherpaOnnxAudioTagging/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTagging/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+index bfd5023d..2b95c60a 100644
+--- a/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxAudioTaggingWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.audio.tagging.wear.os"
+         minSdk = 26
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+diff --git a/android/SherpaOnnxJavaDemo/app/build.gradle b/android/SherpaOnnxJavaDemo/app/build.gradle
+index c59ab06e..039efe01 100644
+--- a/android/SherpaOnnxJavaDemo/app/build.gradle
++++ b/android/SherpaOnnxJavaDemo/app/build.gradle
+@@ -9,8 +9,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 28
+         targetSdk 34
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+@@ -34,5 +34,5 @@ dependencies {
+     implementation 'pub.devrel:easypermissions:3.0.0'
+     implementation 'androidx.core:core-ktx:1.7.0'
+     // implementation files('/Users/fangjun/open-source/sherpa-onnx/android/SherpaOnnxAar/sherpa_onnx/build/outputs/aar/sherpa_onnx-release.aar')
+-    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.15'
++    implementation 'com.github.k2-fsa:sherpa-onnx:v1.12.16'
+ }
+diff --git a/android/SherpaOnnxKws/app/build.gradle b/android/SherpaOnnxKws/app/build.gradle
+index 1e20e730..1d59d8cb 100644
+--- a/android/SherpaOnnxKws/app/build.gradle
++++ b/android/SherpaOnnxKws/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+index 59fe4c9b..5cd52b3c 100644
+--- a/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsr/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+index 103c683f..39ef31e9 100644
+--- a/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
++++ b/android/SherpaOnnxSimulateStreamingAsrWearOs/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.simulate.streaming.asr.wear.os"
+         minSdk = 28
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+         vectorDrawables {
+             useSupportLibrary = true
+         }
+@@ -58,7 +58,7 @@ dependencies {
+     implementation(libs.compose.foundation)
+     implementation(libs.activity.compose)
+     implementation(libs.core.splashscreen)
+-    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.15")
++    implementation("com.github.k2-fsa:sherpa-onnx:v1.12.16")
+     androidTestImplementation(platform(libs.compose.bom))
+     androidTestImplementation(libs.ui.test.junit4)
+     debugImplementation(libs.ui.tooling)
+diff --git a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+index e0128ee0..e538009a 100644
+--- a/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerDiarization/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.diarization"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+index fcd5f076..e49b49c2 100644
+--- a/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpeakerIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.speaker.identification"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+index 889c1efa..98f11773 100644
+--- a/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
++++ b/android/SherpaOnnxSpokenLanguageIdentification/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.slid"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxTts/app/build.gradle b/android/SherpaOnnxTts/app/build.gradle
+index 33d8e55f..2636a9b6 100644
+--- a/android/SherpaOnnxTts/app/build.gradle
++++ b/android/SherpaOnnxTts/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxTtsEngine/app/build.gradle.kts b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+index 3172dd30..1b1d0bc1 100644
+--- a/android/SherpaOnnxTtsEngine/app/build.gradle.kts
++++ b/android/SherpaOnnxTtsEngine/app/build.gradle.kts
+@@ -11,8 +11,8 @@ android {
+         applicationId = "com.k2fsa.sherpa.onnx.tts.engine"
+         minSdk = 21
+         targetSdk = 34
+-        versionCode = 20251022
+-        versionName = "1.12.15"
++        versionCode = 20251113
++        versionName = "1.12.16"
+ 
+         testInstrumentationRunner = "androidx.test.runner.AndroidJUnitRunner"
+         vectorDrawables {
+diff --git a/android/SherpaOnnxVad/app/build.gradle b/android/SherpaOnnxVad/app/build.gradle
+index c13e6886..566e93d2 100644
+--- a/android/SherpaOnnxVad/app/build.gradle
++++ b/android/SherpaOnnxVad/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxVadAsr/app/build.gradle b/android/SherpaOnnxVadAsr/app/build.gradle
+index c13e6886..566e93d2 100644
+--- a/android/SherpaOnnxVadAsr/app/build.gradle
++++ b/android/SherpaOnnxVadAsr/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 33
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/android/SherpaOnnxWebSocket/app/build.gradle b/android/SherpaOnnxWebSocket/app/build.gradle
+index 1ab62085..23547587 100644
+--- a/android/SherpaOnnxWebSocket/app/build.gradle
++++ b/android/SherpaOnnxWebSocket/app/build.gradle
+@@ -11,8 +11,8 @@ android {
+         applicationId "com.k2fsa.sherpa.onnx"
+         minSdk 21
+         targetSdk 32
+-        versionCode 20251022
+-        versionName "1.12.15"
++        versionCode 20251113
++        versionName "1.12.16"
+ 
+         testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
+     }
+diff --git a/build-ios-shared.sh b/build-ios-shared.sh
+index 8159dad9..80ca7b1f 100755
+--- a/build-ios-shared.sh
++++ b/build-ios-shared.sh
+@@ -242,7 +242,7 @@ for d in ios-arm64_x86_64-simulator ios-arm64; do
+ 	<key>CFBundlePackageType</key>
+ 	<string>FMWK</string>
+ 	<key>CFBundleShortVersionString</key>
+-	<string>1.12.15</string>
++	<string>1.12.16</string>
+ 	<key>CFBundleSupportedPlatforms</key>
+ 	<array>
+ 		<string>iPhoneOS</string>
+diff --git a/dart-api-examples/add-punctuations/pubspec.yaml b/dart-api-examples/add-punctuations/pubspec.yaml
+index 93da5a5e..4b1994a0 100644
+--- a/dart-api-examples/add-punctuations/pubspec.yaml
++++ b/dart-api-examples/add-punctuations/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/audio-tagging/pubspec.yaml b/dart-api-examples/audio-tagging/pubspec.yaml
+index c76d38c7..752eec75 100644
+--- a/dart-api-examples/audio-tagging/pubspec.yaml
++++ b/dart-api-examples/audio-tagging/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/keyword-spotter/pubspec.yaml b/dart-api-examples/keyword-spotter/pubspec.yaml
+index 9dba9dba..727da49f 100644
+--- a/dart-api-examples/keyword-spotter/pubspec.yaml
++++ b/dart-api-examples/keyword-spotter/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/non-streaming-asr/pubspec.yaml b/dart-api-examples/non-streaming-asr/pubspec.yaml
+index 63793658..42b1c2b0 100644
+--- a/dart-api-examples/non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speaker-diarization/pubspec.yaml b/dart-api-examples/speaker-diarization/pubspec.yaml
+index 632e39a1..f52042bb 100644
+--- a/dart-api-examples/speaker-diarization/pubspec.yaml
++++ b/dart-api-examples/speaker-diarization/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/speaker-identification/pubspec.yaml b/dart-api-examples/speaker-identification/pubspec.yaml
+index 97742512..bbe878f8 100644
+--- a/dart-api-examples/speaker-identification/pubspec.yaml
++++ b/dart-api-examples/speaker-identification/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+index 39b5ed56..0161e730 100644
+--- a/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
++++ b/dart-api-examples/speech-enhancement-gtcrn/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/spoken-language-identification/pubspec.yaml b/dart-api-examples/spoken-language-identification/pubspec.yaml
+index af5273f0..3248a361 100644
+--- a/dart-api-examples/spoken-language-identification/pubspec.yaml
++++ b/dart-api-examples/spoken-language-identification/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   path: ^1.9.0
+diff --git a/dart-api-examples/streaming-asr/pubspec.yaml b/dart-api-examples/streaming-asr/pubspec.yaml
+index 56c07e66..630ddd72 100644
+--- a/dart-api-examples/streaming-asr/pubspec.yaml
++++ b/dart-api-examples/streaming-asr/pubspec.yaml
+@@ -11,7 +11,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/tts/pubspec.yaml b/dart-api-examples/tts/pubspec.yaml
+index f98e8326..ee263496 100644
+--- a/dart-api-examples/tts/pubspec.yaml
++++ b/dart-api-examples/tts/pubspec.yaml
+@@ -8,7 +8,7 @@ environment:
+ 
+ # Add regular dependencies here.
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+index 79a643c2..005c49e1 100644
+--- a/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
++++ b/dart-api-examples/vad-with-non-streaming-asr/pubspec.yaml
+@@ -10,7 +10,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/dart-api-examples/vad/pubspec.yaml b/dart-api-examples/vad/pubspec.yaml
+index 9b7de990..7206336a 100644
+--- a/dart-api-examples/vad/pubspec.yaml
++++ b/dart-api-examples/vad/pubspec.yaml
+@@ -9,7 +9,7 @@ environment:
+   sdk: ">=3.0.0 <4.0.0"
+ 
+ dependencies:
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   path: ^1.9.0
+   args: ^2.5.0
+ 
+diff --git a/flutter-examples/non_streaming_vad_asr/pubspec.yaml b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+index 3d4aaf90..103aa7ee 100644
+--- a/flutter-examples/non_streaming_vad_asr/pubspec.yaml
++++ b/flutter-examples/non_streaming_vad_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.15
++version: 1.12.16
+ 
+ topics:
+   - speech-recognition
+@@ -31,7 +31,7 @@ dependencies:
+   record: 6.0.0
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/streaming_asr/pubspec.yaml b/flutter-examples/streaming_asr/pubspec.yaml
+index aa986f78..23aab88d 100644
+--- a/flutter-examples/streaming_asr/pubspec.yaml
++++ b/flutter-examples/streaming_asr/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none'
+ 
+-version: 1.12.15
++version: 1.12.16
+ 
+ topics:
+   - speech-recognition
+@@ -30,7 +30,7 @@ dependencies:
+   record: ^6.1.2
+   url_launcher: ^6.2.6
+ 
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+ 
+diff --git a/flutter-examples/tts/pubspec.yaml b/flutter-examples/tts/pubspec.yaml
+index 0e45ba3e..2c393901 100644
+--- a/flutter-examples/tts/pubspec.yaml
++++ b/flutter-examples/tts/pubspec.yaml
+@@ -5,7 +5,7 @@ description: >
+ 
+ publish_to: 'none' # Remove this line if you wish to publish to pub.dev
+ 
+-version: 1.12.15
++version: 1.12.16
+ 
+ environment:
+   sdk: ">=2.17.0 <4.0.0"
+@@ -18,7 +18,7 @@ dependencies:
+   cupertino_icons: ^1.0.6
+   path_provider: ^2.1.3
+   path: ^1.9.0
+-  sherpa_onnx: ^1.12.15
++  sherpa_onnx: ^1.12.16
+   # sherpa_onnx:
+   #   path: ../../flutter/sherpa_onnx
+   url_launcher: 6.2.6
+diff --git a/flutter/sherpa_onnx/pubspec.yaml b/flutter/sherpa_onnx/pubspec.yaml
+index 56b678b1..a04d6a71 100644
+--- a/flutter/sherpa_onnx/pubspec.yaml
++++ b/flutter/sherpa_onnx/pubspec.yaml
+@@ -17,7 +17,7 @@ topics:
+   - voice-activity-detection
+ 
+ # remember to change the version in ../sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+-version: 1.12.15
++version: 1.12.16
+ 
+ homepage: https://github.com/k2-fsa/sherpa-onnx
+ 
+@@ -30,23 +30,23 @@ dependencies:
+   flutter:
+     sdk: flutter
+ 
+-  sherpa_onnx_android: ^1.12.15
++  sherpa_onnx_android: ^1.12.16
+   # sherpa_onnx_android:
+   #   path: ../sherpa_onnx_android
+ 
+-  sherpa_onnx_macos: ^1.12.15
++  sherpa_onnx_macos: ^1.12.16
+   # sherpa_onnx_macos:
+   #   path: ../sherpa_onnx_macos
+ 
+-  sherpa_onnx_linux: ^1.12.15
++  sherpa_onnx_linux: ^1.12.16
+   # sherpa_onnx_linux:
+   #   path: ../sherpa_onnx_linux
+ 
+-  sherpa_onnx_windows: ^1.12.15
++  sherpa_onnx_windows: ^1.12.16
+   # sherpa_onnx_windows:
+   #   path: ../sherpa_onnx_windows
+ 
+-  sherpa_onnx_ios: ^1.12.15
++  sherpa_onnx_ios: ^1.12.16
+   # sherpa_onnx_ios:
+   #   path: ../sherpa_onnx_ios
+ 
+diff --git a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+index ab87f3c6..3119aff1 100644
+--- a/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
++++ b/flutter/sherpa_onnx_ios/ios/sherpa_onnx_ios.podspec
+@@ -7,7 +7,7 @@
+ # https://groups.google.com/g/dart-ffi/c/nUATMBy7r0c
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_ios'
+-  s.version          = '1.12.15'
++  s.version          = '1.12.16'
+   s.summary          = 'A new Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ A new Flutter FFI plugin project.
+diff --git a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+index 894c4470..17def84d 100644
+--- a/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
++++ b/flutter/sherpa_onnx_macos/macos/sherpa_onnx_macos.podspec
+@@ -4,7 +4,7 @@
+ #
+ Pod::Spec.new do |s|
+   s.name             = 'sherpa_onnx_macos'
+-  s.version          = '1.12.15'
++  s.version          = '1.12.16'
+   s.summary          = 'sherpa-onnx Flutter FFI plugin project.'
+   s.description      = <<-DESC
+ sherpa-onnx Flutter FFI plugin project.
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+index 06ac52e5..0228e2c5 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/BuildProfile.ets
+@@ -1,7 +1,7 @@
+ /**
+  * Use these variables when you tailor your ArkTS code. They must be of the const type.
+  */
+-export const HAR_VERSION = '1.12.15';
++export const HAR_VERSION = '1.12.16';
+ export const BUILD_MODE_NAME = 'debug';
+ export const DEBUG = true;
+ export const TARGET_NAME = 'default';
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+index 8f92e35e..d1b4325d 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/README.md
+@@ -23,7 +23,7 @@ or update your `oh-package.json5` to include the following:
+ 
+ ```
+   "dependencies": {
+-    "sherpa_onnx": "1.12.15",
++    "sherpa_onnx": "1.12.16",
+   },
+ ```
+ 
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5 b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+index da1ec3e2..b76d8527 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/oh-package.json5
+@@ -1,6 +1,6 @@
+ {
+   "name": "sherpa_onnx",
+-  "version": "1.12.15",
++  "version": "1.12.16",
+   "description": "On-device speech-to-text, text-to-speech, and speaker diarization using Next-gen Kaldi without Internet connection",
+   "main": "Index.ets",
+   "author": "The next-gen Kaldi team",
+diff --git a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+index 3de6c277..19efd920 100644
+--- a/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerDiarization/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.15"
++    "sherpa_onnx": "1.12.16"
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5 b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+index f6633742..913e059e 100644
+--- a/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxSpeakerIdentification/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.15",
++    "sherpa_onnx": "1.12.16",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+index f6633742..913e059e 100644
+--- a/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxStreamingAsr/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.15",
++    "sherpa_onnx": "1.12.16",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxTts/entry/oh-package.json5 b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+index f6633742..913e059e 100644
+--- a/harmony-os/SherpaOnnxTts/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxTts/entry/oh-package.json5
+@@ -6,7 +6,7 @@
+   "author": "",
+   "license": "",
+   "dependencies": {
+-    "sherpa_onnx": "1.12.15",
++    "sherpa_onnx": "1.12.16",
+   }
+ }
+ 
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/README.md b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+index 6ba991dd..3d6da438 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/README.md
++++ b/harmony-os/SherpaOnnxVadAsr/entry/README.md
+@@ -1,6 +1,6 @@
+ # Introduction
+ 
+-Please download ./sherpa_onnx-v1.12.15.har
++Please download ./sherpa_onnx-v1.12.16.har
+ from <https://huggingface.co/csukuangfj/sherpa-onnx-harmony-os/tree/main/har>
+ 
+ Hint: For users who have no access to huggingface, please use
+diff --git a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5 b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+index f72068ab..a1215a29 100644
+--- a/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
++++ b/harmony-os/SherpaOnnxVadAsr/entry/oh-package.json5
+@@ -7,7 +7,7 @@
+   "license": "",
+   "dependencies": {
+     // please see https://ohpm.openharmony.cn/#/cn/detail/sherpa_onnx
+-    "sherpa_onnx": "1.12.15",
++    "sherpa_onnx": "1.12.16",
+   }
+ }
+ 
+diff --git a/jitpack.yml b/jitpack.yml
+index 63f70826..aefbd313 100644
+--- a/jitpack.yml
++++ b/jitpack.yml
+@@ -2,8 +2,8 @@ jdk:
+   - openjdk17
+ 
+ before_install:
+-  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-1.12.15.aar
++  - wget https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-1.12.16.aar
+ 
+ install:
+-  - FILE="-Dfile=sherpa-onnx-1.12.15.aar"
+-  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.15 -Dpackaging=aar -DgeneratePom=true
++  - FILE="-Dfile=sherpa-onnx-1.12.16.aar"
++  - mvn install:install-file $FILE -DgroupId=com.k2fsa.sherpa.onnx -DartifactId=sherpa-onnx -Dversion=1.12.16 -Dpackaging=aar -DgeneratePom=true
+diff --git a/mfc-examples/README.md b/mfc-examples/README.md
+index a296b953..89f9292b 100644
+--- a/mfc-examples/README.md
++++ b/mfc-examples/README.md
+@@ -5,9 +5,9 @@ for speech recognition.
+ 
+ |Directory| Pre-built exe (x64)|Pre-built exe (x86)| Description|
+ |---------|--------------------|-------------------|------------|
+-|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-asr-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-asr-x86-v1.12.15.exe)| Non-streaming speech recognition|
+-|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-streaming-asr-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-streaming-asr-x86-v1.12.15.exe)| Streaming speech recognition|
+-|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-tts-x64-v1.12.15.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.15/sherpa-onnx-non-streaming-tts-x86-v1.12.15.exe)| Non-streaming text to speech|
++|[./NonStreamingSpeechRecognition](./NonStreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-asr-x86-v1.12.16.exe)| Non-streaming speech recognition|
++|[./StreamingSpeechRecognition](./StreamingSpeechRecognition)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-streaming-asr-x86-v1.12.16.exe)| Streaming speech recognition|
++|[./NonStreamingTextToSpeech](./NonStreamingTextToSpeech)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x64-v1.12.16.exe)|[URL](https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.16/sherpa-onnx-non-streaming-tts-x86-v1.12.16.exe)| Non-streaming text to speech|
+ 
+ Caution: You need to use Windows and install Visual Studio 2022 in order to
+ compile it.
+diff --git a/new-release.sh b/new-release.sh
+index 6570f2f3..d4784850 100755
+--- a/new-release.sh
++++ b/new-release.sh
+@@ -2,11 +2,11 @@
+ 
+ set -ex
+ 
+-old_version_code=20250918
+-new_version_code=20251022
++old_version_code=20251022
++new_version_code=20251113
+ 
+-old_version="1\.12\.14"
+-new_version="1\.12\.15"
++old_version="1\.12\.15"
++new_version="1\.12\.16"
+ 
+ replace_str="s/$old_version/$new_version/g"
+ 
+diff --git a/nodejs-addon-examples/package.json b/nodejs-addon-examples/package.json
+index 6e4d9016..c4f6a333 100644
+--- a/nodejs-addon-examples/package.json
++++ b/nodejs-addon-examples/package.json
+@@ -1,5 +1,5 @@
+ {
+   "dependencies": {
+-    "sherpa-onnx-node": "^1.12.15"
++    "sherpa-onnx-node": "^1.12.16"
+   }
+ }
+diff --git a/nodejs-examples/package.json b/nodejs-examples/package.json
+index a4374ef2..f10c5745 100644
+--- a/nodejs-examples/package.json
++++ b/nodejs-examples/package.json
+@@ -2,7 +2,7 @@
+   "dependencies": {
+     "mic": "^2.1.2",
+     "naudiodon2": "^2.4.0",
+-    "sherpa-onnx": "^1.12.15",
++    "sherpa-onnx": "^1.12.16",
+     "wav": "^1.0.2"
+   }
+ }
+diff --git a/pom.xml b/pom.xml
+index 22b9fbd4..51eb50c8 100644
+--- a/pom.xml
++++ b/pom.xml
+@@ -4,7 +4,7 @@
+     <modelVersion>4.0.0</modelVersion>
+     <groupId>com.k2fsa.sherpa.onnx</groupId>
+     <artifactId>sherpa-onnx-android</artifactId>
+-    <version>1.12.15</version>
++    <version>1.12.16</version>
+     <url>https://github.com/k2-fsa/sherpa-onnx</url>
+     <packaging>pom</packaging>
+     <description>First Android Library</description>
+diff --git a/scripts/wheel/sherpa-onnx-bin/setup.py b/scripts/wheel/sherpa-onnx-bin/setup.py
+index 4821b69f..02dc7925 100644
+--- a/scripts/wheel/sherpa-onnx-bin/setup.py
++++ b/scripts/wheel/sherpa-onnx-bin/setup.py
+@@ -13,7 +13,7 @@ print("bin_files", bin_files)
+ 
+ setup(
+     name="sherpa-onnx-bin",
+-    version="1.12.15",
++    version="1.12.16",
+     description="Binary executables for sherpa-onnx",
+     author="The sherpa-onnx development team",
+     url="https://github.com/k2-fsa/sherpa-onnx",
+@@ -23,7 +23,7 @@ setup(
+     packages=[],
+     data_files=[("Scripts", bin_files) if is_windows() else ("bin", bin_files)],
+     install_requires=[
+-        "sherpa-onnx-core==1.12.15",
++        "sherpa-onnx-core==1.12.16",
+     ],
+     classifiers=[
+         "Programming Language :: Python :: 3",
+diff --git a/scripts/wheel/sherpa-onnx-core/setup.py b/scripts/wheel/sherpa-onnx-core/setup.py
+index 0cfb2b15..a71b3815 100644
+--- a/scripts/wheel/sherpa-onnx-core/setup.py
++++ b/scripts/wheel/sherpa-onnx-core/setup.py
+@@ -23,7 +23,7 @@ def get_binaries():
+ 
+ setup(
+     name="sherpa-onnx-core",
+-    version="1.12.15",
++    version="1.12.16",
+     description="Core shared libraries for sherpa-onnx",
+     packages=["sherpa_onnx"],
+     include_package_data=True,
+diff --git a/setup.py b/setup.py
+index 22e9fec5..6ffc01dc 100644
+--- a/setup.py
++++ b/setup.py
+@@ -105,7 +105,7 @@ setuptools.setup(
+         ],
+     },
+     license="Apache licensed, as found in the LICENSE file",
+-    install_requires=["sherpa-onnx-core==1.12.15"] if need_split_package() else None,
++    install_requires=["sherpa-onnx-core==1.12.16"] if need_split_package() else None,
+ )
+ 
+ with open("sherpa-onnx/python/sherpa_onnx/__init__.py", "r") as f:
+diff --git a/sherpa-onnx/csrc/version.cc b/sherpa-onnx/csrc/version.cc
+index ed8c542c..a7fc063a 100644
+--- a/sherpa-onnx/csrc/version.cc
++++ b/sherpa-onnx/csrc/version.cc
+@@ -7,17 +7,17 @@
+ namespace sherpa_onnx {
+ 
+ const char *GetGitDate() {
+-  static const char *date = "Wed Oct 22 12:00:34 2025";
++  static const char *date = "Thu Nov 13 19:03:01 2025";
+   return date;
+ }
+ 
+ const char *GetGitSha1() {
+-  static const char *sha1 = "8a6dec20";
++  static const char *sha1 = "0f647d32";
+   return sha1;
+ }
+ 
+ const char *GetVersionStr() {
+-  static const char *version = "1.12.15";
++  static const char *version = "1.12.16";
+   return version;
+ }
+ 
+
+commit 0f647d32423d243aa108a43d78b67d87195e1e1e
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 19:03:01 2025 +0800
+
+    Add Koltin and Java API for Omnilingual ASR CTC models (#2783)
+
+diff --git a/.github/workflows/run-java-test.yaml b/.github/workflows/run-java-test.yaml
+index 3f6fda2e..b2167ff0 100644
+--- a/.github/workflows/run-java-test.yaml
++++ b/.github/workflows/run-java-test.yaml
+@@ -108,6 +108,20 @@ jobs:
+           cd ./java-api-examples
+           ./run-version-test.sh
+ 
++      - name:  Run java test (Omnilingual ASR CTC)
++        shell: bash
++        run: |
++          cd ./java-api-examples
++          ./run-non-streaming-decode-file-omnilingual-asr-ctc.sh
++          rm -rf sherpa-onnx-omnilingual-*
++
++      - name:  Run java test (WeNet CTC)
++        shell: bash
++        run: |
++          cd ./java-api-examples
++          ./run-non-streaming-decode-file-wenet-ctc.sh
++          rm -rf sherpa-onnx-wenet*
++
+       - name:  Run java test (Streaming T-one)
+         shell: bash
+         run: |
+diff --git a/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
+new file mode 100644
+index 00000000..cee8bb77
+--- /dev/null
++++ b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
+@@ -0,0 +1,54 @@
++// Copyright 2025 Xiaomi Corporation
++
++// This file shows how to use an offline Omnilingual ASR CTC model,
++// i.e., non-streaming Omnilingual ASR CTC model,
++// to decode files.
++import com.k2fsa.sherpa.onnx.*;
++
++public class NonStreamingDecodeFileOmnilingualAsrCtc {
++  public static void main(String[] args) {
++    // please refer to
++    // https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html
++    // to download model files
++    String model =
++        "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
++
++    String tokens =
++        "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
++
++    String waveFilename =
++        "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
++
++    WaveReader reader = new WaveReader(waveFilename);
++
++    OfflineOmnilingualAsrCtcModelConfig omnilingual =
++        OfflineOmnilingualAsrCtcModelConfig.builder().setModel(model).build();
++
++    OfflineModelConfig modelConfig =
++        OfflineModelConfig.builder()
++            .setOmnilingual(omnilingual)
++            .setTokens(tokens)
++            .setNumThreads(1)
++            .setDebug(true)
++            .build();
++
++    OfflineRecognizerConfig config =
++        OfflineRecognizerConfig.builder()
++            .setOfflineModelConfig(modelConfig)
++            .setDecodingMethod("greedy_search")
++            .build();
++
++    OfflineRecognizer recognizer = new OfflineRecognizer(config);
++    OfflineStream stream = recognizer.createStream();
++    stream.acceptWaveform(reader.getSamples(), reader.getSampleRate());
++
++    recognizer.decode(stream);
++
++    String text = recognizer.getResult(stream).getText();
++
++    System.out.printf("filename:%s\nresult:%s\n", waveFilename, text);
++
++    stream.release();
++    recognizer.release();
++  }
++}
+diff --git a/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh b/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh
+new file mode 100755
+index 00000000..025880ba
+--- /dev/null
++++ b/java-api-examples/run-non-streaming-decode-file-omnilingual-asr-ctc.sh
+@@ -0,0 +1,38 @@
++#!/usr/bin/env bash
++
++set -ex
++
++if [[ ! -f ../build/lib/libsherpa-onnx-jni.dylib  && ! -f ../build/lib/libsherpa-onnx-jni.so ]]; then
++  mkdir -p ../build
++  pushd ../build
++  cmake \
++    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
++    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
++    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
++    -DBUILD_SHARED_LIBS=ON \
++    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
++    -DSHERPA_ONNX_ENABLE_JNI=ON \
++    ..
++
++  make -j4
++  ls -lh lib
++  popd
++fi
++
++if [ ! -f ../sherpa-onnx/java-api/build/sherpa-onnx.jar ]; then
++  pushd ../sherpa-onnx/java-api
++  make
++  popd
++fi
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++
++java \
++  -Djava.library.path=$PWD/../build/lib \
++  -cp ../sherpa-onnx/java-api/build/sherpa-onnx.jar \
++  NonStreamingDecodeFileOmnilingualAsrCtc.java
+diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
+index e338f4d0..6dc1d164 100755
+--- a/kotlin-api-examples/run.sh
++++ b/kotlin-api-examples/run.sh
+@@ -488,6 +488,28 @@ function testOfflineNeMoCanary() {
+   java -Djava.library.path=../build/lib -jar $out_filename
+ }
+ 
++
++function testOfflineOmnilingualAsrCtc() {
++  if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++    tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++    rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  fi
++
++  out_filename=test_offline_omnilingual_asr_ctc.jar
++  kotlinc-jvm -include-runtime -d $out_filename \
++    test_offline_omnilingual_asr_ctc.kt \
++    FeatureConfig.kt \
++    HomophoneReplacerConfig.kt \
++    OfflineRecognizer.kt \
++    OfflineStream.kt \
++    WaveReader.kt \
++    faked-asset-manager.kt
++
++  ls -lh $out_filename
++  java -Djava.library.path=../build/lib -jar $out_filename
++}
++
+ function testOfflineWenetCtc() {
+   if [ ! -f sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10/model.int8.onnx ]; then
+     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+@@ -511,6 +533,7 @@ function testOfflineWenetCtc() {
+ 
+ testVersion
+ 
++testOfflineOmnilingualAsrCtc
+ testOfflineWenetCtc
+ testOfflineNeMoCanary
+ testOfflineSenseVoiceWithHr
+diff --git a/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt b/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt
+new file mode 100644
+index 00000000..a69908c4
+--- /dev/null
++++ b/kotlin-api-examples/test_offline_omnilingual_asr_ctc.kt
+@@ -0,0 +1,31 @@
++package com.k2fsa.sherpa.onnx
++
++fun main() {
++  val recognizer = createOfflineRecognizer()
++  val waveFilename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
++
++  val objArray = WaveReader.readWaveFromFile(
++      filename = waveFilename,
++  )
++  val samples: FloatArray = objArray[0] as FloatArray
++  val sampleRate: Int = objArray[1] as Int
++
++  var stream = recognizer.createStream()
++  stream.acceptWaveform(samples, sampleRate=sampleRate)
++  recognizer.decode(stream)
++
++  var result = recognizer.getResult(stream)
++  println(result)
++
++  stream.release()
++  recognizer.release()
++}
++
++
++fun createOfflineRecognizer(): OfflineRecognizer {
++  val config = OfflineRecognizerConfig(
++      modelConfig = getOfflineModelConfig(type = 44)!!,
++  )
++
++  return OfflineRecognizer(config = config)
++}
+diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
+index d1dee201..c84ef99c 100755
+--- a/scripts/apk/generate-vad-asr-apk-script.py
++++ b/scripts/apk/generate-vad-asr-apk-script.py
+@@ -743,6 +743,22 @@ def get_models():
+ 
+             ls -lh
+ 
++            popd
++            """,
++        ),
++        Model(
++            model_name="sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12",
++            idx=44,
++            lang="1600",
++            lang2="1600_languages",
++            short_name="omnilingual_asr_300M_ctc_int8",
++            cmd="""
++            pushd $model_name
++
++            rm -rfv test_wavs
++
++            ls -lh
++
+             popd
+             """,
+         ),
+diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
+index 597173e4..6bc3c054 100644
+--- a/sherpa-onnx/java-api/Makefile
++++ b/sherpa-onnx/java-api/Makefile
+@@ -37,6 +37,7 @@ java_files += OfflineMoonshineModelConfig.java
+ java_files += OfflineNemoEncDecCtcModelConfig.java
+ java_files += OfflineZipformerCtcModelConfig.java
+ java_files += OfflineWenetCtcModelConfig.java
++java_files += OfflineOmnilingualAsrCtcModelConfig.java
+ java_files += OfflineCanaryModelConfig.java
+ java_files += OfflineSenseVoiceModelConfig.java
+ java_files += OfflineDolphinModelConfig.java
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
+index 3e8c5bf5..19450ead 100644
+--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
+@@ -13,6 +13,7 @@ public class OfflineModelConfig {
+     private final OfflineDolphinModelConfig dolphin;
+     private final OfflineZipformerCtcModelConfig zipformerCtc;
+     private final OfflineWenetCtcModelConfig wenetCtc;
++    private final OfflineOmnilingualAsrCtcModelConfig omnilingual;
+     private final OfflineCanaryModelConfig canary;
+     private final String teleSpeech;
+     private final String tokens;
+@@ -34,6 +35,7 @@ public class OfflineModelConfig {
+         this.zipformerCtc = builder.zipformerCtc;
+         this.canary = builder.canary;
+         this.wenetCtc = builder.wenetCtc;
++        this.omnilingual = builder.omnilingual;
+         this.senseVoice = builder.senseVoice;
+         this.dolphin = builder.dolphin;
+         this.teleSpeech = builder.teleSpeech;
+@@ -86,6 +88,10 @@ public class OfflineModelConfig {
+         return wenetCtc;
+     }
+ 
++    public OfflineOmnilingualAsrCtcModelConfig getOmnilingual() {
++        return omnilingual;
++    }
++
+     public OfflineCanaryModelConfig getCanary() {
+         return canary;
+     }
+@@ -133,6 +139,7 @@ public class OfflineModelConfig {
+         private OfflineDolphinModelConfig dolphin = OfflineDolphinModelConfig.builder().build();
+         private OfflineZipformerCtcModelConfig zipformerCtc = OfflineZipformerCtcModelConfig.builder().build();
+         private OfflineWenetCtcModelConfig wenetCtc = OfflineWenetCtcModelConfig.builder().build();
++        private OfflineOmnilingualAsrCtcModelConfig omnilingual = OfflineOmnilingualAsrCtcModelConfig.builder().build();
+         private OfflineCanaryModelConfig canary = OfflineCanaryModelConfig.builder().build();
+         private String teleSpeech = "";
+         private String tokens = "";
+@@ -177,6 +184,11 @@ public class OfflineModelConfig {
+             return this;
+         }
+ 
++        public Builder setOmnilingual(OfflineOmnilingualAsrCtcModelConfig omnilingual) {
++            this.omnilingual = omnilingual;
++            return this;
++        }
++
+         public Builder setCanary(OfflineCanaryModelConfig canary) {
+             this.canary = canary;
+             return this;
+@@ -242,4 +254,4 @@ public class OfflineModelConfig {
+             return this;
+         }
+     }
+-}
+\ No newline at end of file
++}
+diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java
+new file mode 100644
+index 00000000..2603ce9a
+--- /dev/null
++++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig.java
+@@ -0,0 +1,30 @@
++package com.k2fsa.sherpa.onnx;
++
++public class OfflineOmnilingualAsrCtcModelConfig {
++    private final String model;
++
++    private OfflineOmnilingualAsrCtcModelConfig(Builder builder) {
++        this.model = builder.model;
++    }
++
++    public static Builder builder() {
++        return new Builder();
++    }
++
++    public String getModel() {
++        return model;
++    }
++
++    public static class Builder {
++        private String model = "";
++
++        public OfflineOmnilingualAsrCtcModelConfig build() {
++            return new OfflineOmnilingualAsrCtcModelConfig(this);
++        }
++
++        public Builder setModel(String model) {
++            this.model = model;
++            return this;
++        }
++    }
++}
+diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
+index baa58813..b091670e 100644
+--- a/sherpa-onnx/jni/offline-recognizer.cc
++++ b/sherpa-onnx/jni/offline-recognizer.cc
+@@ -193,6 +193,18 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
+   SHERPA_ONNX_JNI_READ_STRING(ans.model_config.wenet_ctc.model, model,
+                               wenet_ctc_config_cls, wenet_ctc_config);
+ 
++  // omnilingual asr ctc
++  fid = env->GetFieldID(
++      model_config_cls, "omnilingual",
++      "Lcom/k2fsa/sherpa/onnx/OfflineOmnilingualAsrCtcModelConfig;");
++  jobject omnilingual_ctc_config = env->GetObjectField(model_config, fid);
++  jclass omnilingual_ctc_config_cls =
++      env->GetObjectClass(omnilingual_ctc_config);
++
++  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.omnilingual.model, model,
++                              omnilingual_ctc_config_cls,
++                              omnilingual_ctc_config);
++
+   // canary
+   fid = env->GetFieldID(model_config_cls, "canary",
+                         "Lcom/k2fsa/sherpa/onnx/OfflineCanaryModelConfig;");
+diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+index 57287d76..1e94f4f9 100644
+--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
++++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+@@ -40,6 +40,10 @@ data class OfflineWenetCtcModelConfig(
+     var model: String = "",
+ )
+ 
++data class OfflineOmnilingualAsrCtcModelConfig(
++    var model: String = "",
++)
++
+ data class OfflineWhisperModelConfig(
+     var encoder: String = "",
+     var decoder: String = "",
+@@ -85,6 +89,7 @@ data class OfflineModelConfig(
+     var dolphin: OfflineDolphinModelConfig = OfflineDolphinModelConfig(),
+     var zipformerCtc: OfflineZipformerCtcModelConfig = OfflineZipformerCtcModelConfig(),
+     var wenetCtc: OfflineWenetCtcModelConfig = OfflineWenetCtcModelConfig(),
++    var omnilingual: OfflineOmnilingualAsrCtcModelConfig = OfflineOmnilingualAsrCtcModelConfig(),
+     var canary: OfflineCanaryModelConfig = OfflineCanaryModelConfig(),
+     var teleSpeech: String = "",
+     var numThreads: Int = 1,
+@@ -731,6 +736,16 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
+                 modelType = "paraformer",
+             )
+         }
++
++        44 -> {
++            val modelDir = "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12"
++            return OfflineModelConfig(
++                omnilingual = OfflineOmnilingualAsrCtcModelConfig(
++                    model = "$modelDir/model.int8.onnx",
++                ),
++                tokens = "$modelDir/tokens.txt",
++            )
++        }
+     }
+     return null
+ }
+
+commit c218342218e23fccc87fd333a04426dd79e1dd11
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 18:13:15 2025 +0800
+
+    Add Pascal API for Omnilingual ASR CTC models (#2782)
+
+diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
+index df3cf75a..6f4c55e4 100644
+--- a/.github/workflows/pascal.yaml
++++ b/.github/workflows/pascal.yaml
+@@ -126,6 +126,77 @@ jobs:
+             cp -v ../sherpa-onnx/pascal-api/*.pas ../pascal-api-examples/vad-with-non-streaming-asr
+           fi
+ 
++      - name:  Run Pascal test (Non Streaming ASR)
++        shell: bash
++        run: |
++          export PATH=/c/lazarus/fpc/3.2.2/bin/x86_64-win64:$PATH
++
++          cd ./pascal-api-examples
++
++          pushd non-streaming-asr
++
++          ./run-wenet-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-omnilingual-asr-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-nemo-canary.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-zipformer-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-dolphin-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-zipformer-transducer.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-moonshine.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-fire-red-asr.sh
++          rm -rf sherpa-onnx-fire-red-asr*
++          echo "---"
++
++          ./run-whisper.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-nemo-transducer.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-nemo-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-sense-voice.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-telespeech-ctc.sh
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ./run-paraformer.sh
++
++          ./run-paraformer-itn.sh
++
++          rm -rf sherpa-onnx-*
++          echo "---"
++
++          ls -lh
++          popd
++
+       - name:  Run Pascal test (Streaming ASR)
+         shell: bash
+         run: |
+@@ -195,69 +266,6 @@ jobs:
+           ./run-gtcrn.sh
+           ls -lh
+ 
+-      - name:  Run Pascal test (Non Streaming ASR)
+-        shell: bash
+-        run: |
+-          export PATH=/c/lazarus/fpc/3.2.2/bin/x86_64-win64:$PATH
+-
+-          cd ./pascal-api-examples
+-
+-          pushd non-streaming-asr
+-
+-          ./run-nemo-canary.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-zipformer-ctc.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-dolphin-ctc.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-zipformer-transducer.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-moonshine.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-fire-red-asr.sh
+-          rm -rf sherpa-onnx-fire-red-asr*
+-          echo "---"
+-
+-          ./run-whisper.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-nemo-transducer.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-nemo-ctc.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-sense-voice.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-telespeech-ctc.sh
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ./run-paraformer.sh
+-
+-          ./run-paraformer-itn.sh
+-
+-          rm -rf sherpa-onnx-*
+-          echo "---"
+-
+-          ls -lh
+-          popd
+-
+       - name:  Run Pascal test (Speaker diarization)
+         shell: bash
+         run: |
+diff --git a/pascal-api-examples/non-streaming-asr/.gitignore b/pascal-api-examples/non-streaming-asr/.gitignore
+index 635e4bff..d1917ed1 100644
+--- a/pascal-api-examples/non-streaming-asr/.gitignore
++++ b/pascal-api-examples/non-streaming-asr/.gitignore
+@@ -12,3 +12,4 @@ dolphin_ctc
+ zipformer_ctc
+ wenet_ctc
+ nemo_canary
++omnilingual_asr_ctc
+diff --git a/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas b/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas
+new file mode 100644
+index 00000000..00d7e35b
+--- /dev/null
++++ b/pascal-api-examples/non-streaming-asr/omnilingual_asr_ctc.pas
+@@ -0,0 +1,76 @@
++{ Copyright (c)  2025  Xiaomi Corporation }
++
++{
++This file shows how to use a non-streaming Omnilingual ASR CTC model
++to decode files.
++
++You can download the model files from
++https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++}
++
++program omnilingual_asr_ctc;
++
++{$mode objfpc}
++
++uses
++  sherpa_onnx,
++  DateUtils,
++  SysUtils;
++
++var
++  Wave: TSherpaOnnxWave;
++  WaveFilename: AnsiString;
++
++  Config: TSherpaOnnxOfflineRecognizerConfig;
++  Recognizer: TSherpaOnnxOfflineRecognizer;
++  Stream: TSherpaOnnxOfflineStream;
++  RecognitionResult: TSherpaOnnxOfflineRecognizerResult;
++
++  Start: TDateTime;
++  Stop: TDateTime;
++
++  Elapsed: Single;
++  Duration: Single;
++  RealTimeFactor: Single;
++begin
++  Initialize(Config);
++
++  Config.ModelConfig.Omnilingual.Model := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx';
++  Config.ModelConfig.Tokens := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt';
++  Config.ModelConfig.Provider := 'cpu';
++  Config.ModelConfig.NumThreads := 1;
++  Config.ModelConfig.Debug := False;
++
++  WaveFilename := './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
++
++  Wave := SherpaOnnxReadWave(WaveFilename);
++
++  Recognizer := TSherpaOnnxOfflineRecognizer.Create(Config);
++  Stream := Recognizer.CreateStream();
++  Start := Now;
++
++  Stream.AcceptWaveform(Wave.Samples, Wave.SampleRate);
++  Recognizer.Decode(Stream);
++
++  RecognitionResult := Recognizer.GetResult(Stream);
++
++  Stop := Now;
++
++  Elapsed := MilliSecondsBetween(Stop, Start) / 1000;
++  Duration := Length(Wave.Samples) / Wave.SampleRate;
++  RealTimeFactor := Elapsed / Duration;
++
++  WriteLn(RecognitionResult.ToString);
++  WriteLn(Format('NumThreads %d', [Config.ModelConfig.NumThreads]));
++  WriteLn(Format('Elapsed %.3f s', [Elapsed]));
++  WriteLn(Format('Wave duration %.3f s', [Duration]));
++  WriteLn(Format('RTF = %.3f/%.3f = %.3f', [Elapsed, Duration, RealTimeFactor]));
++
++  {Free resources to avoid memory leak.
++
++  Note: You don't need to invoke them for this simple script.
++  However, you have to invoke them in your own large/complex project.
++  }
++  FreeAndNil(Stream);
++  FreeAndNil(Recognizer);
++end.
+diff --git a/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh b/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
+new file mode 100755
+index 00000000..8a21a06f
+--- /dev/null
++++ b/pascal-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
+@@ -0,0 +1,42 @@
++#!/usr/bin/env bash
++
++set -ex
++
++SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
++SHERPA_ONNX_DIR=$(cd $SCRIPT_DIR/../.. && pwd)
++
++echo "SHERPA_ONNX_DIR: $SHERPA_ONNX_DIR"
++
++if [[ ! -f ../../build/install/lib/libsherpa-onnx-c-api.dylib  && ! -f ../../build/install/lib/libsherpa-onnx-c-api.so && ! -f ../../build/install/lib/sherpa-onnx-c-api.dll ]]; then
++  mkdir -p ../../build
++  pushd ../../build
++  cmake \
++    -DCMAKE_INSTALL_PREFIX=./install \
++    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
++    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
++    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
++    -DBUILD_SHARED_LIBS=ON \
++    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
++    ..
++
++  cmake --build . --target install --config Release
++  ls -lh lib
++  popd
++fi
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++fpc \
++  -dSHERPA_ONNX_USE_SHARED_LIBS \
++  -Fu$SHERPA_ONNX_DIR/sherpa-onnx/pascal-api \
++  -Fl$SHERPA_ONNX_DIR/build/install/lib \
++  ./omnilingual_asr_ctc.pas
++
++export LD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$LD_LIBRARY_PATH
++export DYLD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$DYLD_LIBRARY_PATH
++
++./omnilingual_asr_ctc
+diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+index 00c6a068..2357724d 100644
+--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
++++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
+@@ -335,6 +335,11 @@ type
+     function ToString: AnsiString;
+   end;
+ 
++  TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig = record
++    Model: AnsiString;
++    function ToString: AnsiString;
++  end;
++
+   TSherpaOnnxOfflineWhisperModelConfig = record
+     Encoder: AnsiString;
+     Decoder: AnsiString;
+@@ -410,6 +415,7 @@ type
+     ZipformerCtc: TSherpaOnnxOfflineZipformerCtcModelConfig;
+     Canary: TSherpaOnnxOfflineCanaryModelConfig;
+     WenetCtc: TSherpaOnnxOfflineWenetCtcModelConfig;
++    Omnilingual: TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
+     class operator Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineModelConfig);
+     function ToString: AnsiString;
+   end;
+@@ -820,6 +826,9 @@ type
+   SherpaOnnxOfflineWenetCtcModelConfig = record
+     Model: PAnsiChar;
+   end;
++  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig = record
++    Model: PAnsiChar;
++  end;
+   SherpaOnnxOfflineWhisperModelConfig = record
+     Encoder: PAnsiChar;
+     Decoder: PAnsiChar;
+@@ -877,6 +886,7 @@ type
+     ZipformerCtc: SherpaOnnxOfflineZipformerCtcModelConfig;
+     Canary: SherpaOnnxOfflineCanaryModelConfig;
+     WenetCtc: SherpaOnnxOfflineWenetCtcModelConfig;
++    Omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
+   end;
+ 
+   SherpaOnnxOfflineRecognizerConfig = record
+@@ -1704,6 +1714,12 @@ begin
+     [Self.Model]);
+ end;
+ 
++function TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig.ToString: AnsiString;
++begin
++  Result := Format('TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(Model := %s)',
++    [Self.Model]);
++end;
++
+ function TSherpaOnnxOfflineWhisperModelConfig.ToString: AnsiString;
+ begin
+   Result := Format('TSherpaOnnxOfflineWhisperModelConfig(' +
+@@ -1794,7 +1810,8 @@ begin
+     'Dolphin := %s, ' +
+     'ZipformerCtc := %s, ' +
+     'Canary := %s, ' +
+-    'WenetCtc := %s' +
++    'WenetCtc := %s, ' +
++    'Omnilingual := %s' +
+     ')',
+     [Self.Transducer.ToString, Self.Paraformer.ToString,
+      Self.NeMoCtc.ToString, Self.Whisper.ToString, Self.Tdnn.ToString,
+@@ -1802,7 +1819,8 @@ begin
+      Self.ModelType, Self.ModelingUnit, Self.BpeVocab,
+      Self.TeleSpeechCtc, Self.SenseVoice.ToString, Self.Moonshine.ToString,
+      Self.FireRedAsr.ToString, Self.Dolphin.ToString,
+-     Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString
++     Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString,
++     Self.Omnilingual.ToString
+      ]);
+ end;
+ 
+@@ -1882,6 +1900,7 @@ begin
+   C.ModelConfig.Canary.UsePnc := Ord(Config.ModelConfig.Canary.UsePnc);
+ 
+   C.ModelConfig.WenetCtc.Model := PAnsiChar(Config.ModelConfig.WenetCtc.Model);
++  C.ModelConfig.Omnilingual.Model := PAnsiChar(Config.ModelConfig.Omnilingual.Model);
+ 
+   C.LMConfig.Model := PAnsiChar(Config.LMConfig.Model);
+   C.LMConfig.Scale := Config.LMConfig.Scale;
+
+commit 7e71363fbf2213835a013882981cde576c607a4f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 18:12:33 2025 +0800
+
+    Add JavaScript (WebAssembly) API for Omnilingual ASR CTC models (#2781)
+
+diff --git a/.github/scripts/test-nodejs-npm.sh b/.github/scripts/test-nodejs-npm.sh
+index 87d7ffab..16f21b5f 100755
+--- a/.github/scripts/test-nodejs-npm.sh
++++ b/.github/scripts/test-nodejs-npm.sh
+@@ -9,6 +9,14 @@ git status
+ ls -lh
+ ls -lh node_modules
+ 
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++node ./test-offline-omnilingual-asr-ctc.js
++
++rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++
+ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+ tar xvf sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+ rm sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+diff --git a/nodejs-examples/README.md b/nodejs-examples/README.md
+index 46f79b0c..bc8c6832 100644
+--- a/nodejs-examples/README.md
++++ b/nodejs-examples/README.md
+@@ -203,10 +203,26 @@ rm sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03.tar.bz2
+ node ./test-offline-zipformer-ctc.js
+ ```
+ 
++## ./test-offline-omnilingual-asr-ctc.js
++
++[./test-offline-omnilingual-asr-ctc.js](./test-offline-omnilingual-asr-ctc.js) demonstrates
++how to decode a file with a Omnilingual ASR CTC model. In the code we use
++[sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2).
++
++You can use the following command to run it:
++
++```bash
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++node ./test-offline-omnilingual-asr-ctc.js
++```
++
+ ## ./test-offline-wenet-ctc.js
+ 
+ [./test-offline-wenet-ctc.js](./test-offline-wenet-ctc.js) demonstrates
+-how to decode a file with a Wenet CTC model. In the code we use
++how to decode a file with a WeNet CTC model. In the code we use
+ [sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2).
+ 
+ You can use the following command to run it:
+diff --git a/nodejs-examples/test-offline-omnilingual-asr-ctc.js b/nodejs-examples/test-offline-omnilingual-asr-ctc.js
+new file mode 100644
+index 00000000..ffa3d604
+--- /dev/null
++++ b/nodejs-examples/test-offline-omnilingual-asr-ctc.js
+@@ -0,0 +1,37 @@
++// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
++//
++const fs = require('fs');
++const {Readable} = require('stream');
++const wav = require('wav');
++
++const sherpa_onnx = require('sherpa-onnx');
++
++function createOfflineRecognizer() {
++  let config = {
++    modelConfig: {
++      omnilingual: {
++        model:
++            './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx',
++      },
++      tokens:
++          './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt',
++    }
++  };
++
++  return sherpa_onnx.createOfflineRecognizer(config);
++}
++
++const recognizer = createOfflineRecognizer();
++const stream = recognizer.createStream();
++
++const waveFilename =
++    './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
++const wave = sherpa_onnx.readWave(waveFilename);
++stream.acceptWaveform(wave.sampleRate, wave.samples);
++
++recognizer.decode(stream);
++const text = recognizer.getResult(stream).text;
++console.log(text);
++
++stream.free();
++recognizer.free();
+diff --git a/wasm/asr/sherpa-onnx-asr.js b/wasm/asr/sherpa-onnx-asr.js
+index 3ffb35ba..a77d61c1 100644
+--- a/wasm/asr/sherpa-onnx-asr.js
++++ b/wasm/asr/sherpa-onnx-asr.js
+@@ -55,6 +55,10 @@ function freeConfig(config, Module) {
+     freeConfig(config.wenetCtc, Module)
+   }
+ 
++  if ('omnilingual' in config) {
++    freeConfig(config.omnilingual, Module)
++  }
++
+   if ('moonshine' in config) {
+     freeConfig(config.moonshine, Module)
+   }
+@@ -755,6 +759,23 @@ function initSherpaOnnxOfflineWenetCtcModelConfig(config, Module) {
+   }
+ }
+ 
++function initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(config, Module) {
++  const n = Module.lengthBytesUTF8(config.model || '') + 1;
++
++  const buffer = Module._malloc(n);
++
++  const len = 1 * 4;  // 1 pointer
++  const ptr = Module._malloc(len);
++
++  Module.stringToUTF8(config.model || '', buffer, n);
++
++  Module.setValue(ptr, buffer, 'i8*');
++
++  return {
++    buffer: buffer, ptr: ptr, len: len,
++  }
++}
++
+ function initSherpaOnnxOfflineWhisperModelConfig(config, Module) {
+   const encoderLen = Module.lengthBytesUTF8(config.encoder || '') + 1;
+   const decoderLen = Module.lengthBytesUTF8(config.decoder || '') + 1;
+@@ -1025,6 +1046,12 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
+     };
+   }
+ 
++  if (!('omnilingual' in config)) {
++    config.omnilingual = {
++      model: '',
++    };
++  }
++
+   if (!('whisper' in config)) {
+     config.whisper = {
+       encoder: '',
+@@ -1109,9 +1136,13 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
+   const wenetCtc =
+       initSherpaOnnxOfflineWenetCtcModelConfig(config.wenetCtc, Module);
+ 
++  const omnilingual = initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
++      config.omnilingual, Module);
++
+   const len = transducer.len + paraformer.len + nemoCtc.len + whisper.len +
+       tdnn.len + 8 * 4 + senseVoice.len + moonshine.len + fireRedAsr.len +
+-      dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len;
++      dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len +
++      omnilingual.len;
+ 
+   const ptr = Module._malloc(len);
+ 
+@@ -1222,12 +1253,15 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
+   Module._CopyHeap(wenetCtc.ptr, wenetCtc.len, ptr + offset);
+   offset += wenetCtc.len;
+ 
++  Module._CopyHeap(omnilingual.ptr, omnilingual.len, ptr + offset);
++  offset += omnilingual.len;
++
+   return {
+     buffer: buffer, ptr: ptr, len: len, transducer: transducer,
+         paraformer: paraformer, nemoCtc: nemoCtc, whisper: whisper, tdnn: tdnn,
+         senseVoice: senseVoice, moonshine: moonshine, fireRedAsr: fireRedAsr,
+         dolphin: dolphin, zipformerCtc: zipformerCtc, canary: canary,
+-        wenetCtc: wenetCtc,
++        wenetCtc: wenetCtc, omnilingual: omnilingual
+   }
+ }
+ 
+diff --git a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
+index ac8fbfe1..f5ded701 100644
+--- a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
++++ b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
+@@ -15,6 +15,7 @@ static_assert(sizeof(SherpaOnnxOfflineParaformerModelConfig) == 4, "");
+ 
+ static_assert(sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) == 4, "");
+ static_assert(sizeof(SherpaOnnxOfflineWenetCtcModelConfig) == 4, "");
++static_assert(sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) == 4, "");
+ static_assert(sizeof(SherpaOnnxOfflineDolphinModelConfig) == 4, "");
+ static_assert(sizeof(SherpaOnnxOfflineNemoEncDecCtcModelConfig) == 4, "");
+ static_assert(sizeof(SherpaOnnxOfflineWhisperModelConfig) == 5 * 4, "");
+@@ -37,7 +38,8 @@ static_assert(sizeof(SherpaOnnxOfflineModelConfig) ==
+                       sizeof(SherpaOnnxOfflineDolphinModelConfig) +
+                       sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) +
+                       sizeof(SherpaOnnxOfflineCanaryModelConfig) +
+-                      sizeof(SherpaOnnxOfflineWenetCtcModelConfig),
++                      sizeof(SherpaOnnxOfflineWenetCtcModelConfig) +
++                      sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig),
+ 
+               "");
+ static_assert(sizeof(SherpaOnnxFeatureConfig) == 2 * 4, "");
+@@ -86,6 +88,7 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
+   auto zipformer_ctc = &model_config->zipformer_ctc;
+   auto canary = &model_config->canary;
+   auto wenet_ctc = &model_config->wenet_ctc;
++  auto omnilingual = &model_config->omnilingual;
+ 
+   fprintf(stdout, "----------offline transducer model config----------\n");
+   fprintf(stdout, "encoder: %s\n", transducer->encoder);
+@@ -139,6 +142,9 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
+   fprintf(stdout, "----------offline wenet ctc model config----------\n");
+   fprintf(stdout, "model: %s\n", wenet_ctc->model);
+ 
++  fprintf(stdout, "----------offline Omnilingual ASR model config----------\n");
++  fprintf(stdout, "model: %s\n", omnilingual->model);
++
+   fprintf(stdout, "tokens: %s\n", model_config->tokens);
+   fprintf(stdout, "num_threads: %d\n", model_config->num_threads);
+   fprintf(stdout, "provider: %s\n", model_config->provider);
+
+commit 1ab1104a456e17cb51078cfe15e8d980c8c8c722
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 18:11:25 2025 +0800
+
+    Add Dart API for Omnilingual ASR CTC models (#2779)
+
+diff --git a/.github/scripts/test-dart.sh b/.github/scripts/test-dart.sh
+index 863cfe20..a1e1e615 100755
+--- a/.github/scripts/test-dart.sh
++++ b/.github/scripts/test-dart.sh
+@@ -74,6 +74,10 @@ popd
+ 
+ pushd non-streaming-asr
+ 
++echo '----------Omnilingual ASR CTC----------'
++./run-omnilingual-asr-ctc.sh
++rm -rf sherpa-onnx-*
++
+ echo '----------Wenet CTC----------'
+ ./run-wenet-ctc.sh
+ rm -rf sherpa-onnx-*
+diff --git a/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart b/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart
+new file mode 100644
+index 00000000..b1dda8c4
+--- /dev/null
++++ b/dart-api-examples/non-streaming-asr/bin/omnilingual-asr-ctc.dart
+@@ -0,0 +1,52 @@
++// Copyright (c)  2025  Xiaomi Corporation
++import 'dart:io';
++
++import 'package:args/args.dart';
++import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
++
++import './init.dart';
++
++void main(List<String> arguments) async {
++  await initSherpaOnnx();
++
++  final parser = ArgParser()
++    ..addOption('model', help: 'Path to the Omnilingual ASR CTC model')
++    ..addOption('tokens', help: 'Path to tokens.txt')
++    ..addOption('input-wav', help: 'Path to input.wav to transcribe');
++
++  final res = parser.parse(arguments);
++  if (res['model'] == null ||
++      res['tokens'] == null ||
++      res['input-wav'] == null) {
++    print(parser.usage);
++    exit(1);
++  }
++
++  final model = res['model'] as String;
++  final tokens = res['tokens'] as String;
++  final inputWav = res['input-wav'] as String;
++
++  final omnilingual = sherpa_onnx.OfflineOmnilingualAsrCtcModelConfig(model: model);
++
++  final modelConfig = sherpa_onnx.OfflineModelConfig(
++    omnilingual: omnilingual,
++    tokens: tokens,
++    debug: true,
++    numThreads: 1,
++  );
++  final config = sherpa_onnx.OfflineRecognizerConfig(model: modelConfig);
++  final recognizer = sherpa_onnx.OfflineRecognizer(config);
++
++  final waveData = sherpa_onnx.readWave(inputWav);
++  final stream = recognizer.createStream();
++
++  stream.acceptWaveform(
++      samples: waveData.samples, sampleRate: waveData.sampleRate);
++  recognizer.decode(stream);
++
++  final result = recognizer.getResult(stream);
++  print(result.text);
++
++  stream.free();
++  recognizer.free();
++}
+diff --git a/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh b/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
+new file mode 100755
+index 00000000..d0e46e50
+--- /dev/null
++++ b/dart-api-examples/non-streaming-asr/run-omnilingual-asr-ctc.sh
+@@ -0,0 +1,17 @@
++#!/usr/bin/env bash
++
++set -ex
++
++dart pub get
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++dart run \
++  ./bin/omnilingual-asr-ctc.dart \
++  --model ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx \
++  --tokens ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt \
++  --input-wav ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav
+diff --git a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
+index 55126f9b..4fba68bf 100644
+--- a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
++++ b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
+@@ -146,6 +146,27 @@ class OfflineWenetCtcModelConfig {
+   final String model;
+ }
+ 
++class OfflineOmnilingualAsrCtcModelConfig {
++  const OfflineOmnilingualAsrCtcModelConfig({this.model = ''});
++
++  factory OfflineOmnilingualAsrCtcModelConfig.fromJson(Map<String, dynamic> json) {
++    return OfflineOmnilingualAsrCtcModelConfig(
++      model: json['model'] as String? ?? '',
++    );
++  }
++
++  @override
++  String toString() {
++    return 'OfflineOmnilingualAsrCtcModelConfig(model: $model)';
++  }
++
++  Map<String, dynamic> toJson() => {
++        'model': model,
++      };
++
++  final String model;
++}
++
+ class OfflineWhisperModelConfig {
+   const OfflineWhisperModelConfig(
+       {this.encoder = '',
+@@ -371,6 +392,7 @@ class OfflineModelConfig {
+     this.zipformerCtc = const OfflineZipformerCtcModelConfig(),
+     this.canary = const OfflineCanaryModelConfig(),
+     this.wenetCtc = const OfflineWenetCtcModelConfig(),
++    this.omnilingual = const OfflineOmnilingualAsrCtcModelConfig(),
+     required this.tokens,
+     this.numThreads = 1,
+     this.debug = true,
+@@ -431,6 +453,10 @@ class OfflineModelConfig {
+           ? OfflineWenetCtcModelConfig.fromJson(
+               json['wenetCtc'] as Map<String, dynamic>)
+           : const OfflineWenetCtcModelConfig(),
++      omnilingual: json['omnilingual'] != null
++          ? OfflineOmnilingualAsrCtcModelConfig.fromJson(
++              json['omnilingual'] as Map<String, dynamic>)
++          : const OfflineOmnilingualAsrCtcModelConfig(),
+       tokens: json['tokens'] as String,
+       numThreads: json['numThreads'] as int? ?? 1,
+       debug: json['debug'] as bool? ?? true,
+@@ -444,7 +470,7 @@ class OfflineModelConfig {
+ 
+   @override
+   String toString() {
+-    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
++    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, omnilingual: $omnilingual, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
+   }
+ 
+   Map<String, dynamic> toJson() => {
+@@ -460,6 +486,7 @@ class OfflineModelConfig {
+         'zipformerCtc': zipformerCtc.toJson(),
+         'canary': canary.toJson(),
+         'wenetCtc': wenetCtc.toJson(),
++        'omnilingual': omnilingual.toJson(),
+         'tokens': tokens,
+         'numThreads': numThreads,
+         'debug': debug,
+@@ -482,6 +509,7 @@ class OfflineModelConfig {
+   final OfflineZipformerCtcModelConfig zipformerCtc;
+   final OfflineCanaryModelConfig canary;
+   final OfflineWenetCtcModelConfig wenetCtc;
++  final OfflineOmnilingualAsrCtcModelConfig omnilingual;
+ 
+   final String tokens;
+   final int numThreads;
+@@ -719,6 +747,7 @@ class OfflineRecognizer {
+     c.ref.model.canary.usePnc = config.model.canary.usePnc ? 1 : 0;
+ 
+     c.ref.model.wenetCtc.model = config.model.wenetCtc.model.toNativeUtf8();
++    c.ref.model.omnilingual.model = config.model.omnilingual.model.toNativeUtf8();
+ 
+     c.ref.model.tokens = config.model.tokens.toNativeUtf8();
+ 
+@@ -764,6 +793,7 @@ class OfflineRecognizer {
+     calloc.free(c.ref.model.modelType);
+     calloc.free(c.ref.model.provider);
+     calloc.free(c.ref.model.tokens);
++    calloc.free(c.ref.model.omnilingual.model);
+     calloc.free(c.ref.model.wenetCtc.model);
+     calloc.free(c.ref.model.canary.tgtLang);
+     calloc.free(c.ref.model.canary.srcLang);
+diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+index b9147c58..8ce77797 100644
+--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
++++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+@@ -307,6 +307,10 @@ final class SherpaOnnxOfflineWenetCtcModelConfig extends Struct {
+   external Pointer<Utf8> model;
+ }
+ 
++final class SherpaOnnxOfflineOmnilingualAsrCtcModelConfig extends Struct {
++  external Pointer<Utf8> model;
++}
++
+ final class SherpaOnnxOfflineWhisperModelConfig extends Struct {
+   external Pointer<Utf8> encoder;
+   external Pointer<Utf8> decoder;
+@@ -387,6 +391,7 @@ final class SherpaOnnxOfflineModelConfig extends Struct {
+   external SherpaOnnxOfflineZipformerCtcModelConfig zipformerCtc;
+   external SherpaOnnxOfflineCanaryModelConfig canary;
+   external SherpaOnnxOfflineWenetCtcModelConfig wenetCtc;
++  external SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+ }
+ 
+ final class SherpaOnnxOfflineRecognizerConfig extends Struct {
+
+commit 4aee828b2974b000e51bc7aa3c0302673fe93d68
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 18:08:39 2025 +0800
+
+    Add JavaScript (node-addon) API for Omnilingual ASR CTC models (#2780)
+
+diff --git a/.github/scripts/test-nodejs-addon-npm.sh b/.github/scripts/test-nodejs-addon-npm.sh
+index 113d8137..397935c6 100755
+--- a/.github/scripts/test-nodejs-addon-npm.sh
++++ b/.github/scripts/test-nodejs-addon-npm.sh
+@@ -10,7 +10,17 @@ arch=$(node -p "require('os').arch()")
+ platform=$(node -p "require('os').platform()")
+ node_version=$(node -p "process.versions.node.split('.')[0]")
+ 
+-echo "----------non-streaming ASR Wenet CTC----------"
++echo "----------non-streaming ASR Omnilingual ASR CTC----------"
++
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++node ./test_asr_non_streaming_omnilingual_asr_ctc.js
++
++rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++
++echo "----------non-streaming ASR WeNet CTC----------"
+ 
+ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+ tar xvf sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+index 8ef59c4d..0e40f99e 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+@@ -78,6 +78,22 @@ static SherpaOnnxOfflineWenetCtcModelConfig GetOfflineWenetCtcModelConfig(
+   return c;
+ }
+ 
++static SherpaOnnxOfflineOmnilingualAsrCtcModelConfig
++GetOfflineOmnilingualAsrCtcModelConfig(Napi::Object obj) {
++  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig c;
++  memset(&c, 0, sizeof(c));
++
++  if (!obj.Has("omnilingual") || !obj.Get("omnilingual").IsObject()) {
++    return c;
++  }
++
++  Napi::Object o = obj.Get("omnilingual").As<Napi::Object>();
++
++  SHERPA_ONNX_ASSIGN_ATTR_STR(model, model);
++
++  return c;
++}
++
+ static SherpaOnnxOfflineDolphinModelConfig GetOfflineDolphinModelConfig(
+     Napi::Object obj) {
+   SherpaOnnxOfflineDolphinModelConfig c;
+@@ -243,6 +259,7 @@ static SherpaOnnxOfflineModelConfig GetOfflineModelConfig(Napi::Object obj) {
+   c.zipformer_ctc = GetOfflineZipformerCtcModelConfig(o);
+   c.canary = GetOfflineCanaryModelConfig(o);
+   c.wenet_ctc = GetOfflineWenetCtcModelConfig(o);
++  c.omnilingual = GetOfflineOmnilingualAsrCtcModelConfig(o);
+ 
+   SHERPA_ONNX_ASSIGN_ATTR_STR(tokens, tokens);
+   SHERPA_ONNX_ASSIGN_ATTR_INT32(num_threads, numThreads);
+@@ -336,6 +353,7 @@ static void FreeConfig(const SherpaOnnxOfflineRecognizerConfig &c) {
+   SHERPA_ONNX_DELETE_C_STR(c.model_config.canary.tgt_lang);
+ 
+   SHERPA_ONNX_DELETE_C_STR(c.model_config.wenet_ctc.model);
++  SHERPA_ONNX_DELETE_C_STR(c.model_config.omnilingual.model);
+ 
+   SHERPA_ONNX_DELETE_C_STR(c.model_config.tokens);
+   SHERPA_ONNX_DELETE_C_STR(c.model_config.provider);
+diff --git a/nodejs-addon-examples/README.md b/nodejs-addon-examples/README.md
+index 20bc5b6e..de4d6ce7 100644
+--- a/nodejs-addon-examples/README.md
++++ b/nodejs-addon-examples/README.md
+@@ -125,6 +125,7 @@ The following tables list the examples in this folder.
+ |[./test_vad_with_non_streaming_asr_moonshine.js](./test_vad_with_non_streaming_asr_moonshine.js)| Non-streaming speech recognition from a file using [Moonshine](https://github.com/usefulsensors/moonshine) + [Silero VAD](https://github.com/snakers4/silero-vad)|
+ |[./test_asr_non_streaming_nemo_ctc.js](./test_asr_non_streaming_nemo_ctc.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) CTC model with greedy search|
+ |[./test_asr_non_streaming_wenet_ctc.js](./test_asr_non_streaming_wenet_ctc.js)|Non-streaming speech recognition from a file using a [u2pp_conformer_yue](https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/u2pp_conformer_yue) CTC model with greedy search|
++|[./test_asr_non_streaming_omnilingual_asr_ctc.js](./test_asr_non_streaming_omnilingual_asr_ctc.js)|Non-streaming speech recognition from a file using a [Omnilingual-ASR](https://github.com/facebookresearch/omnilingual-asr) CTC model with greedy search|
+ |[./test_asr_non_streaming_nemo_canary.js](./test_asr_non_streaming_nemo_canary.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [Canary](https://k2-fsa.github.io/sherpa/onnx/nemo/canary.html#sherpa-onnx-nemo-canary-180m-flash-en-es-de-fr-int8-english-spanish-german-french) model|
+ |[./test_asr_non_streaming_zipformer_ctc.js](./test_asr_non_streaming_zipformer_ctc.js)|Non-streaming speech recognition from a file using a Zipformer CTC model with greedy search|
+ |[./test_asr_non_streaming_nemo_parakeet_tdt_v2.js](./test_asr_non_streaming_nemo_parakeet_tdt_v2.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [parakeet-tdt-0.6b-v2](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-parakeet-tdt-0-6b-v2-int8-english) model with greedy search|
+@@ -427,7 +428,17 @@ npm install naudiodon2
+ node ./test_vad_asr_non_streaming_nemo_ctc_microphone.js
+ ```
+ 
+-### Non-streaming speech recognition with Wenet CTC models
++### Non-streaming speech recognition with Omnilingual ASR CTC models
++
++```bash
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++node ./test_asr_non_streaming_omnilingual_asr_ctc.js
++```
++
++### Non-streaming speech recognition with WeNet CTC models
+ 
+ ```bash
+ wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
+index 42f25511..88f89e06 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_dolphin_ctc.js
+@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js b/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
+index 1b64e2d1..c9f6e928 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_fire_red_asr.js
+@@ -33,7 +33,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_moonshine.js b/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
+index a0792a24..ebdcf949 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_moonshine.js
+@@ -34,7 +34,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
+index 05668fba..5a106e10 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_canary.js
+@@ -37,7 +37,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-let result = recognizer.getResult(stream)
++let result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
+index 5130a39f..aced80e3 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_ctc.js
+@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js b/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
+index ac3517c3..20b7784e 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_nemo_parakeet_tdt_v2.js
+@@ -35,7 +35,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js
+new file mode 100644
+index 00000000..e55d19c9
+--- /dev/null
++++ b/nodejs-addon-examples/test_asr_non_streaming_omnilingual_asr_ctc.js
+@@ -0,0 +1,48 @@
++// Copyright (c)  2025  Xiaomi Corporation
++const sherpa_onnx = require('sherpa-onnx-node');
++
++// Please download test files from
++// https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++const config = {
++  'featConfig': {
++    'sampleRate': 16000,
++    'featureDim': 80,
++  },
++  'modelConfig': {
++    'omnilingual': {
++      'model':
++          './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx',
++    },
++    'tokens':
++        './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt',
++    'numThreads': 2,
++    'provider': 'cpu',
++    'debug': 1,
++  }
++};
++
++const waveFilename =
++    './sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav';
++
++const recognizer = new sherpa_onnx.OfflineRecognizer(config);
++console.log('Started')
++let start = Date.now();
++const stream = recognizer.createStream();
++const wave = sherpa_onnx.readWave(waveFilename);
++stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
++
++recognizer.decode(stream);
++const result = recognizer.getResult(stream);
++let stop = Date.now();
++console.log('Done')
++
++const elapsed_seconds = (stop - start) / 1000;
++const duration = wave.samples.length / wave.sampleRate;
++const real_time_factor = elapsed_seconds / duration;
++console.log('Wave duration', duration.toFixed(3), 'seconds')
++console.log('Elapsed', elapsed_seconds.toFixed(3), 'seconds')
++console.log(
++    `RTF = ${elapsed_seconds.toFixed(3)}/${duration.toFixed(3)} =`,
++    real_time_factor.toFixed(3))
++console.log(waveFilename)
++console.log('result\n', result)
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_paraformer.js b/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
+index 157ccdea..4f1ecd2e 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_paraformer.js
+@@ -30,7 +30,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js b/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
+index b4a693f5..9e0859c7 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_paraformer_itn.js
+@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js b/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
+index b573cf29..dd79d011 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_sense_voice.js
+@@ -47,7 +47,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js b/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
+index ab2484f7..280c82f2 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_sense_voice_with_hr.js
+@@ -53,7 +53,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_transducer.js b/nodejs-addon-examples/test_asr_non_streaming_transducer.js
+index 3d6dd2ac..e3f22549 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_transducer.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_transducer.js
+@@ -34,7 +34,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
+index d59328ff..e1aa7f6e 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_wenet_ctc.js
+@@ -32,7 +32,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_whisper.js b/nodejs-addon-examples/test_asr_non_streaming_whisper.js
+index da8a32bf..a811a2c0 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_whisper.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_whisper.js
+@@ -33,7 +33,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
+index 3e5b25e9..0dfc61cf 100644
+--- a/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
++++ b/nodejs-addon-examples/test_asr_non_streaming_zipformer_ctc.js
+@@ -30,7 +30,7 @@ const wave = sherpa_onnx.readWave(waveFilename);
+ stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+ 
+ recognizer.decode(stream);
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_ctc.js b/nodejs-addon-examples/test_asr_streaming_ctc.js
+index e5936a31..2f69ea3d 100644
+--- a/nodejs-addon-examples/test_asr_streaming_ctc.js
++++ b/nodejs-addon-examples/test_asr_streaming_ctc.js
+@@ -37,7 +37,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js b/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
+index 3537663c..93940d0e 100644
+--- a/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
++++ b/nodejs-addon-examples/test_asr_streaming_ctc_hlg.js
+@@ -40,7 +40,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_paraformer.js b/nodejs-addon-examples/test_asr_streaming_paraformer.js
+index a03453dd..a87d94b1 100644
+--- a/nodejs-addon-examples/test_asr_streaming_paraformer.js
++++ b/nodejs-addon-examples/test_asr_streaming_paraformer.js
+@@ -38,7 +38,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js b/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
+index 2e7fcf71..c8bd1660 100644
+--- a/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
++++ b/nodejs-addon-examples/test_asr_streaming_t_one_ctc.js
+@@ -34,7 +34,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_transducer.js b/nodejs-addon-examples/test_asr_streaming_transducer.js
+index 3bb3de7c..61898212 100644
+--- a/nodejs-addon-examples/test_asr_streaming_transducer.js
++++ b/nodejs-addon-examples/test_asr_streaming_transducer.js
+@@ -41,7 +41,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_transducer_itn.js b/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
+index 713db644..fce49460 100644
+--- a/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
++++ b/nodejs-addon-examples/test_asr_streaming_transducer_itn.js
+@@ -43,7 +43,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js b/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
+index 471d3084..56a06d52 100644
+--- a/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
++++ b/nodejs-addon-examples/test_asr_streaming_transducer_with_hr.js
+@@ -44,7 +44,7 @@ stream.acceptWaveform({samples: tailPadding, sampleRate: wave.sampleRate});
+ while (recognizer.isReady(stream)) {
+   recognizer.decode(stream);
+ }
+-result = recognizer.getResult(stream)
++const result = recognizer.getResult(stream);
+ let stop = Date.now();
+ console.log('Done')
+ 
+diff --git a/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js b/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
+index adf4f4ea..3b3efbb6 100644
+--- a/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
++++ b/nodejs-addon-examples/test_keyword_spotter_transducer_microphone.js
+@@ -62,7 +62,7 @@ ai.on('data', data => {
+     kws.decode(stream);
+   }
+ 
+-  const keyword = kws.getResult(stream).keyword
++  const keyword = kws.getResult(stream).keyword;
+   if (keyword != '') {
+     display.print(segmentIndex, keyword);
+     segmentIndex += 1;
+diff --git a/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js b/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
+index 91914f72..e772e6ba 100644
+--- a/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
++++ b/nodejs-addon-examples/test_tts_non_streaming_matcha_icefall_en.js
+@@ -10,7 +10,6 @@ function createOfflineTts() {
+       matcha: {
+         acousticModel: './matcha-icefall-en_US-ljspeech/model-steps-3.onnx',
+         vocoder: './vocos-22khz-univ.onnx',
+-        lexicon: './matcha-icefall-en_US-ljspeech/lexicon.txt',
+         tokens: './matcha-icefall-en_US-ljspeech/tokens.txt',
+         dataDir: './matcha-icefall-en_US-ljspeech/espeak-ng-data',
+       },
+
+commit 634f4fdb1da37a45c20fe81a163697b801d7cf6a
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 16:23:38 2025 +0800
+
+    Add Go API for Omnilingual ASR CTC models (#2778)
+
+diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
+index cb18b4b3..e362418e 100644
+--- a/.github/workflows/test-go.yaml
++++ b/.github/workflows/test-go.yaml
+@@ -101,6 +101,7 @@ jobs:
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/keyword-spotting-from-file/
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-canary-decode-files/
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-decode-files/
++            cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-speaker-diarization/
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-tts/
+             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/speaker-identification/
+@@ -140,6 +141,19 @@ jobs:
+           name: ${{ matrix.os }}-libs
+           path: to-upload/
+ 
++      - name: Test non-streaming decoding files with Omnilingual ASR
++        shell: bash
++        run: |
++          cd scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
++          ls -lh
++          go mod tidy
++          cat go.mod
++          go build
++          ls -lh
++
++          ./run.sh
++          rm -rf sherpa-onnx-omnilingual-*
++
+       - name: Test non-streaming TTS
+         shell: bash
+         run: |
+diff --git a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
+index fe764d1b..154f956a 100755
+--- a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
++++ b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
+@@ -2,7 +2,7 @@
+ 
+ set -ex
+ 
+-if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12 ]; then
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
+   curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+   tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+   rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
+diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
+new file mode 100644
+index 00000000..7f0a0a3e
+--- /dev/null
++++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
+@@ -0,0 +1,3 @@
++module non-streaming-omnilingual-asr-ctc-decode-files
++
++go 1.17
+diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
+new file mode 100644
+index 00000000..e910ee80
+--- /dev/null
++++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
+@@ -0,0 +1,97 @@
++package main
++
++import (
++	"bytes"
++	"encoding/binary"
++	"log"
++	"os"
++	"strings"
++
++	sherpa "github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx"
++	"github.com/youpy/go-wav"
++)
++
++func main() {
++	log.SetFlags(log.LstdFlags | log.Lmicroseconds)
++
++	config := sherpa.OfflineRecognizerConfig{}
++
++	config.ModelConfig.Omnilingual.Model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
++	config.ModelConfig.Tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
++
++	waveFilename := "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
++
++	samples, sampleRate := readWave(waveFilename)
++
++	log.Println("Initializing recognizer (may take several seconds)")
++	recognizer := sherpa.NewOfflineRecognizer(&config)
++	log.Println("Recognizer created!")
++	defer sherpa.DeleteOfflineRecognizer(recognizer)
++
++	log.Println("Start decoding!")
++	stream := sherpa.NewOfflineStream(recognizer)
++	defer sherpa.DeleteOfflineStream(stream)
++
++	stream.AcceptWaveform(sampleRate, samples)
++
++	recognizer.Decode(stream)
++	log.Println("Decoding done!")
++	result := stream.GetResult()
++
++	log.Println("Text: " + strings.ToLower(result.Text))
++}
++
++func readWave(filename string) (samples []float32, sampleRate int) {
++	file, _ := os.Open(filename)
++	defer file.Close()
++
++	reader := wav.NewReader(file)
++	format, err := reader.Format()
++	if err != nil {
++		log.Fatalf("Failed to read wave format")
++	}
++
++	if format.AudioFormat != 1 {
++		log.Fatalf("Support only PCM format. Given: %v\n", format.AudioFormat)
++	}
++
++	if format.NumChannels != 1 {
++		log.Fatalf("Support only 1 channel wave file. Given: %v\n", format.NumChannels)
++	}
++
++	if format.BitsPerSample != 16 {
++		log.Fatalf("Support only 16-bit per sample. Given: %v\n", format.BitsPerSample)
++	}
++
++	reader.Duration() // so that it initializes reader.Size
++
++	buf := make([]byte, reader.Size)
++	n, err := reader.Read(buf)
++	if n != int(reader.Size) {
++		log.Fatalf("Failed to read %v bytes. Returned %v bytes\n", reader.Size, n)
++	}
++
++	samples = samplesInt16ToFloat(buf)
++	sampleRate = int(format.SampleRate)
++
++	return
++}
++
++func samplesInt16ToFloat(inSamples []byte) []float32 {
++	numSamples := len(inSamples) / 2
++	outSamples := make([]float32, numSamples)
++
++	for i := 0; i != numSamples; i++ {
++		s := inSamples[i*2 : (i+1)*2]
++
++		var s16 int16
++		buf := bytes.NewReader(s)
++		err := binary.Read(buf, binary.LittleEndian, &s16)
++		if err != nil {
++			log.Fatal("Failed to parse 16-bit sample")
++		}
++		outSamples[i] = float32(s16) / 32768
++	}
++
++	return outSamples
++}
+diff --git a/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
+new file mode 100755
+index 00000000..f996d7cc
+--- /dev/null
++++ b/go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
+@@ -0,0 +1,13 @@
++#!/usr/bin/env bash
++
++set -ex
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++go mod tidy
++go build
++./non-streaming-omnilingual-asr-ctc-decode-files
+diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
+new file mode 100644
+index 00000000..0fac61a7
+--- /dev/null
++++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/go.mod
+@@ -0,0 +1,5 @@
++module non-streaming-omnilingual-asr-ctc-decode-files
++
++go 1.17
++
++replace github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx => ../
+diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go
+new file mode 120000
+index 00000000..68aebe63
+--- /dev/null
++++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/main.go
+@@ -0,0 +1 @@
++../../../../go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/main.go
+\ No newline at end of file
+diff --git a/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
+new file mode 120000
+index 00000000..82b1c19b
+--- /dev/null
++++ b/scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
+@@ -0,0 +1 @@
++../../../../go-api-examples/non-streaming-omnilingual-asr-ctc-decode-files/run.sh
+\ No newline at end of file
+diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
+index 5fe44e22..721db857 100644
+--- a/scripts/go/sherpa_onnx.go
++++ b/scripts/go/sherpa_onnx.go
+@@ -419,6 +419,10 @@ type OfflineWenetCtcModelConfig struct {
+ 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
+ }
+ 
++type OfflineOmnilingualAsrCtcModelConfig struct {
++	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
++}
++
+ type OfflineDolphinModelConfig struct {
+ 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
+ }
+@@ -480,6 +484,7 @@ type OfflineModelConfig struct {
+ 	ZipformerCtc OfflineZipformerCtcModelConfig
+ 	Canary       OfflineCanaryModelConfig
+ 	WenetCtc     OfflineWenetCtcModelConfig
++	Omnilingual     OfflineOmnilingualAsrCtcModelConfig
+ 	Tokens       string // Path to tokens.txt
+ 
+ 	// Number of threads to use for neural network computation
+@@ -583,6 +588,8 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
+ 
+ 	c.model_config.wenet_ctc.model = C.CString(config.ModelConfig.WenetCtc.Model)
+ 
++	c.model_config.omnilingual.model = C.CString(config.ModelConfig.Omnilingual.Model)
++
+ 	c.model_config.tokens = C.CString(config.ModelConfig.Tokens)
+ 
+ 	c.model_config.num_threads = C.int(config.ModelConfig.NumThreads)
+@@ -735,6 +742,11 @@ func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig)
+ 		c.model_config.wenet_ctc.model = nil
+ 	}
+ 
++	if c.model_config.omnilingual.model != nil {
++		C.free(unsafe.Pointer(c.model_config.omnilingual.model))
++		c.model_config.omnilingual.model = nil
++	}
++
+ 	if c.model_config.tokens != nil {
+ 		C.free(unsafe.Pointer(c.model_config.tokens))
+ 		c.model_config.tokens = nil
+
+commit e6062c9cca2cdc2afeebac9de4e90f4c1d702603
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 15:41:32 2025 +0800
+
+    Add Swift API for Omnilingual ASR CTC models (#2776)
+
+diff --git a/.github/scripts/test-swift.sh b/.github/scripts/test-swift.sh
+index 59af61c7..9944d861 100755
+--- a/.github/scripts/test-swift.sh
++++ b/.github/scripts/test-swift.sh
+@@ -9,6 +9,9 @@ ls -lh
+ 
+ ./run-test-version.sh
+ 
++./run-omnilingual-asr-ctc-asr.sh
++rm -rf sherpa-onnx-omnilingual-*
++
+ ./run-decode-file-t-one-streaming.sh
+ rm -rf sherpa-onnx-streaming-*
+ 
+diff --git a/swift-api-examples/.gitignore b/swift-api-examples/.gitignore
+index 2eeba0bb..52857338 100644
+--- a/swift-api-examples/.gitignore
++++ b/swift-api-examples/.gitignore
+@@ -24,3 +24,4 @@ dolphin-ctc-asr
+ tts-kitten-en
+ compute-speaker-embeddings
+ decode-file-t-one-streaming
++omnilingual-asr-ctc
+diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
+index 6c8454c8..b18b8e1a 100644
+--- a/swift-api-examples/SherpaOnnx.swift
++++ b/swift-api-examples/SherpaOnnx.swift
+@@ -368,6 +368,14 @@ func sherpaOnnxOfflineWenetCtcModelConfig(
+   )
+ }
+ 
++func sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
++  model: String = ""
++) -> SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
++  return SherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
++    model: toCPointer(model)
++  )
++}
++
+ func sherpaOnnxOfflineNemoEncDecCtcModelConfig(
+   model: String = ""
+ ) -> SherpaOnnxOfflineNemoEncDecCtcModelConfig {
+@@ -492,7 +500,9 @@ func sherpaOnnxOfflineModelConfig(
+     sherpaOnnxOfflineZipformerCtcModelConfig(),
+   canary: SherpaOnnxOfflineCanaryModelConfig = sherpaOnnxOfflineCanaryModelConfig(),
+   wenetCtc: SherpaOnnxOfflineWenetCtcModelConfig =
+-    sherpaOnnxOfflineWenetCtcModelConfig()
++    sherpaOnnxOfflineWenetCtcModelConfig(),
++  omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig =
++    sherpaOnnxOfflineOmnilingualAsrCtcModelConfig()
+ ) -> SherpaOnnxOfflineModelConfig {
+   return SherpaOnnxOfflineModelConfig(
+     transducer: transducer,
+@@ -514,7 +524,8 @@ func sherpaOnnxOfflineModelConfig(
+     dolphin: dolphin,
+     zipformer_ctc: zipformerCtc,
+     canary: canary,
+-    wenet_ctc: wenetCtc
++    wenet_ctc: wenetCtc,
++    omnilingual: omnilingual
+   )
+ }
+ 
+diff --git a/swift-api-examples/omnilingual-asr-ctc.swift b/swift-api-examples/omnilingual-asr-ctc.swift
+new file mode 100644
+index 00000000..0384e6f6
+--- /dev/null
++++ b/swift-api-examples/omnilingual-asr-ctc.swift
+@@ -0,0 +1,41 @@
++func run() {
++  let model =
++    "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
++  let tokens =
++    "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
++
++  let omnilingual = sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
++    model: model
++  )
++
++  let modelConfig = sherpaOnnxOfflineModelConfig(
++    tokens: tokens,
++    debug: 0,
++    omnilingual: omnilingual
++  )
++
++  let featConfig = sherpaOnnxFeatureConfig()
++  var config = sherpaOnnxOfflineRecognizerConfig(
++    featConfig: featConfig,
++    modelConfig: modelConfig
++  )
++
++  let recognizer = SherpaOnnxOfflineRecognizer(config: &config)
++
++  let filePath = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
++  let audio = SherpaOnnxWaveWrapper.readWave(filename: filePath)
++
++  let result = recognizer.decode(samples: audio.samples, sampleRate: audio.sampleRate)
++
++  print("\nresult is:\n\(result.text)")
++  if result.timestamps.count != 0 {
++    print("\ntimestamps is:\n\(result.timestamps)")
++  }
++}
++
++@main
++struct App {
++  static func main() {
++    run()
++  }
++}
+diff --git a/swift-api-examples/run-omnilingual-asr-ctc-asr.sh b/swift-api-examples/run-omnilingual-asr-ctc-asr.sh
+new file mode 100755
+index 00000000..65870027
+--- /dev/null
++++ b/swift-api-examples/run-omnilingual-asr-ctc-asr.sh
+@@ -0,0 +1,34 @@
++#!/usr/bin/env bash
++
++set -ex
++
++if [ ! -d ../build-swift-macos ]; then
++  echo "Please run ../build-swift-macos.sh first!"
++  exit 1
++fi
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++if [ ! -e ./omnilingual-asr-ctc ]; then
++  # Note: We use -lc++ to link against libc++ instead of libstdc++
++  swiftc \
++    -lc++ \
++    -I ../build-swift-macos/install/include \
++    -import-objc-header ./SherpaOnnx-Bridging-Header.h \
++    ./omnilingual-asr-ctc.swift  ./SherpaOnnx.swift \
++    -L ../build-swift-macos/install/lib/ \
++    -l sherpa-onnx \
++    -l onnxruntime \
++    -o omnilingual-asr-ctc
++
++  strip omnilingual-asr-ctc
++else
++  echo "./omnilingual-asr-ctc exists - skip building"
++fi
++
++export DYLD_LIBRARY_PATH=$PWD/../build-swift-macos/install/lib:$DYLD_LIBRARY_PATH
++./omnilingual-asr-ctc
+
+commit 1832b35070cd8f5df0cf1bf01cc658b0b6adf8c1
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 15:12:20 2025 +0800
+
+    Add C# API for Omnilingual ASR CTC models (#2775)
+
+diff --git a/.github/scripts/test-dot-net.sh b/.github/scripts/test-dot-net.sh
+index 2679d9bb..f65c0f82 100755
+--- a/.github/scripts/test-dot-net.sh
++++ b/.github/scripts/test-dot-net.sh
+@@ -32,6 +32,9 @@ rm -rf sherpa-onnx-nemo-*
+ 
+ cd ../offline-decode-files
+ 
++./run-omnilingual-asr-ctc.sh
++rm -rf sherpa-onnx-*
++
+ ./run-wenet-ctc.sh
+ rm -rf sherpa-onnx-*
+ 
+diff --git a/.gitignore b/.gitignore
+index f0a4c52e..37337ac3 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -156,3 +156,4 @@ am.mvn
+ *bpe.model
+ config.yaml
+ configuration.json
++sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
+diff --git a/dotnet-examples/offline-decode-files/Program.cs b/dotnet-examples/offline-decode-files/Program.cs
+index a133ef0c..4b3d1893 100644
+--- a/dotnet-examples/offline-decode-files/Program.cs
++++ b/dotnet-examples/offline-decode-files/Program.cs
+@@ -87,6 +87,9 @@ class OfflineDecodeFiles
+     [Option("wenet-ctc", Required = false, HelpText = "Path to model.onnx. Used only for Wenet CTC models")]
+     public string WenetCtc { get; set; } = string.Empty;
+ 
++    [Option("omnilingual-asr-ctc", Required = false, HelpText = "Path to model.onnx. Used only for Omnilingual ASR CTC models")]
++    public string Omnilingual { get; set; } = string.Empty;
++
+     [Option("sense-voice-model", Required = false, HelpText = "Path to model.onnx. Used only for SenseVoice CTC models")]
+     public string SenseVoiceModel { get; set; } = string.Empty;
+ 
+@@ -258,6 +261,10 @@ to download pre-trained Tdnn models.
+     {
+       config.ModelConfig.WenetCtc.Model = options.WenetCtc;
+     }
++    else if (!string.IsNullOrEmpty(options.Omnilingual))
++    {
++      config.ModelConfig.Omnilingual.Model = options.Omnilingual;
++    }
+     else if (!string.IsNullOrEmpty(options.WhisperEncoder))
+     {
+       config.ModelConfig.Whisper.Encoder = options.WhisperEncoder;
+diff --git a/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
+new file mode 100755
+index 00000000..fe764d1b
+--- /dev/null
++++ b/dotnet-examples/offline-decode-files/run-omnilingual-asr-ctc.sh
+@@ -0,0 +1,14 @@
++#!/usr/bin/env bash
++
++set -ex
++
++if [ ! -f sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12 ]; then
++  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++  rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++fi
++
++dotnet run \
++  --omnilingual-asr-ctc=./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx \
++  --tokens=./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt \
++  --files ./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav
+diff --git a/scripts/dotnet/OfflineModelConfig.cs b/scripts/dotnet/OfflineModelConfig.cs
+index 6d6a9e90..47205cfe 100644
+--- a/scripts/dotnet/OfflineModelConfig.cs
++++ b/scripts/dotnet/OfflineModelConfig.cs
+@@ -30,6 +30,7 @@ namespace SherpaOnnx
+             ZipformerCtc = new OfflineZipformerCtcModelConfig();
+             Canary = new OfflineCanaryModelConfig();
+             WenetCtc = new OfflineWenetCtcModelConfig();
++            Omnilingual = new OfflineOmnilingualAsrCtcModelConfig();
+         }
+         public OfflineTransducerModelConfig Transducer;
+         public OfflineParaformerModelConfig Paraformer;
+@@ -66,5 +67,6 @@ namespace SherpaOnnx
+         public OfflineZipformerCtcModelConfig ZipformerCtc;
+         public OfflineCanaryModelConfig Canary;
+         public OfflineWenetCtcModelConfig WenetCtc;
++        public OfflineOmnilingualAsrCtcModelConfig Omnilingual;
+     }
+ }
+diff --git a/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs b/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs
+new file mode 100644
+index 00000000..d1dad64e
+--- /dev/null
++++ b/scripts/dotnet/OfflineOmnilingualAsrCtcModel.cs
+@@ -0,0 +1,18 @@
++/// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
++
++using System.Runtime.InteropServices;
++
++namespace SherpaOnnx
++{
++
++    [StructLayout(LayoutKind.Sequential)]
++    public struct OfflineOmnilingualAsrCtcModelConfig
++    {
++        public OfflineOmnilingualAsrCtcModelConfig()
++        {
++            Model = "";
++        }
++        [MarshalAs(UnmanagedType.LPStr)]
++        public string Model;
++    }
++}
+
+commit a0d3e5ea4ce770537166454eef4f71c2c04c4db6
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 14:10:15 2025 +0800
+
+    Add CXX API for Omnilingual ASR CTC models (#2774)
+
+diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
+index c35a92cf..81866989 100644
+--- a/.github/workflows/cxx-api.yaml
++++ b/.github/workflows/cxx-api.yaml
+@@ -78,6 +78,39 @@ jobs:
+             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
+           fi
+ 
++      - name: Test Omnilingual ASR CTC
++        shell: bash
++        run: |
++          name=omnilingual-asr-ctc-cxx-api
++          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
++            -I ./build/install/include \
++            -L ./build/install/lib/ \
++            -l sherpa-onnx-cxx-api \
++            -l sherpa-onnx-c-api \
++            -l onnxruntime
++
++          ls -lh $name
++
++          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
++            ls -lh ./$name
++            ldd ./$name
++            echo "----"
++            readelf -d ./$name
++          fi
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++          tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++          rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++          echo "---"
++
++          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
++          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
++
++          ./$name
++
++          rm -rf sherpa-onnx-omnilingual-*
++          rm -v ./$name
+ 
+       - name: Test Online punctuation
+         shell: bash
+diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
+index 6c47e0a1..2dafd987 100644
+--- a/cxx-api-examples/CMakeLists.txt
++++ b/cxx-api-examples/CMakeLists.txt
+@@ -39,6 +39,9 @@ target_link_libraries(sense-voice-cxx-api sherpa-onnx-cxx-api)
+ add_executable(wenet-ctc-cxx-api ./wenet-ctc-cxx-api.cc)
+ target_link_libraries(wenet-ctc-cxx-api sherpa-onnx-cxx-api)
+ 
++add_executable(omnilingual-asr-ctc-cxx-api ./omnilingual-asr-ctc-cxx-api.cc)
++target_link_libraries(omnilingual-asr-ctc-cxx-api sherpa-onnx-cxx-api)
++
+ add_executable(nemo-canary-cxx-api ./nemo-canary-cxx-api.cc)
+ target_link_libraries(nemo-canary-cxx-api sherpa-onnx-cxx-api)
+ 
+diff --git a/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc b/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
+new file mode 100644
+index 00000000..819d1620
+--- /dev/null
++++ b/cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
+@@ -0,0 +1,75 @@
++// cxx-api-examples/omnilingual-asr-ctc-cxx-api.cc
++// Copyright (c)  2025  Xiaomi Corporation
++
++//
++// This file demonstrates how to use Omnilingual ASR with sherpa-onnx's C++ API.
++// clang-format off
++/*
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++*/
++//
++// clang-format on
++
++#include <chrono>  // NOLINT
++#include <iostream>
++#include <string>
++
++#include "sherpa-onnx/c-api/cxx-api.h"
++
++int32_t main() {
++  using namespace sherpa_onnx::cxx;  // NOLINT
++  OfflineRecognizerConfig config;
++
++  // clang-format off
++  config.model_config.omnilingual.model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
++  config.model_config.tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
++
++  config.model_config.num_threads = 1;
++
++  std::cout << "Loading model\n";
++  OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
++  if (!recognizer.Get()) {
++    std::cerr << "Please check your config\n";
++    return -1;
++  }
++  std::cout << "Loading model done\n";
++
++  std::string wave_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
++  // clang-format on
++
++  Wave wave = ReadWave(wave_filename);
++  if (wave.samples.empty()) {
++    std::cerr << "Failed to read: '" << wave_filename << "'\n";
++    return -1;
++  }
++
++  std::cout << "Start recognition\n";
++  const auto begin = std::chrono::steady_clock::now();
++
++  OfflineStream stream = recognizer.CreateStream();
++  stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
++                        wave.samples.size());
++
++  recognizer.Decode(&stream);
++
++  OfflineRecognizerResult result = recognizer.GetResult(&stream);
++
++  const auto end = std::chrono::steady_clock::now();
++  const float elapsed_seconds =
++      std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
++          .count() /
++      1000.;
++  float duration = wave.samples.size() / static_cast<float>(wave.sample_rate);
++  float rtf = elapsed_seconds / duration;
++
++  std::cout << "text: " << result.text << "\n";
++  printf("Number of threads: %d\n", config.model_config.num_threads);
++  printf("Duration: %.3fs\n", duration);
++  printf("Elapsed seconds: %.3fs\n", elapsed_seconds);
++  printf("(Real time factor) RTF = %.3f / %.3f = %.3f\n", elapsed_seconds,
++         duration, rtf);
++
++  return 0;
++}
+diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
+index 7e42902c..a7e35c55 100644
+--- a/sherpa-onnx/c-api/cxx-api.cc
++++ b/sherpa-onnx/c-api/cxx-api.cc
+@@ -269,6 +269,9 @@ static SherpaOnnxOfflineRecognizerConfig Convert(
+ 
+   c.model_config.wenet_ctc.model = config.model_config.wenet_ctc.model.c_str();
+ 
++  c.model_config.omnilingual.model =
++      config.model_config.omnilingual.model.c_str();
++
+   c.lm_config.model = config.lm_config.model.c_str();
+   c.lm_config.scale = config.lm_config.scale;
+ 
+diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
+index 1cf0eef7..77401e67 100644
+--- a/sherpa-onnx/c-api/cxx-api.h
++++ b/sherpa-onnx/c-api/cxx-api.h
+@@ -268,6 +268,10 @@ struct SHERPA_ONNX_API OfflineWenetCtcModelConfig {
+   std::string model;
+ };
+ 
++struct SHERPA_ONNX_API OfflineOmnilingualAsrCtcModelConfig {
++  std::string model;
++};
++
+ struct SHERPA_ONNX_API OfflineMoonshineModelConfig {
+   std::string preprocessor;
+   std::string encoder;
+@@ -297,6 +301,7 @@ struct SHERPA_ONNX_API OfflineModelConfig {
+   OfflineZipformerCtcModelConfig zipformer_ctc;
+   OfflineCanaryModelConfig canary;
+   OfflineWenetCtcModelConfig wenet_ctc;
++  OfflineOmnilingualAsrCtcModelConfig omnilingual;
+ };
+ 
+ struct SHERPA_ONNX_API OfflineLMConfig {
+
+commit 867d0445f7493de0121a067498cddd8aba3e9fd7
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 12:45:23 2025 +0800
+
+    Add C API for Omnilingual ASR CTC models (#2773)
+
+diff --git a/.github/workflows/c-api.yaml b/.github/workflows/c-api.yaml
+index 561d1d20..b8e1adea 100644
+--- a/.github/workflows/c-api.yaml
++++ b/.github/workflows/c-api.yaml
+@@ -75,6 +75,36 @@ jobs:
+             otool -L ./install/lib/libsherpa-onnx-c-api.dylib
+           fi
+ 
++      - name: Test Omnilingual ASR CTC
++        shell: bash
++        run: |
++          name=omnilingual-asr-ctc-c-api
++          gcc -o $name ./c-api-examples/$name.c \
++            -I ./build/install/include \
++            -L ./build/install/lib/ \
++            -l sherpa-onnx-c-api \
++            -l onnxruntime
++
++          ls -lh $name
++
++          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
++            ldd ./$name
++            echo "----"
++            readelf -d ./$name
++          fi
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++          tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++          rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++
++          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
++          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
++
++          ./$name
++
++          rm $name
++          rm -rf sherpa-onnx-omnilingual-*
++
+       - name: Test Wenet CTC
+         shell: bash
+         run: |
+diff --git a/c-api-examples/CMakeLists.txt b/c-api-examples/CMakeLists.txt
+index cd3d9d4a..b3baed3a 100644
+--- a/c-api-examples/CMakeLists.txt
++++ b/c-api-examples/CMakeLists.txt
+@@ -83,6 +83,9 @@ target_link_libraries(zipformer-c-api sherpa-onnx-c-api)
+ add_executable(wenet-ctc-c-api wenet-ctc-c-api.c)
+ target_link_libraries(wenet-ctc-c-api sherpa-onnx-c-api)
+ 
++add_executable(omnilingual-asr-ctc-c-api omnilingual-asr-ctc-c-api.c)
++target_link_libraries(omnilingual-asr-ctc-c-api sherpa-onnx-c-api)
++
+ add_executable(streaming-zipformer-c-api streaming-zipformer-c-api.c)
+ target_link_libraries(streaming-zipformer-c-api sherpa-onnx-c-api)
+ 
+diff --git a/c-api-examples/omnilingual-asr-ctc-c-api.c b/c-api-examples/omnilingual-asr-ctc-c-api.c
+new file mode 100644
+index 00000000..eabd3c2f
+--- /dev/null
++++ b/c-api-examples/omnilingual-asr-ctc-c-api.c
+@@ -0,0 +1,82 @@
++// c-api-examples/omnilingual-asr-ctc-c-api.c
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++//
++// This file demonstrates how to use Omnilingual ASR with sherpa-onnx's C API.
++// clang-format off
++/*
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++*/
++//
++// clang-format on
++
++#include <stdio.h>
++#include <stdlib.h>
++#include <string.h>
++
++#include "sherpa-onnx/c-api/c-api.h"
++
++int32_t main() {
++  // clang-format off
++  const char *wav_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav";
++  const char *model_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
++  const char *tokens_filename = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt";
++  // clang-format on
++
++  const char *provider = "cpu";
++
++  const SherpaOnnxWave *wave = SherpaOnnxReadWave(wav_filename);
++  if (wave == NULL) {
++    fprintf(stderr, "Failed to read %s\n", wav_filename);
++    return -1;
++  }
++
++  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
++  memset(&omnilingual, 0, sizeof(omnilingual));
++  omnilingual.model = model_filename;
++
++  // Offline model config
++  SherpaOnnxOfflineModelConfig offline_model_config;
++  memset(&offline_model_config, 0, sizeof(offline_model_config));
++  offline_model_config.debug = 1;
++  offline_model_config.num_threads = 1;
++  offline_model_config.provider = provider;
++  offline_model_config.tokens = tokens_filename;
++  offline_model_config.omnilingual = omnilingual;
++
++  // Recognizer config
++  SherpaOnnxOfflineRecognizerConfig recognizer_config;
++  memset(&recognizer_config, 0, sizeof(recognizer_config));
++  recognizer_config.decoding_method = "greedy_search";
++  recognizer_config.model_config = offline_model_config;
++
++  const SherpaOnnxOfflineRecognizer *recognizer =
++      SherpaOnnxCreateOfflineRecognizer(&recognizer_config);
++
++  if (recognizer == NULL) {
++    fprintf(stderr, "Please check your config!\n");
++    SherpaOnnxFreeWave(wave);
++    return -1;
++  }
++
++  const SherpaOnnxOfflineStream *stream =
++      SherpaOnnxCreateOfflineStream(recognizer);
++
++  SherpaOnnxAcceptWaveformOffline(stream, wave->sample_rate, wave->samples,
++                                  wave->num_samples);
++  SherpaOnnxDecodeOfflineStream(recognizer, stream);
++  const SherpaOnnxOfflineRecognizerResult *result =
++      SherpaOnnxGetOfflineStreamResult(stream);
++
++  fprintf(stderr, "Decoded text: %s\n", result->text);
++
++  SherpaOnnxDestroyOfflineRecognizerResult(result);
++  SherpaOnnxDestroyOfflineStream(stream);
++  SherpaOnnxDestroyOfflineRecognizer(recognizer);
++  SherpaOnnxFreeWave(wave);
++
++  return 0;
++}
+diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
+index 75d63569..4d612ae1 100644
+--- a/sherpa-onnx/c-api/c-api.cc
++++ b/sherpa-onnx/c-api/c-api.cc
+@@ -508,6 +508,9 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
+   recognizer_config.model_config.wenet_ctc.model =
+       SHERPA_ONNX_OR(config->model_config.wenet_ctc.model, "");
+ 
++  recognizer_config.model_config.omnilingual.model =
++      SHERPA_ONNX_OR(config->model_config.omnilingual.model, "");
++
+   recognizer_config.lm_config.model =
+       SHERPA_ONNX_OR(config->lm_config.model, "");
+   recognizer_config.lm_config.scale =
+diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
+index 2f22a449..36731e66 100644
+--- a/sherpa-onnx/c-api/c-api.h
++++ b/sherpa-onnx/c-api/c-api.h
+@@ -480,6 +480,10 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineWenetCtcModelConfig {
+   const char *model;
+ } SherpaOnnxOfflineWenetCtcModelConfig;
+ 
++SHERPA_ONNX_API typedef struct SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
++  const char *model;
++} SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
++
+ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
+   SherpaOnnxOfflineTransducerModelConfig transducer;
+   SherpaOnnxOfflineParaformerModelConfig paraformer;
+@@ -506,6 +510,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
+   SherpaOnnxOfflineZipformerCtcModelConfig zipformer_ctc;
+   SherpaOnnxOfflineCanaryModelConfig canary;
+   SherpaOnnxOfflineWenetCtcModelConfig wenet_ctc;
++  SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+ } SherpaOnnxOfflineModelConfig;
+ 
+ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerConfig {
+
+commit 36ef8d1fbc2b757dcb2274361ba7482da1c23d56
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Thu Nov 13 12:25:30 2025 +0800
+
+    Add C++ and Python API for Omnilingual ASR models. (#2772)
+    
+    This PR adds C++ and Python API support for Omnilingual ASR models with CTC decoding, enabling speech recognition across 1600+ languages using models from Facebook Research's omnilingual-asr project.
+    
+    Key changes:
+    
+    - Implements OfflineOmnilingualAsrCtcModel class with feature normalization and model inference
+    - Adds Python bindings through from_omnilingual_asr_ctc() method on OfflineRecognizer
+    - Integrates omnilingual models into existing CTC recognition pipeline with special handling for input shapes and frame shifts
+
+diff --git a/.github/scripts/test-python.sh b/.github/scripts/test-python.sh
+index 9ea01c0c..49523aaf 100755
+--- a/.github/scripts/test-python.sh
++++ b/.github/scripts/test-python.sh
+@@ -8,6 +8,16 @@ log() {
+   echo -e "$(date '+%Y-%m-%d %H:%M:%S') (${fname}:${BASH_LINENO[0]}:${FUNCNAME[1]}) $*"
+ }
+ 
++log "test omnilingual ASR"
++curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++ls -lh sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++
++python3 ./python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
++
++rm -rf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++
+ log "test T-one"
+ 
+ curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-t-one-russian-2025-09-08.tar.bz2
+diff --git a/python-api-examples/offline-fire-red-asr-decode-files.py b/python-api-examples/offline-fire-red-asr-decode-files.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/offline-moonshine-decode-files.py b/python-api-examples/offline-moonshine-decode-files.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/offline-nemo-canary-decode-files.py b/python-api-examples/offline-nemo-canary-decode-files.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/offline-nemo-parakeet-decode-file.py b/python-api-examples/offline-nemo-parakeet-decode-file.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py b/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
+new file mode 100755
+index 00000000..1354ef8d
+--- /dev/null
++++ b/python-api-examples/offline-omnilingual-asr-ctc-decode-files.py
+@@ -0,0 +1,133 @@
++#!/usr/bin/env python3
++
++"""
++This file shows how to use a non-streaming Omnilingual ASR CTC model from
++https://github.com/facebookresearch/omnilingual-asr
++to decode files.
++
++Please download model files from
++https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++
++For instance,
++
++wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
++"""
++
++from pathlib import Path
++
++import numpy as np
++import time
++import sherpa_onnx
++import soundfile as sf
++
++
++def create_recognizer():
++    model = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx"
++    tokens = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/tokens.txt"
++    test_wav_en = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/en.wav"
++    test_wav_de = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/de.wav"
++    test_wav_fr = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/fr.wav"
++    test_wav_es = "./sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/test_wavs/es.wav"
++
++    for f in [model, tokens, test_wav_en, test_wav_de, test_wav_fr, test_wav_es]:
++        if not Path(f).is_file():
++            print(f"{f} does not exist")
++
++            raise ValueError(
++                """Please download model files from
++                https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
++                """
++            )
++    return (
++        sherpa_onnx.OfflineRecognizer.from_omnilingual_asr_ctc(
++            model=model,
++            tokens=tokens,
++            num_threads=1,
++        ),
++        test_wav_en,
++        test_wav_de,
++        test_wav_fr,
++        test_wav_es,
++    )
++
++
++def load_audio(filename):
++    audio, sample_rate = sf.read(filename, dtype="float32", always_2d=True)
++    audio = audio[:, 0]  # only use the first channel
++    if sample_rate != 16000:
++        import librosa
++
++        audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)
++
++    return np.ascontiguousarray(audio)
++
++
++def decode_single_file(recognizer, filename):
++    samples = load_audio(filename)
++
++    start_time = time.time()
++
++    stream = recognizer.create_stream()
++    stream.accept_waveform(sample_rate=16000, waveform=samples)
++    recognizer.decode_stream(stream)
++
++    end_time = time.time()
++    elapsed_seconds = end_time - start_time
++    audio_duration = len(samples) / 16000
++    real_time_factor = elapsed_seconds / audio_duration
++
++    print("---")
++    print(filename)
++    print(stream.result)
++    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
++    print(f"Audio duration in seconds: {audio_duration:.3f}")
++    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
++    print()
++
++
++def decode_multiple_files(recognizer, filenames):
++    streams = []
++
++    start_time = time.time()
++
++    audio_duration = 0
++
++    for filename in filenames:
++        samples = load_audio(filename)
++        audio_duration += len(samples) / 16000
++
++        stream = recognizer.create_stream()
++        stream.accept_waveform(sample_rate=16000, waveform=samples)
++        streams.append(stream)
++
++    recognizer.decode_streams(streams)
++
++    end_time = time.time()
++    elapsed_seconds = end_time - start_time
++    real_time_factor = elapsed_seconds / audio_duration
++
++    for name, stream in zip(filenames, streams):
++        print("---")
++        print(name)
++        print(stream.result)
++        print()
++
++    print(f"Elapsed seconds: {elapsed_seconds:.3f}")
++    print(f"Audio duration in seconds: {audio_duration:.3f}")
++    print(f"RTF: {elapsed_seconds:.3f}/{audio_duration:.3f} = {real_time_factor:.3f}")
++    print()
++    print()
++
++
++def main():
++    recognizer, *filenames = create_recognizer()
++
++    decode_single_file(recognizer, filenames[0])
++    decode_single_file(recognizer, filenames[1])
++    decode_multiple_files(recognizer, filenames[2:])
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/python-api-examples/offline-sense-voice-ctc-decode-files.py b/python-api-examples/offline-sense-voice-ctc-decode-files.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/offline-whisper-decode-files.py b/python-api-examples/offline-whisper-decode-files.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/speaker-identification-with-vad-non-streaming-asr-alsa.py b/python-api-examples/speaker-identification-with-vad-non-streaming-asr-alsa.py
+old mode 100644
+new mode 100755
+diff --git a/python-api-examples/two-pass-wss.py b/python-api-examples/two-pass-wss.py
+old mode 100644
+new mode 100755
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 10e90fd8..a9a09286 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -44,6 +44,8 @@ set(sources
+   offline-moonshine-model.cc
+   offline-nemo-enc-dec-ctc-model-config.cc
+   offline-nemo-enc-dec-ctc-model.cc
++  offline-omnilingual-asr-ctc-model-config.cc
++  offline-omnilingual-asr-ctc-model.cc
+   offline-paraformer-greedy-search-decoder.cc
+   offline-paraformer-model-config.cc
+   offline-paraformer-model.cc
+diff --git a/sherpa-onnx/csrc/offline-ctc-model.cc b/sherpa-onnx/csrc/offline-ctc-model.cc
+index f115af90..4becf1af 100644
+--- a/sherpa-onnx/csrc/offline-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-ctc-model.cc
+@@ -22,6 +22,7 @@
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-dolphin-model.h"
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
++#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-telespeech-ctc-model.h"
+ #include "sherpa-onnx/csrc/offline-wenet-ctc-model.h"
+@@ -123,6 +124,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
+     return std::make_unique<OfflineWenetCtcModel>(config);
+   } else if (!config.telespeech_ctc.empty()) {
+     return std::make_unique<OfflineTeleSpeechCtcModel>(config);
++  } else if (!config.omnilingual.model.empty()) {
++    return std::make_unique<OfflineOmnilingualAsrCtcModel>(config);
+   }
+ 
+   // TODO(fangjun): Refactor it. We don't need to use model_type here
+@@ -187,6 +190,8 @@ std::unique_ptr<OfflineCtcModel> OfflineCtcModel::Create(
+     return std::make_unique<OfflineWenetCtcModel>(mgr, config);
+   } else if (!config.telespeech_ctc.empty()) {
+     return std::make_unique<OfflineTeleSpeechCtcModel>(mgr, config);
++  } else if (!config.omnilingual.model.empty()) {
++    return std::make_unique<OfflineOmnilingualAsrCtcModel>(mgr, config);
+   }
+ 
+   // TODO(fangjun): Refactor it. We don't need to use model_type here
+diff --git a/sherpa-onnx/csrc/offline-model-config.cc b/sherpa-onnx/csrc/offline-model-config.cc
+index 68f98dd2..0171c379 100644
+--- a/sherpa-onnx/csrc/offline-model-config.cc
++++ b/sherpa-onnx/csrc/offline-model-config.cc
+@@ -24,6 +24,7 @@ void OfflineModelConfig::Register(ParseOptions *po) {
+   moonshine.Register(po);
+   dolphin.Register(po);
+   canary.Register(po);
++  omnilingual.Register(po);
+ 
+   po->Register("telespeech-ctc", &telespeech_ctc,
+                "Path to model.onnx for telespeech ctc");
+@@ -148,6 +149,10 @@ bool OfflineModelConfig::Validate() const {
+     return canary.Validate();
+   }
+ 
++  if (!omnilingual.model.empty()) {
++    return omnilingual.Validate();
++  }
++
+   if (!telespeech_ctc.empty() && !FileExists(telespeech_ctc)) {
+     SHERPA_ONNX_LOGE("telespeech_ctc: '%s' does not exist",
+                      telespeech_ctc.c_str());
+@@ -177,6 +182,7 @@ std::string OfflineModelConfig::ToString() const {
+   os << "moonshine=" << moonshine.ToString() << ", ";
+   os << "dolphin=" << dolphin.ToString() << ", ";
+   os << "canary=" << canary.ToString() << ", ";
++  os << "omnilingual=" << omnilingual.ToString() << ", ";
+   os << "telespeech_ctc=\"" << telespeech_ctc << "\", ";
+   os << "tokens=\"" << tokens << "\", ";
+   os << "num_threads=" << num_threads << ", ";
+diff --git a/sherpa-onnx/csrc/offline-model-config.h b/sherpa-onnx/csrc/offline-model-config.h
+index 8164c7f7..6ef84edc 100644
+--- a/sherpa-onnx/csrc/offline-model-config.h
++++ b/sherpa-onnx/csrc/offline-model-config.h
+@@ -11,6 +11,7 @@
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
+ #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
++#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
+ #include "sherpa-onnx/csrc/offline-paraformer-model-config.h"
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-config.h"
+ #include "sherpa-onnx/csrc/offline-tdnn-model-config.h"
+@@ -34,6 +35,7 @@ struct OfflineModelConfig {
+   OfflineMoonshineModelConfig moonshine;
+   OfflineDolphinModelConfig dolphin;
+   OfflineCanaryModelConfig canary;
++  OfflineOmnilingualAsrCtcModelConfig omnilingual;
+   std::string telespeech_ctc;
+ 
+   std::string tokens;
+@@ -68,6 +70,7 @@ struct OfflineModelConfig {
+                      const OfflineMoonshineModelConfig &moonshine,
+                      const OfflineDolphinModelConfig &dolphin,
+                      const OfflineCanaryModelConfig &canary,
++                     const OfflineOmnilingualAsrCtcModelConfig &omnilingual,
+                      const std::string &telespeech_ctc,
+                      const std::string &tokens, int32_t num_threads, bool debug,
+                      const std::string &provider, const std::string &model_type,
+@@ -85,6 +88,7 @@ struct OfflineModelConfig {
+         moonshine(moonshine),
+         dolphin(dolphin),
+         canary(canary),
++        omnilingual(omnilingual),
+         telespeech_ctc(telespeech_ctc),
+         tokens(tokens),
+         num_threads(num_threads),
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
+new file mode 100644
+index 00000000..50ae999c
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
+@@ -0,0 +1,38 @@
++// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
++
++#include <string>
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++void OfflineOmnilingualAsrCtcModelConfig::Register(ParseOptions *po) {
++  po->Register("omnilingual-asr-model", &model,
++               "Path to Omnilingual ASR CTC model");
++}
++
++bool OfflineOmnilingualAsrCtcModelConfig::Validate() const {
++  if (!FileExists(model)) {
++    SHERPA_ONNX_LOGE("Omnilingual ASR CTC model file '%s' does not exist",
++                     model.c_str());
++    return false;
++  }
++
++  return true;
++}
++
++std::string OfflineOmnilingualAsrCtcModelConfig::ToString() const {
++  std::ostringstream os;
++
++  os << "OfflineOmnilingualAsrCtcModelConfig(";
++  os << "model=\"" << model << "\")";
++
++  return os.str();
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
+new file mode 100644
+index 00000000..cfd63a3b
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
+@@ -0,0 +1,32 @@
++// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
++
++#include <string>
++
++#include "sherpa-onnx/csrc/parse-options.h"
++
++namespace sherpa_onnx {
++
++// for
++// https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/omnilingual-asr/test.py
++struct OfflineOmnilingualAsrCtcModelConfig {
++  std::string model;
++
++  OfflineOmnilingualAsrCtcModelConfig() = default;
++
++  explicit OfflineOmnilingualAsrCtcModelConfig(const std::string &model)
++      : model(model) {}
++
++  void Register(ParseOptions *po);
++
++  bool Validate() const;
++
++  std::string ToString() const;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+new file mode 100644
+index 00000000..0ed69a58
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
+@@ -0,0 +1,184 @@
++// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h"
++
++#include <algorithm>
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/onnx-utils.h"
++#include "sherpa-onnx/csrc/session.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++#include "sherpa-onnx/csrc/transpose.h"
++
++namespace sherpa_onnx {
++
++class OfflineOmnilingualAsrCtcModel::Impl {
++ public:
++  explicit Impl(const OfflineModelConfig &config)
++      : config_(config),
++        env_(ORT_LOGGING_LEVEL_ERROR),
++        sess_opts_(GetSessionOptions(config)),
++        allocator_{} {
++    auto buf = ReadFile(config_.omnilingual.model);
++    Init(buf.data(), buf.size());
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const OfflineModelConfig &config)
++      : config_(config),
++        env_(ORT_LOGGING_LEVEL_ERROR),
++        sess_opts_(GetSessionOptions(config)),
++        allocator_{} {
++    auto buf = ReadFile(mgr, config_.omnilingual.model);
++    Init(buf.data(), buf.size());
++  }
++
++  std::vector<Ort::Value> Forward(Ort::Value features,
++                                  Ort::Value /*/features_length*/) {
++    auto out_vec =
++        sess_->Run({}, input_names_ptr_.data(), &features, 1,
++                   output_names_ptr_.data(), output_names_ptr_.size());
++    std::vector<int64_t> logits_shape =
++        out_vec[0].GetTensorTypeAndShapeInfo().GetShape();
++
++    std::vector<int64_t> num_frames(logits_shape[0], logits_shape[1]);
++
++    int64_t shape = logits_shape[0];
++
++    Ort::Value logits_len =
++        Ort::Value::CreateTensor<int64_t>(allocator_, &shape, 1);
++    std::copy(num_frames.begin(), num_frames.end(),
++              logits_len.GetTensorMutableData<int64_t>());
++
++    out_vec.push_back(std::move(logits_len));
++
++    return out_vec;
++  }
++
++  int32_t VocabSize() const { return vocab_size_; }
++
++  OrtAllocator *Allocator() { return allocator_; }
++
++  static void NormalizeFeatures(float *features, int32_t num_frames,
++                                int32_t feat_dim) {
++    if (num_frames != 1) {
++      SHERPA_ONNX_LOGE(
++          "Unexpected error in collecting samples for Omnilingual ASR models!");
++      return;
++    }
++
++    double s = 0;
++    double sq = 0;
++    for (int32_t i = 0; i < feat_dim; ++i) {
++      s += features[i];
++      sq += features[i] * features[i];
++    }
++
++    double mean = s / feat_dim;
++    double inv_stddev = 1 / std::sqrt(sq / feat_dim - mean * mean + 1e-5);
++
++    for (int32_t i = 0; i < feat_dim; ++i) {
++      features[i] = (features[i] - mean) * inv_stddev;
++    }
++  }
++
++ private:
++  void Init(void *model_data, size_t model_data_length) {
++    sess_ = std::make_unique<Ort::Session>(env_, model_data, model_data_length,
++                                           sess_opts_);
++
++    GetInputNames(sess_.get(), &input_names_, &input_names_ptr_);
++
++    GetOutputNames(sess_.get(), &output_names_, &output_names_ptr_);
++
++    // get meta data
++    Ort::ModelMetadata meta_data = sess_->GetModelMetadata();
++    if (config_.debug) {
++      std::ostringstream os;
++      PrintModelMetadata(os, meta_data);
++#if __OHOS__
++      SHERPA_ONNX_LOGE("%{public}s\n", os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
++#endif
++    }
++
++    // get vocab size from the output[0].shape, which is (N, T, vocab_size)
++    vocab_size_ =
++        sess_->GetOutputTypeInfo(0).GetTensorTypeAndShapeInfo().GetShape()[2];
++  }
++
++ private:
++  OfflineModelConfig config_;
++  Ort::Env env_;
++  Ort::SessionOptions sess_opts_;
++  Ort::AllocatorWithDefaultOptions allocator_;
++
++  std::unique_ptr<Ort::Session> sess_;
++
++  std::vector<std::string> input_names_;
++  std::vector<const char *> input_names_ptr_;
++
++  std::vector<std::string> output_names_;
++  std::vector<const char *> output_names_ptr_;
++
++  int32_t vocab_size_ = 0;
++};
++
++OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
++    const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(config)) {}
++
++template <typename Manager>
++OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
++    Manager *mgr, const OfflineModelConfig &config)
++    : impl_(std::make_unique<Impl>(mgr, config)) {}
++
++OfflineOmnilingualAsrCtcModel::~OfflineOmnilingualAsrCtcModel() = default;
++
++std::vector<Ort::Value> OfflineOmnilingualAsrCtcModel::Forward(
++    Ort::Value features, Ort::Value features_length) {
++  return impl_->Forward(std::move(features), std::move(features_length));
++}
++
++int32_t OfflineOmnilingualAsrCtcModel::VocabSize() const {
++  return impl_->VocabSize();
++}
++
++OrtAllocator *OfflineOmnilingualAsrCtcModel::Allocator() const {
++  return impl_->Allocator();
++}
++
++void OfflineOmnilingualAsrCtcModel::NormalizeFeatures(float *features,
++                                                      int32_t num_frames,
++                                                      int32_t feat_dim) const {
++  return impl_->NormalizeFeatures(features, num_frames, feat_dim);
++}
++
++#if __ANDROID_API__ >= 9
++template OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
++    AAssetManager *mgr, const OfflineModelConfig &config);
++#endif
++
++#if __OHOS__
++template OfflineOmnilingualAsrCtcModel::OfflineOmnilingualAsrCtcModel(
++    NativeResourceManager *mgr, const OfflineModelConfig &config);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
+new file mode 100644
+index 00000000..c3653d4d
+--- /dev/null
++++ b/sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
+@@ -0,0 +1,64 @@
++// sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
++#define SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
++#include <memory>
++#include <utility>
++#include <vector>
++
++#include "onnxruntime_cxx_api.h"  // NOLINT
++#include "sherpa-onnx/csrc/offline-ctc-model.h"
++#include "sherpa-onnx/csrc/offline-model-config.h"
++
++namespace sherpa_onnx {
++
++/** This class implements the Omnilingual ASR CTC model
++ * from
++ * https://github.com/facebookresearch/omnilingual-asr
++ *
++ * See
++ * https://github.com/k2-fsa/sherpa-onnx/blob/master/scripts/omnilingual-asr/export-onnx.py
++ */
++class OfflineOmnilingualAsrCtcModel : public OfflineCtcModel {
++ public:
++  explicit OfflineOmnilingualAsrCtcModel(const OfflineModelConfig &config);
++
++  template <typename Manager>
++  OfflineOmnilingualAsrCtcModel(Manager *mgr, const OfflineModelConfig &config);
++
++  ~OfflineOmnilingualAsrCtcModel() override;
++
++  /** Run the forward method of the model.
++   *
++   * @param features  A tensor of shape (N, T, C).
++   * @param features_length  A 1-D tensor of shape (N,) containing number of
++   *                         valid frames in `features` before padding.
++   *                         Its dtype is int64_t.
++   *
++   * @return Return a vector containing:
++   *  - log_probs: A 3-D tensor of shape (N, T', vocab_size).
++   *  - log_probs_length A 1-D tensor of shape (N,). Its dtype is int64_t
++   */
++  std::vector<Ort::Value> Forward(Ort::Value features,
++                                  Ort::Value features_length) override;
++
++  /** Return the vocabulary size of the model
++   */
++  int32_t VocabSize() const override;
++
++  /** Return an allocator for allocating memory
++   */
++  OrtAllocator *Allocator() const override;
++
++  void NormalizeFeatures(float *features, int32_t num_frames,
++                         int32_t feat_dim) const override;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_H_
+diff --git a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+index 491ac27b..78a72fc1 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
++++ b/sherpa-onnx/csrc/offline-recognizer-ctc-impl.h
+@@ -12,6 +12,7 @@
+ #include <utility>
+ #include <vector>
+ 
++#include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-ctc-decoder.h"
+ #include "sherpa-onnx/csrc/offline-ctc-fst-decoder.h"
+ #include "sherpa-onnx/csrc/offline-ctc-greedy-search-decoder.h"
+@@ -154,11 +155,13 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
+     } else if (config_.decoding_method == "greedy_search") {
+       if (!symbol_table_.Contains("<blk>") &&
+           !symbol_table_.Contains("<eps>") &&
+-          !symbol_table_.Contains("<blank>")) {
++          !symbol_table_.Contains("<blank>") &&
++          config_.model_config.omnilingual.model.empty()) {
++        // for omnilingual asr, its blank id is 0
+         SHERPA_ONNX_LOGE(
+             "We expect that tokens.txt contains "
+             "the symbol <blk> or <eps> or <blank> and its ID.");
+-        exit(-1);
++        SHERPA_ONNX_EXIT(-1);
+       }
+ 
+       int32_t blank_id = 0;
+@@ -181,23 +184,33 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
+   }
+ 
+   std::unique_ptr<OfflineStream> CreateStream() const override {
+-    return std::make_unique<OfflineStream>(config_.feat_config);
++    if (config_.model_config.omnilingual.model.empty()) {
++      return std::make_unique<OfflineStream>(config_.feat_config);
++    } else {
++      return std::make_unique<OfflineStream>(OmnilingualAsrTag{});
++    }
+   }
+ 
+   void DecodeStreams(OfflineStream **ss, int32_t n) const override {
+-    if (!model_->SupportBatchProcessing() || (n == 1)) {
+-      // If the model does not support batch process,
++    if (!model_->SupportBatchProcessing() || (n == 1) ||
++        !config_.model_config.omnilingual.model.empty()) {
++      // If the model does not support batch processing,
+       // we process each stream independently.
++      //
++      // omnilingual asr is disabled for batch processing at present
+       for (int32_t i = 0; i != n; ++i) {
+         DecodeStream(ss[i]);
+       }
+       return;
+     }
+ 
++    // Even if the omnilingual asr model can process batch input, the following
++    // code does not support batching raw audio samples.
++
+     auto memory_info =
+         Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+ 
+-    int32_t feat_dim = config_.feat_config.feature_dim;
++    int32_t feat_dim = ss[0]->FeatureDim();
+ 
+     std::vector<Ort::Value> features;
+     features.reserve(n);
+@@ -259,14 +272,17 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
+     auto memory_info =
+         Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeDefault);
+ 
+-    int32_t feat_dim = config_.feat_config.feature_dim;
++    int32_t feat_dim = s->FeatureDim();
+     std::vector<float> f = s->GetFrames();
+ 
+     int32_t num_frames = f.size() / feat_dim;
+ 
+     model_->NormalizeFeatures(f.data(), num_frames, feat_dim);
+ 
+-    std::array<int64_t, 3> shape = {1, num_frames, feat_dim};
++    std::vector<int64_t> shape = {1, num_frames, feat_dim};
++    if (!config_.model_config.omnilingual.model.empty()) {
++      shape = {1, feat_dim};
++    }
+ 
+     Ort::Value x = Ort::Value::CreateTensor(memory_info, f.data(), f.size(),
+                                             shape.data(), shape.size());
+@@ -281,6 +297,10 @@ class OfflineRecognizerCtcImpl : public OfflineRecognizerImpl {
+     auto results = decoder_->Decode(std::move(t[0]), std::move(t[1]));
+     int32_t frame_shift_ms = 10;
+ 
++    if (!config_.model_config.omnilingual.model.empty()) {
++      frame_shift_ms = 20;
++    }
++
+     auto r = Convert(results[0], symbol_table_, frame_shift_ms,
+                      model_->SubsamplingFactor());
+     r.text = ApplyInverseTextNormalization(std::move(r.text));
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index c283c4ae..37279c37 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -101,6 +101,7 @@ std::unique_ptr<OfflineRecognizerImpl> OfflineRecognizerImpl::Create(
+       !config.model_config.zipformer_ctc.model.empty() ||
+       !config.model_config.tdnn.model.empty() ||
+       !config.model_config.wenet_ctc.model.empty() ||
++      !config.model_config.omnilingual.model.empty() ||
+       !config.model_config.dolphin.model.empty()) {
+     return std::make_unique<OfflineRecognizerCtcImpl>(config);
+   }
+diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
+index 9c5d1088..8f757e6a 100644
+--- a/sherpa-onnx/csrc/offline-stream.cc
++++ b/sherpa-onnx/csrc/offline-stream.cc
+@@ -145,6 +145,10 @@ class OfflineStream::Impl {
+     config_.sampling_rate = 16000;
+   }
+ 
++  explicit Impl(OmnilingualAsrTag /*tag*/) : is_omnilingual_asr_(true) {
++    config_.sampling_rate = 16000;
++  }
++
+   void AcceptWaveform(int32_t sampling_rate, const float *waveform, int32_t n) {
+     if (config_.normalize_samples) {
+       AcceptWaveformImpl(sampling_rate, waveform, n);
+@@ -176,7 +180,7 @@ class OfflineStream::Impl {
+       std::vector<float> samples;
+       resampler->Resample(waveform, n, true, &samples);
+ 
+-      if (is_moonshine_) {
++      if (is_moonshine_ || is_omnilingual_asr_) {
+         samples_.insert(samples_.end(), samples.begin(), samples.end());
+       } else if (fbank_) {
+         fbank_->AcceptWaveform(config_.sampling_rate, samples.data(),
+@@ -195,7 +199,7 @@ class OfflineStream::Impl {
+       return;
+     }  // if (sampling_rate != config_.sampling_rate)
+ 
+-    if (is_moonshine_) {
++    if (is_moonshine_ || is_omnilingual_asr_) {
+       samples_.insert(samples_.end(), waveform, waveform + n);
+     } else if (fbank_) {
+       fbank_->AcceptWaveform(sampling_rate, waveform, n);
+@@ -210,7 +214,7 @@ class OfflineStream::Impl {
+   }
+ 
+   int32_t FeatureDim() const {
+-    if (is_moonshine_) {
++    if (is_moonshine_ || is_omnilingual_asr_) {
+       return samples_.size();
+     }
+ 
+@@ -218,7 +222,7 @@ class OfflineStream::Impl {
+   }
+ 
+   std::vector<float> GetFrames() const {
+-    if (is_moonshine_) {
++    if (is_moonshine_ || is_omnilingual_asr_) {
+       return samples_;
+     }
+ 
+@@ -325,8 +329,9 @@ class OfflineStream::Impl {
+   ContextGraphPtr context_graph_;
+   bool is_ced_ = false;
+   bool is_moonshine_ = false;
++  bool is_omnilingual_asr_ = false;
+ 
+-  // used only when is_moonshine_== true
++  // used only when (is_moonshine_ || is_omnilingual_asr_) == true
+   std::vector<float> samples_;
+ };
+ 
+@@ -342,6 +347,9 @@ OfflineStream::OfflineStream(CEDTag tag) : impl_(std::make_unique<Impl>(tag)) {}
+ OfflineStream::OfflineStream(MoonshineTag tag)
+     : impl_(std::make_unique<Impl>(tag)) {}
+ 
++OfflineStream::OfflineStream(OmnilingualAsrTag tag)
++    : impl_(std::make_unique<Impl>(tag)) {}
++
+ OfflineStream::~OfflineStream() = default;
+ 
+ void OfflineStream::AcceptWaveform(int32_t sampling_rate, const float *waveform,
+diff --git a/sherpa-onnx/csrc/offline-stream.h b/sherpa-onnx/csrc/offline-stream.h
+index 1039935d..5e2a514b 100644
+--- a/sherpa-onnx/csrc/offline-stream.h
++++ b/sherpa-onnx/csrc/offline-stream.h
+@@ -57,6 +57,9 @@ struct CEDTag {};
+ // audio samples to features
+ struct MoonshineTag {};
+ 
++// It is based on Wav2Vec, accepting raw audio samples as input
++struct OmnilingualAsrTag {};
++
+ class OfflineStream {
+  public:
+   explicit OfflineStream(const FeatureExtractorConfig &config = {},
+@@ -65,6 +68,7 @@ class OfflineStream {
+   explicit OfflineStream(WhisperTag tag);
+   explicit OfflineStream(CEDTag tag);
+   explicit OfflineStream(MoonshineTag tag);
++  explicit OfflineStream(OmnilingualAsrTag tag);
+   ~OfflineStream();
+ 
+   /**
+diff --git a/sherpa-onnx/python/csrc/CMakeLists.txt b/sherpa-onnx/python/csrc/CMakeLists.txt
+index 4603aab8..0b55ae91 100644
+--- a/sherpa-onnx/python/csrc/CMakeLists.txt
++++ b/sherpa-onnx/python/csrc/CMakeLists.txt
+@@ -17,6 +17,7 @@ set(srcs
+   offline-model-config.cc
+   offline-moonshine-model-config.cc
+   offline-nemo-enc-dec-ctc-model-config.cc
++  offline-omnilingual-asr-ctc-model-config.cc
+   offline-paraformer-model-config.cc
+   offline-punctuation.cc
+   offline-recognizer.cc
+diff --git a/sherpa-onnx/python/csrc/offline-model-config.cc b/sherpa-onnx/python/csrc/offline-model-config.cc
+index dc3c65dc..6c1286b8 100644
+--- a/sherpa-onnx/python/csrc/offline-model-config.cc
++++ b/sherpa-onnx/python/csrc/offline-model-config.cc
+@@ -13,6 +13,7 @@
+ #include "sherpa-onnx/python/csrc/offline-fire-red-asr-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-moonshine-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-nemo-enc-dec-ctc-model-config.h"
++#include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-paraformer-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-sense-voice-model-config.h"
+ #include "sherpa-onnx/python/csrc/offline-tdnn-model-config.h"
+@@ -36,6 +37,7 @@ void PybindOfflineModelConfig(py::module *m) {
+   PybindOfflineMoonshineModelConfig(m);
+   PybindOfflineDolphinModelConfig(m);
+   PybindOfflineCanaryModelConfig(m);
++  PybindOfflineOmnilingualAsrCtcModelConfig(m);
+ 
+   using PyClass = OfflineModelConfig;
+   py::class_<PyClass>(*m, "OfflineModelConfig")
+@@ -50,10 +52,11 @@ void PybindOfflineModelConfig(py::module *m) {
+                     const OfflineSenseVoiceModelConfig &,
+                     const OfflineMoonshineModelConfig &,
+                     const OfflineDolphinModelConfig &,
+-                    const OfflineCanaryModelConfig &, const std::string &,
+-                    const std::string &, int32_t, bool, const std::string &,
++                    const OfflineCanaryModelConfig &,
++                    const OfflineOmnilingualAsrCtcModelConfig &,
++                    const std::string &, const std::string &, int32_t, bool,
+                     const std::string &, const std::string &,
+-                    const std::string &>(),
++                    const std::string &, const std::string &>(),
+            py::arg("transducer") = OfflineTransducerModelConfig(),
+            py::arg("paraformer") = OfflineParaformerModelConfig(),
+            py::arg("nemo_ctc") = OfflineNemoEncDecCtcModelConfig(),
+@@ -66,6 +69,7 @@ void PybindOfflineModelConfig(py::module *m) {
+            py::arg("moonshine") = OfflineMoonshineModelConfig(),
+            py::arg("dolphin") = OfflineDolphinModelConfig(),
+            py::arg("canary") = OfflineCanaryModelConfig(),
++           py::arg("omnilingual") = OfflineOmnilingualAsrCtcModelConfig(),
+            py::arg("telespeech_ctc") = "", py::arg("tokens") = "",
+            py::arg("num_threads") = 1, py::arg("debug") = false,
+            py::arg("provider") = "cpu", py::arg("model_type") = "",
+@@ -82,6 +86,7 @@ void PybindOfflineModelConfig(py::module *m) {
+       .def_readwrite("moonshine", &PyClass::moonshine)
+       .def_readwrite("dolphin", &PyClass::dolphin)
+       .def_readwrite("canary", &PyClass::canary)
++      .def_readwrite("omnilingual", &PyClass::omnilingual)
+       .def_readwrite("telespeech_ctc", &PyClass::telespeech_ctc)
+       .def_readwrite("tokens", &PyClass::tokens)
+       .def_readwrite("num_threads", &PyClass::num_threads)
+diff --git a/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
+new file mode 100644
+index 00000000..0ba3cff0
+--- /dev/null
++++ b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
+@@ -0,0 +1,22 @@
++// sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h"
++
++#include <string>
++
++#include "sherpa-onnx/csrc/offline-omnilingual-asr-ctc-model-config.h"
++
++namespace sherpa_onnx {
++
++void PybindOfflineOmnilingualAsrCtcModelConfig(py::module *m) {
++  using PyClass = OfflineOmnilingualAsrCtcModelConfig;
++  py::class_<PyClass>(*m, "OfflineOmnilingualAsrCtcModelConfig")
++      .def(py::init<>())
++      .def(py::init<const std::string &>(), py::arg("model"))
++      .def_readwrite("model", &PyClass::model)
++      .def("__str__", &PyClass::ToString);
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
+new file mode 100644
+index 00000000..d3770622
+--- /dev/null
++++ b/sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
+@@ -0,0 +1,16 @@
++// sherpa-onnx/python/csrc/offline-omnilingual-asr-ctc-model-config.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
++#define SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
++
++#include "sherpa-onnx/python/csrc/sherpa-onnx.h"
++
++namespace sherpa_onnx {
++
++void PybindOfflineOmnilingualAsrCtcModelConfig(py::module *m);
++
++}
++
++#endif  // SHERPA_ONNX_PYTHON_CSRC_OFFLINE_OMNILINGUAL_ASR_CTC_MODEL_CONFIG_H_
+diff --git a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+index 74eb54c2..1194c664 100644
+--- a/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
++++ b/sherpa-onnx/python/sherpa_onnx/offline_recognizer.py
+@@ -7,6 +7,7 @@ from sherpa_onnx.lib._sherpa_onnx import (
+     FeatureExtractorConfig,
+     HomophoneReplacerConfig,
+     OfflineCanaryModelConfig,
++    OfflineOmnilingualAsrCtcModelConfig,
+     OfflineCtcFstDecoderConfig,
+     OfflineDolphinModelConfig,
+     OfflineFireRedAsrModelConfig,
+@@ -535,6 +536,56 @@ class OfflineRecognizer(object):
+         self.config = recognizer_config
+         return self
+ 
++    @classmethod
++    def from_omnilingual_asr_ctc(
++        cls,
++        model: str,
++        tokens: str,
++        num_threads: int = 1,
++        decoding_method: str = "greedy_search",
++        debug: bool = False,
++        provider: str = "cpu",
++    ):
++        """
++        Please refer to
++        `<https://k2-fsa.github.io/sherpa/onnx/omnilingual-asr/index.html>`_
++        to download pre-trained models.
++
++        Args:
++          model:
++            Path to ``model.onnx``.
++          tokens:
++            Path to ``tokens.txt``. Each line in ``tokens.txt`` contains two
++            columns::
++
++                symbol integer_id
++
++          num_threads:
++            Number of threads for neural network computation.
++          decoding_method:
++            The only supported decoding method is greedy_search.
++          debug:
++            True to show debug messages.
++          provider:
++            onnxruntime execution providers. Valid values are: cpu, cuda, coreml.
++        """
++        self = cls.__new__(cls)
++        model_config = OfflineModelConfig(
++            omnilingual=OfflineOmnilingualAsrCtcModelConfig(model=model),
++            tokens=tokens,
++            num_threads=num_threads,
++            debug=debug,
++            provider=provider,
++        )
++
++        recognizer_config = OfflineRecognizerConfig(
++            model_config=model_config,
++            decoding_method=decoding_method,
++        )
++        self.recognizer = _Recognizer(recognizer_config)
++        self.config = recognizer_config
++        return self
++
+     @classmethod
+     def from_zipformer_ctc(
+         cls,
+
+commit db77fe69993719f3968a4a5b7f0227b0429b909f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Nov 12 20:24:27 2025 +0800
+
+    Begin to export omnilingual-asr to sherpa-onnx (#2770)
+
+diff --git a/.github/workflows/export-omnilingual-asr-to-onnx.yaml b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+new file mode 100644
+index 00000000..332d8d96
+--- /dev/null
++++ b/.github/workflows/export-omnilingual-asr-to-onnx.yaml
+@@ -0,0 +1,171 @@
++name: export-omnilingual-asr-to-onnx
++
++on:
++  push:
++    branches:
++      - export-omnilingual-asr
++  workflow_dispatch:
++
++concurrency:
++  group: export-omnilingual-asr-to-onnx-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  export-omnilingual-asr-to-onnx:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: export omnilingual-asr
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-latest]
++        python-version: ["3.10"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python ${{ matrix.python-version }}
++        uses: actions/setup-python@v5
++        with:
++          python-version: ${{ matrix.python-version }}
++
++      - name: Install dependencies
++        shell: bash
++        run: |
++          sudo apt install libsndfile1
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          pip install fairseq2 \
++            --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.8.0/cpu \
++            torch==2.8.0+cpu -f https://download.pytorch.org/whl/torch \
++            torchaudio==2.8.0+cpu -f https://download.pytorch.org/whl/torchaudio \
++            onnx==1.17.0 \
++            onnxruntime==1.17.1 \
++            soundfile \
++            librosa
++
++          pip install --no-deps omnilingual_asr
++
++          pip install retrying pandas polars pyarrow xxhash
++
++      - name: Setup tmate session
++        if: false
++        uses: mxschmitt/action-tmate@v3
++
++      - name: Run
++        shell: bash
++        run: |
++          cd scripts/omnilingual-asr
++          python3 ./export-onnx.py
++
++          ls -lh *.onnx
++
++          rm README.md
++
++          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/README.md
++          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE
++          curl -SL -O https://raw.githubusercontent.com/facebookresearch/omnilingual-asr/refs/heads/main/LICENSE-CC-BY-4.0.md
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/en.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/es.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/fr.wav
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/de.wav
++
++          echo "---test----"
++          python3 ./test.py
++
++          echo "---collect files----"
++
++          d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
++
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          mv -v model.onnx $d
++          cp -v tokens.txt $d
++          cp -v README.md $d
++          cp -v LICENSE* $d
++          cp -v *.wav $d/test_wavs
++
++          ls -lh $d
++
++          tar cjfv $d.tar.bz2 $d
++          mv $d ../..
++
++          d=sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++
++          mkdir -p $d
++          mkdir -p $d/test_wavs
++
++          mv -v model.int8.onnx $d
++          cp -v tokens.txt $d
++          cp -v README.md $d
++          cp -v LICENSE* $d
++          cp -v *.wav $d/test_wavs
++          ls -lh $d
++
++          tar cjfv $d.tar.bz2 $d
++
++          mv $d ../..
++
++          mv *.tar.bz2 ../../
++
++          cd ../..
++
++          ls -lh *.tar.bz2
++
++      - name: Publish to huggingface
++        env:
++          HF_TOKEN: ${{ secrets.HF_TOKEN }}
++        uses: nick-fields/retry@v3
++        with:
++          max_attempts: 20
++          timeout_seconds: 200
++          shell: bash
++          command: |
++            git config --global user.email "csukuangfj@gmail.com"
++            git config --global user.name "Fangjun Kuang"
++
++            export GIT_LFS_SKIP_SMUDGE=1
++            export GIT_CLONE_PROTECTION_ACTIVE=false
++
++            dirs=(
++              sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-2025-11-12
++              sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12
++            )
++
++            for d in ${dirs[@]}; do
++              rm -rf huggingface
++              git clone https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d huggingface
++              pushd huggingface
++
++              git fetch
++              git pull
++              echo "pwd: $PWD"
++              cp -a ../$d/* .
++
++              git lfs track "*.onnx"
++              git lfs track "*.wav"
++              ls -lh
++              git add .
++
++              ls -lh
++
++              git status
++
++              git commit -m "add models"
++              git push https://csukuangfj:$HF_TOKEN@huggingface.co/csukuangfj/$d main || true
++              popd
++            done
++
++      - name: Release
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models
+diff --git a/scripts/omnilingual-asr/README.md b/scripts/omnilingual-asr/README.md
+new file mode 100644
+index 00000000..342bb4a8
+--- /dev/null
++++ b/scripts/omnilingual-asr/README.md
+@@ -0,0 +1,23 @@
++# Introduction
++
++This folder contains script to export
++https://github.com/facebookresearch/omnilingual-asr
++to sherpa-onnx
++
++See
++https://github.com/k2-fsa/sherpa-onnx/blob/master/.github/workflows/export-omnilingual-asr-to-onnx.yaml
++for usage.
++
++```
++num_frames = round(num_samples / 318 - 1.5)
++num_samples = round(318 * num_frames + 477)
++
++or
++num_frames = round(num_samples / 320)
++
++```
++
++20ms per frame
++
++
++
+diff --git a/scripts/omnilingual-asr/export-onnx.py b/scripts/omnilingual-asr/export-onnx.py
+new file mode 100755
+index 00000000..deaf556d
+--- /dev/null
++++ b/scripts/omnilingual-asr/export-onnx.py
+@@ -0,0 +1,103 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++from typing import Dict
++
++import onnx
++import torch
++from fairseq2.nn.batch_layout import BatchLayout
++from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline
++from onnxruntime.quantization import QuantType, quantize_dynamic
++
++
++def add_meta_data(filename: str, meta_data: Dict[str, str]):
++    """Add meta data to an ONNX model. It is changed in-place.
++
++    Args:
++      filename:
++        Filename of the ONNX model to be changed.
++      meta_data:
++        Key-value pairs.
++    """
++    model = onnx.load(filename)
++    while len(model.metadata_props):
++        model.metadata_props.pop()
++
++    for key, value in meta_data.items():
++        meta = model.metadata_props.add()
++        meta.key = key
++        meta.value = str(value)
++
++
++class ModelWrapper(torch.nn.Module):
++    def __init__(self, model):
++        super().__init__()
++        self.model = model
++
++    def forward(self, x):
++        """
++        Args:
++          x: (N, num_samples), float32
++        """
++        batch_layout = BatchLayout(shape=x.shape, seq_lens=[x.shape[1]])
++        logits, _ = self.model(x, batch_layout)
++        return logits
++
++
++@torch.no_grad()
++def main():
++    pipeline = ASRInferencePipeline(
++        model_card="omniASR_CTC_300M",
++        device="cpu",
++        dtype=torch.float32,
++    )
++
++    vocab_size = pipeline.tokenizer._model.vocabulary_size
++
++    with open("tokens.txt", "w") as f:
++        for i in range(pipeline.tokenizer._model.vocabulary_size):
++            f.write(f"{pipeline.tokenizer._model.index_to_token(i)} {i}\n")
++
++    print("saved to tokens.txt")
++
++    wrapper = ModelWrapper(pipeline.model)
++    wrapper.eval()
++
++    x = torch.rand(1, 16000 * 10)
++    torch.onnx.export(
++        wrapper,
++        x,
++        "model.onnx",
++        opset_version=14,
++        input_names=["x"],
++        output_names=["logits"],
++        dynamic_axes={
++            "x": {0: "N", 1: "num_samples"},
++            "logits": {0: "N", 1: "num_frames"},
++        },
++    )
++
++    meta_data = {
++        "vocab_size": vocab_size,
++        "model_type": "omnilingual-asr",
++        "version": "1",
++        "sample_rate": 16000,
++        "model_author": "facebookresearch",
++        "url": "https://github.com/facebookresearch/omnilingual-asr",
++        "comment": "300M-CTC",
++    }
++
++    add_meta_data("model.onnx", meta_data)
++    print("saved to model.onnx")
++
++    quantize_dynamic(
++        model_input="./model.onnx",
++        model_output="./model.int8.onnx",
++        op_types_to_quantize=["MatMul"],
++        weight_type=QuantType.QUInt8,
++    )
++    print("saved to model.int8.onnx")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/omnilingual-asr/test.py b/scripts/omnilingual-asr/test.py
+new file mode 100755
+index 00000000..9415902a
+--- /dev/null
++++ b/scripts/omnilingual-asr/test.py
+@@ -0,0 +1,118 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import time
++
++import numpy as np
++import onnxruntime as ort
++import soundfile as sf
++
++
++def display(sess):
++    print("==========Input==========")
++    for i in sess.get_inputs():
++        print(i)
++    print("==========Output==========")
++    for i in sess.get_outputs():
++        print(i)
++
++
++class OnnxModel:
++    def __init__(
++        self,
++        filename: str,
++    ):
++        session_opts = ort.SessionOptions()
++        session_opts.inter_op_num_threads = 1
++        session_opts.intra_op_num_threads = 1
++
++        self.model = ort.InferenceSession(
++            filename,
++            sess_options=session_opts,
++            providers=["CPUExecutionProvider"],
++        )
++        display(self.model)
++
++    def __call__(self, x: np.ndarray):
++        logits = self.model.run(
++            [
++                self.model.get_outputs()[0].name,
++            ],
++            {
++                self.model.get_inputs()[0].name: x,
++            },
++        )[0]
++        # [batch_size, T, vocab_size]
++        return logits
++
++
++def load_tokens():
++    id2token = dict()
++    with open("./tokens.txt", encoding="utf-8") as f:
++        for line in f:
++            fields = line.split()
++            if len(fields) == 1:
++                id2token[int(fields[0])] = " "
++            else:
++                t, idx = fields
++                id2token[int(idx)] = t
++    return id2token
++
++
++def load_audio(filename):
++    samples, sr = sf.read(filename, always_2d=True, dtype="float32")
++    samples = samples[:, 0]  # only use the first channel
++    if sr != 16000:
++        import librosa
++
++        samples = librosa.resample(samples, orig_sr=sr, target_sr=16000)
++    if len(samples) / 16000 > 40:
++        raise ValueError(f"{filename} is too long. Support at most 40 seconds")
++
++    mean = np.mean(samples, axis=0, keepdims=True)
++    var = np.var(samples, axis=0, keepdims=True)
++
++    eps = 1e-5
++    return (samples - mean) / np.sqrt(var + eps)
++
++
++def test(filename, wav_file_list, num_iter=10):
++    id2token = load_tokens()
++    model = OnnxModel(filename)
++
++    for it in range(num_iter):
++        for wav in wav_file_list:
++            print(f"---test {filename} with {wav}----iter---{it}")
++            start = time.time()
++            samples = load_audio(wav)
++
++            logits = model(samples[None])
++            ids = logits[0].argmax(axis=-1)
++            ans = []
++            prev = -1
++            blank = 0
++            for i in ids:
++                if i != blank and i != prev:
++                    ans.append(i)
++                prev = i
++
++            words = [id2token[k] for k in ans]
++            end = time.time()
++            elapsed_seconds = end - start
++            audio_duration = samples.shape[0] / 16000
++            real_time_factor = elapsed_seconds / audio_duration
++
++            print("---> text is----", "".join(words))
++            print(f"RTF: {real_time_factor}")
++            print()
++
++
++def main():
++    wav_file_list = ["./en.wav", "./de.wav", "./es.wav", "./fr.wav"]
++    test("./model.onnx", wav_file_list)
++
++    test("./model.int8.onnx", wav_file_list)
++
++
++if __name__ == "__main__":
++    main()
+
+commit f8d6fe5f3af722a23002926cbac390e6ff44a274
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Nov 12 17:19:43 2025 +0800
+
+    Fix missing includes. (#2769)
+
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
+index 6562ef5a..979e621c 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/keyword-spotting.cc
+@@ -1,7 +1,9 @@
+ // scripts/node-addon-api/src/keyword-spotting.cc
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
++#include <memory>
+ #include <sstream>
++#include <string>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+index 96cb4290..8ef59c4d 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+@@ -1,6 +1,7 @@
+ // scripts/node-addon-api/src/non-streaming-asr.cc
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
++#include <memory>
+ #include <sstream>
+ 
+ #include "macros.h"  // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
+index cf23fa75..07b9e068 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speaker-diarization.cc
+@@ -3,7 +3,10 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
+ #include <algorithm>
++#include <memory>
+ #include <sstream>
++#include <utility>
++#include <vector>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
+index 5a847fec..ea3929ea 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-speech-denoiser.cc
+@@ -1,6 +1,8 @@
+ // scripts/node-addon-api/src/non-streaming-speech-denoiser.cc
+ //
+ // Copyright (c)  2025  Xiaomi Corporation
++#include <algorithm>
++#include <memory>
+ #include <sstream>
+ 
+ #include "macros.h"  // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
+index 65cb798c..f958094c 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-tts.cc
+@@ -3,7 +3,10 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
+ #include <algorithm>
++#include <memory>
+ #include <sstream>
++#include <string>
++#include <vector>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
+index 65a27ebc..e8f760de 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/punctuation.cc
+@@ -2,6 +2,7 @@
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include <sstream>
++#include <string>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
+index c2965591..8e34e77e 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/speaker-identification.cc
+@@ -2,7 +2,9 @@
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include <algorithm>
++#include <memory>
+ #include <sstream>
++#include <string>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
+index 0d77139f..a9e8c7e7 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/spoken-language-identification.cc
+@@ -3,6 +3,7 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
+index 0184975e..09808cbb 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/streaming-asr.cc
+@@ -1,7 +1,9 @@
+ // scripts/node-addon-api/src/streaming-asr.cc
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
++#include <memory>
+ #include <sstream>
++#include <string>
+ 
+ #include "macros.h"  // NOLINT
+ #include "napi.h"    // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
+index 0ab9be74..e9ef8335 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/vad.cc
+@@ -3,6 +3,7 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
+ #include <algorithm>
++#include <memory>
+ #include <sstream>
+ 
+ #include "macros.h"  // NOLINT
+diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
+index 23b3a724..b08a3465 100644
+--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
++++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/wave-reader.cc
+@@ -4,6 +4,7 @@
+ 
+ #include <algorithm>
+ #include <sstream>
++#include <string>
+ 
+ #include "napi.h"  // NOLINT
+ #include "sherpa-onnx/c-api/c-api.h"
+diff --git a/scripts/check_style_cpplint.sh b/scripts/check_style_cpplint.sh
+index dcadcd99..6e3f2842 100755
+--- a/scripts/check_style_cpplint.sh
++++ b/scripts/check_style_cpplint.sh
+@@ -26,7 +26,7 @@
+ #  ./scripts/check_style_cpplint.sh 2
+ 
+ 
+-cpplint_version="1.5.4"
++cpplint_version="2.0.2"
+ cur_dir=$(cd $(dirname $BASH_SOURCE) && pwd)
+ sherpa_onnx_dir=$(cd $cur_dir/.. && pwd)
+ 
+diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
+index 3b8f2e6e..7e42902c 100644
+--- a/sherpa-onnx/c-api/cxx-api.cc
++++ b/sherpa-onnx/c-api/cxx-api.cc
+@@ -6,7 +6,9 @@
+ #include <algorithm>
+ #include <cstring>
+ #include <memory>
++#include <string>
+ #include <utility>
++#include <vector>
+ 
+ namespace sherpa_onnx::cxx {
+ 
+diff --git a/sherpa-onnx/csrc/alsa-play.cc b/sherpa-onnx/csrc/alsa-play.cc
+index 5602e389..faf49619 100644
+--- a/sherpa-onnx/csrc/alsa-play.cc
++++ b/sherpa-onnx/csrc/alsa-play.cc
+@@ -7,6 +7,9 @@
+ #include "sherpa-onnx/csrc/alsa-play.h"
+ 
+ #include <algorithm>
++#include <cstdio>
++#include <memory>
++#include <vector>
+ 
+ namespace sherpa_onnx {
+ 
+diff --git a/sherpa-onnx/csrc/alsa.cc b/sherpa-onnx/csrc/alsa.cc
+index a6576109..d136253a 100644
+--- a/sherpa-onnx/csrc/alsa.cc
++++ b/sherpa-onnx/csrc/alsa.cc
+@@ -7,6 +7,9 @@
+ #include "sherpa-onnx/csrc/alsa.h"
+ 
+ #include <algorithm>
++#include <cstdio>
++#include <memory>
++#include <vector>
+ 
+ #include "alsa/asoundlib.h"
+ 
+diff --git a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+index bc081099..54da97b3 100644
+--- a/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-paraformer-model-ascend.cc
+@@ -8,7 +8,8 @@
+ 
+ #include <algorithm>
+ #include <array>
+-#include <mutex>  // NOLINT
++#include <memory>
++#include <mutex>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+index 260b6b9a..9192f0ae 100644
+--- a/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
++++ b/sherpa-onnx/csrc/ascend/offline-sense-voice-model-ascend.cc
+@@ -8,6 +8,7 @@
+ 
+ #include <algorithm>
+ #include <array>
++#include <memory>
+ #include <mutex>  // NOLINT
+ #include <string>
+ #include <utility>
+diff --git a/sherpa-onnx/csrc/ascend/utils.cc b/sherpa-onnx/csrc/ascend/utils.cc
+index 8efca9ba..2977c769 100644
+--- a/sherpa-onnx/csrc/ascend/utils.cc
++++ b/sherpa-onnx/csrc/ascend/utils.cc
+@@ -4,8 +4,11 @@
+ 
+ #include "sherpa-onnx/csrc/ascend/utils.h"
+ 
++#include <memory>
+ #include <sstream>
++#include <string>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/ascend/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/audio-tagging-model-config.cc b/sherpa-onnx/csrc/audio-tagging-model-config.cc
+index ba68c50e..3456df4f 100644
+--- a/sherpa-onnx/csrc/audio-tagging-model-config.cc
++++ b/sherpa-onnx/csrc/audio-tagging-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/audio-tagging-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/audio-tagging.cc b/sherpa-onnx/csrc/audio-tagging.cc
+index 966a1920..b86a11c8 100644
+--- a/sherpa-onnx/csrc/audio-tagging.cc
++++ b/sherpa-onnx/csrc/audio-tagging.cc
+@@ -4,7 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/audio-tagging.h"
+ 
++#include <memory>
+ #include <string>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/base64-decode.cc b/sherpa-onnx/csrc/base64-decode.cc
+index 5723790f..7aa9bd35 100644
+--- a/sherpa-onnx/csrc/base64-decode.cc
++++ b/sherpa-onnx/csrc/base64-decode.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/base64-decode.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/csrc/cat.cc b/sherpa-onnx/csrc/cat.cc
+index a8d748ba..15cc2f34 100644
+--- a/sherpa-onnx/csrc/cat.cc
++++ b/sherpa-onnx/csrc/cat.cc
+@@ -9,6 +9,7 @@
+ #include <numeric>
+ #include <sstream>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/circular-buffer.cc b/sherpa-onnx/csrc/circular-buffer.cc
+index 2ba81807..4af756c6 100644
+--- a/sherpa-onnx/csrc/circular-buffer.cc
++++ b/sherpa-onnx/csrc/circular-buffer.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/circular-buffer.h"
+ 
+ #include <algorithm>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/context-graph-test.cc b/sherpa-onnx/csrc/context-graph-test.cc
+index 5c45b69e..2da44ac4 100644
+--- a/sherpa-onnx/csrc/context-graph-test.cc
++++ b/sherpa-onnx/csrc/context-graph-test.cc
+@@ -4,11 +4,12 @@
+ 
+ #include "sherpa-onnx/csrc/context-graph.h"
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <cmath>
+ #include <map>
+ #include <random>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "gtest/gtest.h"
+diff --git a/sherpa-onnx/csrc/context-graph.cc b/sherpa-onnx/csrc/context-graph.cc
+index 336208b1..f3da0ab6 100644
+--- a/sherpa-onnx/csrc/context-graph.cc
++++ b/sherpa-onnx/csrc/context-graph.cc
+@@ -6,10 +6,12 @@
+ 
+ #include <algorithm>
+ #include <cassert>
++#include <memory>
+ #include <queue>
+ #include <string>
+ #include <tuple>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/fast-clustering-test.cc b/sherpa-onnx/csrc/fast-clustering-test.cc
+index aea4be55..489b0116 100644
+--- a/sherpa-onnx/csrc/fast-clustering-test.cc
++++ b/sherpa-onnx/csrc/fast-clustering-test.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/fast-clustering.h"
+ 
++#include <iostream>
+ #include <vector>
+ 
+ #include "gtest/gtest.h"
+diff --git a/sherpa-onnx/csrc/features.cc b/sherpa-onnx/csrc/features.cc
+index 41848abc..47d12dad 100644
+--- a/sherpa-onnx/csrc/features.cc
++++ b/sherpa-onnx/csrc/features.cc
+@@ -6,8 +6,9 @@
+ 
+ #include <algorithm>
+ #include <memory>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <sstream>
++#include <string>
+ #include <vector>
+ 
+ #include "kaldi-native-fbank/csrc/online-feature.h"
+diff --git a/sherpa-onnx/csrc/file-utils.cc b/sherpa-onnx/csrc/file-utils.cc
+index 25be9bc9..b8361445 100644
+--- a/sherpa-onnx/csrc/file-utils.cc
++++ b/sherpa-onnx/csrc/file-utils.cc
+@@ -8,6 +8,7 @@
+ #include <memory>
+ #include <sstream>
+ #include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/fst-utils.cc b/sherpa-onnx/csrc/fst-utils.cc
+index 5fcf5235..cda51ffb 100644
+--- a/sherpa-onnx/csrc/fst-utils.cc
++++ b/sherpa-onnx/csrc/fst-utils.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/fst-utils.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/csrc/hifigan-vocoder.cc b/sherpa-onnx/csrc/hifigan-vocoder.cc
+index 6703449f..05d896f6 100644
+--- a/sherpa-onnx/csrc/hifigan-vocoder.cc
++++ b/sherpa-onnx/csrc/hifigan-vocoder.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/hifigan-vocoder.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/homophone-replacer.cc b/sherpa-onnx/csrc/homophone-replacer.cc
+index 550b66ec..9dd223d5 100644
+--- a/sherpa-onnx/csrc/homophone-replacer.cc
++++ b/sherpa-onnx/csrc/homophone-replacer.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/homophone-replacer.h"
+ 
+ #include <fstream>
++#include <memory>
+ #include <sstream>
+ #include <string>
+ #include <strstream>
+diff --git a/sherpa-onnx/csrc/hypothesis.cc b/sherpa-onnx/csrc/hypothesis.cc
+index ea332bcb..b6463a12 100644
+--- a/sherpa-onnx/csrc/hypothesis.cc
++++ b/sherpa-onnx/csrc/hypothesis.cc
+@@ -7,6 +7,7 @@
+ 
+ #include <algorithm>
+ #include <utility>
++#include <vector>
+ 
+ namespace sherpa_onnx {
+ 
+diff --git a/sherpa-onnx/csrc/keyword-spotter-impl.cc b/sherpa-onnx/csrc/keyword-spotter-impl.cc
+index b6900e30..350830a7 100644
+--- a/sherpa-onnx/csrc/keyword-spotter-impl.cc
++++ b/sherpa-onnx/csrc/keyword-spotter-impl.cc
+@@ -4,12 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/keyword-spotter-impl.h"
+ 
+-#include "sherpa-onnx/csrc/keyword-spotter-transducer-impl.h"
+-#include "sherpa-onnx/csrc/macros.h"
+-
+-#if SHERPA_ONNX_ENABLE_RKNN
+-#include "sherpa-onnx/csrc/rknn/keyword-spotter-transducer-rknn-impl.h"
+-#endif
++#include <memory>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+@@ -20,6 +15,13 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
++#include "sherpa-onnx/csrc/keyword-spotter-transducer-impl.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++#if SHERPA_ONNX_ENABLE_RKNN
++#include "sherpa-onnx/csrc/rknn/keyword-spotter-transducer-rknn-impl.h"
++#endif
++
+ namespace sherpa_onnx {
+ 
+ std::unique_ptr<KeywordSpotterImpl> KeywordSpotterImpl::Create(
+diff --git a/sherpa-onnx/csrc/keyword-spotter.cc b/sherpa-onnx/csrc/keyword-spotter.cc
+index 615aab9c..ca8ecd11 100644
+--- a/sherpa-onnx/csrc/keyword-spotter.cc
++++ b/sherpa-onnx/csrc/keyword-spotter.cc
+@@ -10,6 +10,7 @@
+ #include <iomanip>
+ #include <memory>
+ #include <sstream>
++#include <string>
+ #include <utility>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+index 77cb5123..8bf7bcac 100644
+--- a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
++++ b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+@@ -6,7 +6,7 @@
+ 
+ #include <codecvt>
+ #include <fstream>
+-#include <regex>  // NOLINT
++#include <regex>
+ #include <sstream>
+ #include <string>
+ #include <strstream>
+@@ -25,8 +25,8 @@
+ #endif
+ 
+ #include "espeak-ng/speak_lib.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+ #include "sherpa-onnx/csrc/phrase-matcher.h"
+@@ -565,7 +565,7 @@ KokoroMultiLangLexicon::KokoroMultiLangLexicon(
+     const std::string &data_dir, const OfflineTtsKokoroModelMetaData &meta_data,
+     bool debug)
+     : impl_(std::make_unique<Impl>(tokens, lexicon, data_dir, meta_data,
+-                                   debug)) {}
++                                   debug)) {}  // NOLINT
+ 
+ template <typename Manager>
+ KokoroMultiLangLexicon::KokoroMultiLangLexicon(
+@@ -573,7 +573,7 @@ KokoroMultiLangLexicon::KokoroMultiLangLexicon(
+     const std::string &data_dir, const OfflineTtsKokoroModelMetaData &meta_data,
+     bool debug)
+     : impl_(std::make_unique<Impl>(mgr, tokens, lexicon, data_dir, meta_data,
+-                                   debug)) {}
++                                   debug)) {}  // NOLINT
+ 
+ std::vector<TokenIDs> KokoroMultiLangLexicon::ConvertTextToTokenIds(
+     const std::string &text, const std::string &voice /*= ""*/) const {
+diff --git a/sherpa-onnx/csrc/lexicon.cc b/sherpa-onnx/csrc/lexicon.cc
+index e1ebaa3c..620a2a3c 100644
+--- a/sherpa-onnx/csrc/lexicon.cc
++++ b/sherpa-onnx/csrc/lexicon.cc
+@@ -10,8 +10,11 @@
+ #include <iomanip>
+ #include <memory>
+ #include <sstream>
++#include <string>
+ #include <strstream>
++#include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/lodr-fst.cc b/sherpa-onnx/csrc/lodr-fst.cc
+index a5d0d218..253352cc 100644
+--- a/sherpa-onnx/csrc/lodr-fst.cc
++++ b/sherpa-onnx/csrc/lodr-fst.cc
+@@ -5,13 +5,18 @@
+ //
+ // Copyright (c)  2025 Tilde SIA (Askars Salimbajevs)
+ 
++#include "sherpa-onnx/csrc/lodr-fst.h"
++
+ #include <algorithm>
++#include <limits>
++#include <memory>
++#include <string>
++#include <unordered_map>
+ #include <utility>
+ #include <vector>
+ 
+-#include "sherpa-onnx/csrc/lodr-fst.h"
+-#include "sherpa-onnx/csrc/log.h"
+ #include "sherpa-onnx/csrc/hypothesis.h"
++#include "sherpa-onnx/csrc/log.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+@@ -21,8 +26,8 @@ int32_t LodrFst::FindBackoffId() {
+ 
+   for (int32_t state = 0; state < fst_->NumStates(); ++state) {
+     fst::ArcIterator<fst::StdConstFst> arc_iter(*fst_, state);
+-    for ( ; !arc_iter.Done(); arc_iter.Next()) {
+-      const auto& arc = arc_iter.Value();
++    for (; !arc_iter.Done(); arc_iter.Next()) {
++      const auto &arc = arc_iter.Value();
+       if (arc.olabel == 0) {  // Check if the output label is epsilon (0)
+         return arc.ilabel;    // Return the input label
+       }
+@@ -35,7 +40,7 @@ int32_t LodrFst::FindBackoffId() {
+ LodrFst::LodrFst(const std::string &fst_path, int32_t backoff_id)
+     : backoff_id_(backoff_id) {
+   fst_ = std::unique_ptr<fst::StdConstFst>(
+-    CastOrConvertToConstFst(fst::StdVectorFst::Read(fst_path)));
++      CastOrConvertToConstFst(fst::StdVectorFst::Read(fst_path)));
+ 
+   if (backoff_id < 0) {
+     // backoff_id_ is not provided, find it automatically
+@@ -49,7 +54,7 @@ LodrFst::LodrFst(const std::string &fst_path, int32_t backoff_id)
+ }
+ 
+ std::vector<std::tuple<int32_t, float>> LodrFst::ProcessBackoffArcs(
+-  int32_t state, float cost) {
++    int32_t state, float cost) {
+   std::vector<std::tuple<int32_t, float>> ans;
+   auto next = GetNextStatesCostsNoBackoff(state, backoff_id_);
+   if (!next.has_value()) {
+@@ -63,7 +68,7 @@ std::vector<std::tuple<int32_t, float>> LodrFst::ProcessBackoffArcs(
+ }
+ 
+ std::optional<std::tuple<int32_t, float>> LodrFst::GetNextStatesCostsNoBackoff(
+-  int32_t state, int32_t label) {
++    int32_t state, int32_t label) {
+   fst::ArcIterator<fst::StdConstFst> arc_iter(*fst_, state);
+   int32_t num_arcs = fst_->NumArcs(state);
+ 
+@@ -84,12 +89,12 @@ std::optional<std::tuple<int32_t, float>> LodrFst::GetNextStatesCostsNoBackoff(
+ }
+ 
+ std::pair<std::vector<int32_t>, std::vector<float>> LodrFst::GetNextStateCosts(
+-  int32_t state, int32_t label) {
++    int32_t state, int32_t label) {
+   std::vector<int32_t> states = {state};
+   std::vector<float> costs = {0};
+ 
+   auto extra_states_costs = ProcessBackoffArcs(state, 0);
+-  for (const auto& [s, c] : extra_states_costs) {
++  for (const auto &[s, c] : extra_states_costs) {
+     states.push_back(s);
+     costs.push_back(c);
+   }
+@@ -140,7 +145,7 @@ float LodrFst::GetFinalCost(int32_t state) {
+ }
+ 
+ LodrStateCost::LodrStateCost(
+-    LodrFst* fst, const std::unordered_map<int32_t, float> &state_cost)
++    LodrFst *fst, const std::unordered_map<int32_t, float> &state_cost)
+     : fst_(fst) {
+   if (state_cost.empty()) {
+     state_cost_[0] = 0.0;
+@@ -151,7 +156,7 @@ LodrStateCost::LodrStateCost(
+ 
+ LodrStateCost LodrStateCost::ForwardOneStep(int32_t label) {
+   std::unordered_map<int32_t, float> state_cost;
+-  for (const auto& [s, c] : state_cost_) {
++  for (const auto &[s, c] : state_cost_) {
+     auto [next_states, next_costs] = fst_->GetNextStateCosts(s, label);
+     for (size_t i = 0; i < next_states.size(); ++i) {
+       int32_t ns = next_states[i];
+@@ -169,10 +174,9 @@ float LodrStateCost::Score() const {
+   if (state_cost_.empty()) {
+     return -std::numeric_limits<float>::infinity();
+   }
+-  auto min_cost = std::min_element(state_cost_.begin(), state_cost_.end(),
+-                                   [](const auto& a, const auto& b) {
+-                                     return a.second < b.second;
+-                                   });
++  auto min_cost = std::min_element(
++      state_cost_.begin(), state_cost_.end(),
++      [](const auto &a, const auto &b) { return a.second < b.second; });
+   return -min_cost->second;
+ }
+ 
+@@ -180,12 +184,10 @@ float LodrStateCost::FinalScore() const {
+   if (state_cost_.empty()) {
+     return -std::numeric_limits<float>::infinity();
+   }
+-  auto min_cost = std::min_element(state_cost_.begin(), state_cost_.end(),
+-                                   [](const auto& a, const auto& b) {
+-                                     return a.second < b.second;
+-                                   });
+-  return -(min_cost->second +
+-           fst_->GetFinalCost(min_cost->first));
++  auto min_cost = std::min_element(
++      state_cost_.begin(), state_cost_.end(),
++      [](const auto &a, const auto &b) { return a.second < b.second; });
++  return -(min_cost->second + fst_->GetFinalCost(min_cost->first));
+ }
+ 
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index b557fe08..b5247d6e 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -27,8 +27,8 @@
+ #endif
+ 
+ #include "espeak-ng/speak_lib.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-canary-model-config.cc b/sherpa-onnx/csrc/offline-canary-model-config.cc
+index 2821c10d..2910d45e 100644
+--- a/sherpa-onnx/csrc/offline-canary-model-config.cc
++++ b/sherpa-onnx/csrc/offline-canary-model-config.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-canary-model-config.h"
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/offline-canary-model.cc b/sherpa-onnx/csrc/offline-canary-model.cc
+index 37471420..5ae52011 100644
+--- a/sherpa-onnx/csrc/offline-canary-model.cc
++++ b/sherpa-onnx/csrc/offline-canary-model.cc
+@@ -6,10 +6,12 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
+ #include <tuple>
+ #include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-canary-model-meta-data.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-ced-model.cc b/sherpa-onnx/csrc/offline-ced-model.cc
+index 241f03b3..b7d03d5a 100644
+--- a/sherpa-onnx/csrc/offline-ced-model.cc
++++ b/sherpa-onnx/csrc/offline-ced-model.cc
+@@ -4,7 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/offline-ced-model.h"
+ 
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-ct-transformer-model.cc b/sherpa-onnx/csrc/offline-ct-transformer-model.cc
+index ee016285..320a3934 100644
+--- a/sherpa-onnx/csrc/offline-ct-transformer-model.cc
++++ b/sherpa-onnx/csrc/offline-ct-transformer-model.cc
+@@ -4,7 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/offline-ct-transformer-model.h"
+ 
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc b/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
+index ca3dcaee..225eaf54 100644
+--- a/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
++++ b/sherpa-onnx/csrc/offline-ctc-fst-decoder.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <string>
+ #include <utility>
++#include <vector>
+ 
+ #include "fst/fstlib.h"
+ #include "kaldi-decoder/csrc/decodable-ctc.h"
+diff --git a/sherpa-onnx/csrc/offline-dolphin-model-config.cc b/sherpa-onnx/csrc/offline-dolphin-model-config.cc
+index 03f4cb57..96ad7e91 100644
+--- a/sherpa-onnx/csrc/offline-dolphin-model-config.cc
++++ b/sherpa-onnx/csrc/offline-dolphin-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-dolphin-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-dolphin-model.cc b/sherpa-onnx/csrc/offline-dolphin-model.cc
+index b8abd5b3..843a0127 100644
+--- a/sherpa-onnx/csrc/offline-dolphin-model.cc
++++ b/sherpa-onnx/csrc/offline-dolphin-model.cc
+@@ -5,8 +5,10 @@
+ #include "sherpa-onnx/csrc/offline-dolphin-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
+index 07e4b875..c27ac0b1 100644
+--- a/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-fire-red-asr-greedy-search-decoder.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <tuple>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc b/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
+index 53eb9337..a4947077 100644
+--- a/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
++++ b/sherpa-onnx/csrc/offline-fire-red-asr-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-fire-red-asr-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
+index f2103892..b5677f06 100644
+--- a/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
++++ b/sherpa-onnx/csrc/offline-fire-red-asr-model.cc
+@@ -6,10 +6,12 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
+ #include <tuple>
+ #include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-lm.cc b/sherpa-onnx/csrc/offline-lm.cc
+index a452915c..fab209d0 100644
+--- a/sherpa-onnx/csrc/offline-lm.cc
++++ b/sherpa-onnx/csrc/offline-lm.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-lm.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <utility>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
+index 3e2d77f6..dba09023 100644
+--- a/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-moonshine-greedy-search-decoder.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <algorithm>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-moonshine-model-config.cc b/sherpa-onnx/csrc/offline-moonshine-model-config.cc
+index c687507e..93743fb0 100644
+--- a/sherpa-onnx/csrc/offline-moonshine-model-config.cc
++++ b/sherpa-onnx/csrc/offline-moonshine-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-moonshine-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-moonshine-model.cc b/sherpa-onnx/csrc/offline-moonshine-model.cc
+index 7c66b351..59ef8483 100644
+--- a/sherpa-onnx/csrc/offline-moonshine-model.cc
++++ b/sherpa-onnx/csrc/offline-moonshine-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-moonshine-model.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
+index 9ea26f81..fd5ef33f 100644
+--- a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
+index 7759c101..6eaa8bf2 100644
+--- a/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.cc
+@@ -4,6 +4,11 @@
+ 
+ #include "sherpa-onnx/csrc/offline-nemo-enc-dec-ctc-model.h"
+ 
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model-config.cc b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+index b4d0ee9a..e71b77bb 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model-config.cc
++++ b/sherpa-onnx/csrc/offline-paraformer-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-paraformer-model-config.h"
+ 
++#include <memory>
+ #include <string>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/offline-paraformer-model.cc b/sherpa-onnx/csrc/offline-paraformer-model.cc
+index 5b8586ef..1ff5c640 100644
+--- a/sherpa-onnx/csrc/offline-paraformer-model.cc
++++ b/sherpa-onnx/csrc/offline-paraformer-model.cc
+@@ -5,8 +5,10 @@
+ #include "sherpa-onnx/csrc/offline-paraformer-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-punctuation-impl.cc b/sherpa-onnx/csrc/offline-punctuation-impl.cc
+index ed943fcc..002127ef 100644
+--- a/sherpa-onnx/csrc/offline-punctuation-impl.cc
++++ b/sherpa-onnx/csrc/offline-punctuation-impl.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-punctuation-impl.h"
+ 
++#include <memory>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/offline-punctuation-model-config.cc b/sherpa-onnx/csrc/offline-punctuation-model-config.cc
+index e98fe00b..d5ed624c 100644
+--- a/sherpa-onnx/csrc/offline-punctuation-model-config.cc
++++ b/sherpa-onnx/csrc/offline-punctuation-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-punctuation-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-punctuation.cc b/sherpa-onnx/csrc/offline-punctuation.cc
+index 140e7203..ccaa8119 100644
+--- a/sherpa-onnx/csrc/offline-punctuation.cc
++++ b/sherpa-onnx/csrc/offline-punctuation.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-punctuation.h"
+ 
++#include <string>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/offline-recognizer-impl.cc b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+index a5482bef..c283c4ae 100644
+--- a/sherpa-onnx/csrc/offline-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/offline-recognizer-impl.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer-impl.h"
+ 
++#include <memory>
+ #include <string>
+ #include <strstream>
+ #include <utility>
+diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
+index 1d2271f2..8a02fc16 100644
+--- a/sherpa-onnx/csrc/offline-recognizer.cc
++++ b/sherpa-onnx/csrc/offline-recognizer.cc
+@@ -5,6 +5,8 @@
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+ 
+ #include <memory>
++#include <string>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-rnn-lm.cc b/sherpa-onnx/csrc/offline-rnn-lm.cc
+index bdc2f903..5a92c796 100644
+--- a/sherpa-onnx/csrc/offline-rnn-lm.cc
++++ b/sherpa-onnx/csrc/offline-rnn-lm.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-rnn-lm.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+index fc9884dd..cc18a11a 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-sense-voice-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-sense-voice-model.cc b/sherpa-onnx/csrc/offline-sense-voice-model.cc
+index 95664e22..588cdd37 100644
+--- a/sherpa-onnx/csrc/offline-sense-voice-model.cc
++++ b/sherpa-onnx/csrc/offline-sense-voice-model.cc
+@@ -5,8 +5,10 @@
+ #include "sherpa-onnx/csrc/offline-sense-voice-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-source-separation-impl.cc b/sherpa-onnx/csrc/offline-source-separation-impl.cc
+index 3a68ab2a..ec2da1fe 100644
+--- a/sherpa-onnx/csrc/offline-source-separation-impl.cc
++++ b/sherpa-onnx/csrc/offline-source-separation-impl.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <memory>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-source-separation-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-model-config.cc
+index 00dcbb8e..50a0f8c7 100644
+--- a/sherpa-onnx/csrc/offline-source-separation-model-config.cc
++++ b/sherpa-onnx/csrc/offline-source-separation-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-source-separation-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
+index 0dc3ee6e..873d9977 100644
+--- a/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
++++ b/sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-source-separation-spleeter-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc b/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
+index e3c16511..0dee12be 100644
+--- a/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
++++ b/sherpa-onnx/csrc/offline-source-separation-spleeter-model.cc
+@@ -171,7 +171,7 @@ class OfflineSourceSeparationSpleeterModel::Impl {
+ };
+ 
+ OfflineSourceSeparationSpleeterModel::~OfflineSourceSeparationSpleeterModel() =
+-    default;
++    default;  // NOLINT
+ 
+ OfflineSourceSeparationSpleeterModel::OfflineSourceSeparationSpleeterModel(
+     const OfflineSourceSeparationModelConfig &config)
+diff --git a/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc b/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
+index f95ea307..eaae2833 100644
+--- a/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
++++ b/sherpa-onnx/csrc/offline-source-separation-uvr-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-source-separation-uvr-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-source-separation.cc b/sherpa-onnx/csrc/offline-source-separation.cc
+index d352d9ab..acee51fc 100644
+--- a/sherpa-onnx/csrc/offline-source-separation.cc
++++ b/sherpa-onnx/csrc/offline-source-separation.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-source-separation.h"
+ 
+ #include <memory>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/offline-source-separation-impl.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h b/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
+index 8573c749..e0a890d7 100644
+--- a/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
++++ b/sherpa-onnx/csrc/offline-speaker-diarization-pyannote-impl.h
+@@ -41,11 +41,11 @@ struct PairHash {
+ };
+ }  // namespace
+ 
+-using Matrix2D =
+-    Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
++using Matrix2D = Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
++                               Eigen::RowMajor>;  // NOLINT
+ 
+-using Matrix2DInt32 =
+-    Eigen::Matrix<int32_t, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
++using Matrix2DInt32 = Eigen::Matrix<int32_t, Eigen::Dynamic, Eigen::Dynamic,
++                                    Eigen::RowMajor>;  // NOLINT
+ 
+ using FloatRowVector = Eigen::Matrix<float, 1, Eigen::Dynamic>;
+ using Int32RowVector = Eigen::Matrix<int32_t, 1, Eigen::Dynamic>;
+diff --git a/sherpa-onnx/csrc/offline-speaker-diarization-result.cc b/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
+index 59695728..6a600f4d 100644
+--- a/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
++++ b/sherpa-onnx/csrc/offline-speaker-diarization-result.cc
+@@ -6,10 +6,12 @@
+ 
+ #include <algorithm>
+ #include <array>
++#include <cstdio>
+ #include <sstream>
+ #include <string>
+ #include <unordered_set>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc b/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
+index 53671ec4..6fb39849 100644
+--- a/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
++++ b/sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-speaker-segmentation-pyannote-model.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+@@ -112,18 +113,18 @@ class OfflineSpeakerSegmentationPyannoteModel::Impl {
+ };
+ 
+ OfflineSpeakerSegmentationPyannoteModel::
+-    OfflineSpeakerSegmentationPyannoteModel(
++    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
+         const OfflineSpeakerSegmentationModelConfig &config)
+-    : impl_(std::make_unique<Impl>(config)) {}
++    : impl_(std::make_unique<Impl>(config)) {}  // NOLINT
+ 
+ template <typename Manager>
+ OfflineSpeakerSegmentationPyannoteModel::
+-    OfflineSpeakerSegmentationPyannoteModel(
++    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
+         Manager *mgr, const OfflineSpeakerSegmentationModelConfig &config)
+-    : impl_(std::make_unique<Impl>(mgr, config)) {}
++    : impl_(std::make_unique<Impl>(mgr, config)) {}  // NOLINT
+ 
+ OfflineSpeakerSegmentationPyannoteModel::
+-    ~OfflineSpeakerSegmentationPyannoteModel() = default;
++    ~OfflineSpeakerSegmentationPyannoteModel() = default;  // NOLINT
+ 
+ const OfflineSpeakerSegmentationPyannoteModelMetaData &
+ OfflineSpeakerSegmentationPyannoteModel::GetModelMetaData() const {
+@@ -137,14 +138,14 @@ Ort::Value OfflineSpeakerSegmentationPyannoteModel::Forward(
+ 
+ #if __ANDROID_API__ >= 9
+ template OfflineSpeakerSegmentationPyannoteModel::
+-    OfflineSpeakerSegmentationPyannoteModel(
++    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
+         AAssetManager *mgr,
+         const OfflineSpeakerSegmentationModelConfig &config);
+ #endif
+ 
+ #if __OHOS__
+ template OfflineSpeakerSegmentationPyannoteModel::
+-    OfflineSpeakerSegmentationPyannoteModel(
++    OfflineSpeakerSegmentationPyannoteModel(  // NOLINT
+         NativeResourceManager *mgr,
+         const OfflineSpeakerSegmentationModelConfig &config);
+ #endif
+diff --git a/sherpa-onnx/csrc/offline-speech-denoiser.cc b/sherpa-onnx/csrc/offline-speech-denoiser.cc
+index afdd4d9f..1ef56500 100644
+--- a/sherpa-onnx/csrc/offline-speech-denoiser.cc
++++ b/sherpa-onnx/csrc/offline-speech-denoiser.cc
+@@ -4,7 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-speech-denoiser.h"
+ 
+-#include "sherpa-onnx/csrc/offline-speech-denoiser-impl.h"
++#include <string>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+@@ -15,6 +15,8 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
++#include "sherpa-onnx/csrc/offline-speech-denoiser-impl.h"
++
+ namespace sherpa_onnx {
+ 
+ void OfflineSpeechDenoiserConfig::Register(ParseOptions *po) {
+diff --git a/sherpa-onnx/csrc/offline-stream.cc b/sherpa-onnx/csrc/offline-stream.cc
+index 0e7fe3b9..9c5d1088 100644
+--- a/sherpa-onnx/csrc/offline-stream.cc
++++ b/sherpa-onnx/csrc/offline-stream.cc
+@@ -9,7 +9,10 @@
+ #include <cmath>
+ #include <iomanip>
+ #include <limits>
++#include <memory>
++#include <string>
+ #include <utility>
++#include <vector>
+ 
+ #include "kaldi-native-fbank/csrc/online-feature.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc b/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
+index ca23210f..bc2a5d4b 100644
+--- a/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-tdnn-ctc-model.cc
+@@ -4,7 +4,10 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tdnn-ctc-model.h"
+ 
++#include <memory>
++#include <string>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-tdnn-model-config.cc b/sherpa-onnx/csrc/offline-tdnn-model-config.cc
+index be1b11cd..b8d3dced 100644
+--- a/sherpa-onnx/csrc/offline-tdnn-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tdnn-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tdnn-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc b/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
+index f6b2574b..c1c38a21 100644
+--- a/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-telespeech-ctc-model.cc
+@@ -4,6 +4,11 @@
+ 
+ #include "sherpa-onnx/csrc/offline-telespeech-ctc-model.h"
+ 
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+index 6fd3bf40..98fe78da 100644
+--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-decoder.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <iterator>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+ #include "sherpa-onnx/csrc/packed-sequence.h"
+diff --git a/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc b/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
+index e39e3154..4b338d26 100644
+--- a/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
++++ b/sherpa-onnx/csrc/offline-transducer-greedy-search-nemo-decoder.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <iterator>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-transducer-model-config.cc b/sherpa-onnx/csrc/offline-transducer-model-config.cc
+index 72fcfefb..0d99776c 100644
+--- a/sherpa-onnx/csrc/offline-transducer-model-config.cc
++++ b/sherpa-onnx/csrc/offline-transducer-model-config.cc
+@@ -4,6 +4,7 @@
+ #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/offline-transducer-model.cc b/sherpa-onnx/csrc/offline-transducer-model.cc
+index a08854fe..0ee94da6 100644
+--- a/sherpa-onnx/csrc/offline-transducer-model.cc
++++ b/sherpa-onnx/csrc/offline-transducer-model.cc
+@@ -5,7 +5,9 @@
+ #include "sherpa-onnx/csrc/offline-transducer-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+diff --git a/sherpa-onnx/csrc/offline-transducer-nemo-model.cc b/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
+index 82a973cb..01e73926 100644
+--- a/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
++++ b/sherpa-onnx/csrc/offline-transducer-nemo-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-transducer-nemo-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-tts-character-frontend.cc b/sherpa-onnx/csrc/offline-tts-character-frontend.cc
+index 968e287c..6a1df6e8 100644
+--- a/sherpa-onnx/csrc/offline-tts-character-frontend.cc
++++ b/sherpa-onnx/csrc/offline-tts-character-frontend.cc
+@@ -7,9 +7,13 @@
+ #include <codecvt>
+ #include <fstream>
+ #include <locale>
++#include <memory>
+ #include <sstream>
++#include <string>
+ #include <strstream>
++#include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc b/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
+index 690c72e6..2534b1be 100644
+--- a/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-kitten-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-kitten-model-config.h"
+ 
++#include <string>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-kitten-model.cc b/sherpa-onnx/csrc/offline-tts-kitten-model.cc
+index f8497e1a..d1d857f9 100644
+--- a/sherpa-onnx/csrc/offline-tts-kitten-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-kitten-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-tts-kitten-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc b/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
+index b6481bb2..06ab74d2 100644
+--- a/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-kokoro-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-kokoro-model-config.h"
+ 
++#include <string>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-kokoro-model.cc b/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
+index 599aa64c..d973d7ce 100644
+--- a/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-kokoro-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-tts-kokoro-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+index 9b379f44..90cf6289 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-matcha-model-config.h"
+ 
++#include <string>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model.cc b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+index 316a8a5f..b0e42deb 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-tts-matcha-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-tts-model-config.cc b/sherpa-onnx/csrc/offline-tts-model-config.cc
+index 8d804dc9..df7bf06f 100644
+--- a/sherpa-onnx/csrc/offline-tts-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/csrc/offline-tts-vits-model-config.cc b/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
+index cd638f64..66face5c 100644
+--- a/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-vits-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-vits-model-config.h"
+ 
++#include <string>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-vits-model.cc b/sherpa-onnx/csrc/offline-tts-vits-model.cc
+index d8bf7325..af2e090d 100644
+--- a/sherpa-onnx/csrc/offline-tts-vits-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-vits-model.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/offline-tts-vits-model.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+index be81068e..0cf582c7 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend-test.cc
+@@ -4,10 +4,13 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
+ 
++#include <string>
++#include <vector>
++
+ #include "espeak-ng/speak_lib.h"
+ #include "gtest/gtest.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+index 12714a44..075b8d0c 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-frontend.cc
+@@ -7,10 +7,14 @@
+ #include <codecvt>
+ #include <fstream>
+ #include <locale>
+-#include <regex>  // NOLINT
++#include <memory>
++#include <regex>
+ #include <sstream>
++#include <string>
+ #include <strstream>
++#include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+@@ -23,8 +27,8 @@
+ 
+ #include "cppinyin/csrc/cppinyin.h"
+ #include "espeak-ng/speak_lib.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-frontend.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+index a5291ffa..453bd6f6 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model-config.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/offline-tts-zipvoice-model-config.h"
+ 
++#include <string>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+index 4ff7d833..05f324c9 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+@@ -7,6 +7,7 @@
+ #include <algorithm>
+ #include <cstring>
+ #include <iostream>
++#include <memory>
+ #include <random>
+ #include <string>
+ #include <utility>
+diff --git a/sherpa-onnx/csrc/offline-websocket-server-impl.cc b/sherpa-onnx/csrc/offline-websocket-server-impl.cc
+index b34ebcaa..80f43938 100644
+--- a/sherpa-onnx/csrc/offline-websocket-server-impl.cc
++++ b/sherpa-onnx/csrc/offline-websocket-server-impl.cc
+@@ -5,6 +5,11 @@
+ #include "sherpa-onnx/csrc/offline-websocket-server-impl.h"
+ 
+ #include <algorithm>
++#include <iostream>
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+@@ -44,7 +49,7 @@ void OfflineWebsocketDecoderConfig::Validate() const {
+ OfflineWebsocketDecoder::OfflineWebsocketDecoder(OfflineWebsocketServer *server)
+     : config_(server->GetConfig().decoder_config),
+       server_(server),
+-      recognizer_(config_.recognizer_config) {}
++      recognizer_(config_.recognizer_config) {}  // NOLINT
+ 
+ void OfflineWebsocketDecoder::Push(connection_hdl hdl, ConnectionDataPtr d) {
+   std::lock_guard<std::mutex> lock(mutex_);
+diff --git a/sherpa-onnx/csrc/offline-websocket-server.cc b/sherpa-onnx/csrc/offline-websocket-server.cc
+index eb55413d..4be72b05 100644
+--- a/sherpa-onnx/csrc/offline-websocket-server.cc
++++ b/sherpa-onnx/csrc/offline-websocket-server.cc
+@@ -2,7 +2,9 @@
+ //
+ // Copyright (c)  2022-2023  Xiaomi Corporation
+ 
+-#include "asio.hpp"
++#include <vector>
++
++#include "asio.hpp"  // NOLINT
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/offline-websocket-server-impl.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc b/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
+index 2493971a..b65d1d24 100644
+--- a/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/offline-wenet-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-wenet-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-wenet-ctc-model.cc b/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
+index 31ffe5b6..e7dd0684 100644
+--- a/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-wenet-ctc-model.cc
+@@ -4,6 +4,11 @@
+ 
+ #include "sherpa-onnx/csrc/offline-wenet-ctc-model.h"
+ 
++#include <memory>
++#include <string>
++#include <utility>
++#include <vector>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc b/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
+index ad76a31e..5dbb7697 100644
+--- a/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
++++ b/sherpa-onnx/csrc/offline-whisper-greedy-search-decoder.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <algorithm>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-whisper-model-config.cc b/sherpa-onnx/csrc/offline-whisper-model-config.cc
+index 708531f7..6afa1f51 100644
+--- a/sherpa-onnx/csrc/offline-whisper-model-config.cc
++++ b/sherpa-onnx/csrc/offline-whisper-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-whisper-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-whisper-model.cc b/sherpa-onnx/csrc/offline-whisper-model.cc
+index 5443d37a..3921c3b0 100644
+--- a/sherpa-onnx/csrc/offline-whisper-model.cc
++++ b/sherpa-onnx/csrc/offline-whisper-model.cc
+@@ -6,10 +6,12 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
+ #include <tuple>
+ #include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+@@ -405,13 +407,13 @@ const std::vector<int32_t> &OfflineWhisperModel::GetAllLanguageIDs() const {
+   return impl_->GetAllLanguageIDs();
+ }
+ 
+-const std::unordered_map<std::string, int32_t>
+-    &OfflineWhisperModel::GetLang2ID() const {
++const std::unordered_map<std::string, int32_t> &
++OfflineWhisperModel::GetLang2ID() const {
+   return impl_->GetLang2ID();
+ }
+ 
+-const std::unordered_map<int32_t, std::string>
+-    &OfflineWhisperModel::GetID2Lang() const {
++const std::unordered_map<int32_t, std::string> &
++OfflineWhisperModel::GetID2Lang() const {
+   return impl_->GetID2Lang();
+ }
+ 
+diff --git a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
+index 633bfac8..7d5f8a06 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.cc
+@@ -4,6 +4,10 @@
+ 
+ #include "sherpa-onnx/csrc/offline-zipformer-audio-tagging-model-config.h"
+ 
++#include <memory>
++#include <string>
++#include <utility>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
+index f464074e..3a19d4cf 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.cc
+@@ -4,7 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/offline-zipformer-audio-tagging-model.h"
+ 
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+index e03e841f..fd5e0321 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-zipformer-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc b/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
+index 356537e9..4a51e179 100644
+--- a/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
++++ b/sherpa-onnx/csrc/offline-zipformer-ctc-model.cc
+@@ -4,7 +4,10 @@
+ 
+ #include "sherpa-onnx/csrc/offline-zipformer-ctc-model.h"
+ 
++#include <memory>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/online-cnn-bilstm-model.cc b/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
+index 2ca270ca..b2489831 100644
+--- a/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
++++ b/sherpa-onnx/csrc/online-cnn-bilstm-model.cc
+@@ -4,7 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/online-cnn-bilstm-model.h"
+ 
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/online-lm.cc b/sherpa-onnx/csrc/online-lm.cc
+index dfec00cc..1aaec374 100644
+--- a/sherpa-onnx/csrc/online-lm.cc
++++ b/sherpa-onnx/csrc/online-lm.cc
+@@ -6,6 +6,7 @@
+ #include "sherpa-onnx/csrc/online-lm.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <utility>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc b/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
+index c3c22b97..4c8ae94c 100644
+--- a/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/online-nemo-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-nemo-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-nemo-ctc-model.cc b/sherpa-onnx/csrc/online-nemo-ctc-model.cc
+index bbe4f1c7..dfce793d 100644
+--- a/sherpa-onnx/csrc/online-nemo-ctc-model.cc
++++ b/sherpa-onnx/csrc/online-nemo-ctc-model.cc
+@@ -6,7 +6,10 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/online-paraformer-model-config.cc b/sherpa-onnx/csrc/online-paraformer-model-config.cc
+index 25a99262..a2ee8029 100644
+--- a/sherpa-onnx/csrc/online-paraformer-model-config.cc
++++ b/sherpa-onnx/csrc/online-paraformer-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-paraformer-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-paraformer-model.cc b/sherpa-onnx/csrc/online-paraformer-model.cc
+index e75b70a4..5e71c019 100644
+--- a/sherpa-onnx/csrc/online-paraformer-model.cc
++++ b/sherpa-onnx/csrc/online-paraformer-model.cc
+@@ -6,7 +6,10 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/online-punctuation-impl.cc b/sherpa-onnx/csrc/online-punctuation-impl.cc
+index ebdbc848..84031354 100644
+--- a/sherpa-onnx/csrc/online-punctuation-impl.cc
++++ b/sherpa-onnx/csrc/online-punctuation-impl.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-punctuation-impl.h"
+ 
++#include <memory>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/online-punctuation-model-config.cc b/sherpa-onnx/csrc/online-punctuation-model-config.cc
+index 5dab600f..c7bc0610 100644
+--- a/sherpa-onnx/csrc/online-punctuation-model-config.cc
++++ b/sherpa-onnx/csrc/online-punctuation-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-punctuation-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-punctuation.cc b/sherpa-onnx/csrc/online-punctuation.cc
+index 6435b1c4..a7f83c9b 100644
+--- a/sherpa-onnx/csrc/online-punctuation.cc
++++ b/sherpa-onnx/csrc/online-punctuation.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-punctuation.h"
+ 
++#include <string>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/online-recognizer-impl.cc b/sherpa-onnx/csrc/online-recognizer-impl.cc
+index 7b96c5b8..1397a8ed 100644
+--- a/sherpa-onnx/csrc/online-recognizer-impl.cc
++++ b/sherpa-onnx/csrc/online-recognizer-impl.cc
+@@ -4,8 +4,11 @@
+ 
+ #include "sherpa-onnx/csrc/online-recognizer-impl.h"
+ 
++#include <memory>
++#include <string>
+ #include <strstream>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+@@ -244,8 +247,8 @@ OnlineRecognizerImpl::OnlineRecognizerImpl(Manager *mgr,
+         itn_list_.push_back(
+             std::make_unique<kaldifst::TextNormalizer>(std::move(r)));
+       }  // for (; !reader->Done(); reader->Next())
+-    }    // for (const auto &f : files)
+-  }      // if (!config.rule_fars.empty())
++    }  // for (const auto &f : files)
++  }  // if (!config.rule_fars.empty())
+   if (!config.hr.lexicon.empty() && !config.hr.rule_fsts.empty()) {
+     auto hr_config = config.hr;
+     hr_config.debug = config.model_config.debug;
+diff --git a/sherpa-onnx/csrc/online-recognizer.cc b/sherpa-onnx/csrc/online-recognizer.cc
+index 338a92f3..29177a10 100644
+--- a/sherpa-onnx/csrc/online-recognizer.cc
++++ b/sherpa-onnx/csrc/online-recognizer.cc
+@@ -10,6 +10,7 @@
+ #include <iomanip>
+ #include <memory>
+ #include <sstream>
++#include <string>
+ #include <utility>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/online-rnn-lm.cc b/sherpa-onnx/csrc/online-rnn-lm.cc
+index 8c5b1b45..1b333bc6 100644
+--- a/sherpa-onnx/csrc/online-rnn-lm.cc
++++ b/sherpa-onnx/csrc/online-rnn-lm.cc
+@@ -6,6 +6,7 @@
+ #include "sherpa-onnx/csrc/online-rnn-lm.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+@@ -49,7 +50,7 @@ class OnlineRnnLM::Impl {
+     // if LODR enabled, we need to update the LODR state
+     if (lodr_fst_ != nullptr) {
+       auto next_lodr_state = std::make_unique<LodrStateCost>(
+-                            hyp->lodr_state->ForwardOneStep(hyp->ys.back()));
++          hyp->lodr_state->ForwardOneStep(hyp->ys.back()));
+       // calculate the score of the latest token
+       auto score = next_lodr_state->Score() - hyp->lodr_state->Score();
+       hyp->lodr_state = std::move(next_lodr_state);
+@@ -108,7 +109,8 @@ class OnlineRnnLM::Impl {
+           // apply LODR to hyp score
+           if (lodr_fst_ != nullptr) {
+             // We scale LODR scale with LM scale to replicate Icefall code
+-            lodr_fst_->ComputeScore(config_.lodr_scale*scale, &h, context_size);
++            lodr_fst_->ComputeScore(config_.lodr_scale * scale, &h,
++                                    context_size);
+           }
+ 
+           // update NN LM states in hyp
+@@ -178,8 +180,8 @@ class OnlineRnnLM::Impl {
+     ComputeInitStates();
+ 
+     if (!config_.lodr_fst.empty()) {
+-      lodr_fst_ = std::make_unique<LodrFst>(LodrFst(config_.lodr_fst,
+-                                                    config_.lodr_backoff_id));
++      lodr_fst_ = std::make_unique<LodrFst>(
++          LodrFst(config_.lodr_fst, config_.lodr_backoff_id));
+     }
+   }
+ 
+diff --git a/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc b/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
+index b37e8802..e7b48ec2 100644
+--- a/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/online-t-one-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-t-one-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-t-one-ctc-model.cc b/sherpa-onnx/csrc/online-t-one-ctc-model.cc
+index 2bb4ccf5..5c498443 100644
+--- a/sherpa-onnx/csrc/online-t-one-ctc-model.cc
++++ b/sherpa-onnx/csrc/online-t-one-ctc-model.cc
+@@ -6,7 +6,10 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc b/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
+index a76db5b7..ab445f02 100644
+--- a/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
++++ b/sherpa-onnx/csrc/online-transducer-greedy-search-nemo-decoder.cc
+@@ -8,6 +8,7 @@
+ #include <algorithm>
+ #include <iterator>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/online-stream.h"
+diff --git a/sherpa-onnx/csrc/online-transducer-model-config.cc b/sherpa-onnx/csrc/online-transducer-model-config.cc
+index dd757271..10c2846d 100644
+--- a/sherpa-onnx/csrc/online-transducer-model-config.cc
++++ b/sherpa-onnx/csrc/online-transducer-model-config.cc
+@@ -4,6 +4,7 @@
+ #include "sherpa-onnx/csrc/online-transducer-model-config.h"
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/online-transducer-model.cc b/sherpa-onnx/csrc/online-transducer-model.cc
+index 286fd9cd..157350f2 100644
+--- a/sherpa-onnx/csrc/online-transducer-model.cc
++++ b/sherpa-onnx/csrc/online-transducer-model.cc
+@@ -17,6 +17,7 @@
+ #include <memory>
+ #include <sstream>
+ #include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/online-websocket-client.cc b/sherpa-onnx/csrc/online-websocket-client.cc
+index 2e5ac059..71998e55 100644
+--- a/sherpa-onnx/csrc/online-websocket-client.cc
++++ b/sherpa-onnx/csrc/online-websocket-client.cc
+@@ -4,6 +4,7 @@
+ #include <chrono>  // NOLINT
+ #include <fstream>
+ #include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/online-websocket-server-impl.cc b/sherpa-onnx/csrc/online-websocket-server-impl.cc
+index 8925ebb4..e5ac2b4b 100644
+--- a/sherpa-onnx/csrc/online-websocket-server-impl.cc
++++ b/sherpa-onnx/csrc/online-websocket-server-impl.cc
+@@ -4,6 +4,10 @@
+ 
+ #include "sherpa-onnx/csrc/online-websocket-server-impl.h"
+ 
++#include <iostream>
++#include <memory>
++#include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+diff --git a/sherpa-onnx/csrc/online-websocket-server-impl.h b/sherpa-onnx/csrc/online-websocket-server-impl.h
+index 4e0582db..b7bd189b 100644
+--- a/sherpa-onnx/csrc/online-websocket-server-impl.h
++++ b/sherpa-onnx/csrc/online-websocket-server-impl.h
+@@ -9,14 +9,14 @@
+ #include <fstream>
+ #include <map>
+ #include <memory>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <set>
+ #include <string>
+ #include <unordered_set>
+ #include <utility>
+ #include <vector>
+ 
+-#include "asio.hpp"
++#include "asio.hpp"  // NOLINT
+ #include "sherpa-onnx/csrc/online-recognizer.h"
+ #include "sherpa-onnx/csrc/online-stream.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/online-websocket-server.cc b/sherpa-onnx/csrc/online-websocket-server.cc
+index 6ba7a198..2e976207 100644
+--- a/sherpa-onnx/csrc/online-websocket-server.cc
++++ b/sherpa-onnx/csrc/online-websocket-server.cc
+@@ -2,7 +2,9 @@
+ //
+ // Copyright (c)  2022-2023  Xiaomi Corporation
+ 
+-#include "asio.hpp"
++#include <vector>
++
++#include "asio.hpp"  // NOLINT
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/online-websocket-server-impl.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc b/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
+index a47b3e16..2d881e76 100644
+--- a/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/online-wenet-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-wenet-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-wenet-ctc-model.cc b/sherpa-onnx/csrc/online-wenet-ctc-model.cc
+index 9024481d..9d78b151 100644
+--- a/sherpa-onnx/csrc/online-wenet-ctc-model.cc
++++ b/sherpa-onnx/csrc/online-wenet-ctc-model.cc
+@@ -6,7 +6,10 @@
+ 
+ #include <algorithm>
+ #include <cmath>
++#include <memory>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc b/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
+index ed9e7b8a..925e33e6 100644
+--- a/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
++++ b/sherpa-onnx/csrc/online-zipformer2-ctc-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-zipformer2-ctc-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc b/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
+index f7cccc43..aa083418 100644
+--- a/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
++++ b/sherpa-onnx/csrc/online-zipformer2-ctc-model.cc
+@@ -7,8 +7,11 @@
+ #include <algorithm>
+ #include <cassert>
+ #include <cmath>
++#include <memory>
+ #include <numeric>
+ #include <string>
++#include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/onnx-utils.cc b/sherpa-onnx/csrc/onnx-utils.cc
+index fd10c02c..a33252de 100644
+--- a/sherpa-onnx/csrc/onnx-utils.cc
++++ b/sherpa-onnx/csrc/onnx-utils.cc
+@@ -11,6 +11,7 @@
+ #include <numeric>
+ #include <sstream>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "onnxruntime_cxx_api.h"  // NOLINT
+@@ -258,7 +259,7 @@ void PrintShape(const Ort::Value *v) {
+     os << i << ", ";
+   }
+   os << "\n";
+-  fprintf(stderr, "%s", os.str().c_str());
++  SHERPA_ONNX_LOGE("%s", os.str().c_str());
+ }
+ 
+ template <typename T /*= float*/>
+@@ -270,7 +271,7 @@ void Print1D(const Ort::Value *v) {
+     os << d[i] << " ";
+   }
+   os << "\n";
+-  fprintf(stderr, "%s\n", os.str().c_str());
++  SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+ }
+ 
+ template void Print1D<int64_t>(const Ort::Value *v);
+@@ -289,7 +290,7 @@ void Print2D(const Ort::Value *v) {
+     }
+     os << "\n";
+   }
+-  fprintf(stderr, "%s\n", os.str().c_str());
++  SHERPA_ONNX_LOGE("%s\n", os.str().c_str());
+ }
+ 
+ template void Print2D<int64_t>(const Ort::Value *v);
+@@ -300,15 +301,15 @@ void Print3D(const Ort::Value *v) {
+   const float *d = v->GetTensorData<float>();
+ 
+   for (int32_t p = 0; p != static_cast<int32_t>(shape[0]); ++p) {
+-    fprintf(stderr, "---plane %d---\n", p);
++    SHERPA_ONNX_LOGE("---plane %d---\n", p);
+     for (int32_t r = 0; r != static_cast<int32_t>(shape[1]); ++r) {
+       for (int32_t c = 0; c != static_cast<int32_t>(shape[2]); ++c, ++d) {
+-        fprintf(stderr, "%.3f ", *d);
++        SHERPA_ONNX_LOGE("%.3f ", *d);
+       }
+-      fprintf(stderr, "\n");
++      SHERPA_ONNX_LOGE("\n");
+     }
+   }
+-  fprintf(stderr, "\n");
++  SHERPA_ONNX_LOGE("\n");
+ }
+ 
+ void Print4D(const Ort::Value *v) {
+@@ -316,19 +317,19 @@ void Print4D(const Ort::Value *v) {
+   const float *d = v->GetTensorData<float>();
+ 
+   for (int32_t p = 0; p != static_cast<int32_t>(shape[0]); ++p) {
+-    fprintf(stderr, "---plane %d---\n", p);
++    SHERPA_ONNX_LOGE("---plane %d---\n", p);
+     for (int32_t q = 0; q != static_cast<int32_t>(shape[1]); ++q) {
+-      fprintf(stderr, "---subplane %d---\n", q);
++      SHERPA_ONNX_LOGE("---subplane %d---\n", q);
+       for (int32_t r = 0; r != static_cast<int32_t>(shape[2]); ++r) {
+         for (int32_t c = 0; c != static_cast<int32_t>(shape[3]); ++c, ++d) {
+-          fprintf(stderr, "%.3f ", *d);
++          SHERPA_ONNX_LOGE("%.3f ", *d);
+         }
+-        fprintf(stderr, "\n");
++        SHERPA_ONNX_LOGE("\n");
+       }
+-      fprintf(stderr, "\n");
++      SHERPA_ONNX_LOGE("\n");
+     }
+   }
+-  fprintf(stderr, "\n");
++  SHERPA_ONNX_LOGE("\n");
+ }
+ 
+ Ort::Value Repeat(OrtAllocator *allocator, Ort::Value *cur_encoder_out,
+diff --git a/sherpa-onnx/csrc/packed-sequence-test.cc b/sherpa-onnx/csrc/packed-sequence-test.cc
+index eda38914..8ed182f0 100644
+--- a/sherpa-onnx/csrc/packed-sequence-test.cc
++++ b/sherpa-onnx/csrc/packed-sequence-test.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/packed-sequence.h"
+ 
++#include <cstdio>
+ #include <numeric>
+ 
+ #include "gtest/gtest.h"
+diff --git a/sherpa-onnx/csrc/packed-sequence.cc b/sherpa-onnx/csrc/packed-sequence.cc
+index 1c3fe91c..240345a7 100644
+--- a/sherpa-onnx/csrc/packed-sequence.cc
++++ b/sherpa-onnx/csrc/packed-sequence.cc
+@@ -8,6 +8,7 @@
+ #include <cassert>
+ #include <numeric>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/slice.h"
+ #include "sherpa-onnx/csrc/transpose.h"
+diff --git a/sherpa-onnx/csrc/parse-options.cc b/sherpa-onnx/csrc/parse-options.cc
+index c0dd9595..68ba1467 100644
+--- a/sherpa-onnx/csrc/parse-options.cc
++++ b/sherpa-onnx/csrc/parse-options.cc
+@@ -17,6 +17,7 @@
+ #include <cstring>
+ #include <fstream>
+ #include <iomanip>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/log.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/phrase-matcher.cc b/sherpa-onnx/csrc/phrase-matcher.cc
+index 35e09555..bf2069e6 100644
+--- a/sherpa-onnx/csrc/phrase-matcher.cc
++++ b/sherpa-onnx/csrc/phrase-matcher.cc
+@@ -5,7 +5,10 @@
+ 
+ #include <algorithm>
+ #include <sstream>
++#include <string>
++#include <unordered_set>
+ #include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+diff --git a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+index 21200dd3..8b9e4474 100644
+--- a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
++++ b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+@@ -8,10 +8,11 @@
+ #include <fstream>
+ #include <locale>
+ #include <map>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <sstream>
+ #include <string>
+ #include <strstream>
++#include <unordered_map>
+ #include <utility>
+ #include <vector>
+ 
+@@ -25,8 +26,8 @@
+ #endif
+ 
+ #include "espeak-ng/speak_lib.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/piper-phonemize-test.cc b/sherpa-onnx/csrc/piper-phonemize-test.cc
+index 47e2c57b..65abfabd 100644
+--- a/sherpa-onnx/csrc/piper-phonemize-test.cc
++++ b/sherpa-onnx/csrc/piper-phonemize-test.cc
+@@ -2,10 +2,15 @@
+ //
+ // Copyright (c)  2023  Xiaomi Corporation
+ 
++#include <iostream>
++#include <map>
++#include <string>
++#include <vector>
++
+ #include "espeak-ng/speak_lib.h"
+ #include "gtest/gtest.h"
+-#include "phoneme_ids.hpp"
+-#include "phonemize.hpp"
++#include "phoneme_ids.hpp"  // NOLINT
++#include "phonemize.hpp"    // NOLINT
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/provider-config.cc b/sherpa-onnx/csrc/provider-config.cc
+index 165e2d9a..dc93723c 100644
+--- a/sherpa-onnx/csrc/provider-config.cc
++++ b/sherpa-onnx/csrc/provider-config.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/provider-config.h"
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/provider.cc b/sherpa-onnx/csrc/provider.cc
+index 3baed32c..af3759c7 100644
+--- a/sherpa-onnx/csrc/provider.cc
++++ b/sherpa-onnx/csrc/provider.cc
+@@ -6,6 +6,7 @@
+ 
+ #include <algorithm>
+ #include <cctype>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
+index 82155a6b..2276cd79 100644
+--- a/sherpa-onnx/csrc/qnn-config.cc
++++ b/sherpa-onnx/csrc/qnn-config.cc
+@@ -5,6 +5,7 @@
+ #include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ #include <sstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+index 8ed9c281..df4299ac 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-backend.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+@@ -8,6 +8,7 @@
+ #include <stdio.h>
+ 
+ #include <cstdint>
++#include <memory>
+ #include <sstream>
+ #include <string>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
+index d3c53788..735995ea 100644
+--- a/sherpa-onnx/csrc/qnn/qnn-model.cc
++++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
+@@ -617,7 +617,7 @@ QnnModel::QnnModel(const std::string &binary_context_file,
+                    const std::string &system_lib, const QnnBackend *backend,
+                    BinaryContextTag tag)
+     : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
+-                                   tag)) {}
++                                   tag)) {}  // NOLINT
+ 
+ bool QnnModel::SaveBinaryContext(const std::string &filename) const {
+   return impl_->SaveBinaryContext(filename);
+diff --git a/sherpa-onnx/csrc/regex-lang-test.cc b/sherpa-onnx/csrc/regex-lang-test.cc
+index 11df85cc..7bd63440 100644
+--- a/sherpa-onnx/csrc/regex-lang-test.cc
++++ b/sherpa-onnx/csrc/regex-lang-test.cc
+@@ -2,7 +2,10 @@
+ //
+ // Copyright (c)  2025  Xiaomi Corporation
+ 
++#include <iostream>
+ #include <regex>  // NOLINT
++#include <string>
++#include <vector>
+ 
+ #include "gtest/gtest.h"
+ #include "sherpa-onnx/csrc/text-utils.cc"
+diff --git a/sherpa-onnx/csrc/resample.cc b/sherpa-onnx/csrc/resample.cc
+index c2a768ee..f393deec 100644
+--- a/sherpa-onnx/csrc/resample.cc
++++ b/sherpa-onnx/csrc/resample.cc
+@@ -29,6 +29,7 @@
+ #include <cstdio>
+ #include <cstdlib>
+ #include <type_traits>
++#include <vector>
+ 
+ #ifndef M_2PI
+ #define M_2PI 6.283185307179586476925286766559005
+diff --git a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+index 1a44eafb..2a13deb8 100644
+--- a/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
++++ b/sherpa-onnx/csrc/rknn/online-zipformer-transducer-model-rknn.cc
+@@ -442,7 +442,7 @@ class OnlineZipformerTransducerModelRknn::Impl {
+ };
+ 
+ OnlineZipformerTransducerModelRknn::~OnlineZipformerTransducerModelRknn() =
+-    default;
++    default;  // NOLINT
+ 
+ OnlineZipformerTransducerModelRknn::OnlineZipformerTransducerModelRknn(
+     const OnlineModelConfig &config)
+diff --git a/sherpa-onnx/csrc/rknn/utils.cc b/sherpa-onnx/csrc/rknn/utils.cc
+index 5f092c61..d0522d6e 100644
+--- a/sherpa-onnx/csrc/rknn/utils.cc
++++ b/sherpa-onnx/csrc/rknn/utils.cc
+@@ -7,6 +7,7 @@
+ #include <string.h>
+ 
+ #include <sstream>
++#include <string>
+ #include <unordered_map>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/sherpa-display.h b/sherpa-onnx/csrc/sherpa-display.h
+index 0fb2dc4d..2da5d31d 100644
+--- a/sherpa-onnx/csrc/sherpa-display.h
++++ b/sherpa-onnx/csrc/sherpa-display.h
+@@ -5,6 +5,7 @@
+ 
+ #include <stdlib.h>
+ 
++#include <cstdio>
+ #include <ctime>
+ #include <iomanip>
+ #include <sstream>
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
+index 6a5b701e..da1b8c87 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-audio-tagging.cc
+@@ -7,8 +7,11 @@
+ #include <stdlib.h>
+ 
+ #include <algorithm>
+-#include <mutex>   // NOLINT
++#include <mutex>  // NOLINT
++#include <string>
+ #include <thread>  // NOLINT
++#include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/audio-tagging.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
+index a14ff5c7..1404afb3 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline-speaker-identification.cc
+@@ -10,7 +10,11 @@
+ #include <fstream>
+ #include <mutex>  // NOLINT
+ #include <sstream>
++#include <string>
+ #include <thread>  // NOLINT
++#include <unordered_map>
++#include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
+index b69ec6cd..8b144f76 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-alsa-offline.cc
+@@ -8,9 +8,12 @@
+ 
+ #include <algorithm>
+ #include <cctype>  // std::tolower
+-#include <chrono>  // NOLINT
+-#include <mutex>   // NOLINT
+-#include <thread>  // NOLINT
++#include <chrono>
++#include <mutex>
++#include <string>
++#include <thread>
++#include <utility>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
+index 2aae076d..c070a23e 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-alsa.cc
+@@ -8,6 +8,8 @@
+ #include <algorithm>
+ #include <cctype>  // std::tolower
+ #include <cstdint>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/display.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
+index cfa46dc9..0213e0a0 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter-alsa.cc
+@@ -7,6 +7,8 @@
+ 
+ #include <algorithm>
+ #include <cstdint>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/display.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
+index e52ef8ac..050b4f50 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-keyword-spotter.cc
+@@ -4,10 +4,12 @@
+ 
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <iomanip>
+ #include <iostream>
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/keyword-spotter.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
+index b94ba09a..03b4e7d3 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-audio-tagging.cc
+@@ -8,8 +8,10 @@
+ 
+ #include <algorithm>
+ #include <cctype>  // std::tolower
+-#include <mutex>   // NOLINT
+-#include <thread>  // NOLINT
++#include <mutex>
++#include <thread>
++#include <utility>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/audio-tagging.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
+index 3d22f0d0..da6bd6f5 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline-speaker-identification.cc
+@@ -8,9 +8,13 @@
+ 
+ #include <algorithm>
+ #include <fstream>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <sstream>
+-#include <thread>  // NOLINT
++#include <string>
++#include <thread>
++#include <unordered_map>
++#include <utility>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
+index 5a012a8b..3dab1fee 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-microphone-offline.cc
+@@ -8,8 +8,10 @@
+ 
+ #include <algorithm>
+ #include <cctype>  // std::tolower
+-#include <mutex>   // NOLINT
+-#include <thread>  // NOLINT
++#include <mutex>
++#include <thread>
++#include <utility>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-microphone.cc b/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
+index b70797a3..f18cdc3e 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-microphone.cc
+@@ -9,6 +9,8 @@
+ #include <algorithm>
+ #include <clocale>
+ #include <cwctype>
++#include <string>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/display.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
+index 9367b017..0e5de242 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-audio-tagging.cc
+@@ -3,6 +3,9 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include <stdio.h>
+ 
++#include <string>
++#include <vector>
++
+ #include "sherpa-onnx/csrc/audio-tagging.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+ #include "sherpa-onnx/csrc/wave-reader.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
+index 0ec31487..fcd2de38 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-denoiser.cc
+@@ -3,7 +3,9 @@
+ // Copyright (c)  2025  Xiaomi Corporation
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-speech-denoiser.h"
+ #include "sherpa-onnx/csrc/wave-reader.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
+index 1b563943..a3148a21 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-parallel.cc
+@@ -5,11 +5,12 @@
+ #include <stdio.h>
+ 
+ #include <atomic>
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <fstream>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <string>
+-#include <thread>  // NOLINT
++#include <thread>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
+index 7f220734..b7891d85 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-punctuation.cc
+@@ -3,7 +3,8 @@
+ // Copyright (c)  2022-2024  Xiaomi Corporation
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/offline-punctuation.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
+index 31cda85f..a27f1d3a 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-speaker-diarization.cc
+@@ -2,6 +2,11 @@
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
++#include <cstdio>
++#include <iostream>
++#include <string>
++#include <vector>
++
+ #include "sherpa-onnx/csrc/offline-speaker-diarization.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+ #include "sherpa-onnx/csrc/wave-reader.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
+index c5915cbc..401c0d84 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play-alsa.cc
+@@ -11,9 +11,11 @@
+ #include <algorithm>
+ #include <chrono>              // NOLINT
+ #include <condition_variable>  // NOLINT
++#include <cstdio>
+ #include <fstream>
+ #include <mutex>  // NOLINT
+ #include <queue>
++#include <string>
+ #include <thread>  // NOLINT
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
+index a8ec6b5e..ae496566 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts-play.cc
+@@ -5,12 +5,15 @@
+ #include <signal.h>
+ 
+ #include <algorithm>
+-#include <chrono>              // NOLINT
+-#include <condition_variable>  // NOLINT
++#include <chrono>
++#include <condition_variable>
++#include <cstdio>
+ #include <fstream>
+-#include <mutex>  // NOLINT
++#include <mutex>
+ #include <queue>
+-#include <thread>  // NOLINT
++#include <string>
++#include <thread>
++#include <utility>
+ #include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
+index 0426367b..7c12af13 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-tts.cc
+@@ -3,7 +3,9 @@
+ // Copyright (c)  2023  Xiaomi Corporation
+ 
+ #include <chrono>  // NOLINT
++#include <cstdio>
+ #include <fstream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/offline-tts.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+index b424fba3..ad2e3352 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline-zeroshot-tts.cc
+@@ -2,8 +2,11 @@
+ //
+ // Copyright (c)  2025  Xiaomi Corporation
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
++#include <cstdio>
+ #include <fstream>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-tts.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-offline.cc b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+index 5509a861..41fb3a4f 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-offline.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-offline.cc
+@@ -4,8 +4,9 @@
+ 
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc b/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
+index faca83b9..35783a11 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-online-punctuation.cc
+@@ -4,8 +4,9 @@
+ 
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <iostream>
++#include <string>
+ 
+ #include "sherpa-onnx/csrc/online-punctuation.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
+index 225dcc20..4de8dbb2 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa-offline-asr.cc
+@@ -7,7 +7,10 @@
+ #include <stdlib.h>
+ 
+ #include <algorithm>
++#include <memory>
+ #include <mutex>  // NOLINT
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/circular-buffer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
+index dcf7c85a..d96d3884 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-alsa.cc
+@@ -8,6 +8,9 @@
+ 
+ #include <algorithm>
+ #include <iomanip>
++#include <memory>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/alsa.h"
+ #include "sherpa-onnx/csrc/voice-activity-detector.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
+index 5940f1d6..b8823983 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-offline-asr.cc
+@@ -7,7 +7,10 @@
+ #include <stdlib.h>
+ 
+ #include <algorithm>
+-#include <mutex>  // NOLINT
++#include <memory>
++#include <mutex>
++#include <utility>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/circular-buffer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
+index fa7afe4c..27ea6587 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone-simulated-streaming-asr.cc
+@@ -7,9 +7,10 @@
+ #include <stdlib.h>
+ 
+ #include <algorithm>
+-#include <chrono>              // NOLINT
+-#include <condition_variable>  // NOLINT
+-#include <mutex>               // NOLINT
++#include <chrono>
++#include <condition_variable>
++#include <memory>
++#include <mutex>
+ #include <queue>
+ #include <string>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
+index 49802e86..e774dacb 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-microphone.cc
+@@ -7,7 +7,10 @@
+ #include <stdlib.h>
+ 
+ #include <algorithm>
+-#include <mutex>  // NOLINT
++#include <memory>
++#include <mutex>
++#include <utility>
++#include <vector>
+ 
+ #include "portaudio.h"  // NOLINT
+ #include "sherpa-onnx/csrc/circular-buffer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
+index 94d7ff6b..32097856 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-offline-asr.cc
+@@ -4,8 +4,11 @@
+ 
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <algorithm>
++#include <chrono>
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc b/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
+index 4ccc8245..9fc57220 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad-with-online-asr.cc
+@@ -9,8 +9,10 @@
+ #include <stdio.h>
+ 
+ #include <algorithm>
+-#include <chrono>  // NOLINT
++#include <chrono>
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/online-recognizer.h"
+@@ -138,13 +140,13 @@ for a list of pre-trained models to download.
+     samples = std::move(out_samples);
+     fprintf(stderr, "Resampling done\n");
+   }
+-  const float tail_padding_len =  1.28;  // related to model chunk-size
+-  std::vector<float> tail_paddings(
+-      static_cast<int>(tail_padding_len * 16000));
++  const float tail_padding_len = 1.28;  // related to model chunk-size
++  std::vector<float> tail_paddings(static_cast<int>(tail_padding_len * 16000));
+ 
+   fprintf(stderr, "Started!\n");
+   int32_t window_size = vad_config.ten_vad.model.empty()
+-    ? vad_config.silero_vad.window_size : vad_config.ten_vad.window_size;
++                            ? vad_config.silero_vad.window_size
++                            : vad_config.ten_vad.window_size;
+   int32_t offset = 0;
+   int32_t segment_id = 0;
+   bool speech_started = false;
+@@ -178,8 +180,8 @@ for a list of pre-trained models to download.
+       }
+       auto text = recognizer.GetResult(s.get()).text;
+       if (!text.empty()) {
+-        fprintf(stderr, "vad segment(%d:%.3f-%.3f) results: %s\n",
+-            segment_id, start_time, end_time, text.c_str());
++        fprintf(stderr, "vad segment(%d:%.3f-%.3f) results: %s\n", segment_id,
++                start_time, end_time, text.c_str());
+       }
+       vad->Pop();
+     }
+diff --git a/sherpa-onnx/csrc/sherpa-onnx-vad.cc b/sherpa-onnx/csrc/sherpa-onnx-vad.cc
+index 7cf82c3b..b452f729 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx-vad.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx-vad.cc
+@@ -7,6 +7,9 @@
+ 
+ #include <algorithm>
+ #include <iomanip>
++#include <memory>
++#include <string>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/voice-activity-detector.h"
+ #include "sherpa-onnx/csrc/wave-reader.h"
+diff --git a/sherpa-onnx/csrc/sherpa-onnx.cc b/sherpa-onnx/csrc/sherpa-onnx.cc
+index 5470925b..14783fd8 100644
+--- a/sherpa-onnx/csrc/sherpa-onnx.cc
++++ b/sherpa-onnx/csrc/sherpa-onnx.cc
+@@ -4,10 +4,12 @@
+ 
+ #include <stdio.h>
+ 
+-#include <chrono>  // NOLINT
++#include <chrono>
+ #include <iomanip>
+ #include <iostream>
++#include <memory>
+ #include <string>
++#include <utility>
+ #include <vector>
+ 
+ #include "sherpa-onnx/csrc/online-recognizer.h"
+diff --git a/sherpa-onnx/csrc/silero-vad-model-config.cc b/sherpa-onnx/csrc/silero-vad-model-config.cc
+index a21ac63b..2623a179 100644
+--- a/sherpa-onnx/csrc/silero-vad-model-config.cc
++++ b/sherpa-onnx/csrc/silero-vad-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/silero-vad-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/silero-vad-model.cc b/sherpa-onnx/csrc/silero-vad-model.cc
+index 8a2db3db..00a0f399 100644
+--- a/sherpa-onnx/csrc/silero-vad-model.cc
++++ b/sherpa-onnx/csrc/silero-vad-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/silero-vad-model.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
+index 3abb21a2..6b94bc96 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-extractor-impl.cc
+@@ -3,6 +3,8 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include "sherpa-onnx/csrc/speaker-embedding-extractor-impl.h"
+ 
++#include <memory>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
+index b2192431..2c870761 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-extractor-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/speaker-embedding-extractor-model.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
+index 66f79aac..fe652b1e 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/speaker-embedding-extractor-nemo-model.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/speaker-embedding-extractor.cc b/sherpa-onnx/csrc/speaker-embedding-extractor.cc
+index 5d52fb2f..237678b1 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-extractor.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-extractor.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/speaker-embedding-extractor.h"
+ 
++#include <memory>
++#include <string>
+ #include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+diff --git a/sherpa-onnx/csrc/speaker-embedding-manager-test.cc b/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
+index 6f115ca5..12a624c5 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-manager-test.cc
+@@ -4,6 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
+ 
++#include <string>
++#include <vector>
++
+ #include "gtest/gtest.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/csrc/speaker-embedding-manager.cc b/sherpa-onnx/csrc/speaker-embedding-manager.cc
+index 44f41370..c61597c5 100644
+--- a/sherpa-onnx/csrc/speaker-embedding-manager.cc
++++ b/sherpa-onnx/csrc/speaker-embedding-manager.cc
+@@ -5,16 +5,18 @@
+ #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
+ 
+ #include <algorithm>
++#include <string>
+ #include <unordered_map>
+ #include <utility>
++#include <vector>
+ 
+ #include "Eigen/Dense"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+ namespace sherpa_onnx {
+ 
+-using FloatMatrix =
+-    Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
++using FloatMatrix = Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic,
++                                  Eigen::RowMajor>;  // NOLINT
+ 
+ class SpeakerEmbeddingManager::Impl {
+  public:
+diff --git a/sherpa-onnx/csrc/spoken-language-identification.cc b/sherpa-onnx/csrc/spoken-language-identification.cc
+index 3797586a..4880c5b8 100644
+--- a/sherpa-onnx/csrc/spoken-language-identification.cc
++++ b/sherpa-onnx/csrc/spoken-language-identification.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/spoken-language-identification.h"
+ 
++#include <memory>
+ #include <string>
+ 
+ #if __ANDROID_API__ >= 9
+diff --git a/sherpa-onnx/csrc/stack.cc b/sherpa-onnx/csrc/stack.cc
+index 1d0fac51..76689d72 100644
+--- a/sherpa-onnx/csrc/stack.cc
++++ b/sherpa-onnx/csrc/stack.cc
+@@ -8,7 +8,9 @@
+ #include <functional>
+ #include <numeric>
+ #include <utility>
++#include <vector>
+ 
++#include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+ 
+ namespace sherpa_onnx {
+@@ -26,9 +28,9 @@ static bool Compare(const std::vector<int64_t> &a,
+ 
+ static void PrintShape(const std::vector<int64_t> &a) {
+   for (auto i : a) {
+-    fprintf(stderr, "%d ", static_cast<int32_t>(i));
++    SHERPA_ONNX_LOGE("%d ", static_cast<int32_t>(i));
+   }
+-  fprintf(stderr, "\n");
++  SHERPA_ONNX_LOGE("\n");
+ }
+ 
+ template <typename T /*=float*/>
+@@ -41,12 +43,12 @@ Ort::Value Stack(OrtAllocator *allocator,
+     auto s = values[i]->GetTensorTypeAndShapeInfo().GetShape();
+     bool ret = Compare(v0_shape, s);
+     if (!ret) {
+-      fprintf(stderr, "Incorrect shape in Stack !\n");
++      SHERPA_ONNX_LOGE("Incorrect shape in Stack !\n");
+ 
+-      fprintf(stderr, "Shape for tensor 0: ");
++      SHERPA_ONNX_LOGE("Shape for tensor 0: ");
+       PrintShape(v0_shape);
+ 
+-      fprintf(stderr, "Shape for tensor %d: ", i);
++      SHERPA_ONNX_LOGE("Shape for tensor %d: ", i);
+       PrintShape(s);
+ 
+       exit(-1);
+diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
+index 2bc2c7f4..eafd4958 100644
+--- a/sherpa-onnx/csrc/symbol-table.cc
++++ b/sherpa-onnx/csrc/symbol-table.cc
+@@ -11,6 +11,7 @@
+ #include <sstream>
+ #include <string>
+ #include <strstream>
++#include <unordered_map>
+ #include <utility>
+ 
+ #if __ANDROID_API__ >= 9
+diff --git a/sherpa-onnx/csrc/ten-vad-model-config.cc b/sherpa-onnx/csrc/ten-vad-model-config.cc
+index 0e7bcba0..77061ec1 100644
+--- a/sherpa-onnx/csrc/ten-vad-model-config.cc
++++ b/sherpa-onnx/csrc/ten-vad-model-config.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/ten-vad-model-config.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+ 
+diff --git a/sherpa-onnx/csrc/text-utils-test.cc b/sherpa-onnx/csrc/text-utils-test.cc
+index 3240fe37..1fef71e4 100644
+--- a/sherpa-onnx/csrc/text-utils-test.cc
++++ b/sherpa-onnx/csrc/text-utils-test.cc
+@@ -4,8 +4,13 @@
+ 
+ #include "sherpa-onnx/csrc/text-utils.h"
+ 
+-#include <regex>  // NOLINT
++#include <cstdio>
++#include <cstring>
++#include <iostream>
++#include <regex>
+ #include <sstream>
++#include <string>
++#include <vector>
+ 
+ #include "gtest/gtest.h"
+ 
+diff --git a/sherpa-onnx/csrc/text2token-test.cc b/sherpa-onnx/csrc/text2token-test.cc
+index 0ad912df..be04a15e 100644
+--- a/sherpa-onnx/csrc/text2token-test.cc
++++ b/sherpa-onnx/csrc/text2token-test.cc
+@@ -3,8 +3,10 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
+ #include <fstream>
++#include <memory>
+ #include <sstream>
+ #include <string>
++#include <vector>
+ 
+ #include "gtest/gtest.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/csrc/unbind-test.cc b/sherpa-onnx/csrc/unbind-test.cc
+index 8159685b..b2b02cb6 100644
+--- a/sherpa-onnx/csrc/unbind-test.cc
++++ b/sherpa-onnx/csrc/unbind-test.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/unbind.h"
+ 
++#include <vector>
++
+ #include "gtest/gtest.h"
+ #include "sherpa-onnx/csrc/cat.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/csrc/utfcpp-test.cc b/sherpa-onnx/csrc/utfcpp-test.cc
+index fcc3ae74..ef06c1cb 100644
+--- a/sherpa-onnx/csrc/utfcpp-test.cc
++++ b/sherpa-onnx/csrc/utfcpp-test.cc
+@@ -3,7 +3,9 @@
+ // Copyright (c)  2023  Xiaomi Corporation
+ 
+ #include <cctype>
++#include <iostream>
+ #include <string>
++#include <vector>
+ 
+ #include "gtest/gtest.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+diff --git a/sherpa-onnx/csrc/vad-model.cc b/sherpa-onnx/csrc/vad-model.cc
+index 781b7fe0..90658383 100644
+--- a/sherpa-onnx/csrc/vad-model.cc
++++ b/sherpa-onnx/csrc/vad-model.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/vad-model.h"
+ 
++#include <memory>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/vocoder.cc b/sherpa-onnx/csrc/vocoder.cc
+index d9821cec..21963b3d 100644
+--- a/sherpa-onnx/csrc/vocoder.cc
++++ b/sherpa-onnx/csrc/vocoder.cc
+@@ -4,6 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/vocoder.h"
+ 
++#include <memory>
++#include <vector>
++
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+ #include "android/asset_manager_jni.h"
+diff --git a/sherpa-onnx/csrc/vocos-vocoder.cc b/sherpa-onnx/csrc/vocos-vocoder.cc
+index a869bebd..26ac3b4d 100644
+--- a/sherpa-onnx/csrc/vocos-vocoder.cc
++++ b/sherpa-onnx/csrc/vocos-vocoder.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/vocos-vocoder.h"
+ 
++#include <memory>
+ #include <string>
+ #include <utility>
+ #include <vector>
+diff --git a/sherpa-onnx/csrc/voice-activity-detector.cc b/sherpa-onnx/csrc/voice-activity-detector.cc
+index be8b8811..c82a77e3 100644
+--- a/sherpa-onnx/csrc/voice-activity-detector.cc
++++ b/sherpa-onnx/csrc/voice-activity-detector.cc
+@@ -5,8 +5,10 @@
+ #include "sherpa-onnx/csrc/voice-activity-detector.h"
+ 
+ #include <algorithm>
++#include <memory>
+ #include <queue>
+ #include <utility>
++#include <vector>
+ 
+ #if __ANDROID_API__ >= 9
+ #include "android/asset_manager.h"
+diff --git a/sherpa-onnx/csrc/wave-reader.cc b/sherpa-onnx/csrc/wave-reader.cc
+index 90db0d51..56fa2718 100644
+--- a/sherpa-onnx/csrc/wave-reader.cc
++++ b/sherpa-onnx/csrc/wave-reader.cc
+@@ -7,6 +7,7 @@
+ #include <cassert>
+ #include <cstdint>
+ #include <fstream>
++#include <string>
+ #include <utility>
+ #include <vector>
+ 
+diff --git a/sherpa-onnx/jni/audio-tagging.cc b/sherpa-onnx/jni/audio-tagging.cc
+index 767761d1..8f560006 100644
+--- a/sherpa-onnx/jni/audio-tagging.cc
++++ b/sherpa-onnx/jni/audio-tagging.cc
+@@ -4,6 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/audio-tagging.h"
+ 
++#include <memory>
++#include <vector>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/jni.cc b/sherpa-onnx/jni/jni.cc
+index 42fd0d72..19df23a7 100644
+--- a/sherpa-onnx/jni/jni.cc
++++ b/sherpa-onnx/jni/jni.cc
+@@ -5,6 +5,7 @@
+ //                2023       Zhaoming
+ 
+ #include <fstream>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/onnx-utils.h"
+diff --git a/sherpa-onnx/jni/keyword-spotter.cc b/sherpa-onnx/jni/keyword-spotter.cc
+index 4002a6ab..df5aa6d4 100644
+--- a/sherpa-onnx/jni/keyword-spotter.cc
++++ b/sherpa-onnx/jni/keyword-spotter.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/keyword-spotter.h"
+ 
++#include <memory>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/offline-punctuation.cc b/sherpa-onnx/jni/offline-punctuation.cc
+index b94d5c26..37ddf669 100644
+--- a/sherpa-onnx/jni/offline-punctuation.cc
++++ b/sherpa-onnx/jni/offline-punctuation.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-punctuation.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
+index 772fecc3..baa58813 100644
+--- a/sherpa-onnx/jni/offline-recognizer.cc
++++ b/sherpa-onnx/jni/offline-recognizer.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-recognizer.h"
+ 
++#include <memory>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+ #include "sherpa-onnx/jni/common.h"
+diff --git a/sherpa-onnx/jni/offline-speaker-diarization.cc b/sherpa-onnx/jni/offline-speaker-diarization.cc
+index a6de825c..8a888b2b 100644
+--- a/sherpa-onnx/jni/offline-speaker-diarization.cc
++++ b/sherpa-onnx/jni/offline-speaker-diarization.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/offline-speaker-diarization.h"
+ 
++#include <vector>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/online-punctuation.cc b/sherpa-onnx/jni/online-punctuation.cc
+index 8b87352d..e562f192 100644
+--- a/sherpa-onnx/jni/online-punctuation.cc
++++ b/sherpa-onnx/jni/online-punctuation.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-punctuation.h"
+ 
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/online-recognizer.cc b/sherpa-onnx/jni/online-recognizer.cc
+index b518ffe8..c2486866 100644
+--- a/sherpa-onnx/jni/online-recognizer.cc
++++ b/sherpa-onnx/jni/online-recognizer.cc
+@@ -4,6 +4,8 @@
+ 
+ #include "sherpa-onnx/csrc/online-recognizer.h"
+ 
++#include <memory>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/csrc/text-utils.h"
+ #include "sherpa-onnx/jni/common.h"
+diff --git a/sherpa-onnx/jni/speaker-embedding-extractor.cc b/sherpa-onnx/jni/speaker-embedding-extractor.cc
+index 16cf02ae..b6821d44 100644
+--- a/sherpa-onnx/jni/speaker-embedding-extractor.cc
++++ b/sherpa-onnx/jni/speaker-embedding-extractor.cc
+@@ -3,6 +3,9 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include "sherpa-onnx/csrc/speaker-embedding-extractor.h"
+ 
++#include <memory>
++#include <vector>
++
+ #include "sherpa-onnx/jni/common.h"
+ 
+ namespace sherpa_onnx {
+diff --git a/sherpa-onnx/jni/speaker-embedding-manager.cc b/sherpa-onnx/jni/speaker-embedding-manager.cc
+index d73a8788..3c169fdb 100644
+--- a/sherpa-onnx/jni/speaker-embedding-manager.cc
++++ b/sherpa-onnx/jni/speaker-embedding-manager.cc
+@@ -3,6 +3,9 @@
+ // Copyright (c)  2024  Xiaomi Corporation
+ #include "sherpa-onnx/csrc/speaker-embedding-manager.h"
+ 
++#include <string>
++#include <vector>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/spoken-language-identification.cc b/sherpa-onnx/jni/spoken-language-identification.cc
+index 4003a061..80b2ce55 100644
+--- a/sherpa-onnx/jni/spoken-language-identification.cc
++++ b/sherpa-onnx/jni/spoken-language-identification.cc
+@@ -4,6 +4,9 @@
+ 
+ #include "sherpa-onnx/csrc/spoken-language-identification.h"
+ 
++#include <memory>
++#include <string>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/jni/common.h"
+ 
+diff --git a/sherpa-onnx/jni/wave-reader.cc b/sherpa-onnx/jni/wave-reader.cc
+index ebed51b0..a6c31a8a 100644
+--- a/sherpa-onnx/jni/wave-reader.cc
++++ b/sherpa-onnx/jni/wave-reader.cc
+@@ -4,6 +4,7 @@
+ #include "sherpa-onnx/csrc/wave-reader.h"
+ 
+ #include <fstream>
++#include <vector>
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
+diff --git a/sherpa-onnx/python/csrc/faked-alsa.cc b/sherpa-onnx/python/csrc/faked-alsa.cc
+index 26ce28ff..765c81b5 100644
+--- a/sherpa-onnx/python/csrc/faked-alsa.cc
++++ b/sherpa-onnx/python/csrc/faked-alsa.cc
+@@ -2,6 +2,8 @@
+ //
+ // Copyright (c)  2024  Xiaomi Corporation
+ 
++#include <vector>
++
+ #include "sherpa-onnx/csrc/macros.h"
+ #include "sherpa-onnx/python/csrc/alsa.h"
+ 
+
+commit bf2924489f83553fa7880b8d8f16f2374cf4b4c4
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Wed Nov 12 13:58:38 2025 +0800
+
+    Add QnnConfig. (#2768)
+
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 577048ec..10e90fd8 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -160,6 +160,10 @@ list(APPEND sources
+   offline-zipformer-audio-tagging-model.cc
+ )
+ 
++list(APPEND sources
++  qnn-config.cc
++)
++
+ # punctuation
+ list(APPEND sources
+   offline-ct-transformer-model.cc
+diff --git a/sherpa-onnx/csrc/offline-recognizer.cc b/sherpa-onnx/csrc/offline-recognizer.cc
+index 44cc3848..1d2271f2 100644
+--- a/sherpa-onnx/csrc/offline-recognizer.cc
++++ b/sherpa-onnx/csrc/offline-recognizer.cc
+@@ -136,6 +136,11 @@ std::string OfflineRecognizerConfig::ToString() const {
+   os << "model_config=" << model_config.ToString() << ", ";
+   os << "lm_config=" << lm_config.ToString() << ", ";
+   os << "ctc_fst_decoder_config=" << ctc_fst_decoder_config.ToString() << ", ";
++
++  if (!qnn_config.backend_lib.empty()) {
++    os << "qnn_config=" << qnn_config.ToString() << ", ";
++  }
++
+   os << "decoding_method=\"" << decoding_method << "\", ";
+   os << "max_active_paths=" << max_active_paths << ", ";
+   os << "hotwords_file=\"" << hotwords_file << "\", ";
+diff --git a/sherpa-onnx/csrc/offline-recognizer.h b/sherpa-onnx/csrc/offline-recognizer.h
+index 1fcc1016..ae2c23a4 100644
+--- a/sherpa-onnx/csrc/offline-recognizer.h
++++ b/sherpa-onnx/csrc/offline-recognizer.h
+@@ -17,6 +17,7 @@
+ #include "sherpa-onnx/csrc/offline-stream.h"
+ #include "sherpa-onnx/csrc/offline-transducer-model-config.h"
+ #include "sherpa-onnx/csrc/parse-options.h"
++#include "sherpa-onnx/csrc/qnn-config.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -27,6 +28,7 @@ struct OfflineRecognizerConfig {
+   OfflineModelConfig model_config;
+   OfflineLMConfig lm_config;
+   OfflineCtcFstDecoderConfig ctc_fst_decoder_config;
++  QnnConfig qnn_config;
+ 
+   std::string decoding_method = "greedy_search";
+   int32_t max_active_paths = 4;
+diff --git a/sherpa-onnx/csrc/qnn-config.cc b/sherpa-onnx/csrc/qnn-config.cc
+new file mode 100644
+index 00000000..82155a6b
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn-config.cc
+@@ -0,0 +1,74 @@
++// sherpa-onnx/csrc/qnn-config.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn-config.h"
++
++#include <sstream>
++
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++namespace sherpa_onnx {
++
++void QnnConfig::Register(ParseOptions *po) {
++  po->Register("qnn-backend-lib", &backend_lib,
++               "Path to libQnnHtp.so "
++               "Used only when provider is qnn."
++               "Leave it empty if you don't use qnn");
++
++  po->Register(
++      "qnn-context-binary", &context_binary,
++      "Path to model.bin. Used only when provider is qnn."
++      "If it exists, libmodel.so is ignored."
++      "If it does not exist, Context binary is saved to this path so that "
++      "it is loaded the next time you run it. You can leave it empty if you "
++      "don't use qnn");
++
++  po->Register("qnn-system-lib", &system_lib,
++               "Required and used only when --qnn-context-binary is not empty "
++               "and exists. You can leave it empty if you don't use qnn.");
++}
++
++bool QnnConfig::Validate() const {
++  if (backend_lib.empty()) {
++    SHERPA_ONNX_LOGE("Please provide path to libQnnHtp.so if you use qnn");
++    return false;
++  }
++
++  if (!FileExists(backend_lib)) {
++    SHERPA_ONNX_LOGE("--qnn-backend-lib: '%s' does not exist",
++                     backend_lib.c_str());
++    return false;
++  }
++
++  if (!context_binary.empty() && FileExists(context_binary)) {
++    if (system_lib.empty()) {
++      SHERPA_ONNX_LOGE(
++          "Please provide --qnn-system-lib when you provide "
++          "--qnn-context-binary");
++      return false;
++    }
++
++    if (!FileExists(system_lib)) {
++      SHERPA_ONNX_LOGE("--qnn-system-lib: '%s' does not exist",
++                       system_lib.c_str());
++      return false;
++    }
++  }
++
++  return true;
++}
++
++std::string QnnConfig::ToString() const {
++  std::ostringstream os;
++
++  os << "QnnConfig(";
++  os << "backend_lib=\"" << backend_lib << "\", ";
++  os << "context_binary=\"" << context_binary << "\", ";
++  os << "system_lib=\"" << system_lib << "\")";
++
++  return os.str();
++}
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn-config.h b/sherpa-onnx/csrc/qnn-config.h
+new file mode 100644
+index 00000000..01db111e
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn-config.h
+@@ -0,0 +1,38 @@
++// sherpa-onnx/csrc/qnn-config.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_QNN_CONFIG_H_
++#define SHERPA_ONNX_CSRC_QNN_CONFIG_H_
++
++#include <string>
++
++#include "sherpa-onnx/csrc/parse-options.h"
++
++namespace sherpa_onnx {
++
++struct QnnConfig {
++  // Path to the backend library, e.g.,
++  // /some/path/to/libQnnHtp.so
++  std::string backend_lib;
++
++  // If it exists, you need to also provide system_lib.
++  // In this case, the model lib, i.e., libmodel.so, is ignored
++  //
++  // If it does not exist and if the user want to save the context binary,
++  // it will save it to this path.
++  std::string context_binary;
++
++  // Required and used only when context_binary exists
++  // Example value: /some/path/to/libQnnSystem.so
++  std::string system_lib;
++
++  std::string ToString() const;
++
++  void Register(ParseOptions *po);
++
++  bool Validate() const;
++};
++
++}  // namespace sherpa_onnx
++#endif  // SHERPA_ONNX_CSRC_QNN_CONFIG_H_
+
+commit 8d09756cc6bc67aced7cecbdf6a851d2ad4c0cf1
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 11 19:11:28 2025 +0800
+
+    Begin to add qnn C API (#2766)
+
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 6c1564be..92a3ff01 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -61,6 +61,7 @@ option(SHERPA_ONNX_ENABLE_SANITIZER "Whether to enable ubsan and asan" OFF)
+ option(SHERPA_ONNX_BUILD_C_API_EXAMPLES "Whether to enable C API examples" ${SUGGEST_BUILD_BINARIES})
+ option(SHERPA_ONNX_ENABLE_RKNN "Whether to build for RKNN NPU " OFF)
+ option(SHERPA_ONNX_ENABLE_ASCEND_NPU "Whether to build for Ascend NPU " OFF)
++option(SHERPA_ONNX_ENABLE_QNN "Whether to build for Qualcomm NPU" OFF)
+ 
+ set(SHERPA_ONNX_LINUX_ARM64_GPU_ONNXRUNTIME_VERSION "1.11.0" CACHE STRING "Used only for Linux ARM64 GPU. Set to 1.11.0 if you use CUDA 10.2 and cudnn8. Set it to 1.16.0 if you use CUDA 11.4 and cudnn8. Set it to 1.18.0 if you use CUDA 12.2 and cudnn8. Set it to 1.18.1 if you use CUDA 12.6 and cudnn9")
+ 
+@@ -179,6 +180,7 @@ message(STATUS "SHERPA_ONNX_ENABLE_SANITIZER: ${SHERPA_ONNX_ENABLE_SANITIZER}")
+ message(STATUS "SHERPA_ONNX_BUILD_C_API_EXAMPLES: ${SHERPA_ONNX_BUILD_C_API_EXAMPLES}")
+ message(STATUS "SHERPA_ONNX_ENABLE_RKNN: ${SHERPA_ONNX_ENABLE_RKNN}")
+ message(STATUS "SHERPA_ONNX_ENABLE_ASCEND_NPU: ${SHERPA_ONNX_ENABLE_ASCEND_NPU}")
++message(STATUS "SHERPA_ONNX_ENABLE_QNN: ${SHERPA_ONNX_ENABLE_QNN}")
+ message(STATUS "SHERPA_ONNX_LINK_D3D: ${SHERPA_ONNX_LINK_D3D}")
+ 
+ if(BUILD_SHARED_LIBS OR SHERPA_ONNX_ENABLE_JNI)
+@@ -337,6 +339,44 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+   message(STATUS "Build with Ascend NPU")
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_QNN)
++  if(NOT DEFINED ENV{QNN_SDK_ROOT})
++      message(FATAL_ERROR "\
++      Please specify the installation directory of the QNN SDK toolkit.
++      For instance, if it is installed in
++
++        /mnt/sdb/open-source/qairt/2.33.0.250327
++
++      You can run
++
++        source /mnt/sdb/open-source/qairt/2.33.0.250327/bin/envsetup.sh
++
++      which will give you the following output
++
++      [INFO] AISW SDK environment set
++      [INFO] QNN_SDK_ROOT: /mnt/sdb/open-source/qairt/2.33.0.250327
++      [INFO] SNPE_ROOT: /mnt/sdb/open-source/qairt/2.33.0.250327
++
++      Then run
++
++        echo $QNN_SDK_ROOT
++
++      It should print:
++
++        /mnt/sdb/open-source/qairt/2.33.0.250327
++
++      You can choose a version of QNN SDK by yourself. You don't need
++      to use 2.33.0.250327
++      ")
++  endif()
++
++  set(QNN_SDK_ROOT $ENV{QNN_SDK_ROOT})
++
++  if(NOT EXISTS ${QNN_SDK_ROOT}/include/QNN/QnnInterface.h)
++    message(FATAL_ERROR "${QNN_SDK_ROOT}/include/QNN/QnnInterface.h does not exist")
++  endif()
++endif()
++
+ if(UNIX AND NOT APPLE AND NOT SHERPA_ONNX_ENABLE_WASM AND NOT CMAKE_SYSTEM_NAME STREQUAL Android AND NOT CMAKE_SYSTEM_NAME STREQUAL OHOS)
+   check_include_file_cxx(alsa/asoundlib.h SHERPA_ONNX_HAS_ALSA)
+   if(SHERPA_ONNX_HAS_ALSA)
+diff --git a/build-android-arm64-v8a.sh b/build-android-arm64-v8a.sh
+index 7de2e681..1491c8ba 100755
+--- a/build-android-arm64-v8a.sh
++++ b/build-android-arm64-v8a.sh
+@@ -101,6 +101,10 @@ if [ -z $SHERPA_ONNX_ENABLE_RKNN ]; then
+   SHERPA_ONNX_ENABLE_RKNN=OFF
+ fi
+ 
++if [ -z $SHERPA_ONNX_ENABLE_QNN ]; then
++  SHERPA_ONNX_ENABLE_QNN=OFF
++fi
++
+ if [ $SHERPA_ONNX_ENABLE_RKNN == ON ]; then
+   rknn_version=2.2.0
+   if [ ! -d ./librknnrt-android ]; then
+@@ -130,6 +134,10 @@ if [ -z $SHERPA_ONNX_ENABLE_C_API ]; then
+   SHERPA_ONNX_ENABLE_C_API=OFF
+ fi
+ 
++if [ -z $SHERPA_ONNX_ANDROID_PLATFORM ]; then
++  SHERPA_ONNX_ANDROID_PLATFORM=android-21
++fi
++
+ if [ -z $SHERPA_ONNX_ENABLE_JNI ]; then
+   SHERPA_ONNX_ENABLE_JNI=ON
+ fi
+@@ -153,8 +161,9 @@ cmake -DCMAKE_TOOLCHAIN_FILE="$ANDROID_NDK/build/cmake/android.toolchain.cmake"
+     -DSHERPA_ONNX_ENABLE_C_API=$SHERPA_ONNX_ENABLE_C_API \
+     -DCMAKE_INSTALL_PREFIX=./install \
+     -DSHERPA_ONNX_ENABLE_RKNN=$SHERPA_ONNX_ENABLE_RKNN \
++    -DSHERPA_ONNX_ENABLE_QNN=$SHERPA_ONNX_ENABLE_QNN \
+     -DANDROID_ABI="arm64-v8a" \
+-    -DANDROID_PLATFORM=android-21 ..
++    -DANDROID_PLATFORM=$SHERPA_ONNX_ANDROID_PLATFORM ..
+ 
+     # By default, it links to libc++_static.a
+     # -DANDROID_STL=c++_shared \
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index d44e8e8a..577048ec 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -200,6 +200,14 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+   )
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_QNN)
++  list(APPEND sources
++    ./qnn/qnn-backend.cc
++    ./qnn/qnn-model.cc
++    ./qnn/utils.cc
++  )
++endif()
++
+ if(SHERPA_ONNX_ENABLE_TTS)
+   list(APPEND sources
+     character-lexicon.cc
+@@ -314,6 +322,10 @@ if(SHERPA_ONNX_ENABLE_ASCEND_NPU)
+     )
+ endif()
+ 
++if(SHERPA_ONNX_ENABLE_QNN)
++  target_include_directories(sherpa-onnx-core PRIVATE ${QNN_SDK_ROOT}/include/QNN)
++endif()
++
+ if(TARGET onnxruntime)
+   target_link_libraries(sherpa-onnx-core onnxruntime)
+ else()
+diff --git a/sherpa-onnx/csrc/qnn/macros.h b/sherpa-onnx/csrc/qnn/macros.h
+new file mode 100644
+index 00000000..58c65cf3
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/macros.h
+@@ -0,0 +1,19 @@
++// sherpa-onnx/csrc/qnn/macros.h
++//
++// Copyright      2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_QNN_MACROS_H_
++#define SHERPA_ONNX_CSRC_QNN_MACROS_H_
++
++#include "sherpa-onnx/csrc/macros.h"
++
++#define SHERPA_ONNX_QNN_CHECK(ret, msg, ...)                             \
++  do {                                                                   \
++    if (ret != QNN_SUCCESS) {                                            \
++      SHERPA_ONNX_LOGE("Return code is: %d", static_cast<int32_t>(ret)); \
++      SHERPA_ONNX_LOGE(msg, ##__VA_ARGS__);                              \
++      SHERPA_ONNX_EXIT(-1);                                              \
++    }                                                                    \
++  } while (0)
++
++#endif  // SHERPA_ONNX_CSRC_QNN_MACROS_H_
+diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.cc b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+new file mode 100644
+index 00000000..8ed9c281
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/qnn-backend.cc
+@@ -0,0 +1,251 @@
++// sherpa-onnx/csrc/qnn/qnn-backend.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
++
++#include <dlfcn.h>
++#include <stdio.h>
++
++#include <cstdint>
++#include <sstream>
++#include <string>
++#include <vector>
++
++#include "QnnInterface.h"
++#include "System/QnnSystemInterface.h"
++#include "sherpa-onnx/csrc/qnn/macros.h"
++#include "sherpa-onnx/csrc/qnn/utils.h"
++
++namespace sherpa_onnx {
++
++class QnnBackend::Impl {
++ public:
++  explicit Impl(const std::string &backend_lib) {
++    bool ok = InitQnnInterface(backend_lib);
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to init qnn interface from '%s'",
++                       backend_lib.c_str());
++      return;
++    }
++
++    InitLog();
++    InitBackend();
++    InitDevice();
++
++    is_initialized_ = true;
++  }
++
++  ~Impl() {
++    if (context_handle_) {
++      auto ret = qnn_interface_.contextFree(context_handle_, nullptr);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextFree");
++    }
++
++    if (device_handle_) {
++      auto ret = qnn_interface_.deviceFree(device_handle_);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call deviceFree");
++    }
++
++    if (backend_handle_) {
++      auto ret = qnn_interface_.backendFree(backend_handle_);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendFree");
++    }
++
++    if (log_handle_) {
++      auto ret = qnn_interface_.logFree(log_handle_);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call logFree");
++    }
++  }
++
++  void InitContext() {
++    if (context_handle_) {
++      SHERPA_ONNX_LOGE("context handle is already initialized");
++      return;
++    }
++
++    auto ret = qnn_interface_.contextCreate(backend_handle_, device_handle_,
++                                            context_config_, &context_handle_);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextCreate");
++  }
++
++  void InitContext(Qnn_ContextHandle_t t) { context_handle_ = t; }
++
++  Qnn_LogHandle_t LogHandle() const { return log_handle_; }
++
++  Qnn_BackendHandle_t BackendHandle() const { return backend_handle_; }
++
++  Qnn_DeviceHandle_t DeviceHandle() const { return device_handle_; }
++
++  Qnn_ContextHandle_t ContextHandle() const { return context_handle_; }
++
++  QNN_INTERFACE_VER_TYPE QnnInterface() const { return qnn_interface_; }
++
++  QnnLog_Level_t LogLevel() const { return log_level_; }
++
++  bool IsInitialized() const { return is_initialized_; }
++
++ private:
++  bool InitQnnInterface(const std::string &backend_lib) {
++    backend_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
++        dlopen(backend_lib.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
++    if (!backend_lib_handle_) {
++      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
++                       backend_lib.c_str(), dlerror());
++      return false;
++    }
++    SHERPA_ONNX_LOGE("loaded %s", backend_lib.c_str());
++
++    const char *symbol = "QnnInterface_getProviders";
++    auto get_interface_providers =
++        reinterpret_cast<QnnInterfaceGetProvidersFnType>(
++            dlsym(backend_lib_handle_.get(), symbol));
++    if (!get_interface_providers) {
++      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
++                       dlerror());
++      return false;
++    }
++    SHERPA_ONNX_LOGE("Got %s", symbol);
++
++    const QnnInterface_t **interface_providers = nullptr;
++    uint32_t num_providers = 0;
++
++    auto ret = get_interface_providers(&interface_providers, &num_providers);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call get_interface_providers");
++
++    if (!interface_providers) {
++      SHERPA_ONNX_LOGE("interface_providers is nullptr");
++      return false;
++    }
++
++    if (num_providers == 0) {
++      SHERPA_ONNX_LOGE("Number of providers is 0");
++      return false;
++    }
++
++    bool found_valid_interface = false;
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("QNN_API_VERSION_MAJOR: %d", QNN_API_VERSION_MAJOR);
++      SHERPA_ONNX_LOGE("QNN_API_VERSION_MINOR: %d", QNN_API_VERSION_MINOR);
++      SHERPA_ONNX_LOGE("QNN_API_VERSION_PATCH: %d", QNN_API_VERSION_PATCH);
++    }
++
++    for (size_t idx = 0; idx < num_providers; ++idx) {
++      auto p = interface_providers[idx];
++
++      if (debug_) {
++        std::ostringstream os;
++        os << "---" << idx << "----\n";
++        os << "backendId: " << p->backendId << "\n";
++        os << "coreApiVersion.major: " << p->apiVersion.coreApiVersion.major
++           << "\n";
++        os << "coreApiVersion.minor: " << p->apiVersion.coreApiVersion.minor
++           << "\n";
++        os << "coreApiVersion.patch: " << p->apiVersion.coreApiVersion.patch
++           << "\n";
++
++        os << "backendApiVersion.major: "
++           << p->apiVersion.backendApiVersion.major << "\n";
++        os << "backendApiVersion.minor: "
++           << p->apiVersion.backendApiVersion.minor << "\n";
++        os << "backendApiVersion.patch: "
++           << p->apiVersion.backendApiVersion.patch << "\n";
++        SHERPA_ONNX_LOGE("%s", os.str().c_str());
++      }
++
++      qnn_interface_ = p->QNN_INTERFACE_VER_NAME;
++      found_valid_interface = true;
++      break;
++    }
++
++    if (!found_valid_interface) {
++      SHERPA_ONNX_LOGE("Failed to find valid interface");
++      return false;
++    }
++
++    if (debug_) {
++      const char *build_id = nullptr;
++      ret = qnn_interface_.backendGetBuildId(&build_id);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendGetBuildId()");
++
++      SHERPA_ONNX_LOGE("backend build ID: %s", build_id);
++    }
++
++    return true;
++  }
++
++  void InitLog() {
++    auto ret = qnn_interface_.logCreate(LogCallback, log_level_, &log_handle_);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call logCreate");
++  }
++
++  void InitBackend() {
++    auto ret = qnn_interface_.backendCreate(log_handle_, backend_config_,
++                                            &backend_handle_);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call backendCreate");
++  }
++
++  void InitDevice() {
++    auto ret =
++        qnn_interface_.deviceCreate(log_handle_, nullptr, &device_handle_);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call deviceCreate");
++  }
++
++ private:
++  bool debug_ = true;
++  std::unique_ptr<void, decltype(&dlclose)> backend_lib_handle_{nullptr,
++                                                                &dlclose};
++
++  QNN_INTERFACE_VER_TYPE qnn_interface_;
++
++  QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_WARN;
++  // QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_INFO;
++  // QnnLog_Level_t log_level_ = QNN_LOG_LEVEL_VERBOSE;
++
++  Qnn_LogHandle_t log_handle_ = nullptr;
++
++  const QnnBackend_Config_t **backend_config_ = nullptr;
++  Qnn_BackendHandle_t backend_handle_ = nullptr;
++
++  Qnn_DeviceHandle_t device_handle_ = nullptr;
++
++  Qnn_ContextHandle_t context_handle_ = nullptr;
++  const QnnContext_Config_t **context_config_ = nullptr;
++  bool is_initialized_ = false;
++};
++
++QnnBackend::~QnnBackend() = default;
++
++QnnBackend::QnnBackend(const std::string &backend_lib)
++    : impl_(std::make_unique<Impl>(backend_lib)) {}
++
++void QnnBackend::InitContext() const { impl_->InitContext(); }
++
++void QnnBackend::InitContext(Qnn_ContextHandle_t context_handle) const {
++  impl_->InitContext(context_handle);
++}
++
++Qnn_LogHandle_t QnnBackend::LogHandle() const { return impl_->LogHandle(); }
++
++Qnn_BackendHandle_t QnnBackend::BackendHandle() const {
++  return impl_->BackendHandle();
++}
++
++Qnn_DeviceHandle_t QnnBackend::DeviceHandle() const {
++  return impl_->DeviceHandle();
++}
++
++Qnn_ContextHandle_t QnnBackend::ContextHandle() const {
++  return impl_->ContextHandle();
++}
++
++QNN_INTERFACE_VER_TYPE QnnBackend::QnnInterface() const {
++  return impl_->QnnInterface();
++}
++
++QnnLog_Level_t QnnBackend::LogLevel() const { return impl_->LogLevel(); }
++
++bool QnnBackend::IsInitialized() const { return impl_->IsInitialized(); }
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn/qnn-backend.h b/sherpa-onnx/csrc/qnn/qnn-backend.h
+new file mode 100644
+index 00000000..62049a69
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/qnn-backend.h
+@@ -0,0 +1,36 @@
++// sherpa-onnx/csrc/qnn/qnn-backend.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
++#define SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
++
++#include <memory>
++#include <string>
++
++#include "QnnInterface.h"
++
++namespace sherpa_onnx {
++
++class QnnBackend {
++ public:
++  explicit QnnBackend(const std::string &backend_lib);
++  ~QnnBackend();
++
++  void InitContext() const;
++  void InitContext(Qnn_ContextHandle_t context_handle) const;
++  Qnn_LogHandle_t LogHandle() const;
++  Qnn_BackendHandle_t BackendHandle() const;
++  Qnn_DeviceHandle_t DeviceHandle() const;
++  Qnn_ContextHandle_t ContextHandle() const;
++  QNN_INTERFACE_VER_TYPE QnnInterface() const;
++  QnnLog_Level_t LogLevel() const;
++  bool IsInitialized() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_QNN_BACKEND_H_
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.cc b/sherpa-onnx/csrc/qnn/qnn-model.cc
+new file mode 100644
+index 00000000..d3c53788
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/qnn-model.cc
+@@ -0,0 +1,665 @@
++// sherpa-onnx/csrc/qnn/qnn-model.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/qnn/qnn-model.h"
++
++#include <dlfcn.h>
++
++#include <fstream>
++#include <memory>
++#include <string>
++#include <unordered_map>
++#include <utility>
++#include <vector>
++
++#include "sherpa-onnx/csrc/qnn/macros.h"
++#include "sherpa-onnx/csrc/qnn/qnn-backend.h"
++#include "sherpa-onnx/csrc/qnn/utils.h"
++
++namespace sherpa_onnx {
++
++class QnnModel::Impl {
++ public:
++  Impl(const std::string &model_so, const QnnBackend *backend)
++      : backend_(backend) {
++    bool ok = InitModel(model_so);
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to load '%s'", model_so.c_str());
++      return;
++    }
++
++    ok = InitSymbols();
++    if (!ok) {
++      SHERPA_ONNX_LOGE("Failed to get model symbols from '%s'",
++                       model_so.c_str());
++      return;
++    }
++
++    InitGraph();
++
++    PostInit();
++  }
++
++  Impl(const std::string &binary_context_file, const std::string &system_lib,
++       const QnnBackend *backend, BinaryContextTag)
++      : backend_(backend) {
++    bool ok = LoadSystemLib(binary_context_file, system_lib);
++    if (!ok) {
++      return;
++    }
++
++    PostInit();
++  }
++
++  bool LoadSystemLib(const std::string &binary_context_file,
++                     const std::string &system_lib) {
++    system_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
++        dlopen(system_lib.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
++    if (!system_lib_handle_) {
++      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
++                       system_lib.c_str(), dlerror());
++      return false;
++    }
++    SHERPA_ONNX_LOGE("loaded %s", system_lib.c_str());
++
++    auto get_system_interface_providers =
++        reinterpret_cast<QnnSystemInterfaceGetProvidersFnType>(
++            dlsym(system_lib_handle_.get(), "QnnSystemInterface_getProviders"));
++
++    if (!get_system_interface_providers) {
++      SHERPA_ONNX_LOGE("Failed to get QnnSystemInterface_getProviders");
++      return false;
++    }
++
++    const QnnSystemInterface_t **system_interface_providers = nullptr;
++    uint32_t num_providers = 0;
++    if (get_system_interface_providers(&system_interface_providers,
++                                       &num_providers) != QNN_SUCCESS) {
++      SHERPA_ONNX_LOGE("Failed to get system interface providers.");
++      return false;
++    }
++
++    if (!system_interface_providers) {
++      SHERPA_ONNX_LOGE(
++          "Failed to get system interface providers: null "
++          "interface providers received.");
++      return false;
++    }
++
++    if (!num_providers) {
++      SHERPA_ONNX_LOGE(
++          "Failed to get interface providers: 0 interface providers.");
++      return false;
++    }
++
++    for (uint32_t i = 0; i < num_providers; ++i) {
++      if (debug_) {
++        SHERPA_ONNX_LOGE("QNN_SYSTEM_API_VERSION_MAJOR: %d",
++                         static_cast<int32_t>(QNN_SYSTEM_API_VERSION_MAJOR));
++        SHERPA_ONNX_LOGE("QNN_SYSTEM_API_VERSION_MINOR: %d",
++                         static_cast<int32_t>(QNN_SYSTEM_API_VERSION_MINOR));
++        SHERPA_ONNX_LOGE(
++            "systemApiVersion.major: %d",
++            static_cast<int32_t>(
++                system_interface_providers[i]->systemApiVersion.major));
++        SHERPA_ONNX_LOGE(
++            "systemApiVersion.minor: %d",
++            static_cast<int32_t>(
++                system_interface_providers[i]->systemApiVersion.minor));
++      }
++
++      qnn_system_interface_ =
++          system_interface_providers[i]->QNN_SYSTEM_INTERFACE_VER_NAME;
++    }
++
++    // read file into a buffer
++    std::vector<uint8_t> buffer = ReadFile<uint8_t>(binary_context_file);
++
++    QnnSystemContext_Handle_t sys_ctx_handle = nullptr;
++    if (qnn_system_interface_.systemContextCreate(&sys_ctx_handle) !=
++        QNN_SUCCESS) {
++      SHERPA_ONNX_LOGE("Could not create system handle.");
++      return false;
++    }
++
++    const QnnSystemContext_BinaryInfo_t *binary_info = nullptr;
++    Qnn_ContextBinarySize_t binary_info_size = 0;
++
++    auto ret = qnn_system_interface_.systemContextGetBinaryInfo(
++        sys_ctx_handle, static_cast<void *>(buffer.data()), buffer.size(),
++        &binary_info, &binary_info_size);
++    if (ret != QNN_SUCCESS) {
++      SHERPA_ONNX_LOGE("Failed to get context binary info from '%s'",
++                       binary_context_file.c_str());
++
++      qnn_system_interface_.systemContextFree(sys_ctx_handle);
++      return false;
++    }
++
++    const GraphConfigInfo **graph_configs_info = nullptr;
++
++    uint32_t graph_configs_info_count = 0;
++    GraphInfo **graphs_info = nullptr;
++    uint32_t graphs_count = 0;
++
++    if (!CopyMetadataToGraphsInfo(binary_info, graphs_info, graphs_count)) {
++      SHERPA_ONNX_LOGE("Failed to call CopyMetadataToGraphsInfo");
++
++      qnn_system_interface_.systemContextFree(sys_ctx_handle);
++      return false;
++    }
++
++    qnn_system_interface_.systemContextFree(sys_ctx_handle);
++
++    auto free_graphs_info = [&graphs_info, &graphs_count] {
++      for (uint32_t i = 0; i < graphs_count; ++i) {
++        for (uint32_t k = 0; k < graphs_info[i]->num_input_tensors; ++k) {
++          FreeTensor(&graphs_info[i]->input_tensors[k]);
++        }
++
++        for (uint32_t k = 0; k < graphs_info[i]->num_output_tensors; ++k) {
++          FreeTensor(&graphs_info[i]->output_tensors[k]);
++        }
++
++        free(graphs_info[i]->input_tensors);
++        free(graphs_info[i]->output_tensors);
++
++        free(graphs_info[i]->graph_name);
++      }
++
++      free(graphs_info[0]);
++      free(graphs_info);
++    };
++
++    if (graphs_count > 1) {
++      SHERPA_ONNX_LOGE("Only the first graph is used");
++    }
++
++    Qnn_ContextHandle_t context_handle = nullptr;
++
++    if (backend_->QnnInterface().contextCreateFromBinary(
++            backend_->BackendHandle(), backend_->DeviceHandle(),
++            context_config_, static_cast<void *>(buffer.data()), buffer.size(),
++            &context_handle, nullptr) != QNN_SUCCESS) {
++      free_graphs_info();
++      SHERPA_ONNX_LOGE("Could not create context from binary.");
++      return false;
++    }
++
++    backend_->InitContext(context_handle);
++
++    if (backend_->QnnInterface().graphRetrieve(
++            context_handle, (*graphs_info)[0].graph_name,
++            &((*graphs_info)[0].graph)) != QNN_SUCCESS) {
++      free_graphs_info();
++      SHERPA_ONNX_LOGE("Unable to retrieve graph handle for graph %d", 0);
++      return false;
++    }
++
++    graph_handle_ = (*graphs_info)[0].graph;
++
++    InitInputTensors((*graphs_info)[0]);
++    InitOutputTensors((*graphs_info)[0]);
++
++    free_graphs_info();
++
++    return true;
++  }
++
++  ~Impl() = default;
++
++  bool SaveBinaryContext(const std::string &filename) {
++    auto qnn_interface = backend_->QnnInterface();
++
++    if (!qnn_interface.contextGetBinarySize ||
++        !qnn_interface.contextGetBinary) {
++      SHERPA_ONNX_LOGE(
++          "contextGetBinarySizeFnHandle or "
++          "contextGetBinaryFnHandle is nullptr.");
++      return false;
++    }
++
++    uint64_t required_buffer_size{0};
++    auto ret = qnn_interface.contextGetBinarySize(backend_->ContextHandle(),
++                                                  &required_buffer_size);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextGetBinarySize");
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("context binary size: %.3f MB",
++                       static_cast<float>(required_buffer_size) / 1024 / 1024);
++    }
++    std::vector<uint8_t> saveBuffer(required_buffer_size);
++    uint64_t writtenBufferSize{0};
++
++    ret = qnn_interface.contextGetBinary(
++        backend_->ContextHandle(), reinterpret_cast<void *>(saveBuffer.data()),
++        required_buffer_size, &writtenBufferSize);
++
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call contextGetBinary");
++
++    if (required_buffer_size < writtenBufferSize) {
++      SHERPA_ONNX_LOGE(
++          "Illegal written buffer size %d bytes. Cannot exceed "
++          "allocated memory of %d bytes",
++          static_cast<int32_t>(writtenBufferSize),
++          static_cast<int32_t>(required_buffer_size));
++      return false;
++    }
++    std::ofstream ofs(filename, std::ios::binary | std::ios::trunc);
++    ofs.write(reinterpret_cast<const char *>(saveBuffer.data()),
++              saveBuffer.size());
++
++    if (!ofs) {
++      SHERPA_ONNX_LOGE("Failed to write '%s'", filename.c_str());
++      return false;
++    }
++
++    return true;
++  }
++
++  const std::vector<std::string> &InputTensorNames() const {
++    return input_tensor_names_;
++  }
++
++  const std::vector<std::string> &OutputTensorNames() const {
++    return output_tensor_names_;
++  }
++
++  std::vector<int32_t> TensorShape(const std::string &name) const {
++    std::vector<int32_t> shape;
++
++    if (!HasTensor(name)) {
++      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
++      return shape;
++    }
++
++    auto t = name2tensor_.at(name);
++
++    shape = {t->v1.dimensions, t->v1.dimensions + t->v1.rank};
++
++    return shape;
++  }
++
++  int32_t TensorSizeInBytes(const std::string &name) const {
++    if (!HasTensor(name)) {
++      return 0;
++    }
++
++    return name2tensor_.at(name)->v1.clientBuf.dataSize;
++  }
++
++  bool HasTensor(const std::string &name) const {
++    return name2tensor_.count(name);
++  }
++
++  bool SetInputTensorData(const std::string &name, const float *p, int32_t n) {
++    if (!HasTensor(name)) {
++      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
++      return false;
++    }
++
++    auto t = name2tensor_.at(name);
++    if (t->v1.dataType != QNN_DATATYPE_UFIXED_POINT_16) {
++      SHERPA_ONNX_LOGE(
++          "tensor '%s' should be of type "
++          "QNN_DATATYPE_UFIXED_POINT_16, but it is %s",
++          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
++      return false;
++    }
++
++    if (t->v1.quantizeParams.quantizationEncoding !=
++        QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
++      SHERPA_ONNX_LOGE(
++          "tensor '%s' should be quantized with "
++          "QNN_QUANTIZATION_ENCODING_SCALE_OFFSET, but it is %s",
++          name.c_str(),
++          QuantizationEncodingToString(
++              t->v1.quantizeParams.quantizationEncoding)
++              .c_str());
++      return false;
++    }
++
++    if (n * sizeof(uint16_t) != t->v1.clientBuf.dataSize) {
++      SHERPA_ONNX_LOGE("tensor '%s' expects %d bytes, but you provide %d bytes",
++                       name.c_str(),
++                       static_cast<int32_t>(t->v1.clientBuf.dataSize),
++                       static_cast<int32_t>(n * sizeof(uint16_t)));
++      return false;
++    }
++
++    FillData(t, p, n);
++    SHERPA_ONNX_LOGE("set %s", name.c_str());
++
++    return true;
++  }
++
++  bool SetInputTensorData(const std::string &name, const int32_t *p,
++                          int32_t n) {
++    if (!HasTensor(name)) {
++      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
++      return false;
++    }
++
++    auto t = name2tensor_.at(name);
++    if (t->v1.dataType != QNN_DATATYPE_INT_32) {
++      SHERPA_ONNX_LOGE(
++          "tensor '%s' should be of type "
++          "QNN_DATATYPE_INT_32, but it is %s",
++          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
++      return false;
++    }
++
++    if (n * sizeof(int32_t) != t->v1.clientBuf.dataSize) {
++      SHERPA_ONNX_LOGE("tensor '%s' expects %d bytes, but you provide %d bytes",
++                       name.c_str(),
++                       static_cast<int32_t>(t->v1.clientBuf.dataSize),
++                       static_cast<int32_t>(n * sizeof(int32_t)));
++      return false;
++    }
++
++    FillData(t, p, n);
++    SHERPA_ONNX_LOGE("set %s", name.c_str());
++
++    return true;
++  }
++
++  std::vector<float> GetOutputTensorData(const std::string &name) {
++    if (!HasTensor(name)) {
++      SHERPA_ONNX_LOGE("No such tensor '%s'", name.c_str());
++      return {};
++    }
++
++    auto t = name2tensor_.at(name);
++    if (t->v1.dataType != QNN_DATATYPE_UFIXED_POINT_16) {
++      SHERPA_ONNX_LOGE(
++          "tensor '%s' should be of type "
++          "QNN_DATATYPE_UFIXED_POINT_16, but it is %s",
++          name.c_str(), TensorDataTypeToString(t->v1.dataType).c_str());
++      return {};
++    }
++
++    if (t->v1.quantizeParams.quantizationEncoding !=
++        QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
++      SHERPA_ONNX_LOGE(
++          "tensor '%s' should be quantized with "
++          "QNN_QUANTIZATION_ENCODING_SCALE_OFFSET, but it is %s",
++          name.c_str(),
++          QuantizationEncodingToString(
++              t->v1.quantizeParams.quantizationEncoding)
++              .c_str());
++      return {};
++    }
++
++    int32_t n = t->v1.clientBuf.dataSize / sizeof(uint16_t);
++    std::vector<float> ans(n);
++
++    GetData(t, ans.data(), n);
++
++    return ans;
++  }
++
++  bool Run() {
++    std::vector<Qnn_Tensor_t> input_tensors_raw;
++    std::vector<Qnn_Tensor_t> output_tensors_raw;
++
++    input_tensors_raw.reserve(input_tensors_.size());
++    output_tensors_raw.reserve(output_tensors_.size());
++
++    for (const auto &p : input_tensors_) {
++      input_tensors_raw.push_back(*p);
++    }
++
++    for (const auto &p : output_tensors_) {
++      output_tensors_raw.push_back(*p);
++    }
++
++    auto ret = backend_->QnnInterface().graphExecute(
++        graph_handle_, input_tensors_raw.data(), input_tensors_raw.size(),
++        output_tensors_raw.data(), output_tensors_raw.size(), nullptr, nullptr);
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to run graphExecute");
++
++    return true;
++  }
++
++  bool IsInitialized() const { return is_initialized_; }
++
++ private:
++  void PostInit() {
++    AllocateBuffer();
++    SetupPointers();
++
++    is_initialized_ = true;
++  }
++
++  bool InitModel(const std::string &model_so) {
++    model_lib_handle_ = std::unique_ptr<void, decltype(&dlclose)>(
++        dlopen(model_so.c_str(), RTLD_NOW | RTLD_LOCAL), &dlclose);
++    if (!model_lib_handle_) {
++      SHERPA_ONNX_LOGE("Failed to dlopen '%s'. Error is: '%s'",
++                       model_so.c_str(), dlerror());
++      return false;
++    }
++    SHERPA_ONNX_LOGE("loaded %s", model_so.c_str());
++
++    return true;
++  }
++
++  bool InitSymbols() {
++    const char *symbol = "QnnModel_composeGraphs";
++
++    compose_graphs_fn_handle_ = reinterpret_cast<ComposeGraphsFnHandleType>(
++        dlsym(model_lib_handle_.get(), symbol));
++    if (!compose_graphs_fn_handle_) {
++      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
++                       dlerror());
++      return false;
++    }
++
++    symbol = "QnnModel_freeGraphsInfo";
++    free_graph_info_fn_handle_ = reinterpret_cast<FreeGraphInfoFnHandleType>(
++        dlsym(model_lib_handle_.get(), symbol));
++    if (!free_graph_info_fn_handle_) {
++      SHERPA_ONNX_LOGE("Failed to dlsym for '%s'. Error is: '%s'", symbol,
++                       dlerror());
++      return false;
++    }
++    return true;
++  }
++
++  void InitGraph() {
++    const GraphConfigInfo **graph_configs_info = nullptr;
++
++    uint32_t graph_configs_info_count = 0;
++    GraphInfo **graphs_info = nullptr;
++    uint32_t graphs_count = 0;
++
++    auto ret = compose_graphs_fn_handle_(
++        backend_->BackendHandle(), backend_->QnnInterface(),
++        backend_->ContextHandle(), graph_configs_info, graph_configs_info_count,
++        &graphs_info, &graphs_count, debug_, LogCallback, backend_->LogLevel());
++    SHERPA_ONNX_QNN_CHECK(ret, "Failed to call compose_graphs_fn_handle_");
++
++    SHERPA_ONNX_LOGE("graphs_count: %d", (int32_t)graphs_count);
++
++    for (uint32_t i = 0; i < graphs_count; ++i) {
++      if (debug_) {
++        SHERPA_ONNX_LOGE(
++            "Finalizing graph %d/%d: '%s'", static_cast<int32_t>(i),
++            static_cast<int32_t>(graphs_count), (*graphs_info)[i].graph_name);
++      }
++      ret = backend_->QnnInterface().graphFinalize((*graphs_info)[i].graph,
++                                                   nullptr, nullptr);
++      SHERPA_ONNX_QNN_CHECK(ret, "Failed to call graph_finalize");
++    }
++
++    if (graphs_count > 1) {
++      SHERPA_ONNX_LOGE("We only use the first graph: %s",
++                       (*graphs_info)[0].graph_name);
++    }
++
++    InitInputTensors((*graphs_info)[0]);
++    InitOutputTensors((*graphs_info)[0]);
++
++    graph_handle_ = (*graphs_info)[0].graph;
++  }
++
++  void InitInputTensors(GraphInfo graph) {
++    input_tensors_.reserve(graph.num_input_tensors);
++    input_tensor_names_.reserve(graph.num_input_tensors);
++
++    for (uint32_t i = 0; i < graph.num_input_tensors; ++i) {
++      SHERPA_ONNX_LOGE("input %d", (int)i);
++      auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
++
++      CopyTensorInfo(graph.input_tensors[i], *p);
++      PrintTensor(p->v2);
++
++      std::string name = p->v1.name;
++      name2tensor_[name] = p.get();
++      input_tensor_names_.push_back(std::move(name));
++
++      input_tensors_.push_back(std::move(p));
++    }
++  }
++
++  void InitOutputTensors(GraphInfo graph) {
++    output_tensors_.reserve(graph.num_output_tensors);
++    output_tensor_names_.reserve(graph.num_output_tensors);
++    for (uint32_t i = 0; i < graph.num_output_tensors; ++i) {
++      auto p = TensorPtr(new Qnn_Tensor_t(QNN_TENSOR_INIT), &FreeTensor);
++
++      CopyTensorInfo(graph.output_tensors[i], *p);
++
++      SHERPA_ONNX_LOGE("output %d", (int)i);
++      PrintTensor(p->v2);
++
++      std::string name = p->v1.name;
++      name2tensor_[name] = p.get();
++      output_tensor_names_.push_back(std::move(name));
++
++      output_tensors_.push_back(std::move(p));
++    }
++  }
++
++  void AllocateBuffer() {
++    uint32_t n = 0;
++    for (const auto &p : name2tensor_) {
++      n += p.second->v1.clientBuf.dataSize;
++    }
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("Allocate %d bytes, or %.3f MB", static_cast<int32_t>(n),
++                       static_cast<float>(n) / 1024 / 1024);
++    }
++
++    buffer_.resize(n);
++  }
++
++  void SetupPointers() {
++    uint8_t *p = buffer_.data();
++    uint32_t n = 0;
++    for (auto &t : input_tensors_) {
++      t->v1.clientBuf.data = p;
++      p += t->v1.clientBuf.dataSize;
++    }
++
++    for (auto &t : output_tensors_) {
++      t->v1.clientBuf.data = p;
++      p += t->v1.clientBuf.dataSize;
++    }
++
++    if (debug_) {
++      if (p == buffer_.data() + buffer_.size()) {
++        SHERPA_ONNX_LOGE("Setup pointers successfully.");
++      } else {
++        SHERPA_ONNX_LOGE("Bad things happened in setting up pointers.");
++      }
++    }
++  }
++
++ private:
++  bool debug_ = true;
++  std::unique_ptr<void, decltype(&dlclose)> model_lib_handle_{nullptr,
++                                                              &dlclose};
++
++  std::unique_ptr<void, decltype(&dlclose)> system_lib_handle_{nullptr,
++                                                               &dlclose};
++
++  QNN_SYSTEM_INTERFACE_VER_TYPE qnn_system_interface_;
++
++  ComposeGraphsFnHandleType compose_graphs_fn_handle_ = nullptr;
++  FreeGraphInfoFnHandleType free_graph_info_fn_handle_ = nullptr;
++
++  std::vector<TensorPtr> input_tensors_;
++  std::vector<TensorPtr> output_tensors_;
++
++  std::vector<std::string> input_tensor_names_;
++  std::vector<std::string> output_tensor_names_;
++
++  std::unordered_map<std::string, Qnn_Tensor_t *> name2tensor_;
++
++  std::vector<uint8_t> buffer_;
++  const QnnBackend *backend_ = nullptr;
++
++  Qnn_GraphHandle_t graph_handle_ = nullptr;
++
++  const QnnContext_Config_t **context_config_ = nullptr;
++  bool is_initialized_ = false;
++};
++
++QnnModel::~QnnModel() = default;
++
++QnnModel::QnnModel(const std::string &model_so, const QnnBackend *backend)
++    : impl_(std::make_unique<Impl>(model_so, backend)) {}
++
++QnnModel::QnnModel(const std::string &binary_context_file,
++                   const std::string &system_lib, const QnnBackend *backend,
++                   BinaryContextTag tag)
++    : impl_(std::make_unique<Impl>(binary_context_file, system_lib, backend,
++                                   tag)) {}
++
++bool QnnModel::SaveBinaryContext(const std::string &filename) const {
++  return impl_->SaveBinaryContext(filename);
++}
++
++const std::vector<std::string> &QnnModel::InputTensorNames() const {
++  return impl_->InputTensorNames();
++}
++
++const std::vector<std::string> &QnnModel::OutputTensorNames() const {
++  return impl_->OutputTensorNames();
++}
++
++std::vector<int32_t> QnnModel::TensorShape(const std::string &name) const {
++  return impl_->TensorShape(name);
++}
++
++int32_t QnnModel::TensorSizeInBytes(const std::string &name) const {
++  return impl_->TensorSizeInBytes(name);
++}
++
++bool QnnModel::HasTensor(const std::string &name) const {
++  return impl_->HasTensor(name);
++}
++
++bool QnnModel::SetInputTensorData(const std::string &name, const float *p,
++                                  int32_t n) const {
++  return impl_->SetInputTensorData(name, p, n);
++}
++
++bool QnnModel::SetInputTensorData(const std::string &name, const int32_t *p,
++                                  int32_t n) const {
++  return impl_->SetInputTensorData(name, p, n);
++}
++
++std::vector<float> QnnModel::GetOutputTensorData(
++    const std::string &name) const {
++  return impl_->GetOutputTensorData(name);
++}
++
++bool QnnModel::Run() const { return impl_->Run(); }
++
++bool QnnModel::IsInitialized() const { return impl_->IsInitialized(); }
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/qnn/qnn-model.h b/sherpa-onnx/csrc/qnn/qnn-model.h
+new file mode 100644
+index 00000000..d4cbfa3d
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/qnn-model.h
+@@ -0,0 +1,55 @@
++// sherpa-onnx/csrc/qnn/qnn-model.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
++#define SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
++
++#include <memory>
++#include <string>
++#include <vector>
++
++#include "QnnInterface.h"
++
++namespace sherpa_onnx {
++
++class QnnBackend;
++
++struct BinaryContextTag {};
++
++class QnnModel {
++ public:
++  QnnModel(const std::string &model_so, const QnnBackend *backend);
++  QnnModel(const std::string &binary_context_file,
++           const std::string &system_lib, const QnnBackend *backend,
++           BinaryContextTag tag);
++  ~QnnModel();
++
++  bool SaveBinaryContext(const std::string &filename) const;
++
++  const std::vector<std::string> &InputTensorNames() const;
++  const std::vector<std::string> &OutputTensorNames() const;
++
++  std::vector<int32_t> TensorShape(const std::string &name) const;
++  int32_t TensorSizeInBytes(const std::string &name) const;
++
++  bool HasTensor(const std::string &name) const;
++
++  bool SetInputTensorData(const std::string &name, const float *p,
++                          int32_t n) const;
++
++  bool SetInputTensorData(const std::string &name, const int32_t *p,
++                          int32_t n) const;
++
++  std::vector<float> GetOutputTensorData(const std::string &name) const;
++
++  bool Run() const;
++  bool IsInitialized() const;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_QNN_QNN_MODEL_H_
+diff --git a/sherpa-onnx/csrc/qnn/utils.cc b/sherpa-onnx/csrc/qnn/utils.cc
+new file mode 100644
+index 00000000..20c498f7
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/utils.cc
+@@ -0,0 +1,546 @@
++// sherpa-onnx/csrc/qnn/utils.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#include "sherpa-onnx/csrc/qnn/utils.h"
++
++#include <math.h>
++#include <stdio.h>
++
++#include <algorithm>
++#include <functional>
++#include <numeric>
++#include <sstream>
++#include <string>
++
++#include "sherpa-onnx/csrc/qnn/macros.h"
++
++#define SHERPA_ONNX_TO_STRING(s) \
++  case s:                        \
++    return #s
++
++std::string TensorTypeToString(Qnn_TensorType_t t) {
++  switch (t) {
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_WRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_READ);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_APP_READWRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_NATIVE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_STATIC);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_NULL);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_STATIC);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_NATIVE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_WRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_READ);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UPDATEABLE_APP_READWRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_WRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_READ);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_OPTIONAL_APP_READWRITE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSOR_TYPE_UNDEFINED);
++  }
++  return "Unknown";
++}
++
++std::string QuantizationEncodingToString(Qnn_QuantizationEncoding_t q) {
++  switch (q) {
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_SCALE_OFFSET);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_AXIS_SCALE_OFFSET);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BW_SCALE_OFFSET);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BW_AXIS_SCALE_OFFSET);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BLOCK);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_BLOCKWISE_EXPANSION);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_VECTOR);
++    SHERPA_ONNX_TO_STRING(QNN_QUANTIZATION_ENCODING_UNDEFINED);
++  }
++  return "Unknown";
++}
++
++std::string TensorDataTypeToString(Qnn_DataType_t t) {
++  switch (t) {
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_8);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_16);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_32);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_INT_64);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_8);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_16);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_32);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UINT_64);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_16);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_32);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_FLOAT_64);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_4);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_8);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_16);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_SFIXED_POINT_32);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_4);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_8);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_16);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UFIXED_POINT_32);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_BOOL_8);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_STRING);
++    SHERPA_ONNX_TO_STRING(QNN_DATATYPE_UNDEFINED);
++  }
++  return "unknown";
++}
++
++std::string TensorMemTypeToString(Qnn_TensorMemType_t t) {
++  switch (t) {
++    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_RAW);
++    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_MEMHANDLE);
++    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_RETRIEVE_RAW);
++    SHERPA_ONNX_TO_STRING(QNN_TENSORMEMTYPE_UNDEFINED);
++  }
++  return "Unknown";
++}
++
++#undef SHERPA_ONNX_TO_STRING
++
++// quantized = float / scale - offset;
++void FillData(Qnn_Tensor_t *t, const float *data, int32_t n) {
++  float scale = t->v1.quantizeParams.scaleOffsetEncoding.scale;
++  int32_t offset = t->v1.quantizeParams.scaleOffsetEncoding.offset;
++
++  size_t bit_width = 16;
++  double true_bit_width_max = pow(2, bit_width) - 1;
++  double encoding_min = offset * scale;
++  double encoding_max = (true_bit_width_max + offset) * scale;
++  double encoding_range = encoding_max - encoding_min;
++
++  uint16_t *out = reinterpret_cast<uint16_t *>(t->v1.clientBuf.data);
++
++  for (size_t i = 0; i < n; ++i) {
++    int32_t quantized_value =
++        round(true_bit_width_max * (data[i] - encoding_min) / encoding_range);
++
++    if (quantized_value < 0) {
++      quantized_value = 0;
++    } else if (quantized_value > static_cast<int32_t>(true_bit_width_max)) {
++      quantized_value = static_cast<int32_t>(true_bit_width_max);
++    }
++    out[i] = static_cast<uint16_t>(quantized_value);
++  }
++}
++
++void FillData(Qnn_Tensor_t *t, const int32_t *data, int32_t n) {
++  int32_t *out = reinterpret_cast<int32_t *>(t->v1.clientBuf.data);
++  std::copy(data, data + n, out);
++}
++
++void GetData(const Qnn_Tensor_t *t, float *data, int32_t n) {
++  double scale = t->v1.quantizeParams.scaleOffsetEncoding.scale;
++  double offset = t->v1.quantizeParams.scaleOffsetEncoding.offset;
++
++  const uint16_t *p = reinterpret_cast<const uint16_t *>(t->v1.clientBuf.data);
++  for (int32_t i = 0; i < n; ++i) {
++    double quantizedValue = static_cast<double>(p[i]);
++    data[i] = (quantizedValue + offset) * scale;
++  }
++}
++
++static void FreeTensorV1(Qnn_Tensor_t *t) {
++  free(const_cast<char *>(t->v1.name));
++
++  delete[] t->v1.dimensions;
++}
++
++static void FreeTensorV2(Qnn_Tensor_t *t) {
++  free(const_cast<char *>(t->v2.name));
++
++  delete[] t->v2.dimensions;
++  delete[] t->v2.isDynamicDimensions;
++}
++
++void FreeTensor(Qnn_Tensor_t *t) {
++  if (t->version == QNN_TENSOR_VERSION_1) {
++    FreeTensorV1(t);
++  } else if (t->version == QNN_TENSOR_VERSION_2) {
++    FreeTensorV2(t);
++  } else {
++    SHERPA_ONNX_LOGE("Unknown tensor version: %d", t->version);
++  }
++}
++
++uint32_t GetSizeInBytes(const uint32_t *dimensions, uint32_t n,
++                        Qnn_DataType_t type) {
++  if (n == 0) {
++    return 0;
++  }
++
++  auto count = std::accumulate(dimensions, dimensions + n, 1,
++                               std::multiplies<uint32_t>());
++
++  uint32_t b = 1;
++  switch (type) {
++    case QNN_DATATYPE_INT_8:
++      b = 1;
++      break;
++    case QNN_DATATYPE_INT_16:
++      b = 2;
++      break;
++    case QNN_DATATYPE_INT_32:
++      b = 4;
++      break;
++    case QNN_DATATYPE_INT_64:
++      b = 8;
++      break;
++    case QNN_DATATYPE_UINT_8:
++      b = 1;
++      break;
++    case QNN_DATATYPE_UINT_16:
++      b = 2;
++      break;
++    case QNN_DATATYPE_UINT_32:
++      b = 4;
++      break;
++    case QNN_DATATYPE_UINT_64:
++      b = 8;
++      break;
++    case QNN_DATATYPE_FLOAT_16:
++      b = 2;
++      break;
++    case QNN_DATATYPE_FLOAT_32:
++      b = 4;
++      break;
++    case QNN_DATATYPE_FLOAT_64:
++      b = 8;
++      break;
++    case QNN_DATATYPE_SFIXED_POINT_8:
++      b = 1;
++      break;
++    case QNN_DATATYPE_SFIXED_POINT_16:
++      b = 2;
++      break;
++    case QNN_DATATYPE_SFIXED_POINT_32:
++      b = 4;
++      break;
++    case QNN_DATATYPE_UFIXED_POINT_8:
++      b = 1;
++      break;
++    case QNN_DATATYPE_UFIXED_POINT_16:
++      b = 2;
++      break;
++    case QNN_DATATYPE_UFIXED_POINT_32:
++      b = 4;
++      break;
++    case QNN_DATATYPE_BOOL_8:
++      b = 1;
++      break;
++    default:
++      SHERPA_ONNX_LOGE("Unsupported data type: %s",
++                       TensorDataTypeToString(type).c_str());
++      break;
++  }
++
++  return count * b;
++}
++
++template <typename T>
++void CopyDimensions(const T *src, uint32_t n, T **dst) {
++  if (!src || n == 0) {
++    *dst = nullptr;
++    return;
++  }
++
++  *dst = new T[n];
++  std::copy(src, src + n, *dst);
++}
++
++static void CopyQuantizeParams(const Qnn_QuantizeParams_t &src,
++                               Qnn_QuantizeParams_t &dst) {  // NOLINT
++  dst.encodingDefinition = src.encodingDefinition;
++  dst.quantizationEncoding = src.quantizationEncoding;
++
++  switch (src.quantizationEncoding) {
++    case QNN_QUANTIZATION_ENCODING_SCALE_OFFSET:
++      dst.scaleOffsetEncoding = src.scaleOffsetEncoding;
++      break;
++    case QNN_QUANTIZATION_ENCODING_UNDEFINED:
++      // do nothing in this case
++      break;
++    default:
++      SHERPA_ONNX_LOGE(
++          "Unsupported quantizationEncoding: %s",
++          QuantizationEncodingToString(src.quantizationEncoding).c_str());
++  }
++}
++
++static void CopyTensorInfoV1(const Qnn_Tensor_t &src,
++                             Qnn_Tensor_t &dst) {  // NOLINT
++  dst.version = src.version;
++  dst.v1.id = src.v1.id;
++  if (src.v1.name) {
++    dst.v1.name = strdup(src.v1.name);
++  } else {
++    dst.v1.name = strdup("");
++  }
++
++  dst.v1.type = src.v1.type;
++  dst.v1.dataFormat = src.v1.dataFormat;
++  dst.v1.dataType = src.v1.dataType;
++
++  CopyQuantizeParams(src.v1.quantizeParams, dst.v1.quantizeParams);
++
++  dst.v1.rank = src.v1.rank;
++
++  CopyDimensions(src.v1.dimensions, src.v1.rank, &dst.v1.dimensions);
++
++  dst.v1.memType = src.v1.memType;
++  if (dst.v1.memType != QNN_TENSORMEMTYPE_RAW) {
++    SHERPA_ONNX_LOGE("Unsupported mem type: %s",
++                     TensorMemTypeToString(dst.v1.memType).c_str());
++  } else {
++    dst.v1.clientBuf.data = nullptr;
++    dst.v1.clientBuf.dataSize =
++        GetSizeInBytes(dst.v1.dimensions, dst.v1.rank, dst.v1.dataType);
++  }
++}
++
++static void CopyTensorInfoV2(const Qnn_Tensor_t &src,
++                             Qnn_Tensor_t &dst) {  // NOLINT
++  dst.version = src.version;
++  dst.v2.id = src.v2.id;
++  if (src.v2.name) {
++    dst.v2.name = strdup(src.v2.name);
++  } else {
++    dst.v2.name = strdup("");
++  }
++
++  dst.v2.type = src.v2.type;
++  dst.v2.dataFormat = src.v2.dataFormat;
++  dst.v2.dataType = src.v2.dataType;
++
++  CopyQuantizeParams(src.v2.quantizeParams, dst.v2.quantizeParams);
++
++  dst.v2.rank = src.v2.rank;
++
++  CopyDimensions(src.v2.dimensions, src.v2.rank, &dst.v2.dimensions);
++
++  dst.v2.memType = src.v2.memType;
++  if (dst.v2.memType != QNN_TENSORMEMTYPE_RAW) {
++    SHERPA_ONNX_LOGE("Unsupported mem type: %s",
++                     TensorMemTypeToString(dst.v2.memType).c_str());
++  } else {
++    dst.v2.clientBuf.data = nullptr;
++    dst.v2.clientBuf.dataSize =
++        GetSizeInBytes(dst.v2.dimensions, dst.v2.rank, dst.v2.dataType);
++  }
++
++  CopyDimensions(src.v2.isDynamicDimensions, src.v2.rank,
++                 &dst.v2.isDynamicDimensions);
++
++  dst.v2.sparseParams.type = src.v2.sparseParams.type;
++  dst.v2.sparseParams.hybridCoo.numSpecifiedElements =
++      src.v2.sparseParams.hybridCoo.numSpecifiedElements;
++  dst.v2.sparseParams.hybridCoo.numSparseDimensions =
++      src.v2.sparseParams.hybridCoo.numSparseDimensions;
++  dst.v2.isProduced = src.v2.isProduced;
++}
++
++void CopyTensorInfo(const Qnn_Tensor_t &src, Qnn_Tensor_t &dst) {  // NOLINT
++  if (src.version == QNN_TENSOR_VERSION_1) {
++    CopyTensorInfoV1(src, dst);
++  } else if (src.version == QNN_TENSOR_VERSION_2) {
++    CopyTensorInfoV2(src, dst);
++  } else {
++    SHERPA_ONNX_LOGE("Unknown tensor version: %d", dst.version);
++  }
++}
++
++void LogCallback(const char *fmt, QnnLog_Level_t level, uint64_t timestamp,
++                 va_list args) {
++  std::string s;
++  switch (level) {
++    case QNN_LOG_LEVEL_ERROR:
++      s = "ERROR";
++      break;
++    case QNN_LOG_LEVEL_WARN:
++      s = "WARN";
++      break;
++    case QNN_LOG_LEVEL_INFO:
++      s = "INFO";
++      break;
++    case QNN_LOG_LEVEL_DEBUG:
++      s = "DEBUG";
++      break;
++    case QNN_LOG_LEVEL_VERBOSE:
++      s = "VERBOSE";
++      break;
++    case QNN_LOG_LEVEL_MAX:
++      s = "UNKNOWN";
++      break;
++  }
++
++  double ms = timestamp / 1000000.0;
++  fprintf(stdout, "%8.1fms [%-7s] ", ms, s.c_str());
++  vfprintf(stdout, fmt, args);
++}
++
++void PrintTensor(Qnn_TensorV2_t t) {
++  std::ostringstream os;
++  os << "  id: " << t.id << "\n";
++  os << "  name: " << t.name << "\n";
++  os << "  type: " << TensorTypeToString(t.type) << "\n";
++  os << "  data format: " << t.dataFormat << "\n";
++  os << "  data type: " << TensorDataTypeToString(t.dataType) << "\n";
++  os << "  quantize info: \n";
++  auto qp = t.quantizeParams;
++  os << "    encodingDefinition: " << std::hex << "0x" << qp.encodingDefinition
++     << std::dec << "\n";
++  os << "    quantizationEncoding: "
++     << QuantizationEncodingToString(qp.quantizationEncoding) << "\n";
++  if (qp.quantizationEncoding == QNN_QUANTIZATION_ENCODING_SCALE_OFFSET) {
++    Qnn_ScaleOffset_t s = qp.scaleOffsetEncoding;
++    os << "     scale: " << s.scale << "\n";
++    os << "     offset: " << s.offset << "\n";
++  }
++  os << "  rank: " << t.rank << "\n";
++  os << "  dimensions: ";
++  for (int32_t i = 0; i < t.rank; ++i) {
++    os << t.dimensions[i] << ", ";
++    if (i + 1 == t.rank) {
++      os << "\n";
++    }
++  }
++  os << "  memType: " << TensorMemTypeToString(t.memType) << "\n";
++  if (t.memType == QNN_TENSORMEMTYPE_RAW) {
++    os << " memType raw data size: " << t.clientBuf.dataSize << "\n";
++  }
++  os << "  isDynamicDimensions: "
++     << ((t.isDynamicDimensions != nullptr) ? "True" : "False") << "\n";
++  os << "  isProduced: " << static_cast<int32_t>(t.isProduced) << "\n";
++
++  SHERPA_ONNX_LOGE("%s", os.str().c_str());
++}
++
++static bool CopyGraphsInfoV3(const QnnSystemContext_GraphInfoV3_t *src,
++                             GraphInfo *dst) {
++  if (src->graphName) {
++    dst->graph_name = strdup(src->graphName);
++  } else {
++    dst->graph_name = strdup("");
++  }
++
++  dst->input_tensors = nullptr;
++  dst->num_input_tensors = 0;
++
++  if (src->graphInputs) {
++    dst->input_tensors = reinterpret_cast<Qnn_Tensor_t *>(
++        calloc(src->numGraphInputs, sizeof(Qnn_Tensor_t)));
++
++    for (uint32_t i = 0; i < src->numGraphInputs; ++i) {
++      dst->input_tensors[i] = QNN_TENSOR_INIT;
++
++      CopyTensorInfo(src->graphInputs[i], dst->input_tensors[i]);
++    }
++
++    dst->num_input_tensors = src->numGraphInputs;
++  }
++
++  dst->output_tensors = nullptr;
++  dst->num_output_tensors = 0;
++
++  if (src->graphOutputs) {
++    dst->output_tensors = reinterpret_cast<Qnn_Tensor_t *>(
++        calloc(src->numGraphOutputs, sizeof(Qnn_Tensor_t)));
++
++    for (uint32_t i = 0; i < src->numGraphOutputs; ++i) {
++      dst->output_tensors[i] = QNN_TENSOR_INIT;
++
++      CopyTensorInfo(src->graphOutputs[i], dst->output_tensors[i]);
++    }
++
++    dst->num_output_tensors = src->numGraphOutputs;
++  }
++
++  return true;
++}
++
++static bool CopyGraphsInfo(const QnnSystemContext_GraphInfo_t *graphs_input,
++                           uint32_t num_graphs,
++                           GraphInfo **&graphs_info) {  // NOLINT
++  if (num_graphs == 0) {
++    SHERPA_ONNX_LOGE("empty graphs");
++    graphs_info = nullptr;
++    return false;
++  }
++
++  SHERPA_ONNX_LOGE("version: %d", (int)graphs_input[0].version);
++
++  // remember to free graphs_info
++  graphs_info =
++      reinterpret_cast<GraphInfo **>(calloc(num_graphs, sizeof(GraphInfo *)));
++
++  GraphInfo *graph_info_arr =
++      reinterpret_cast<GraphInfo *>(calloc(num_graphs, sizeof(GraphInfo)));
++
++  if (!graphs_info || !graph_info_arr) {
++    SHERPA_ONNX_LOGE("Failure to allocate memory for *graphInfo");
++    return false;
++  }
++
++  for (uint32_t i = 0; i < num_graphs; ++i) {
++    switch (graphs_input[i].version) {
++      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_1:
++        SHERPA_ONNX_LOGE("Unsupported version: %d",
++                         static_cast<int32_t>(graphs_input[i].version));
++        return false;
++
++      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_2:
++        SHERPA_ONNX_LOGE("Unsupported version: %d",
++                         static_cast<int32_t>(graphs_input[i].version));
++        return false;
++
++      case QNN_SYSTEM_CONTEXT_GRAPH_INFO_VERSION_3: {
++        bool ok =
++            CopyGraphsInfoV3(&graphs_input[i].graphInfoV3, &graph_info_arr[i]);
++        if (!ok) {
++          SHERPA_ONNX_LOGE("Failed to copy graphs info v3");
++        }
++        graphs_info[i] = graph_info_arr + i;
++
++        break;
++      }
++
++      default:
++        SHERPA_ONNX_LOGE("Unsupported version: %d",
++                         static_cast<int32_t>(graphs_input[i].version));
++        return false;
++    }
++  }
++
++  return true;
++}
++
++bool CopyMetadataToGraphsInfo(const QnnSystemContext_BinaryInfo_t *binary_info,
++                              GraphInfo **&graphs_info,  // NOLINT
++                              uint32_t &graphs_count) {  // NOLINT
++  graphs_count = 0;
++
++  switch (binary_info->version) {
++    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_1: {
++      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
++                       binary_info->version);
++      return false;
++    }
++    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_2: {
++      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
++                       binary_info->version);
++      return false;
++    }
++    case QNN_SYSTEM_CONTEXT_BINARY_INFO_VERSION_3: {
++      bool ok = CopyGraphsInfo(binary_info->contextBinaryInfoV3.graphs,
++                               binary_info->contextBinaryInfoV3.numGraphs,
++                               graphs_info);
++
++      if (!ok) {
++        SHERPA_ONNX_LOGE("Failed while copying graphs Info v3.");
++        return false;
++      }
++      graphs_count = binary_info->contextBinaryInfoV3.numGraphs;
++      return true;
++    }
++    default: {
++      SHERPA_ONNX_LOGE("Unsupported binary context version: %d",
++                       binary_info->version);
++      return false;
++    }
++  }
++}
+diff --git a/sherpa-onnx/csrc/qnn/utils.h b/sherpa-onnx/csrc/qnn/utils.h
+new file mode 100644
+index 00000000..53663ecd
+--- /dev/null
++++ b/sherpa-onnx/csrc/qnn/utils.h
+@@ -0,0 +1,94 @@
++// sherpa-onnx/csrc/qnn/utils.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++#ifndef SHERPA_ONNX_CSRC_QNN_UTILS_H_
++#define SHERPA_ONNX_CSRC_QNN_UTILS_H_
++#include <stdio.h>
++
++#include <cstdint>
++#include <memory>
++#include <string>
++#include <vector>
++
++#include "QnnInterface.h"
++#include "System/QnnSystemInterface.h"
++#include "sherpa-onnx/csrc/macros.h"
++
++template <typename T>
++std::vector<T> ReadFile(const std::string &filename) {
++  FILE *fp = fopen(filename.c_str(), "rb");
++  if (!fp) {
++    SHERPA_ONNX_LOGE("Failed to open '%s'", filename.c_str());
++    return {};
++  }
++
++  fseek(fp, 0, SEEK_END);
++  int32_t n = ftell(fp);
++  fseek(fp, 0, SEEK_SET);
++
++  std::vector<T> ans(n / sizeof(T));
++  fread(ans.data(), sizeof(T), ans.size(), fp);
++  fclose(fp);
++
++  return ans;
++}
++
++void PrintTensor(Qnn_TensorV2_t t);
++
++// float -> uint16_t
++void FillData(Qnn_Tensor_t *t, const float *data, int32_t n);
++
++// int32_t -> int32_t
++void FillData(Qnn_Tensor_t *t, const int32_t *data, int32_t n);
++
++// uint16_t -> float
++void GetData(const Qnn_Tensor_t *t, float *data, int32_t n);
++
++void FreeTensor(Qnn_Tensor_t *t);
++
++using TensorPtr = std::unique_ptr<Qnn_Tensor_t, decltype(&FreeTensor)>;
++
++void CopyTensorInfo(const Qnn_Tensor_t &src, Qnn_Tensor_t &dst);  // NOLINT
++
++std::string QuantizationEncodingToString(Qnn_QuantizationEncoding_t q);
++
++std::string TensorDataTypeToString(Qnn_DataType_t t);
++
++using QnnInterfaceGetProvidersFnType = Qnn_ErrorHandle_t (*)(
++    const QnnInterface_t ***provider_list, uint32_t *num_providers);
++
++using QnnSystemInterfaceGetProvidersFnType = Qnn_ErrorHandle_t (*)(
++    const QnnSystemInterface_t ***provider_list, uint32_t *num_providers);
++
++struct GraphInfo {
++  Qnn_GraphHandle_t graph;
++  char *graph_name;
++  Qnn_Tensor_t *input_tensors;
++  uint32_t num_input_tensors;
++  Qnn_Tensor_t *output_tensors;
++  uint32_t num_output_tensors;
++};
++
++struct GraphConfigInfo {
++  char *graph_name;
++  const QnnGraph_Config_t **graph_configs;
++};
++
++using ComposeGraphsFnHandleType = Qnn_ErrorHandle_t (*)(
++    Qnn_BackendHandle_t backend_handle, QNN_INTERFACE_VER_TYPE interface,
++    Qnn_ContextHandle_t context_handle,
++    const GraphConfigInfo **graphs_config_info,
++    const uint32_t num_graphs_config_info, GraphInfo ***graphs_info,
++    uint32_t *num_graphs_info, bool debug, QnnLog_Callback_t logCallback,
++    QnnLog_Level_t max_log_level);
++
++using FreeGraphInfoFnHandleType =
++    Qnn_ErrorHandle_t (*)(GraphInfo ***, uint32_t num_graphs_info);
++
++void LogCallback(const char *fmt, QnnLog_Level_t level, uint64_t timestamp,
++                 va_list args);
++
++bool CopyMetadataToGraphsInfo(const QnnSystemContext_BinaryInfo_t *binary_info,
++                              GraphInfo **&graphs_info,  // NOLINT
++                              uint32_t &graphs_count);   // NOLINT
++#endif  // SHERPA_ONNX_CSRC_QNN_UTILS_H_
+
+commit 2b81e4d54236b04095334c84122dc0925201746a
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Tue Nov 11 14:29:02 2025 +0800
+
+    Support passing multiple lexicon files for matcha tts models. (#2765)
+
+diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
+index e5c9711d..1cf0eef7 100644
+--- a/sherpa-onnx/c-api/cxx-api.h
++++ b/sherpa-onnx/c-api/cxx-api.h
+@@ -742,7 +742,7 @@ struct OnlinePunctuationModelConfig {
+   std::string cnn_bilstm;
+   std::string bpe_vocab;
+   int32_t num_threads = 1;
+-  int32_t debug = false;
++  bool debug = false;
+   std::string provider = "cpu";
+ };
+ 
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+index 11f58f7c..b557fe08 100644
+--- a/sherpa-onnx/csrc/matcha-tts-lexicon.cc
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -57,10 +57,7 @@ class MatchaTtsLexicon::Impl {
+       InitTokens(is);
+     }
+ 
+-    {
+-      std::ifstream is(lexicon);
+-      InitLexicon(is);
+-    }
++    InitLexicon(lexicon);
+ 
+     if (data_dir.empty()) {
+       SHERPA_ONNX_LOGE("Please provide data dir for this model");
+@@ -86,8 +83,11 @@ class MatchaTtsLexicon::Impl {
+       InitTokens(is);
+     }
+ 
+-    {
+-      auto buf = ReadFile(mgr, lexicon);
++    std::vector<std::string> files;
++    SplitStringToVector(lexicon, ",", false, &files);
++    for (const auto &f : files) {
++      auto buf = ReadFile(mgr, f);
++
+       std::istrstream is(buf.data(), buf.size());
+       InitLexicon(is);
+     }
+@@ -103,7 +103,7 @@ class MatchaTtsLexicon::Impl {
+   std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
+     std::string text = _text;
+     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
+-        {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
++        {"", ","}, {"", ","}, {"", ";"}, {"", ":"},
+         {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
+     };
+     for (const auto &p : replace_str_pairs) {
+@@ -205,6 +205,18 @@ class MatchaTtsLexicon::Impl {
+       this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
+ 
+       if (IsPunct(w)) {
++        if (debug_) {
++          std::ostringstream os;
++          std::string sep;
++          os << "new sentence: [";
++          for (auto i : this_sentence) {
++            os << sep << i;
++            sep = ", ";
++          }
++          os << "]";
++          SHERPA_ONNX_LOGE("%s", os.str().c_str());
++        }
++
+         ans.emplace_back(std::move(this_sentence));
+         this_sentence = {};
+       }
+@@ -220,7 +232,6 @@ class MatchaTtsLexicon::Impl {
+  private:
+   std::vector<int32_t> ConvertWordToIds(const std::string &w) const {
+     std::vector<int32_t> ans;
+-
+     if (word2ids_.count(w)) {
+       ans = word2ids_.at(w);
+     } else if (token2id_.count(w)) {
+@@ -255,11 +266,15 @@ class MatchaTtsLexicon::Impl {
+       }
+     }
+ 
++    if (IsAlphaOrPunct(w.front())) {
++      ans.push_back(token2id_.at(" "));
++    }
++
+     if (debug_) {
+       std::ostringstream os;
+       os << w << ": ";
+       for (auto i : ans) {
+-        os << id2token_.at(i) << " ";
++        os << "'" << id2token_.at(i) << "'(" << i << ")" << ",";
+       }
+ #if __OHOS__
+       SHERPA_ONNX_LOGE("%{public}s", os.str().c_str());
+@@ -274,29 +289,6 @@ class MatchaTtsLexicon::Impl {
+   void InitTokens(std::istream &is) {
+     token2id_ = ReadTokens(is);
+ 
+-    std::vector<std::pair<std::string, std::string>> puncts = {
+-        {",", ""}, {".", ""}, {"!", ""}, {"?", ""}, {":", ""},
+-        {"\"", ""}, {"\"", ""}, {"'", ""},  {"'", ""},  {";", ""},
+-    };
+-
+-    for (const auto &p : puncts) {
+-      if (token2id_.count(p.first) && !token2id_.count(p.second)) {
+-        token2id_[p.second] = token2id_[p.first];
+-      }
+-
+-      if (!token2id_.count(p.first) && token2id_.count(p.second)) {
+-        token2id_[p.first] = token2id_[p.second];
+-      }
+-    }
+-
+-    if (!token2id_.count("") && token2id_.count("")) {
+-      token2id_[""] = token2id_[""];
+-    }
+-
+-    if (!token2id_.count(";") && token2id_.count(",")) {
+-      token2id_[";"] = token2id_[","];
+-    }
+-
+     if (debug_) {
+       for (const auto &p : token2id_) {
+         id2token_[p.second] = p.first;
+@@ -327,6 +319,20 @@ class MatchaTtsLexicon::Impl {
+     }
+   }
+ 
++  void InitLexicon(const std::string &lexicon) {
++    if (lexicon.empty()) {
++      SHERPA_ONNX_LOGE("Empty lexicon!");
++      return;
++    }
++
++    std::vector<std::string> files;
++    SplitStringToVector(lexicon, ",", false, &files);
++    for (const auto &f : files) {
++      std::ifstream is(f);
++      InitLexicon(is);
++    }
++  }
++
+   void InitLexicon(std::istream &is) {
+     std::string word;
+     std::vector<std::string> token_list;
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+index 1fdbad53..9b379f44 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+@@ -8,6 +8,7 @@
+ 
+ #include "sherpa-onnx/csrc/file-utils.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/text-utils.h"
+ 
+ namespace sherpa_onnx {
+ 
+@@ -15,8 +16,10 @@ void OfflineTtsMatchaModelConfig::Register(ParseOptions *po) {
+   po->Register("matcha-acoustic-model", &acoustic_model,
+                "Path to matcha acoustic model");
+   po->Register("matcha-vocoder", &vocoder, "Path to matcha vocoder");
+-  po->Register("matcha-lexicon", &lexicon,
+-               "Path to lexicon.txt for Matcha models");
++  po->Register(
++      "matcha-lexicon", &lexicon,
++      "Path to lexicon.txt for Matcha models. You can pass multiple "
++      "files separated by comma , e.g., lexicon.txt,lexicon2.txt,lexicon3.txt");
+   po->Register("matcha-tokens", &tokens,
+                "Path to tokens.txt for Matcha models");
+   po->Register("matcha-data-dir", &data_dir,
+@@ -82,9 +85,17 @@ bool OfflineTtsMatchaModelConfig::Validate() const {
+     }
+   }
+ 
+-  if (!lexicon.empty() && !FileExists(lexicon)) {
+-    SHERPA_ONNX_LOGE("--matcha-lexicon: '%s' does not exist", lexicon.c_str());
+-    return false;
++  if (!lexicon.empty()) {
++    std::vector<std::string> files;
++    SplitStringToVector(lexicon, ",", false, &files);
++    for (const auto &f : files) {
++      if (!FileExists(f)) {
++        SHERPA_ONNX_LOGE(
++            "lexicon '%s' does not exist. Please re-check --matcha-lexicon",
++            f.c_str());
++        return false;
++      }
++    }
+   }
+ 
+   if (!dict_dir.empty()) {
+diff --git a/sherpa-onnx/csrc/phrase-matcher.cc b/sherpa-onnx/csrc/phrase-matcher.cc
+index bea8bded..35e09555 100644
+--- a/sherpa-onnx/csrc/phrase-matcher.cc
++++ b/sherpa-onnx/csrc/phrase-matcher.cc
+@@ -56,34 +56,43 @@ class PhraseMatcher::Impl {
+     int32_t num_words = static_cast<int32_t>(words.size());
+     for (int32_t i = 0; i < num_words;) {
+       int32_t start = i;
+-      int32_t end = std::min(i + max_search_len_ - 1, num_words - 1);
+ 
+       std::string w;
+-      while (end > start) {
+-        auto this_word = GetWord(words, start, end);
+-        if (debug_) {
++
++      if (!IsAlphaOrPunct(words[i].front())) {
++        int32_t end = std::min(i + max_search_len_ - 1, num_words - 1);
++
++        while (end > start) {
++          auto this_word = GetWord(words, start, end);
++          if (IsAlphaOrPunct(this_word.back())) {
++            --end;
++            continue;
++          }
++
++          if (debug_) {
+ #if __OHOS__
+-          SHERPA_ONNX_LOGE("%{public}d-%{public}d: %{public}s", start, end,
+-                           this_word.c_str());
++            SHERPA_ONNX_LOGE("%{public}d-%{public}d: %{public}s", start, end,
++                             this_word.c_str());
+ #else
+-          SHERPA_ONNX_LOGE("%d-%d: %s", start, end, this_word.c_str());
++            SHERPA_ONNX_LOGE("%d-%d: %s", start, end, this_word.c_str());
+ #endif
+-        }
+-        if (lexicon_->count(this_word)) {
+-          i = end + 1;
+-          w = std::move(this_word);
+-          if (debug_) {
++          }
++          if (lexicon_->count(this_word)) {
++            i = end + 1;
++            w = std::move(this_word);
++            if (debug_) {
+ #if __OHOS__
+-            SHERPA_ONNX_LOGE("matched %{public}d-%{public}d: %{public}s", start,
+-                             end, w.c_str());
++              SHERPA_ONNX_LOGE("matched %{public}d-%{public}d: %{public}s",
++                               start, end, w.c_str());
+ #else
+-            SHERPA_ONNX_LOGE("matched %d-%d: %s", start, end, w.c_str());
++              SHERPA_ONNX_LOGE("matched %d-%d: %s", start, end, w.c_str());
+ #endif
++            }
++            break;
+           }
+-          break;
+-        }
+ 
+-        end -= 1;
++          end -= 1;
++        }
+       }
+ 
+       if (w.empty()) {
+diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
+index 760ac329..993f2bb0 100644
+--- a/sherpa-onnx/csrc/text-utils.cc
++++ b/sherpa-onnx/csrc/text-utils.cc
+@@ -808,6 +808,8 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
+   return ans;
+ }
+ 
++bool IsAlphaOrPunct(int ch) { return std::isalpha(ch) || std::ispunct(ch); }
++
+ bool IsPunct(const std::string &s) {
+   static const std::unordered_set<std::string> puncts = {
+       ",",  ".",  "!",  "?", ":", "\"", "'", "",
+diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
+index 6f56dc53..5a3fff09 100644
+--- a/sherpa-onnx/csrc/text-utils.h
++++ b/sherpa-onnx/csrc/text-utils.h
+@@ -165,6 +165,8 @@ std::string ToUpperAscii(const std::string &str);
+ // unchanged)
+ std::string ToLowerAscii(const std::string &str);
+ 
++bool IsAlphaOrPunct(int ch);
++
+ // Detect if a codepoint is a CJK character
+ bool IsCJK(char32_t cp);
+ 
+
+commit 5509c1b8cad17db01ca4b40795520593d0c41269
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 10 18:22:26 2025 +0800
+
+    Fix zipvoice. (#2764)
+    
+    Fixes #2701
+    
+    The pull request addresses a critical memory management issue in OfflineTtsZipvoiceModel::Impl::Run. The previous implementation had a potential use-after-free bug by directly returning an Ort::Value pointing to the internal buffer of a std::vector (out_data), which would be deallocated when out_data went out of scope. The updated code correctly allocates memory for the Ort::Value using allocator_ and then copies the data, ensuring proper memory ownership and preventing undefined behavior. This is a crucial fix for the stability and correctness of the application.
+
+diff --git a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+index 7c04622b..4ff7d833 100644
+--- a/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-zipvoice-model.cc
+@@ -176,9 +176,14 @@ class OfflineTtsZipvoiceModel::Impl {
+                   keep_frames * feat_dim * sizeof(float));
+     }
+     std::vector<int64_t> out_shape = {batch_size, keep_frames, feat_dim};
+-    return Ort::Value::CreateTensor<float>(memory_info, out_data.data(),
+-                                           out_data.size(), out_shape.data(),
+-                                           out_shape.size());
++
++    Ort::Value ans = Ort::Value::CreateTensor<float>(
++        allocator_, out_shape.data(), out_shape.size());
++
++    std::copy(out_data.begin(), out_data.end(),
++              ans.GetTensorMutableData<float>());
++
++    return ans;
+   }
+ 
+  private:
+
+commit 60736d2f7e46c74b6fe13fe95af40dc68650320f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 10 18:07:23 2025 +0800
+
+    Support MatchaTTS models for Chinese+English. (#2763)
+
+diff --git a/scripts/matcha-tts/zh-en/README.md b/scripts/matcha-tts/zh-en/README.md
+index ed2ecbca..0fba2f75 100644
+--- a/scripts/matcha-tts/zh-en/README.md
++++ b/scripts/matcha-tts/zh-en/README.md
+@@ -9,3 +9,25 @@ vocos-16khz-univ.onnx
+ You can download it from 
+  https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010/resolve/master/vocos-16khz-univ.onnx
+ or
++
++```
++{'am': './model-steps-3.onnx', 'vocoder': './vocos-16khz-univ.onnx', 'tokens': './tokens.txt', 'lexicon': './lexicon.txt', 'text': '. It supports both English ', 'out_wav': 'generated.wav'}
++
++{'use_eos_bos': '1', 'modelscope_url': 'https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010', 'sample_rate': '16000', 'language': 'chinese English', 'model_type': 'matcha-tts', 'n_speakers': '1', 'model_author': 'dengcunqin', 'version': '1', 'pad_id': '0', 'voice': 'zh en-us', 'demo_url': 'https://www.tulingyun.com/tts.html', 'num_ode_steps': '3'}
++
++NodeArg(name='x', type='tensor(int64)', shape=['N', 'L'])
++NodeArg(name='x_length', type='tensor(int64)', shape=['N'])
++NodeArg(name='noise_scale', type='tensor(float)', shape=[1])
++NodeArg(name='length_scale', type='tensor(float)', shape=[1])
++-----
++NodeArg(name='mel', type='tensor(float)', shape=['N', 80, 'L'])
++
++vocos {'modelscope_url': 'https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010', 'use_eos_bos': '1', 'n_speakers': '1', 'sample_rate': '16000', 'pad_id': '0', 'language': 'chinese English', 'model_type': 'matcha-tts vocos', 'voice': 'zh en-us', 'version': '1', 'demo_url': 'https://www.tulingyun.com/tts.html', 'model_author': 'dengcunqin'}
++
++----------vocos----------
++NodeArg(name='mels', type='tensor(float)', shape=['batch_size', 80, 'time'])
++-----
++NodeArg(name='mag', type='tensor(float)', shape=['batch_size', 'Clipmag_dim_1', 'time'])
++NodeArg(name='x', type='tensor(float)', shape=['batch_size', 'Cosx_dim_1', 'time'])
++NodeArg(name='y', type='tensor(float)', shape=['batch_size', 'Cosx_dim_1', 'time'])
++```
+diff --git a/scripts/matcha-tts/zh-en/generate_lexicon.py b/scripts/matcha-tts/zh-en/generate_lexicon.py
+index bcc7e8dc..15bc5f44 100755
+--- a/scripts/matcha-tts/zh-en/generate_lexicon.py
++++ b/scripts/matcha-tts/zh-en/generate_lexicon.py
+@@ -8,6 +8,11 @@ load_phrases_dict(
+         "": [["yin2"], ["hang2"], ["hang2"], ["zhang3"]],
+     }
+ )
++user_defined = {
++    "": ["wei1", "tiao2"],
++    "": ["zhe4", "ge4"],
++    "": ["fang1", "bian2", "de1"],
++}
+ 
+ 
+ def main():
+@@ -32,7 +37,12 @@ def main():
+ 
+             f.write(f"{w} {tokens}\n")
+ 
++        for key, value in user_defined.items():
++            f.write(f"{key} {' '.join(value)}\n")
++
+         for key in phrases:
++            if key in user_defined:
++                continue
+             tokens = lazy_pinyin(key, style=Style.TONE3, tone_sandhi=True)
+             for i in range(len(tokens)):
+                 if tokens[i] == "shei2":
+diff --git a/sherpa-onnx/csrc/CMakeLists.txt b/sherpa-onnx/csrc/CMakeLists.txt
+index 6d6bb53c..d44e8e8a 100644
+--- a/sherpa-onnx/csrc/CMakeLists.txt
++++ b/sherpa-onnx/csrc/CMakeLists.txt
+@@ -206,6 +206,7 @@ if(SHERPA_ONNX_ENABLE_TTS)
+     hifigan-vocoder.cc
+     kokoro-multi-lang-lexicon.cc
+     lexicon.cc
++    matcha-tts-lexicon.cc
+     melo-tts-lexicon.cc
+     offline-tts-character-frontend.cc
+     offline-tts-frontend.cc
+@@ -220,8 +221,8 @@ if(SHERPA_ONNX_ENABLE_TTS)
+     offline-tts-vits-model-config.cc
+     offline-tts-vits-model.cc
+     offline-tts-zipvoice-frontend.cc
+-    offline-tts-zipvoice-model.cc
+     offline-tts-zipvoice-model-config.cc
++    offline-tts-zipvoice-model.cc
+     offline-tts.cc
+     piper-phonemize-lexicon.cc
+     vocoder.cc
+diff --git a/sherpa-onnx/csrc/character-lexicon.cc b/sherpa-onnx/csrc/character-lexicon.cc
+index 2dc1ad6e..60d48b55 100644
+--- a/sherpa-onnx/csrc/character-lexicon.cc
++++ b/sherpa-onnx/csrc/character-lexicon.cc
+@@ -34,14 +34,6 @@
+ 
+ namespace sherpa_onnx {
+ 
+-static bool IsPunct(const std::string &s) {
+-  static const std::unordered_set<std::string> puncts = {
+-      ",",  ".",  "!",  "?", ":", "\"", "'", "",
+-      "", "", "", "", "", "",  "",
+-  };
+-  return puncts.count(s);
+-}
+-
+ class CharacterLexicon::Impl {
+  public:
+   Impl(const std::string &lexicon, const std::string &tokens, bool debug)
+diff --git a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+index 38ac2fd6..77cb5123 100644
+--- a/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
++++ b/sherpa-onnx/csrc/kokoro-multi-lang-lexicon.cc
+@@ -4,6 +4,7 @@
+ 
+ #include "sherpa-onnx/csrc/kokoro-multi-lang-lexicon.h"
+ 
++#include <codecvt>
+ #include <fstream>
+ #include <regex>  // NOLINT
+ #include <sstream>
+@@ -23,8 +24,6 @@
+ #include "rawfile/raw_file_manager.h"
+ #endif
+ 
+-#include <codecvt>
+-
+ #include "espeak-ng/speak_lib.h"
+ #include "phoneme_ids.hpp"
+ #include "phonemize.hpp"
+@@ -72,9 +71,6 @@ class KokoroMultiLangLexicon::Impl {
+     // we cannot convert text to lowercase here since it will affect
+     // how piper_phonemize handles punctuations inside the text
+     std::string text = _text;
+-    if (debug_) {
+-      SHERPA_ONNX_LOGE("After converting to lowercase:\n%s", text.c_str());
+-    }
+ 
+     std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
+         {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.cc b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+new file mode 100644
+index 00000000..11f58f7c
+--- /dev/null
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.cc
+@@ -0,0 +1,432 @@
++// sherpa-onnx/csrc/matcha-tts-lexicon.cc
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
++
++#include <algorithm>
++#include <codecvt>
++#include <fstream>
++#include <memory>
++#include <regex>  // NOLINT
++#include <sstream>
++#include <string>
++#include <strstream>
++#include <unordered_map>
++#include <unordered_set>
++#include <utility>
++#include <vector>
++
++#if __ANDROID_API__ >= 9
++#include "android/asset_manager.h"
++#include "android/asset_manager_jni.h"
++#endif
++
++#if __OHOS__
++#include "rawfile/raw_file_manager.h"
++#endif
++
++#include "espeak-ng/speak_lib.h"
++#include "phoneme_ids.hpp"
++#include "phonemize.hpp"
++#include "sherpa-onnx/csrc/file-utils.h"
++#include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/onnx-utils.h"
++#include "sherpa-onnx/csrc/phrase-matcher.h"
++#include "sherpa-onnx/csrc/symbol-table.h"
++#include "sherpa-onnx/csrc/text-utils.h"
++
++namespace sherpa_onnx {
++
++void CallPhonemizeEspeak(const std::string &text,
++                         piper::eSpeakPhonemeConfig &config,  // NOLINT
++                         std::vector<std::vector<piper::Phoneme>> *phonemes);
++
++class MatchaTtsLexicon::Impl {
++ public:
++  Impl(const std::string &lexicon, const std::string &tokens,
++       const std::string &data_dir, bool debug)
++      : debug_(debug) {
++    if (lexicon.empty()) {
++      SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    {
++      std::ifstream is(tokens);
++      InitTokens(is);
++    }
++
++    {
++      std::ifstream is(lexicon);
++      InitLexicon(is);
++    }
++
++    if (data_dir.empty()) {
++      SHERPA_ONNX_LOGE("Please provide data dir for this model");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitEspeak(data_dir);  // See ./piper-phonemize-lexicon.cc
++  }
++
++  template <typename Manager>
++  Impl(Manager *mgr, const std::string &lexicon, const std::string &tokens,
++       const std::string &data_dir, bool debug)
++      : debug_(debug) {
++    if (lexicon.empty()) {
++      SHERPA_ONNX_LOGE("Please provide lexicon.txt for this model");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    {
++      auto buf = ReadFile(mgr, tokens);
++      std::istrstream is(buf.data(), buf.size());
++
++      InitTokens(is);
++    }
++
++    {
++      auto buf = ReadFile(mgr, lexicon);
++      std::istrstream is(buf.data(), buf.size());
++      InitLexicon(is);
++    }
++
++    if (data_dir.empty()) {
++      SHERPA_ONNX_LOGE("Please provide data dir for this model");
++      SHERPA_ONNX_EXIT(-1);
++    }
++
++    InitEspeak(data_dir);  // See ./piper-phonemize-lexicon.cc
++  }
++
++  std::vector<TokenIDs> ConvertTextToTokenIds(const std::string &_text) const {
++    std::string text = _text;
++    std::vector<std::pair<std::string, std::string>> replace_str_pairs = {
++        {"", ","}, {":", ","},  {"", ","}, {"", ";"},   {"", ":"},
++        {"", "."}, {"", "?"}, {"", "!"}, {"\\s+", " "},
++    };
++    for (const auto &p : replace_str_pairs) {
++      std::regex re(p.first);
++      text = std::regex_replace(text, re, p.second);
++    }
++
++    if (debug_) {
++      SHERPA_ONNX_LOGE("After replacing punctuations and merging spaces:\n%s",
++                       text.c_str());
++    }
++
++    std::vector<std::string> words = SplitUtf8(text);
++
++    if (debug_) {
++#if __OHOS__
++      SHERPA_ONNX_LOGE("input text:\n%{public}s", _text.c_str());
++      SHERPA_ONNX_LOGE("after replacing punctuations:\n%{public}s",
++                       text.c_str());
++#else
++      SHERPA_ONNX_LOGE("input text:\n%s", _text.c_str());
++      SHERPA_ONNX_LOGE("after replacing punctuations:\n%s", text.c_str());
++#endif
++
++      std::ostringstream os;
++      std::string sep = "";
++      for (const auto &w : words) {
++        os << sep << w;
++        sep = "_";
++      }
++
++#if __OHOS__
++      SHERPA_ONNX_LOGE("after splitting into UTF8:\n%{public}s",
++                       os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("after splitting into UTF8:\n%s", os.str().c_str());
++#endif
++    }
++
++    // remove spaces after punctuations
++    std::vector<std::string> words2 = std::move(words);
++    words.reserve(words2.size());
++
++    for (int32_t i = 0; i < words2.size(); ++i) {
++      if (i == 0) {
++        words.push_back(std::move(words2[i]));
++      } else if (words2[i] == " ") {
++        if (words.back() == " " || IsPunct(words.back())) {
++          continue;
++        } else {
++          words.push_back(std::move(words2[i]));
++        }
++      } else if (IsPunct(words2[i])) {
++        if (words.back() == " " || IsPunct(words.back())) {
++          continue;
++        } else {
++          words.push_back(std::move(words2[i]));
++        }
++      } else {
++        words.push_back(std::move(words2[i]));
++      }
++    }
++
++    if (debug_) {
++      std::ostringstream os;
++      std::string sep = "";
++      for (const auto &w : words) {
++        os << sep << w;
++        sep = "_";
++      }
++
++#if __OHOS__
++      SHERPA_ONNX_LOGE("after removing spaces after punctuations:\n%{public}s",
++                       os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("after removing spaces after punctuations:\n%s",
++                       os.str().c_str());
++#endif
++    }
++
++    std::vector<TokenIDs> ans;
++    std::vector<int64_t> this_sentence;
++
++    PhraseMatcher matcher(&all_words_, words, debug_);
++
++    std::vector<int32_t> ids;
++    for (const std::string &w : matcher) {
++      ids = ConvertWordToIds(w);
++
++      if (ids.empty()) {
++#if __OHOS__
++        SHERPA_ONNX_LOGE("Ignore OOV '%{public}s'", w.c_str());
++#else
++        SHERPA_ONNX_LOGE("Ignore OOV '%s'", w.c_str());
++#endif
++        continue;
++      }
++
++      this_sentence.insert(this_sentence.end(), ids.begin(), ids.end());
++
++      if (IsPunct(w)) {
++        ans.emplace_back(std::move(this_sentence));
++        this_sentence = {};
++      }
++    }  // for (const std::string &w : matcher)
++
++    if (!this_sentence.empty()) {
++      ans.emplace_back(std::move(this_sentence));
++    }
++
++    return ans;
++  }
++
++ private:
++  std::vector<int32_t> ConvertWordToIds(const std::string &w) const {
++    std::vector<int32_t> ans;
++
++    if (word2ids_.count(w)) {
++      ans = word2ids_.at(w);
++    } else if (token2id_.count(w)) {
++      ans = {token2id_.at(w)};
++    } else {
++      if (ContainsCJK(w)) {
++        std::vector<std::string> words = SplitUtf8(w);
++        for (const auto &word : words) {
++          if (word2ids_.count(word)) {
++            auto ids = ConvertWordToIds(word);
++            ans.insert(ans.end(), ids.begin(), ids.end());
++          }
++        }
++      } else {
++        SHERPA_ONNX_LOGE("use espeak for %s", w.c_str());
++        // use espeak
++        piper::eSpeakPhonemeConfig config;
++        config.voice = "en-us";
++        std::vector<std::vector<piper::Phoneme>> phonemes;
++        CallPhonemizeEspeak(w, config, &phonemes);
++        for (const auto &ps : phonemes) {
++          for (const auto &p : ps) {
++            if (phoneme2id_.count(p)) {
++              ans.push_back(phoneme2id_.at(p));
++            } else {
++              SHERPA_ONNX_LOGE(
++                  "Skip unknown phonemes. Unicode codepoint: \\U+%04x. for %s",
++                  static_cast<uint32_t>(p), w.c_str());
++            }
++          }
++        }
++      }
++    }
++
++    if (debug_) {
++      std::ostringstream os;
++      os << w << ": ";
++      for (auto i : ans) {
++        os << id2token_.at(i) << " ";
++      }
++#if __OHOS__
++      SHERPA_ONNX_LOGE("%{public}s", os.str().c_str());
++#else
++      SHERPA_ONNX_LOGE("%s", os.str().c_str());
++#endif
++    }
++
++    return ans;
++  }
++
++  void InitTokens(std::istream &is) {
++    token2id_ = ReadTokens(is);
++
++    std::vector<std::pair<std::string, std::string>> puncts = {
++        {",", ""}, {".", ""}, {"!", ""}, {"?", ""}, {":", ""},
++        {"\"", ""}, {"\"", ""}, {"'", ""},  {"'", ""},  {";", ""},
++    };
++
++    for (const auto &p : puncts) {
++      if (token2id_.count(p.first) && !token2id_.count(p.second)) {
++        token2id_[p.second] = token2id_[p.first];
++      }
++
++      if (!token2id_.count(p.first) && token2id_.count(p.second)) {
++        token2id_[p.first] = token2id_[p.second];
++      }
++    }
++
++    if (!token2id_.count("") && token2id_.count("")) {
++      token2id_[""] = token2id_[""];
++    }
++
++    if (!token2id_.count(";") && token2id_.count(",")) {
++      token2id_[";"] = token2id_[","];
++    }
++
++    if (debug_) {
++      for (const auto &p : token2id_) {
++        id2token_[p.second] = p.first;
++      }
++    }
++
++    std::wstring_convert<std::codecvt_utf8<char32_t>, char32_t> conv;
++    std::u32string s;
++    for (const auto &p : token2id_) {
++      if ((p.first.front() == '<' && p.first.back() == '>') ||
++          p.first.back() == '1' || p.first.back() == '2' ||
++          p.first.back() == '3' || p.first.back() == '4' ||
++          p.first.back() == '5') {
++        continue;
++      }
++      s = conv.from_bytes(p.first);
++
++      if (s.size() != 1) {
++        SHERPA_ONNX_LOGE("Error for token %s with id %d", p.first.c_str(),
++                         p.second);
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      char32_t c = s[0];
++      if (!phoneme2id_.count(c)) {
++        phoneme2id_.insert({c, p.second});
++      }
++    }
++  }
++
++  void InitLexicon(std::istream &is) {
++    std::string word;
++    std::vector<std::string> token_list;
++    std::string line;
++    std::string phone;
++    int32_t line_num = 0;
++
++    while (std::getline(is, line)) {
++      ++line_num;
++
++      std::istringstream iss(line);
++
++      token_list.clear();
++
++      iss >> word;
++      ToLowerCase(&word);
++
++      if (word2ids_.count(word)) {
++#if __OHOS__
++        SHERPA_ONNX_LOGE(
++            "Duplicated word: %{public}s at line %{public}d:%{public}s. Ignore "
++            "it.",
++            word.c_str(), line_num, line.c_str());
++#else
++        SHERPA_ONNX_LOGE("Duplicated word: %s at line %d:%s. Ignore it.",
++                         word.c_str(), line_num, line.c_str());
++#endif
++        continue;
++      }
++
++      while (iss >> phone) {
++        token_list.push_back(std::move(phone));
++      }
++
++      std::vector<int32_t> ids = ConvertTokensToIds(token2id_, token_list);
++      if (ids.empty()) {
++        if (debug_) {
++#if __OHOS__
++          SHERPA_ONNX_LOGE("Empty token ids for '%{public}s'", line.c_str());
++#else
++          SHERPA_ONNX_LOGE("Empty token ids for '%s'", line.c_str());
++#endif
++        }
++        continue;
++      }
++
++      word2ids_.insert({std::move(word), std::move(ids)});
++    }
++
++    for (const auto &[key, _] : word2ids_) {
++      all_words_.insert(key);
++    }
++  }
++
++ private:
++  // lexicon.txt is saved in word2ids_
++  std::unordered_map<std::string, std::vector<int32_t>> word2ids_;
++  std::unordered_set<std::string> all_words_;
++
++  // tokens.txt is saved in token2id_
++  std::unordered_map<std::string, int32_t> token2id_;
++  std::unordered_map<char32_t, int32_t> phoneme2id_;
++
++  std::unordered_map<int32_t, std::string> id2token_;
++
++  bool debug_ = false;
++};  // namespace sherpa_onnx
++
++MatchaTtsLexicon::~MatchaTtsLexicon() = default;
++
++MatchaTtsLexicon::MatchaTtsLexicon(const std::string &lexicon,
++                                   const std::string &tokens,
++                                   const std::string &data_dir, bool debug)
++    : impl_(std::make_unique<Impl>(lexicon, tokens, data_dir, debug)) {}
++
++template <typename Manager>
++MatchaTtsLexicon::MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
++                                   const std::string &tokens,
++                                   const std::string &data_dir, bool debug)
++    : impl_(std::make_unique<Impl>(mgr, lexicon, tokens, data_dir, debug)) {}
++
++std::vector<TokenIDs> MatchaTtsLexicon::ConvertTextToTokenIds(
++    const std::string &text, const std::string & /*unused_voice = ""*/) const {
++  return impl_->ConvertTextToTokenIds(text);
++}
++
++#if __ANDROID_API__ >= 9
++template MatchaTtsLexicon::MatchaTtsLexicon(AAssetManager *mgr,
++                                            const std::string &lexicon,
++                                            const std::string &tokens,
++                                            const std::string &data_dir,
++                                            bool debug);
++#endif
++
++#if __OHOS__
++template MatchaTtsLexicon::MatchaTtsLexicon(NativeResourceManager *mgr,
++                                            const std::string &lexicon,
++                                            const std::string &tokens,
++                                            const std::string &data_dir,
++                                            bool debug);
++#endif
++
++}  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/matcha-tts-lexicon.h b/sherpa-onnx/csrc/matcha-tts-lexicon.h
+new file mode 100644
+index 00000000..f9da31a6
+--- /dev/null
++++ b/sherpa-onnx/csrc/matcha-tts-lexicon.h
+@@ -0,0 +1,41 @@
++// sherpa-onnx/csrc/matcha-tts-lexicon.h
++//
++// Copyright (c)  2025  Xiaomi Corporation
++
++#ifndef SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
++#define SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
++
++#include <memory>
++#include <string>
++#include <unordered_map>
++#include <vector>
++
++#include "sherpa-onnx/csrc/offline-tts-frontend.h"
++
++namespace sherpa_onnx {
++
++// For Chinese+English matcha tts
++class MatchaTtsLexicon : public OfflineTtsFrontend {
++ public:
++  ~MatchaTtsLexicon() override;
++
++  MatchaTtsLexicon(const std::string &lexicon, const std::string &tokens,
++                   const std::string &data_dir, bool debug);
++
++  template <typename Manager>
++  MatchaTtsLexicon(Manager *mgr, const std::string &lexicon,
++                   const std::string &tokens, const std::string &data_dir,
++                   bool debug);
++
++  std::vector<TokenIDs> ConvertTextToTokenIds(
++      const std::string &text,
++      const std::string &unused_voice = "") const override;
++
++ private:
++  class Impl;
++  std::unique_ptr<Impl> impl_;
++};
++
++}  // namespace sherpa_onnx
++
++#endif  // SHERPA_ONNX_CSRC_MATCHA_TTS_LEXICON_H_
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-impl.h b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+index dc887ab2..3588b2d2 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-impl.h
++++ b/sherpa-onnx/csrc/offline-tts-matcha-impl.h
+@@ -4,6 +4,7 @@
+ #ifndef SHERPA_ONNX_CSRC_OFFLINE_TTS_MATCHA_IMPL_H_
+ #define SHERPA_ONNX_CSRC_OFFLINE_TTS_MATCHA_IMPL_H_
+ 
++#include <algorithm>
+ #include <memory>
+ #include <string>
+ #include <strstream>
+@@ -16,6 +17,7 @@
+ #include "sherpa-onnx/csrc/character-lexicon.h"
+ #include "sherpa-onnx/csrc/lexicon.h"
+ #include "sherpa-onnx/csrc/macros.h"
++#include "sherpa-onnx/csrc/matcha-tts-lexicon.h"
+ #include "sherpa-onnx/csrc/melo-tts-lexicon.h"
+ #include "sherpa-onnx/csrc/offline-tts-character-frontend.h"
+ #include "sherpa-onnx/csrc/offline-tts-frontend.h"
+@@ -32,8 +34,26 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+  public:
+   explicit OfflineTtsMatchaImpl(const OfflineTtsConfig &config)
+       : config_(config),
+-        model_(std::make_unique<OfflineTtsMatchaModel>(config.model)),
+-        vocoder_(Vocoder::Create(config.model)) {
++        model_(std::make_unique<OfflineTtsMatchaModel>(config.model)) {
++    const auto &meta_data = model_->GetMetaData();
++    if (meta_data.need_vocoder) {
++      if (config.model.matcha.vocoder.empty()) {
++        SHERPA_ONNX_LOGE("Please provide vocoder for this model");
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      if (!FileExists(config.model.matcha.vocoder)) {
++        SHERPA_ONNX_LOGE("Please vocoder '%s' does not exist",
++                         config.model.matcha.vocoder.c_str());
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      vocoder_ = Vocoder::Create(config.model);
++    } else if (!config.model.matcha.vocoder.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You don't need to provide vocoder for this model. Ignore it");
++    }
++
+     InitFrontend();
+ 
+     if (!config.rule_fsts.empty()) {
+@@ -84,13 +104,38 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+         SHERPA_ONNX_LOGE("FST archives loaded!");
+       }
+     }
++
++    if (meta_data.sample_rate == 16000 && meta_data.is_zh_en == 1) {
++      if (!Contains(config.model.matcha.vocoder, "16") &&
++          Contains(config.model.matcha.vocoder, "2")) {
++        SHERPA_ONNX_LOGE(
++            "This Chinese+English TTS model requires a 16khz Vocoder.");
++        SHERPA_ONNX_LOGE("You should use vocos-16khz-univ.onnx.");
++        SHERPA_ONNX_LOGE(
++            "Please re-download a vocoder from "
++            "https://github.com/k2-fsa/sherpa-onnx/releases/tag/"
++            "vocoder-models.");
++      }
++    }
+   }
+ 
+   template <typename Manager>
+   OfflineTtsMatchaImpl(Manager *mgr, const OfflineTtsConfig &config)
+       : config_(config),
+-        model_(std::make_unique<OfflineTtsMatchaModel>(mgr, config.model)),
+-        vocoder_(Vocoder::Create(mgr, config.model)) {
++        model_(std::make_unique<OfflineTtsMatchaModel>(mgr, config.model)) {
++    const auto &meta_data = model_->GetMetaData();
++    if (meta_data.need_vocoder) {
++      if (config.model.matcha.vocoder.empty()) {
++        SHERPA_ONNX_LOGE("Please provide vocoder for this model");
++        SHERPA_ONNX_EXIT(-1);
++      }
++
++      vocoder_ = Vocoder::Create(mgr, config.model);
++    } else if (!config.model.matcha.vocoder.empty()) {
++      SHERPA_ONNX_LOGE(
++          "You don't need to provide vocoder for this model. Ignore it");
++    }
++
+     InitFrontend(mgr);
+ 
+     if (!config.rule_fsts.empty()) {
+@@ -142,6 +187,19 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+         }  // for (; !reader->Done(); reader->Next())
+       }  // for (const auto &f : files)
+     }  // if (!config.rule_fars.empty())
++
++    if (meta_data.sample_rate == 16000 && meta_data.is_zh_en == 1) {
++      if (!Contains(config.model.matcha.vocoder, "16") &&
++          Contains(config.model.matcha.vocoder, "2")) {
++        SHERPA_ONNX_LOGE(
++            "This Chinese+English TTS model requires a 16khz Vocoder.");
++        SHERPA_ONNX_LOGE("You should use vocos-16khz-univ.onnx.");
++        SHERPA_ONNX_LOGE(
++            "Please re-download a vocoder from "
++            "https://github.com/k2-fsa/sherpa-onnx/releases/tag/"
++            "vocoder-models.");
++      }
++    }
+   }
+ 
+   int32_t SampleRate() const override {
+@@ -325,7 +383,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+     // from assets to disk
+     const auto &meta_data = model_->GetMetaData();
+ 
+-    if (meta_data.jieba || meta_data.voice == "zh en-us") {
++    if (meta_data.is_zh_en) {
++      frontend_ = std::make_unique<MatchaTtsLexicon>(
++          mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
++          config_.model.matcha.data_dir, config_.model.debug);
++    } else if (meta_data.jieba) {
+       frontend_ = std::make_unique<CharacterLexicon>(
+           mgr, config_.model.matcha.lexicon, config_.model.matcha.tokens,
+           config_.model.debug);
+@@ -334,7 +396,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+           mgr, config_.model.matcha.tokens, config_.model.matcha.data_dir,
+           meta_data);
+     } else {
+-      SHERPA_ONNX_LOGE("jieba + espeaker-ng is not supported yet");
++      SHERPA_ONNX_LOGE("Unsupported matcha tts model. Please ask for help");
+       SHERPA_ONNX_EXIT(-1);
+     }
+   }
+@@ -342,7 +404,11 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+   void InitFrontend() {
+     const auto &meta_data = model_->GetMetaData();
+ 
+-    if (meta_data.jieba || meta_data.voice == "zh en-us") {
++    if (meta_data.is_zh_en) {
++      frontend_ = std::make_unique<MatchaTtsLexicon>(
++          config_.model.matcha.lexicon, config_.model.matcha.tokens,
++          config_.model.matcha.data_dir, config_.model.debug);
++    } else if (meta_data.jieba) {
+       frontend_ = std::make_unique<CharacterLexicon>(
+           config_.model.matcha.lexicon, config_.model.matcha.tokens,
+           config_.model.debug);
+@@ -351,7 +417,7 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+           config_.model.matcha.tokens, config_.model.matcha.data_dir,
+           meta_data);
+     } else {
+-      SHERPA_ONNX_LOGE("jieba + espeaker-ng is not supported yet");
++      SHERPA_ONNX_LOGE("Unsupported matcha tts model. Please ask for help");
+       SHERPA_ONNX_EXIT(-1);
+     }
+   }
+@@ -376,11 +442,24 @@ class OfflineTtsMatchaImpl : public OfflineTtsImpl {
+     Ort::Value x_tensor = Ort::Value::CreateTensor(
+         memory_info, x.data(), x.size(), x_shape.data(), x_shape.size());
+ 
++    GeneratedAudio ans;
++
+     Ort::Value mel = model_->Run(std::move(x_tensor), sid, speed);
+ 
+-    GeneratedAudio ans;
++    const auto &meta_data = model_->GetMetaData();
++    if (meta_data.need_vocoder) {
++      ans.samples = vocoder_->Run(std::move(mel));
++    } else {
++      std::vector<int64_t> shape = mel.GetTensorTypeAndShapeInfo().GetShape();
++      int64_t num_samples = 1;
++      for (auto s : shape) {
++        num_samples *= s;
++      }
++      ans.samples.resize(num_samples);
++      auto p = mel.GetTensorData<float>();
++      std::copy(p, p + num_samples, ans.samples.data());
++    }
+ 
+-    ans.samples = vocoder_->Run(std::move(mel));
+     ans.sample_rate = model_->GetMetaData().sample_rate;
+ 
+     float silence_scale = config_.silence_scale;
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+index 10c66fc6..1fdbad53 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model-config.cc
+@@ -42,16 +42,6 @@ bool OfflineTtsMatchaModelConfig::Validate() const {
+     return false;
+   }
+ 
+-  if (vocoder.empty()) {
+-    SHERPA_ONNX_LOGE("Please provide --matcha-vocoder");
+-    return false;
+-  }
+-
+-  if (!FileExists(vocoder)) {
+-    SHERPA_ONNX_LOGE("--matcha-vocoder: '%s' does not exist", vocoder.c_str());
+-    return false;
+-  }
+-
+   if (tokens.empty()) {
+     SHERPA_ONNX_LOGE("Please provide --matcha-tokens");
+     return false;
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h b/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
+index b4ec6c4d..1e83e16e 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model-meta-data.h
+@@ -22,6 +22,8 @@ struct OfflineTtsMatchaModelMetaData {
+   int32_t use_eos_bos = 0;
+   int32_t pad_id = 0;
+   int32_t add_blank = 1;
++  int32_t is_zh_en = 0;
++  bool need_vocoder = true;
+ 
+   std::string voice;
+ };
+diff --git a/sherpa-onnx/csrc/offline-tts-matcha-model.cc b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+index 992109b3..316a8a5f 100644
+--- a/sherpa-onnx/csrc/offline-tts-matcha-model.cc
++++ b/sherpa-onnx/csrc/offline-tts-matcha-model.cc
+@@ -171,6 +171,11 @@ class OfflineTtsMatchaModel::Impl {
+       // for models from
+       // https://modelscope.cn/models/dengcunqin/matcha_tts_zh_en_20251010
+       meta_data_.add_blank = 0;
++      meta_data_.is_zh_en = 1;
++    }
++
++    if (output_names_.front() == "audio_output") {
++      meta_data_.need_vocoder = false;
+     }
+   }
+ 
+diff --git a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+index 2a3c3daa..21200dd3 100644
+--- a/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
++++ b/sherpa-onnx/csrc/piper-phonemize-lexicon.cc
+@@ -92,7 +92,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
+     iss >> std::ws;
+     if (!iss.eof()) {
+       SHERPA_ONNX_LOGE("Error when reading tokens: %s", line.c_str());
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ 
+     s = conv.from_bytes(sym);
+@@ -105,7 +105,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
+ 
+       SHERPA_ONNX_LOGE("Error when reading tokens at Line %s. size: %d",
+                        line.c_str(), static_cast<int32_t>(s.size()));
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ 
+     char32_t c = s[0];
+@@ -113,7 +113,7 @@ static std::unordered_map<char32_t, int32_t> ReadTokens(std::istream &is) {
+     if (token2id.count(c)) {
+       SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
+                        sym.c_str(), line.c_str(), token2id.at(c));
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ 
+     token2id.insert({c, id});
+@@ -322,7 +322,7 @@ void InitEspeak(const std::string &data_dir) {
+           "Failed to initialize espeak-ng with data dir: %s. Return code is: "
+           "%d",
+           data_dir.c_str(), result);
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+   });
+ }
+@@ -541,7 +541,7 @@ std::vector<TokenIDs> PiperPhonemizeLexicon::ConvertTextToTokenIdsVits(
+ 
+   } else {
+     SHERPA_ONNX_LOGE("Unsupported model");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+ 
+   return ans;
+diff --git a/sherpa-onnx/csrc/symbol-table.cc b/sherpa-onnx/csrc/symbol-table.cc
+index 15fdf125..2bc2c7f4 100644
+--- a/sherpa-onnx/csrc/symbol-table.cc
++++ b/sherpa-onnx/csrc/symbol-table.cc
+@@ -130,14 +130,14 @@ std::unordered_map<std::string, int32_t> ReadTokens(
+     iss >> std::ws;
+     if (!iss.eof()) {
+       SHERPA_ONNX_LOGE("Error: %s", line.c_str());
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ 
+ #if 0
+     if (token2id.count(sym)) {
+       SHERPA_ONNX_LOGE("Duplicated token %s. Line %s. Existing ID: %d",
+                        sym.c_str(), line.c_str(), token2id.at(sym));
+-      exit(-1);
++      SHERPA_ONNX_EXIT(-1);
+     }
+ #endif
+     if (id2token) {
+diff --git a/sherpa-onnx/csrc/text-utils.cc b/sherpa-onnx/csrc/text-utils.cc
+index 020c6041..760ac329 100644
+--- a/sherpa-onnx/csrc/text-utils.cc
++++ b/sherpa-onnx/csrc/text-utils.cc
+@@ -16,6 +16,7 @@
+ #include <sstream>
+ #include <string>
+ #include <unordered_map>
++#include <unordered_set>
+ #include <utility>
+ #include <vector>
+ 
+@@ -708,6 +709,14 @@ bool EndsWith(const std::string &haystack, const std::string &needle) {
+   return std::equal(needle.rbegin(), needle.rend(), haystack.rbegin());
+ }
+ 
++bool Contains(const std::string &haystack, const std::string &needle) {
++  if (needle.size() > haystack.size()) {
++    return false;
++  }
++
++  return haystack.find(needle) != std::string::npos;
++}
++
+ std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size) {
+   std::vector<std::string> ans;
+   if (chunk_size < 1 || chunk_size > s.size()) {
+@@ -799,4 +808,12 @@ std::string GetWord(const std::vector<std::string> &words, int32_t start,
+   return ans;
+ }
+ 
++bool IsPunct(const std::string &s) {
++  static const std::unordered_set<std::string> puncts = {
++      ",",  ".",  "!",  "?", ":", "\"", "'", "",
++      "", "", "", "", "", "",  "",
++  };
++  return puncts.count(s);
++}
++
+ }  // namespace sherpa_onnx
+diff --git a/sherpa-onnx/csrc/text-utils.h b/sherpa-onnx/csrc/text-utils.h
+index 369ba2e6..6f56dc53 100644
+--- a/sherpa-onnx/csrc/text-utils.h
++++ b/sherpa-onnx/csrc/text-utils.h
+@@ -147,6 +147,8 @@ std::string ToString(const std::wstring &s);
+ 
+ bool EndsWith(const std::string &haystack, const std::string &needle);
+ 
++bool Contains(const std::string &haystack, const std::string &needle);
++
+ std::vector<std::string> SplitString(const std::string &s, int32_t chunk_size);
+ 
+ // Converts a UTF-8 std::string to a UTF-32 std::u32string
+@@ -176,6 +178,8 @@ bool StringToBool(const std::string &s);
+ std::string GetWord(const std::vector<std::string> &words, int32_t start,
+                     int32_t end);
+ 
++bool IsPunct(const std::string &s);
++
+ }  // namespace sherpa_onnx
+ 
+ #endif  // SHERPA_ONNX_CSRC_TEXT_UTILS_H_
+diff --git a/sherpa-onnx/csrc/vocoder.cc b/sherpa-onnx/csrc/vocoder.cc
+index 38c1e4f7..d9821cec 100644
+--- a/sherpa-onnx/csrc/vocoder.cc
++++ b/sherpa-onnx/csrc/vocoder.cc
+@@ -76,15 +76,12 @@ static ModelType GetModelType(char *model_data, size_t model_data_length,
+ std::unique_ptr<Vocoder> Vocoder::Create(const OfflineTtsModelConfig &config) {
+   std::vector<char> buffer;
+   if (!config.matcha.vocoder.empty()) {
+-    SHERPA_ONNX_LOGE("Using matcha vocoder: %s", config.matcha.vocoder.c_str());
+     buffer = ReadFile(config.matcha.vocoder);
+   } else if (!config.zipvoice.vocoder.empty()) {
+-    SHERPA_ONNX_LOGE("Using zipvoice vocoder: %s",
+-                     config.zipvoice.vocoder.c_str());
+     buffer = ReadFile(config.zipvoice.vocoder);
+   } else {
+     SHERPA_ONNX_LOGE("No vocoder model provided in the config!");
+-    exit(-1);
++    SHERPA_ONNX_EXIT(-1);
+   }
+   auto model_type = GetModelType(buffer.data(), buffer.size(), config.debug);
+ 
+
+commit 9beac05020a484cbf626d1305d95b8070af8479f
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 10 14:15:32 2025 +0800
+
+    Export models to Ascend 910B3 (#2761)
+
+diff --git a/.github/workflows/export-paraformer-to-ascend-npu.yaml b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+index da9a3d2a..2fbd89f1 100644
+--- a/.github/workflows/export-paraformer-to-ascend-npu.yaml
++++ b/.github/workflows/export-paraformer-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-paraformer-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - ascend-npu-acl-api
++      - ascend-910b3
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -62,6 +62,27 @@ jobs:
+             framework: "WSChuan-ASR"
+             cann: "8.2"
+ 
++          # ===== Ascend 910B3 =====
++          - soc_version: "910B3"
++            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
++            framework: "FunASR"
++            cann: "8.0"
++
++          - soc_version: "910B3"
++            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
++            framework: "WSChuan-ASR"
++            cann: "8.0"
++
++          - soc_version: "910B3"
++            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
++            framework: "FunASR"
++            cann: "8.2"
++
++          - soc_version: "910B3"
++            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
++            framework: "WSChuan-ASR"
++            cann: "8.2"
++
+           # ===== Ascend 310 =====
+           - soc_version: "310P3"
+             image: "gpustack/devel-ascendai-cann:8.0.rc3.beta1-310p-ubuntu20.04-v2"
+@@ -339,7 +360,7 @@ jobs:
+           overwrite: true
+           repo_name: k2-fsa/sherpa-onnx
+           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: asr-models
++          tag: asr-models-ascend
+ 
+       - name: Release
+         if: github.repository_owner == 'k2-fsa'
+@@ -348,4 +369,4 @@ jobs:
+           file_glob: true
+           file: ./*.tar.bz2
+           overwrite: true
+-          tag: asr-models
++          tag: asr-models-ascend
+diff --git a/.github/workflows/export-sense-voice-to-ascend-npu.yaml b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+index df235fe1..4f10df79 100644
+--- a/.github/workflows/export-sense-voice-to-ascend-npu.yaml
++++ b/.github/workflows/export-sense-voice-to-ascend-npu.yaml
+@@ -3,7 +3,7 @@ name: export-sense-voice-to-ascend-npu
+ on:
+   push:
+     branches:
+-      - ascend-npu-310
++      - ascend-910b3
+   workflow_dispatch:
+ 
+ concurrency:
+@@ -62,6 +62,27 @@ jobs:
+             framework: "WSYue-ASR"
+             cann: "8.2"
+ 
++          # ===== Ascend 910B3 =====
++          - soc_version: "910B3"
++            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
++            framework: "FunASR"
++            cann: "8.0"
++
++          - soc_version: "910B3"
++            image: "gpustack/ascendai-cann:8.0.RC3-910b-ubuntu20.04-py3.9"
++            framework: "WSYue-ASR"
++            cann: "8.0"
++
++          - soc_version: "910B3"
++            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
++            framework: "FunASR"
++            cann: "8.2"
++
++          - soc_version: "910B3"
++            image: "gpustack/devel-ascendai-cann:8.2.rc1-910b-ubuntu20.04-v2"
++            framework: "WSYue-ASR"
++            cann: "8.2"
++
+ 
+           # ===== Ascend 310 =====
+           - soc_version: "310P3"
+@@ -279,7 +300,7 @@ jobs:
+           overwrite: true
+           repo_name: k2-fsa/sherpa-onnx
+           repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
+-          tag: asr-models
++          tag: asr-models-ascend
+ 
+       - name: Release
+         if: github.repository_owner == 'k2-fsa'
+@@ -288,4 +309,4 @@ jobs:
+           file_glob: true
+           file: ./*.tar.bz2
+           overwrite: true
+-          tag: asr-models
++          tag: asr-models-ascend
+
+commit 049d525cb39f2dd7334f6d407c841caa071c5182
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 10 11:52:20 2025 +0800
+
+    Export sense voice to qnn (#2760)
+
+diff --git a/.github/workflows/export-sense-voice-to-qnn.yaml b/.github/workflows/export-sense-voice-to-qnn.yaml
+new file mode 100644
+index 00000000..73c8a4ec
+--- /dev/null
++++ b/.github/workflows/export-sense-voice-to-qnn.yaml
+@@ -0,0 +1,475 @@
++name: export-sense-voice-to-qnn
++
++on:
++  push:
++    branches:
++      - export-sense-voice-qnn-2
++  workflow_dispatch:
++
++concurrency:
++  group: export-sense-voice-to-qnn-${{ github.ref }}
++  cancel-in-progress: true
++
++jobs:
++  export-sense-voice-to-qnn:
++    if: github.repository_owner == 'k2-fsa' || github.repository_owner == 'csukuangfj'
++    name: ${{ matrix.framework }} ${{ matrix.input_in_seconds }}
++    runs-on: ${{ matrix.os }}
++    strategy:
++      fail-fast: false
++      matrix:
++        os: [ubuntu-22.04]
++        python-version: ["3.10"]
++        input_in_seconds: ["5", "8", "10", "13", "15", "18", "20", "23", "25", "28", "30"]
++        framework: ["FunASR", "WSYue-ASR"]
++
++    steps:
++      - uses: actions/checkout@v4
++
++      - name: Setup Python ${{ matrix.python-version }}
++        uses: actions/setup-python@v5
++        with:
++          python-version: ${{ matrix.python-version }}
++
++      - name: Display NDK HOME
++        shell: bash
++        run: |
++          echo "ANDROID_NDK_LATEST_HOME: ${ANDROID_NDK_LATEST_HOME}"
++          ls -lh ${ANDROID_NDK_LATEST_HOME}
++
++      - name: Create Python virtual environment
++        shell: bash
++        run: |
++          python3 -m venv py310
++          which python3
++          source py310/bin/activate
++          which python3
++
++      - name: Show ndk-build help
++        shell: bash
++        run: |
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          ndk-build --help
++
++      - name: Download toolkit
++        shell: bash
++        run: |
++          curl -SL -O https://huggingface.co/csukuangfj/qnn-toolkit/resolve/main/v2.33.0.250327.zip
++          ls -lh v2.33.0.250327.zip
++
++      - name: Unzip toolkit
++        shell: bash
++        run: |
++          unzip v2.33.0.250327.zip
++
++      - name: Show
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---ls -lh qairt---"
++
++          ls -lh qairt
++
++          echo "---"
++
++      - name: Install linux dependencies
++        shell: bash
++        run: |
++          ls -lh
++
++          echo "---"
++
++          ls -lh qairt
++
++          cd qairt/2.33.0.250327/bin
++          source envsetup.sh
++
++          yes | sudo ${QNN_SDK_ROOT}/bin/check-linux-dependency.sh || true
++
++      - name: Install Python dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          cd qairt/2.33.0.250327/bin
++          source envsetup.sh
++
++          python3 -m pip install \
++            mock \
++            numpy \
++            opencv-python \
++            optuna \
++            packaging \
++            pandas \
++            paramiko \
++            pathlib2 \
++            pillow \
++            plotly \
++            protobuf \
++            psutil \
++            pydantic \
++            pytest \
++            pyyaml \
++            rich \
++            scikit-optimize \
++            scipy \
++            six \
++            tabulate \
++            typing-extensions \
++            xlsxwriter
++
++          python3 "${QNN_SDK_ROOT}/bin/check-python-dependency" || true
++
++          which python3
++
++      - name: Install onnx dependencies
++        shell: bash
++        run: |
++          source py310/bin/activate
++          python3 -m pip install --upgrade \
++            torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch \
++            kaldi_native_fbank \
++            pip \
++            "numpy<2" \
++            onnx==1.17.0 \
++            onnxruntime==1.17.1 \
++            soundfile \
++            librosa \
++            onnxsim \
++            sentencepiece \
++            pyyaml
++
++          which python3
++
++      - name: Show qnn-onnx-converter help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-onnx-converter --help
++
++      - name: Show qnn-model-lib-generator help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-model-lib-generator --help
++
++      - name: Show qnn-net-run help
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          qnn-net-run --help
++
++      - name: Run SenseVoice from FunAsr
++        if: matrix.framework == 'FunASR'
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++
++          cd scripts/sense-voice/qnn
++
++          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
++          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/model.pt
++          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ja.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/ko.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav
++
++          rm -f README.md || true
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/README.md
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/LICENSE
++
++          echo "export to onnx"
++          t=${{ matrix.input_in_seconds }}
++
++          echo "----$t---"
++          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 17
++
++          ls -lh *.onnx
++
++          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
++
++          echo "test exported onnx models"
++
++          echo "----------$t----------"
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ja.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./ko.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav
++
++          echo "export to qnn"
++          echo "----------$t----------"
++          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")
++
++          echo "num_frames: $num_frames"
++
++          ./generate_test_data.py  --num-frames $num_frames --wav ./zh.wav
++          mv input0.raw zh-input0.raw
++          mv input1.raw zh-input1.raw
++          echo "zh-input0.raw zh-input1.raw" > input_list.txt
++
++          for w in ja ko en yue; do
++            ./generate_test_data.py  --num-frames $num_frames --wav ./$w.wav
++            mv input0.raw $w-input0.raw
++            mv input1.raw $w-input1.raw
++            echo "$w-input0.raw $w-input1.raw" >> input_list.txt
++          done
++
++          cat ./input_list.txt
++
++          qnn-onnx-converter \
++            --input_network model-$t-seconds.onnx \
++            --output_path ./model-$t-seconds-quantized \
++            --out_node logits \
++            --input_list ./input_list.txt \
++            --use_native_input_files  \
++            --input_dtype x float32 \
++            --input_dtype prompt int32 \
++            --act_bitwidth 16 \
++            --bias_bitwidth 32 \
++            --input_layout x NTF
++          ls -lh
++          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
++          echo "----"
++          ls -lh
++
++          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++            -c "model-$t-seconds-quantized.cpp" \
++            -b "model-$t-seconds-quantized.bin" \
++            -o model_libs > /dev/null 2>&1
++
++          ls -lh model_libs/*/
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "collect results"
++
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v README.md $d
++            cp -v LICENSE $d
++            cp -v model_libs/$p/lib*.so $d/libmodel.so
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++
++            echo "num_frames=$num_frames" > $d/info.txt
++            echo "target=$p" >> $d/info.txt
++
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../../..
++
++      - name: Run SenseVoice from WSYue-ASR
++        if: matrix.framework == 'WSYue-ASR'
++        shell: bash
++        run: |
++          source py310/bin/activate
++
++          pushd qairt/2.33.0.250327/bin
++          source envsetup.sh
++          popd
++
++          export PATH=${ANDROID_NDK_LATEST_HOME}:$PATH
++          export LDFLAGS="-Wl,-z,max-page-size=16384"
++
++          cd scripts/sense-voice/qnn
++
++          curl -SL -O https://huggingface.co/ASLP-lab/WSYue-ASR/resolve/main/sensevoice_small_yue/model.pt
++
++          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/am.mvn
++          curl -SL -O https://hf-mirror.com/FunAudioLLM/SenseVoiceSmall/resolve/main/chn_jpn_yue_eng_ko_spectok.bpe.model
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/en.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/yue.wav
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/resolve/main/test_wavs/zh.wav
++
++          for i in $(seq 0 17); do
++            curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/test_wavs/yue-$i.wav
++          done
++
++          rm -f README.md || true
++
++          curl -SL -O https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2025-09-09/resolve/main/README.md
++
++          echo "export to onnx"
++          t=${{ matrix.input_in_seconds }}
++
++          echo "----$t---"
++
++          export model_author="ASLP-lab"
++          export comment="ASLP-lab/WSYue-ASR"
++          export url="https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/sensevoice_small_yue"
++
++          python3 ./export-onnx.py --input-len-in-seconds $t --opset-version 17
++
++          ls -lh *.onnx
++
++          python3 ../../pyannote/segmentation/show-onnx.py --filename ./model-$t-seconds.onnx
++
++          echo "test exported onnx models"
++
++          echo "----------$t----------"
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./en.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue.wav
++          python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./zh.wav
++
++          for i in $(seq 0 17); do
++            echo "yue-$i.wav"
++            python3 ./test_onnx.py --model model-$t-seconds.onnx --tokens ./tokens.txt --wave ./yue-$i.wav
++          done
++
++          echo "export to qnn"
++          echo "----------$t----------"
++          num_frames=$(python3 -c "print(int($t*100 / 6 + 0.5))")
++
++          echo "num_frames: $num_frames"
++
++          ./generate_test_data.py  --num-frames $num_frames --wav ./zh.wav
++          mv input0.raw zh-input0.raw
++          mv input1.raw zh-input1.raw
++          echo "zh-input0.raw zh-input1.raw" > input_list.txt
++
++          for w in en yue; do
++            ./generate_test_data.py  --num-frames $num_frames --wav ./$w.wav
++            mv input0.raw $w-input0.raw
++            mv input1.raw $w-input1.raw
++            echo "$w-input0.raw $w-input1.raw" >> input_list.txt
++          done
++
++          for i in $(seq 0 17); do
++            echo "yue-$i.wav"
++            ./generate_test_data.py  --num-frames $num_frames --wav ./yue-$i.wav
++            mv input0.raw $i-input0.raw
++            mv input1.raw $i-input1.raw
++            echo "$i-input0.raw $i-input1.raw" >> input_list.txt
++          done
++
++          cat ./input_list.txt
++
++          qnn-onnx-converter \
++            --input_network model-$t-seconds.onnx \
++            --output_path ./model-$t-seconds-quantized \
++            --out_node logits \
++            --input_list ./input_list.txt \
++            --use_native_input_files  \
++            --input_dtype x float32 \
++            --input_dtype prompt int32 \
++            --act_bitwidth 16 \
++            --bias_bitwidth 32 \
++            --input_layout x NTF
++          ls -lh
++          mv model-$t-seconds-quantized model-$t-seconds-quantized.cpp
++          echo "----"
++          ls -lh
++
++          python3 "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator" \
++            -c "model-$t-seconds-quantized.cpp" \
++            -b "model-$t-seconds-quantized.bin" \
++            -o model_libs > /dev/null 2>&1
++
++          ls -lh model_libs/*/
++
++          readelf -lW model_libs/*/lib*.so
++
++          echo "collect results"
++          for p in x86_64-linux-clang aarch64-android; do
++            if [[ $p == x86_64-linux-clang ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8-linux-x64
++            elif [[ $p == aarch64-android ]]; then
++              d=sherpa-onnx-qnn-$t-seconds-sense-voice-zh-en-ja-ko-yue-2025-09-09-int8-android-aarch64
++            else
++              echo "Unknown $p"
++              exit -1
++            fi
++
++            mkdir -p $d
++            mkdir -p $d/test_wavs
++
++            cp -v README.md $d
++            cp -v model_libs/$p/lib*.so $d/libmodel.so
++            cp -v tokens.txt $d
++            cp -v *.wav $d/test_wavs
++
++            echo "num_frames=$num_frames" > $d/info.txt
++            echo "target=$p" >> $d/info.txt
++
++            ls -lh $d
++            tar cjfv $d.tar.bz2 $d
++            ls -lh *.tar.bz2
++            rm -rf $d
++          done
++
++          echo "----show---"
++          ls -lh *.tar.bz2
++
++          mv *.tar.bz2 ../../..
++
++      - uses: actions/upload-artifact@v4
++        with:
++          name: ${{ matrix.framework }}-${{ matrix.input_in_seconds }}-seconds
++          path: ./scripts/sense-voice/qnn/*.json
++
++      - name: Release
++        if: github.repository_owner == 'csukuangfj'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          repo_name: k2-fsa/sherpa-onnx
++          repo_token: ${{ secrets.UPLOAD_GH_SHERPA_ONNX_TOKEN }}
++          tag: asr-models-qnn
++
++      - name: Release
++        if: github.repository_owner == 'k2-fsa'
++        uses: svenstaro/upload-release-action@v2
++        with:
++          file_glob: true
++          file: ./*.tar.bz2
++          overwrite: true
++          tag: asr-models-qnn
+diff --git a/scripts/sense-voice/ascend-npu/test_om.py b/scripts/sense-voice/ascend-npu/test_om.py
+index ff6a11c1..29c6187b 100755
+--- a/scripts/sense-voice/ascend-npu/test_om.py
++++ b/scripts/sense-voice/ascend-npu/test_om.py
+@@ -1,13 +1,11 @@
+ #!/usr/bin/env python3
+ # Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
+ 
+-import argparse
+ from typing import Tuple
+ 
+ import kaldi_native_fbank as knf
+ import numpy as np
+ import soundfile as sf
+-import torch
+ from ais_bench.infer.interface import InferSession
+ 
+ 
+diff --git a/scripts/sense-voice/qnn/.gitignore b/scripts/sense-voice/qnn/.gitignore
+new file mode 100644
+index 00000000..e51c42e9
+--- /dev/null
++++ b/scripts/sense-voice/qnn/.gitignore
+@@ -0,0 +1 @@
++*.raw
+diff --git a/scripts/sense-voice/qnn/decode_logits.py b/scripts/sense-voice/qnn/decode_logits.py
+new file mode 100755
+index 00000000..27047940
+--- /dev/null
++++ b/scripts/sense-voice/qnn/decode_logits.py
+@@ -0,0 +1,34 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++import numpy as np
++
++
++def load_tokens(filename):
++    ans = dict()
++    i = 0
++    with open(filename, encoding="utf-8") as f:
++        for line in f:
++            ans[i] = line.strip().split()[0]
++            i += 1
++    return ans
++
++
++logits = np.fromfile("./logits.raw", dtype=np.float32).reshape((-1, 25055))
++
++idx = logits.argmax(axis=-1)
++print("idx", idx)
++print(len(idx))
++prev = -1
++ids = []
++for i in idx:
++    if i != prev:
++        ids.append(i)
++    prev = i
++ids = [i for i in ids if i != 0]
++print(ids)
++
++tokens = load_tokens("./tokens.txt")
++text = "".join([tokens[i] for i in ids])
++
++text = text.replace("_", " ")
++print(text)
+diff --git a/scripts/sense-voice/qnn/export-onnx.py b/scripts/sense-voice/qnn/export-onnx.py
+new file mode 120000
+index 00000000..06c05488
+--- /dev/null
++++ b/scripts/sense-voice/qnn/export-onnx.py
+@@ -0,0 +1 @@
++../rknn/export-onnx.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/qnn/generate_test_data.py b/scripts/sense-voice/qnn/generate_test_data.py
+new file mode 100755
+index 00000000..e816f536
+--- /dev/null
++++ b/scripts/sense-voice/qnn/generate_test_data.py
+@@ -0,0 +1,119 @@
++#!/usr/bin/env python3
++# Copyright      2025  Xiaomi Corp.        (authors: Fangjun Kuang)
++
++import argparse
++from typing import Tuple
++
++import kaldi_native_fbank as knf
++import numpy as np
++import soundfile as sf
++
++
++def get_args():
++    parser = argparse.ArgumentParser(
++        formatter_class=argparse.ArgumentDefaultsHelpFormatter
++    )
++
++    parser.add_argument(
++        "--num-frames",
++        type=int,
++        required=True,
++    )
++
++    parser.add_argument(
++        "--wav",
++        type=str,
++        required=True,
++    )
++    return parser.parse_args()
++
++
++def load_audio(filename: str) -> Tuple[np.ndarray, int]:
++    data, sample_rate = sf.read(
++        filename,
++        always_2d=True,
++        dtype="float32",
++    )
++    data = data[:, 0]  # use only the first channel
++    samples = np.ascontiguousarray(data)
++    return samples, sample_rate
++
++
++def compute_feat(
++    samples,
++    sample_rate,
++    window_size: int = 7,  # lfr_m
++    window_shift: int = 6,  # lfr_n
++):
++    opts = knf.FbankOptions()
++    opts.frame_opts.dither = 0
++    opts.frame_opts.snip_edges = False
++    opts.frame_opts.window_type = "hamming"
++    opts.frame_opts.samp_freq = sample_rate
++    opts.mel_opts.num_bins = 80
++
++    online_fbank = knf.OnlineFbank(opts)
++    online_fbank.accept_waveform(sample_rate, (samples * 32768).tolist())
++    online_fbank.input_finished()
++
++    features = np.stack(
++        [online_fbank.get_frame(i) for i in range(online_fbank.num_frames_ready)]
++    )
++    assert features.data.contiguous is True
++    assert features.dtype == np.float32, features.dtype
++
++    T = (features.shape[0] - window_size) // window_shift + 1
++    features = np.lib.stride_tricks.as_strided(
++        features,
++        shape=(T, features.shape[1] * window_size),
++        strides=((window_shift * features.shape[1]) * 4, 4),
++    )
++
++    return np.copy(features)
++
++
++def main():
++    args = get_args()
++    print(vars(args))
++
++    samples, sample_rate = load_audio(args.wav)
++    if sample_rate != 16000:
++        import librosa
++
++        samples = librosa.resample(samples, orig_sr=sample_rate, target_sr=16000)
++        sample_rate = 16000
++
++    features = compute_feat(
++        samples=samples,
++        sample_rate=sample_rate,
++    )
++    print("features.shape", features.shape)
++    if features.shape[0] > args.num_frames:
++        features = features[: args.num_frames]
++    elif features.shape[0] < args.num_frames:
++        pad_width = ((0, args.num_frames - features.shape[0]), (0, 0))
++        features = np.pad(features, pad_width, mode="constant", constant_values=0)
++
++    features.tofile("input0.raw")
++
++    language_auto = 0
++    language_zh = 3
++    language_en = 4
++    language_yue = 7
++    language_ya = 11
++    language_ko = 12
++    language_nospeech = 13
++
++    language = language_auto
++
++    with_itn = 14
++    without_itn = 15
++
++    text_norm = with_itn
++
++    prompt = np.array([language, 1, 2, text_norm], dtype=np.int32)
++    prompt.tofile("input1.raw")
++
++
++if __name__ == "__main__":
++    main()
+diff --git a/scripts/sense-voice/qnn/test_onnx.py b/scripts/sense-voice/qnn/test_onnx.py
+new file mode 120000
+index 00000000..dfb3c92a
+--- /dev/null
++++ b/scripts/sense-voice/qnn/test_onnx.py
+@@ -0,0 +1 @@
++../rknn/test_onnx.py
+\ No newline at end of file
+diff --git a/scripts/sense-voice/rknn/export-onnx.py b/scripts/sense-voice/rknn/export-onnx.py
+index 6ee01c60..7a0161f8 100755
+--- a/scripts/sense-voice/rknn/export-onnx.py
++++ b/scripts/sense-voice/rknn/export-onnx.py
+@@ -25,6 +25,12 @@ def get_args():
+         how long the model can process.
+         """,
+     )
++
++    parser.add_argument(
++        "--opset-version",
++        type=int,
++        default=13,
++    )
+     return parser.parse_args()
+ 
+ 
+@@ -118,7 +124,7 @@ def main():
+     text_norm = 15
+     prompt = torch.tensor([language, 1, 2, text_norm], dtype=torch.int32)
+ 
+-    opset_version = 13
++    opset_version = args.opset_version
+     filename = f"model-{input_len_in_seconds}-seconds.onnx"
+     torch.onnx.export(
+         model,
+
+commit 42d20cccfd994583f921ab22405f24a2d528ef34
+Author: Fangjun Kuang <csukuangfj@gmail.com>
+Date:   Mon Nov 10 11:30:41 2025 +0800
+
+    Add cxx API for online punctuation models (#2759)
+
+diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
+index 891c8140..c35a92cf 100644
+--- a/.github/workflows/cxx-api.yaml
++++ b/.github/workflows/cxx-api.yaml
+@@ -78,6 +78,75 @@ jobs:
+             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
+           fi
+ 
++
++      - name: Test Online punctuation
++        shell: bash
++        run: |
++          name=online-punctuation-cxx-api
++          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
++            -I ./build/install/include \
++            -L ./build/install/lib/ \
++            -l sherpa-onnx-cxx-api \
++            -l sherpa-onnx-c-api \
++            -l onnxruntime
++
++          ls -lh $name
++
++          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
++            ls -lh ./$name
++            ldd ./$name
++            echo "----"
++            readelf -d ./$name
++          fi
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++          tar xvf sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++          rm sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++
++          echo "---"
++
++          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
++          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
++
++          ./$name
++
++          rm -rf sherpa-onnx-online-punct-*
++          rm -v ./$name
++
++      - name: Test Offline punctuation
++        shell: bash
++        run: |
++          name=offline-punctuation-cxx-api
++          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
++            -I ./build/install/include \
++            -L ./build/install/lib/ \
++            -l sherpa-onnx-cxx-api \
++            -l sherpa-onnx-c-api \
++            -l onnxruntime
++
++          ls -lh $name
++
++          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
++            ls -lh ./$name
++            ldd ./$name
++            echo "----"
++            readelf -d ./$name
++          fi
++
++          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++          tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++          rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++
++          echo "---"
++
++          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
++          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
++
++          ./$name
++
++          rm -rf sherpa-onnx-punct-*
++          rm -v ./$name
++
+       - name: Test CED audio tagging
+         shell: bash
+         run: |
+diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
+index 3e4104b5..6c47e0a1 100644
+--- a/cxx-api-examples/CMakeLists.txt
++++ b/cxx-api-examples/CMakeLists.txt
+@@ -42,8 +42,11 @@ target_link_libraries(wenet-ctc-cxx-api sherpa-onnx-cxx-api)
+ add_executable(nemo-canary-cxx-api ./nemo-canary-cxx-api.cc)
+ target_link_libraries(nemo-canary-cxx-api sherpa-onnx-cxx-api)
+ 
+-add_executable(punctuation-cxx-api ./punctuation-cxx-api.cc)
+-target_link_libraries(punctuation-cxx-api sherpa-onnx-cxx-api)
++add_executable(offline-punctuation-cxx-api ./offline-punctuation-cxx-api.cc)
++target_link_libraries(offline-punctuation-cxx-api sherpa-onnx-cxx-api)
++
++add_executable(online-punctuation-cxx-api ./online-punctuation-cxx-api.cc)
++target_link_libraries(online-punctuation-cxx-api sherpa-onnx-cxx-api)
+ 
+ if(SHERPA_ONNX_ENABLE_PORTAUDIO)
+   add_executable(sense-voice-simulate-streaming-microphone-cxx-api
+diff --git a/cxx-api-examples/punctuation-cxx-api.cc b/cxx-api-examples/offline-punctuation-cxx-api.cc
+similarity index 77%
+rename from cxx-api-examples/punctuation-cxx-api.cc
+rename to cxx-api-examples/offline-punctuation-cxx-api.cc
+index e803ff9a..f20a9fc3 100644
+--- a/cxx-api-examples/punctuation-cxx-api.cc
++++ b/cxx-api-examples/offline-punctuation-cxx-api.cc
+@@ -1,11 +1,12 @@
+-// cxx-api-examples/punctuation-cxx-api.cc
++// cxx-api-examples/offline-punctuation-cxx-api.cc
+ // Copyright (c)  2025  Xiaomi Corporation
+ 
+ // To use punctuation model:
+-// wget
+-// https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
+-// tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
+-// rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12.tar.bz2
++// clang-format off
++// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++// tar xvf sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++// rm sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8.tar.bz2
++// clang-format on
+ 
+ #include <iostream>
+ #include <string>
+@@ -17,8 +18,8 @@ int32_t main() {
+ 
+   OfflinePunctuationConfig punctuation_config;
+   punctuation_config.model.ct_transformer =
+-      "./sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12/"
+-      "model.onnx";
++      "./sherpa-onnx-punct-ct-transformer-zh-en-vocab272727-2024-04-12-int8/"
++      "model.int8.onnx";
+   punctuation_config.model.num_threads = 1;
+   punctuation_config.model.debug = false;
+   punctuation_config.model.provider = "cpu";
+diff --git a/cxx-api-examples/online-punctuation-cxx-api.cc b/cxx-api-examples/online-punctuation-cxx-api.cc
+new file mode 100644
+index 00000000..cbba6d86
+--- /dev/null
++++ b/cxx-api-examples/online-punctuation-cxx-api.cc
+@@ -0,0 +1,41 @@
++// cxx-api-examples/online-punctuation-cxx-api.cc
++// Copyright (c)  2025  Xiaomi Corporation
++
++// To use punctuation model:
++// clang-format off
++// wget https://github.com/k2-fsa/sherpa-onnx/releases/download/punctuation-models/sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++// tar xvf sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++// rm sherpa-onnx-online-punct-en-2024-08-06.tar.bz2
++// clang-format on
++
++#include <iostream>
++#include <string>
++
++#include "sherpa-onnx/c-api/cxx-api.h"
++
++int32_t main() {
++  using namespace sherpa_onnx::cxx;  // NOLINT
++
++  OnlinePunctuationConfig punctuation_config;
++  punctuation_config.model.cnn_bilstm =
++      "sherpa-onnx-online-punct-en-2024-08-06/model.int8.onnx";
++  punctuation_config.model.bpe_vocab =
++      "sherpa-onnx-online-punct-en-2024-08-06/bpe.vocab";
++  punctuation_config.model.num_threads = 1;
++  punctuation_config.model.debug = false;
++  punctuation_config.model.provider = "cpu";
++
++  OnlinePunctuation punct = OnlinePunctuation::Create(punctuation_config);
++  if (!punct.Get()) {
++    std::cerr
++        << "Failed to create punctuation model. Please check your config\n";
++    return -1;
++  }
++
++  std::string text = "how are you i am fine thank you";
++  std::string text_with_punct = punct.AddPunctuation(text);
++  std::cout << "Original text: " << text << std::endl;
++  std::cout << "With punctuation: " << text_with_punct << std::endl;
++
++  return 0;
++}
+diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
+index dffc1407..3b8f2e6e 100644
+--- a/sherpa-onnx/c-api/cxx-api.cc
++++ b/sherpa-onnx/c-api/cxx-api.cc
+@@ -836,6 +836,38 @@ std::string OfflinePunctuation::AddPunctuation(const std::string &text) const {
+   return ans;
+ }
+ 
++// ============================================================
++// For Online Punctuation
++// ============================================================
++OnlinePunctuation OnlinePunctuation::Create(
++    const OnlinePunctuationConfig &config) {
++  struct SherpaOnnxOnlinePunctuationConfig c;
++  memset(&c, 0, sizeof(c));
++  c.model.cnn_bilstm = config.model.cnn_bilstm.c_str();
++  c.model.bpe_vocab = config.model.bpe_vocab.c_str();
++  c.model.num_threads = config.model.num_threads;
++  c.model.debug = config.model.debug;
++  c.model.provider = config.model.provider.c_str();
++
++  const SherpaOnnxOnlinePunctuation *punct =
++      SherpaOnnxCreateOnlinePunctuation(&c);
++  return OnlinePunctuation(punct);
++}
++
++OnlinePunctuation::OnlinePunctuation(const SherpaOnnxOnlinePunctuation *p)
++    : MoveOnly<OnlinePunctuation, SherpaOnnxOnlinePunctuation>(p) {}
++
++void OnlinePunctuation::Destroy(const SherpaOnnxOnlinePunctuation *p) const {
++  SherpaOnnxDestroyOnlinePunctuation(p);
++}
++
++std::string OnlinePunctuation::AddPunctuation(const std::string &text) const {
++  const char *result = SherpaOnnxOnlinePunctuationAddPunct(p_, text.c_str());
++  std::string ans(result);
++  SherpaOnnxOnlinePunctuationFreeText(result);
++  return ans;
++}
++
+ // ============================================================
+ // For Audio tagging
+ // ============================================================
+diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
+index fa718db9..e5c9711d 100644
+--- a/sherpa-onnx/c-api/cxx-api.h
++++ b/sherpa-onnx/c-api/cxx-api.h
+@@ -735,6 +735,35 @@ class SHERPA_ONNX_API OfflinePunctuation
+   explicit OfflinePunctuation(const SherpaOnnxOfflinePunctuation *p);
+ };
+ 
++// ============================================================================
++// Online Punctuation
++// ============================================================================
++struct OnlinePunctuationModelConfig {
++  std::string cnn_bilstm;
++  std::string bpe_vocab;
++  int32_t num_threads = 1;
++  int32_t debug = false;
++  std::string provider = "cpu";
++};
++
++struct OnlinePunctuationConfig {
++  OnlinePunctuationModelConfig model;
++};
++
++class SHERPA_ONNX_API OnlinePunctuation
++    : public MoveOnly<OnlinePunctuation, SherpaOnnxOnlinePunctuation> {
++ public:
++  static OnlinePunctuation Create(const OnlinePunctuationConfig &config);
++
++  void Destroy(const SherpaOnnxOnlinePunctuation *p) const;
++
++  // Add punctuations to the input text and return it.
++  std::string AddPunctuation(const std::string &text) const;
++
++ private:
++  explicit OnlinePunctuation(const SherpaOnnxOnlinePunctuation *p);
++};
++
+ // ============================================================================
+ // Audio tagging
+ // ============================================================================
diff --git a/whats_new.md b/whats_new.md
deleted file mode 100644
index 56f9b135..00000000
--- a/whats_new.md
+++ /dev/null
@@ -1,47 +0,0 @@
-# sherpa-onnx  November2025 refresh
-
-The `integrate/local-changes` branch now tracks upstream `k2-fsa/sherpa-onnx` at commit `e4f48ce` plus the local iOS/macOS integration work from autumn 2025. This document highlights the notable additions, platform-specific upgrades, and risks introduced by the current sync.
-
-## New
-- **ZipVoice zeroshot TTS**  the C API now exposes `SherpaOnnxOfflineTtsZipvoiceModelConfig`, enabling textencoder + flowmatching + vocoder pipelines with guidance and RMS controls. Swift glue in `thirdparty/sherpa-onnx_swift` can construct the new config (tokens, text model, flowmatching model, `pinyin_dict`, etc.) and route it through `SherpaOnnxOfflineTtsModelConfig`.
-- **Tone/T-one CTC streaming ASR**  online model configs gained a dedicated `t_one_ctc` slot. Swift wrappers were updated so iOS code can load the new Chinese tone-aware checkpoints without editing generated headers.
-- **Wenet CTC offline models**  offline recognizer configs include a `wenet_ctc` field, allowing direct use of community WeNet checkpoints in C/Swift bindings.
-- **XCFramework merger pipeline**  new `build-xcframework.sh` collects the freshly built iOS (device + simulator) and macOS static archives into `build-apple/sherpa-onnx.xcframework`, simplifying reuse inside `aiassistant`.
-- **Merge-prep automation**  `git_prepare_merge.sh` snapshots the fork, hard-syncs `master` with upstream, exports conflict metadata, and leaves the repo ready for manual conflict resolution.
-
-## Improvements
-- **iOS/macOS build reproducibility**  `build-ios.sh` now queries `xcrun` for SDK and toolchain paths, guaranteeing the correct (26.1) SDK even when Xcode upgrades. Explicit `clang`/`clang++`/`libtool` paths eliminate the compiler not found and `-static` link failures seen on Apple Silicon.
-- **SwiftUI TTS sample hardening**  `ContentView.swift` and `ViewModel.swift` now:
-  - create a fresh `SherpaOnnxOfflineTtsWrapper` for each synthesis, guard against empty text, and perform generation on a background queue;
-  - persist `SherpaOnnxGeneratedAudioWrapper` to avoid premature deallocation;
-  - enforce CPU provider / single-threaded execution for Kokoro models and add an INT8 variant with diagnostics (file-size checks, rule FST wiring, etc.);
-  - wrap audio session setup and playback state transitions in helper methods with meaningful logging.
-- **Xcode project hygiene**  resource folders (e.g., `kokoro-82m`) are covered by file-system synced groups, shared schemes were added for iOS and macOS targets, and `objectVersion` bumped to 70 to align with Xcode 16 toolchains.
-- **Repo hygiene**  `.gitignore` now excludes large Kokoro resource folders copied into the SwiftUI sample so checkouts stay lightweight.
-
-## Fixes
-- **CPU-only Kokoro path**  forcing `provider: "cpu"` and `numThreads = 1` prevents ONNX Runtime from defaulting to unsupported CoreML accelerators on iOS/macOS, which previously produced empty audio buffers.
-- **Crashy concurrent playback**  the SwiftUI demo now serializes generation via `isGenerating`, retains `AVAudioPlayer`, and validates sample energy before saving/playing WAV output, resolving previous crashes when tapping Generate repeatedly.
-- **SDK drift**  `build-ios.sh` detected the mismatch between legacy iPhoneSimulator18.5 paths and the installed 26.1 SDK; the new script fails fast with clear diagnostics when `xcode-select` points elsewhere.
-
-## Breaking / Risky Changes
-- **Massive upstream delta**  `out_prepare_manual_20251109-060454/local_diffstat_vs_upstream.log` shows extensive edits/removals across CI workflows, Android/Flutter/Dart examples, and python/node/go bindings. Downstream automation that referenced removed files (e.g., `export-*-ascend` workflows, Flutter VAD sample) will break unless updated.
-- **Xcode 16 requirement**  the updated `.pbxproj` uses `PBXFileSystemSynchronizedBuildFileExceptionSet` and `objectVersion = 70`. Builds on older Xcode releases will fail to parse the project file.
-- **Resource handling**  the SwiftUI sample and `.gitignore` expect `ios-swiftui/SherpaOnnxTts/kokoro-82m/` assets to be restored manually after a clean checkout. Failing to do so results in runtime crashes when the app looks for bundled models.
-- **INT8 Kokoro config**  experimental INT8 support introduces strict single-threading and higher logging volume. Running with the wrong ONNX Runtime delegate or missing dict assets can still cause silent output; treat the INT8 path as beta.
-- **CI coverage gaps**  many upstream GitHub workflows were deleted or heavily modified. Until replacement pipelines are verified, there is reduced automated test coverage for non-Apple targets.
-
-## Docs / CI / Build
-- Removed a large set of specialized workflow files (Ascend/RKNN exporters, Pascal sample builders, etc.) while consolidating others; ensure your fork does not rely on the deleted automation.
-- `README.md` and `CHANGELOG.md` were touched but not yet rewritten; use this `whats_new.md` for the current state until an upstream changelog lands.
-
-## Upgrade Notes & Testing Recommendations
-1. **Rebuild native artifacts**  run `./build-ios.sh`, `./build-swift-macos.sh`, then `./build-xcframework.sh`. Verify `build-apple/sherpa-onnx.xcframework` contains all three slices before consuming it in `aiassistant`.
-2. **Sample validation**  launch `ios-swiftui/SherpaOnnxTts` on device/simulator and test:
-   - standard Kokoro multi-lang model,
-   - the new INT8 model path,
-   - ZipVoice (if assets are available) to ensure the Swift glue passes configs correctly.
-3. **Regression sweep**  because CI coverage shrank, manually spot-check at least one ASR (online + offline) and one VAD binary build on macOS/iOS.
-4. **Resource packaging**  confirm `.gitignore` exclusions do not hide critical assets in release archives. For xcframework distribution, embed the `kokoro-82m` resources within your main app bundle.
-
-Proceed with caution if downstream scripts referenced deleted workflow files or expect older C API layoutsthe new configs (`t_one_ctc`, `wenet_ctc`, `zipvoice`) are required fields in struct initializers now.
diff --git a/whatsnew.md b/whatsnew.md
new file mode 100644
index 00000000..d381019b
--- /dev/null
+++ b/whatsnew.md
@@ -0,0 +1,209 @@
+# Sherpa-ONNX Upgrade to v1.12.20
+
+## Overview
+
+This document summarizes the upgrade from sherpa-onnx v1.12.15 to v1.12.20 (December 2025).
+
+**Upgrade Date**: December 27, 2025
+**Previous Version**: 1.12.15
+**New Version**: 1.12.20
+
+---
+
+## New Features
+
+### ASR (Automatic Speech Recognition)
+
+#### 1. Google MedASR CTC Model Support (#2935)
+- **Purpose**: Medical speech recognition optimized for healthcare terminology
+- **Platforms**: All platforms including iOS
+- **API**: New `OfflineRecognizer.from_medasr_ctc()` factory method
+- **Benefits**: Specialized medical vocabulary recognition for clinical applications
+
+#### 2. Fun-ASR-Nano-2512 Support (#2906)
+Comprehensive multi-dialect and multi-language ASR model with:
+
+| Feature | Description |
+|---------|-------------|
+| **Far-field Recognition** | Optimized for distance pickup and high-noise scenarios (93% accuracy) |
+| **Chinese Dialects** | 7 major dialects: Wu, Cantonese, Min, Hakka, Gan, Xiang, Jin |
+| **Regional Accents** | 26 regions including Henan, Sichuan, Guangdong, etc. |
+| **Multi-language** | 31 languages with East/Southeast Asian focus |
+| **Music Background** | Enhanced lyric recognition under music interference |
+
+#### 3. GigaAM v3 Export (#2901)
+- New generation of ASR models exported to sherpa-onnx format
+
+### TTS (Text-to-Speech)
+
+#### 1. Matcha TTS Chinese+English (#2882, #2885)
+- APK builds for Matcha TTS supporting mixed Chinese+English synthesis
+- WASM space deployment for web applications
+- Improved space handling between English words (#2858)
+
+#### 2. Zipvoice Improvements (#2887, #2890, #2892, #2894)
+- New testing scripts for Zipvoice ONNX models
+- Uploaded optimized Zipvoice ONNX models
+- **Removed cppinyin dependency** - simplified build process
+- Shorter, cleaner model naming conventions
+
+### NPU/Accelerator Support
+
+#### 1. Qualcomm NPU (QNN) for Paraformer (#2931, #2932)
+- C++ runtime for Paraformer ASR with QNN acceleration
+- Android demo application
+- QNN context binary loading for faster startup (#2877)
+
+#### 2. Axera NPU Support (#2867, #2870, #2872)
+- Support for Axera ax630, ax650, and axcl backends
+- CI integration for Axera NPU testing
+- Refactored examples for better integration
+
+#### 3. Ascend 910B4 Export (#2878)
+- Model export support for Huawei Ascend AI processors
+
+### Other Improvements
+
+- **Streaming VAD optimization** (#2876): Better output when no voice detected for extended periods
+- **SenseVoice refactoring** (#2873): Cleaner implementation
+- **Paraformer refactoring** (#2874): Improved code structure
+- **Token-level confidence scores** (#2843): For offline transducer models
+- **Spacemit RISC-V support** (#2837): New CPU platform support
+
+---
+
+## Bug Fixes
+
+| Issue | Fix |
+|-------|-----|
+| #2939 | Fixed Ort::Value tensor view creation - corrected memory info handling |
+| #2909 | Fixed NPM package publishing |
+| #2905 | Fixed typos in URLs |
+| #2893 | Fixed build errors |
+| #2838 | Fixed building without TTS for C API |
+
+---
+
+## Build System Changes
+
+### Library Dependency Removal
+**Critical Change**: `libcppinyin_core.a` has been removed from the build:
+
+```diff
+# build-ios.sh changes
+- for f in libcppinyin_core.a libkaldi-native-fbank-core.a ...
++ for f in libkaldi-native-fbank-core.a ...
+
+# libtool merge changes
+- build/simulator/lib/libcppinyin_core.a \
+  build/simulator/lib/libkaldi-native-fbank-core.a \
+```
+
+This simplifies the build and removes the cppinyin pinyin library dependency from core sherpa-onnx.
+
+### CI/Platform Updates
+- Updated macOS CI from `macos-13` to `macos-15-intel`
+- Swift workflow updates for new macOS runners
+- Updated Flutter plugin versions to 1.12.20
+
+---
+
+## iOS/macOS Specific Changes
+
+### Swift API Updates
+- Error message improvements in `SherpaOnnx.swift` (better audio generation error reporting)
+- Model configuration updates for new ASR models
+
+### Build Script Changes
+The upstream `build-ios.sh` and `build-swift-macos.sh` scripts have been modified to:
+1. Remove `libcppinyin_core.a` from library merging
+2. Simplify the static library linking process
+
+---
+
+## Risk Assessment
+
+### High Risk
+
+| Risk | Description | Mitigation |
+|------|-------------|------------|
+| **API Compatibility** | New ASR model types (MedASR, Fun-ASR-Nano) add new config structures | Verify Swift wrapper compatibility; add new model support if needed |
+| **cppinyin Removal** | Pinyin functionality may be affected if directly used | Review if any code depends on cppinyin; most iOS apps use higher-level APIs |
+
+### Medium Risk
+
+| Risk | Description | Mitigation |
+|------|-------------|------------|
+| **Memory Info Change** | Ort::Value view creation now uses tensor's own memory info (#2939) | Verify TTS/ASR operations work correctly |
+| **Build Script Sync** | Library list changes in build scripts | Ensure custom build scripts don't reference removed libraries |
+
+### Low Risk
+
+| Risk | Description | Mitigation |
+|------|-------------|------------|
+| **QNN/NPU Changes** | Qualcomm/Axera NPU features | Android-only; no iOS impact |
+| **CI Platform Updates** | macOS runner changes | Build automation concern only |
+
+---
+
+## Custom Build Script Analysis
+
+### `build-xcframework.sh` Status: **NO CHANGES REQUIRED**
+
+The custom `build-xcframework.sh` script at `thirdparty/sherpa-onnx/build-xcframework.sh` is a wrapper that:
+
+1. Calls `./build-ios.sh` to generate iOS xcframework
+2. Calls `./build-swift-macos.sh` to generate macOS xcframework
+3. Merges them into a unified `build-apple/sherpa-onnx.xcframework`
+
+Since the library changes (cppinyin removal) are handled internally by the upstream scripts, the wrapper script correctly delegates to them and **does not need modification**.
+
+### Verification Checklist
+
+- [ ] Rebuild xcframework with updated sherpa-onnx
+- [ ] Verify TTS synthesis works (Kokoro model)
+- [ ] Verify VAD detection works (Silero VAD)
+- [ ] Verify ASR works if used (Whisper/Paraformer)
+- [ ] Check for any Swift compilation warnings
+- [ ] Test on both iOS device and simulator
+- [ ] Verify macOS Catalyst build works
+
+---
+
+## Migration Notes
+
+### For Existing Code
+
+No breaking changes for existing functionality. The upgrade is backward compatible with:
+- Existing TTS models (Kokoro, Matcha, VITS)
+- Existing VAD models (Silero VAD)
+- Existing ASR models (Whisper, SenseVoice)
+
+### For New Features
+
+To use new ASR models, additional Swift API wrappers may need to be added:
+
+```swift
+// Example: Future MedASR support might look like
+let config = OfflineMedAsrCtcModelConfig(
+    model: "path/to/model.onnx"
+)
+```
+
+---
+
+## Recommended Actions
+
+1. **Rebuild xcframework**: Run `./build-xcframework.sh` to generate updated binaries
+2. **Test existing functionality**: Verify TTS/VAD/ASR continue working
+3. **Update wrapper if needed**: Add Swift wrappers for new model types if required
+4. **Review memory usage**: The Ort::Value fix may affect memory patterns
+
+---
+
+## References
+
+- [Sherpa-ONNX GitHub Releases](https://github.com/k2-fsa/sherpa-onnx/releases)
+- [Sherpa-ONNX Documentation](https://k2-fsa.github.io/sherpa/onnx/)
+- [Fun-ASR-Nano Model](https://huggingface.co/FunAudioLLM/Fun-ASR-Nano-2512)
+- [Google MedASR](https://github.com/Google-Health/medasr)

commit 1f124600f506b45426c3b05f7fcc588ed373d03e
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:53:00 2025 +0800

    Add Kotlin and Java API for Google MedAsr model (#2956)

diff --git a/.github/workflows/run-java-test.yaml b/.github/workflows/run-java-test.yaml
index b2167ff0..2b250f6c 100644
--- a/.github/workflows/run-java-test.yaml
+++ b/.github/workflows/run-java-test.yaml
@@ -108,6 +108,14 @@ jobs:
           cd ./java-api-examples
           ./run-version-test.sh
 
+      - name:  Run java test (MedASR CTC)
+        shell: bash
+        run: |
+          cd ./java-api-examples
+          ./run-non-streaming-decode-file-medasr-ctc.sh
+          rm -rf sherpa-onnx-medasr-*
+
+
       - name:  Run java test (Omnilingual ASR CTC)
         shell: bash
         run: |
diff --git a/java-api-examples/NonStreamingDecodeFileMedAsrCtc.java b/java-api-examples/NonStreamingDecodeFileMedAsrCtc.java
new file mode 100644
index 00000000..d419bda3
--- /dev/null
+++ b/java-api-examples/NonStreamingDecodeFileMedAsrCtc.java
@@ -0,0 +1,51 @@
+// Copyright 2025 Xiaomi Corporation
+
+// This file shows how to use an offline Google MedASR CTC model,
+// i.e., non-streaming MedASR CTC model,
+// to decode files.
+import com.k2fsa.sherpa.onnx.*;
+
+public class NonStreamingDecodeFileMedAsrCtc {
+  public static void main(String[] args) {
+    // please refer to
+    // https://k2-fsa.github.io/sherpa/onnx/medasr/index.html
+    // to download model files
+    String model = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx";
+
+    String tokens = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt";
+
+    String waveFilename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav";
+
+    WaveReader reader = new WaveReader(waveFilename);
+
+    OfflineMedAsrCtcModelConfig medasr =
+        OfflineMedAsrCtcModelConfig.builder().setModel(model).build();
+
+    OfflineModelConfig modelConfig =
+        OfflineModelConfig.builder()
+            .setMedAsr(medasr)
+            .setTokens(tokens)
+            .setNumThreads(1)
+            .setDebug(true)
+            .build();
+
+    OfflineRecognizerConfig config =
+        OfflineRecognizerConfig.builder()
+            .setOfflineModelConfig(modelConfig)
+            .setDecodingMethod("greedy_search")
+            .build();
+
+    OfflineRecognizer recognizer = new OfflineRecognizer(config);
+    OfflineStream stream = recognizer.createStream();
+    stream.acceptWaveform(reader.getSamples(), reader.getSampleRate());
+
+    recognizer.decode(stream);
+
+    String text = recognizer.getResult(stream).getText();
+
+    System.out.printf("filename:%s\nresult:%s\n", waveFilename, text);
+
+    stream.release();
+    recognizer.release();
+  }
+}
diff --git a/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
index cee8bb77..08b68981 100644
--- a/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
+++ b/java-api-examples/NonStreamingDecodeFileOmnilingualAsrCtc.java
@@ -8,7 +8,7 @@ import com.k2fsa.sherpa.onnx.*;
 public class NonStreamingDecodeFileOmnilingualAsrCtc {
   public static void main(String[] args) {
     // please refer to
-    // https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html
+    // https://k2-fsa.github.io/sherpa/onnx/omnilingual-asr/index.html
     // to download model files
     String model =
         "sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12/model.int8.onnx";
diff --git a/java-api-examples/run-non-streaming-decode-file-medasr-ctc.sh b/java-api-examples/run-non-streaming-decode-file-medasr-ctc.sh
new file mode 100755
index 00000000..927f5ed9
--- /dev/null
+++ b/java-api-examples/run-non-streaming-decode-file-medasr-ctc.sh
@@ -0,0 +1,38 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [[ ! -f ../build/lib/libsherpa-onnx-jni.dylib  && ! -f ../build/lib/libsherpa-onnx-jni.so ]]; then
+  mkdir -p ../build
+  pushd ../build
+  cmake \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    -DSHERPA_ONNX_ENABLE_JNI=ON \
+    ..
+
+  make -j4
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f ../sherpa-onnx/java-api/build/sherpa-onnx.jar ]; then
+  pushd ../sherpa-onnx/java-api
+  make
+  popd
+fi
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+
+java \
+  -Djava.library.path=$PWD/../build/lib \
+  -cp ../sherpa-onnx/java-api/build/sherpa-onnx.jar \
+  NonStreamingDecodeFileMedAsrCtc.java
diff --git a/kotlin-api-examples/run.sh b/kotlin-api-examples/run.sh
index 427d19fb..d14ead48 100755
--- a/kotlin-api-examples/run.sh
+++ b/kotlin-api-examples/run.sh
@@ -234,10 +234,10 @@ function testOfflineAsr() {
     rm sherpa-onnx-moonshine-tiny-en-int8.tar.bz2
   fi
 
-  if [ ! -f ./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt ]; then
-    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-    tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-    rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
+  if [ ! -f ./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/tokens.txt ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+    tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+    rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
   fi
 
   if [ ! -f ./sherpa-onnx-whisper-tiny.en/tiny.en-encoder.int8.onnx ]; then
@@ -439,10 +439,10 @@ function testOfflineSpeechDenoiser() {
 }
 
 function testOfflineSenseVoiceWithHr() {
-  if [ ! -f ./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt ]; then
-    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-    tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-    rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
+  if [ ! -f ./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/tokens.txt ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+    tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+    rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
   fi
 
   if [ ! -d dict ]; then
@@ -515,6 +515,28 @@ function testOfflineOmnilingualAsrCtc() {
   java -Djava.library.path=../build/lib -jar $out_filename
 }
 
+function testOfflineMedAsrCtc() {
+  if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+    curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+    tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+    rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  fi
+
+  out_filename=test_offline_medasr_ctc.jar
+  kotlinc-jvm -include-runtime -d $out_filename \
+    test_offline_medasr_ctc.kt \
+    FeatureConfig.kt \
+    QnnConfig.kt \
+    HomophoneReplacerConfig.kt \
+    OfflineRecognizer.kt \
+    OfflineStream.kt \
+    WaveReader.kt \
+    faked-asset-manager.kt
+
+  ls -lh $out_filename
+  java -Djava.library.path=../build/lib -jar $out_filename
+}
+
 function testOfflineWenetCtc() {
   if [ ! -f sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10/model.int8.onnx ]; then
     curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-wenetspeech-yue-u2pp-conformer-ctc-zh-en-cantonese-int8-2025-09-10.tar.bz2
@@ -539,6 +561,7 @@ function testOfflineWenetCtc() {
 
 testVersion
 
+testOfflineMedAsrCtc
 testOfflineOmnilingualAsrCtc
 testOfflineWenetCtc
 testOfflineNeMoCanary
diff --git a/kotlin-api-examples/test_offline_asr.kt b/kotlin-api-examples/test_offline_asr.kt
index 6cee007e..665c8191 100644
--- a/kotlin-api-examples/test_offline_asr.kt
+++ b/kotlin-api-examples/test_offline_asr.kt
@@ -15,7 +15,7 @@ fun test(type: Int) {
     2 -> "./sherpa-onnx-whisper-tiny.en/test_wavs/0.wav"
     5 -> "./sherpa-onnx-zipformer-multi-zh-hans-2023-9-2/test_wavs/1.wav"
     6 -> "./sherpa-onnx-nemo-ctc-en-citrinet-512/test_wavs/8k.wav"
-    15 -> "./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/test_wavs/zh.wav"
+    15 -> "./sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/test_wavs/zh.wav"
     21 -> "./sherpa-onnx-moonshine-tiny-en-int8/test_wavs/0.wav"
     24 -> "./sherpa-onnx-fire-red-asr-large-zh_en-2025-02-16/test_wavs/0.wav"
     25 -> "./sherpa-onnx-dolphin-base-ctc-multi-lang-int8-2025-04-02/test_wavs/0.wav"
diff --git a/kotlin-api-examples/test_offline_medasr_ctc.kt b/kotlin-api-examples/test_offline_medasr_ctc.kt
new file mode 100644
index 00000000..53198043
--- /dev/null
+++ b/kotlin-api-examples/test_offline_medasr_ctc.kt
@@ -0,0 +1,31 @@
+package com.k2fsa.sherpa.onnx
+
+fun main() {
+  val recognizer = createOfflineRecognizer()
+  val waveFilename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav"
+
+  val objArray = WaveReader.readWaveFromFile(
+      filename = waveFilename,
+  )
+  val samples: FloatArray = objArray[0] as FloatArray
+  val sampleRate: Int = objArray[1] as Int
+
+  var stream = recognizer.createStream()
+  stream.acceptWaveform(samples, sampleRate=sampleRate)
+  recognizer.decode(stream)
+
+  var result = recognizer.getResult(stream)
+  println(result)
+
+  stream.release()
+  recognizer.release()
+}
+
+
+fun createOfflineRecognizer(): OfflineRecognizer {
+  val config = OfflineRecognizerConfig(
+      modelConfig = getOfflineModelConfig(type = 45)!!,
+  )
+
+  return OfflineRecognizer(config = config)
+}
diff --git a/nodejs-examples/README.md b/nodejs-examples/README.md
index 72fba48b..c6cd4cbb 100644
--- a/nodejs-examples/README.md
+++ b/nodejs-examples/README.md
@@ -287,9 +287,9 @@ how to decode a file with a non-streaming SenseVoice model with homophone replac
 You can use the following command to run it:
 
 ```bash
-curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
 
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/hr-files/dict.tar.bz2
 tar xf dict.tar.bz2
@@ -309,9 +309,9 @@ how to decode a file with a non-streaming SenseVoice model.
 You can use the following command to run it:
 
 ```bash
-wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
-rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+tar xvf sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
+rm sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17.tar.bz2
 
 node ./test-offline-sense-voice.js
 ```
diff --git a/nodejs-examples/test-offline-sense-voice-with-hr.js b/nodejs-examples/test-offline-sense-voice-with-hr.js
index ebf3ba47..e652eb70 100644
--- a/nodejs-examples/test-offline-sense-voice-with-hr.js
+++ b/nodejs-examples/test-offline-sense-voice-with-hr.js
@@ -6,11 +6,12 @@ function createOfflineRecognizer() {
   let modelConfig = {
     senseVoice: {
       model:
-          './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx',
+          './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/model.int8.onnx',
       language: '',
       useInverseTextNormalization: 1,
     },
-    tokens: './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt',
+    tokens:
+        './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/tokens.txt',
   };
 
   let config = {
diff --git a/nodejs-examples/test-offline-sense-voice.js b/nodejs-examples/test-offline-sense-voice.js
index ff3d829b..6fc0c725 100644
--- a/nodejs-examples/test-offline-sense-voice.js
+++ b/nodejs-examples/test-offline-sense-voice.js
@@ -6,11 +6,12 @@ function createOfflineRecognizer() {
   let modelConfig = {
     senseVoice: {
       model:
-          './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx',
+          './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/model.int8.onnx',
       language: '',
       useInverseTextNormalization: 1,
     },
-    tokens: './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt',
+    tokens:
+        './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/tokens.txt',
   };
 
   let config = {
@@ -24,7 +25,7 @@ const recognizer = createOfflineRecognizer();
 const stream = recognizer.createStream();
 
 const waveFilename =
-    './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/test_wavs/zh.wav';
+    './sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17/test_wavs/zh.wav';
 const wave = sherpa_onnx.readWave(waveFilename);
 stream.acceptWaveform(wave.sampleRate, wave.samples);
 
diff --git a/scripts/apk/generate-vad-asr-apk-script.py b/scripts/apk/generate-vad-asr-apk-script.py
index e9c8caa8..020ffa57 100755
--- a/scripts/apk/generate-vad-asr-apk-script.py
+++ b/scripts/apk/generate-vad-asr-apk-script.py
@@ -758,6 +758,22 @@ def get_models():
 
             ls -lh
 
+            popd
+            """,
+        ),
+        Model(
+            model_name="sherpa-onnx-medasr-ctc-en-int8-2025-12-25",
+            idx=45,
+            lang="en",
+            lang2="",
+            short_name="google_medasr_ctc_int8",
+            cmd="""
+            pushd $model_name
+
+            rm -rfv test_wavs
+
+            ls -lh
+
             popd
             """,
         ),
diff --git a/sherpa-onnx/java-api/Makefile b/sherpa-onnx/java-api/Makefile
index 3206d7b9..8b921ace 100644
--- a/sherpa-onnx/java-api/Makefile
+++ b/sherpa-onnx/java-api/Makefile
@@ -39,6 +39,7 @@ java_files += OfflineNemoEncDecCtcModelConfig.java
 java_files += OfflineZipformerCtcModelConfig.java
 java_files += OfflineWenetCtcModelConfig.java
 java_files += OfflineOmnilingualAsrCtcModelConfig.java
+java_files += OfflineMedAsrCtcModelConfig.java
 java_files += OfflineCanaryModelConfig.java
 java_files += OfflineSenseVoiceModelConfig.java
 java_files += OfflineDolphinModelConfig.java
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineMedAsrCtcModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineMedAsrCtcModelConfig.java
new file mode 100644
index 00000000..986fdd9a
--- /dev/null
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineMedAsrCtcModelConfig.java
@@ -0,0 +1,30 @@
+package com.k2fsa.sherpa.onnx;
+
+public class OfflineMedAsrCtcModelConfig {
+    private final String model;
+
+    private OfflineMedAsrCtcModelConfig(Builder builder) {
+        this.model = builder.model;
+    }
+
+    public static Builder builder() {
+        return new Builder();
+    }
+
+    public String getModel() {
+        return model;
+    }
+
+    public static class Builder {
+        private String model = "";
+
+        public OfflineMedAsrCtcModelConfig build() {
+            return new OfflineMedAsrCtcModelConfig(this);
+        }
+
+        public Builder setModel(String model) {
+            this.model = model;
+            return this;
+        }
+    }
+}
diff --git a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
index 19450ead..ef20fb7b 100644
--- a/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
+++ b/sherpa-onnx/java-api/src/main/java/com/k2fsa/sherpa/onnx/OfflineModelConfig.java
@@ -14,6 +14,7 @@ public class OfflineModelConfig {
     private final OfflineZipformerCtcModelConfig zipformerCtc;
     private final OfflineWenetCtcModelConfig wenetCtc;
     private final OfflineOmnilingualAsrCtcModelConfig omnilingual;
+    private final OfflineMedAsrCtcModelConfig medasr;
     private final OfflineCanaryModelConfig canary;
     private final String teleSpeech;
     private final String tokens;
@@ -36,6 +37,7 @@ public class OfflineModelConfig {
         this.canary = builder.canary;
         this.wenetCtc = builder.wenetCtc;
         this.omnilingual = builder.omnilingual;
+        this.medasr = builder.medasr;
         this.senseVoice = builder.senseVoice;
         this.dolphin = builder.dolphin;
         this.teleSpeech = builder.teleSpeech;
@@ -92,6 +94,10 @@ public class OfflineModelConfig {
         return omnilingual;
     }
 
+    public OfflineMedAsrCtcModelConfig getMedAsr() {
+        return medasr;
+    }
+
     public OfflineCanaryModelConfig getCanary() {
         return canary;
     }
@@ -140,6 +146,7 @@ public class OfflineModelConfig {
         private OfflineZipformerCtcModelConfig zipformerCtc = OfflineZipformerCtcModelConfig.builder().build();
         private OfflineWenetCtcModelConfig wenetCtc = OfflineWenetCtcModelConfig.builder().build();
         private OfflineOmnilingualAsrCtcModelConfig omnilingual = OfflineOmnilingualAsrCtcModelConfig.builder().build();
+        private OfflineMedAsrCtcModelConfig medasr = OfflineMedAsrCtcModelConfig.builder().build();
         private OfflineCanaryModelConfig canary = OfflineCanaryModelConfig.builder().build();
         private String teleSpeech = "";
         private String tokens = "";
@@ -189,6 +196,11 @@ public class OfflineModelConfig {
             return this;
         }
 
+        public Builder setMedAsr(OfflineMedAsrCtcModelConfig medasr) {
+            this.medasr = medasr;
+            return this;
+        }
+
         public Builder setCanary(OfflineCanaryModelConfig canary) {
             this.canary = canary;
             return this;
diff --git a/sherpa-onnx/jni/offline-recognizer.cc b/sherpa-onnx/jni/offline-recognizer.cc
index fa2f7dff..a7877d96 100644
--- a/sherpa-onnx/jni/offline-recognizer.cc
+++ b/sherpa-onnx/jni/offline-recognizer.cc
@@ -258,6 +258,15 @@ static OfflineRecognizerConfig GetOfflineConfig(JNIEnv *env, jobject config,
                               omnilingual_ctc_config_cls,
                               omnilingual_ctc_config);
 
+  // medasr ctc
+  fid = env->GetFieldID(model_config_cls, "medasr",
+                        "Lcom/k2fsa/sherpa/onnx/OfflineMedAsrCtcModelConfig;");
+  jobject medasr_ctc_config = env->GetObjectField(model_config, fid);
+  jclass medasr_ctc_config_cls = env->GetObjectClass(medasr_ctc_config);
+
+  SHERPA_ONNX_JNI_READ_STRING(ans.model_config.medasr.model, model,
+                              medasr_ctc_config_cls, medasr_ctc_config);
+
   // canary
   fid = env->GetFieldID(model_config_cls, "canary",
                         "Lcom/k2fsa/sherpa/onnx/OfflineCanaryModelConfig;");
diff --git a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
index 7b7958fa..64e6531e 100644
--- a/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
+++ b/sherpa-onnx/kotlin-api/OfflineRecognizer.kt
@@ -46,6 +46,10 @@ data class OfflineOmnilingualAsrCtcModelConfig(
     var model: String = "",
 )
 
+data class OfflineMedAsrCtcModelConfig(
+    var model: String = "",
+)
+
 data class OfflineWhisperModelConfig(
     var encoder: String = "",
     var decoder: String = "",
@@ -93,6 +97,7 @@ data class OfflineModelConfig(
     var zipformerCtc: OfflineZipformerCtcModelConfig = OfflineZipformerCtcModelConfig(),
     var wenetCtc: OfflineWenetCtcModelConfig = OfflineWenetCtcModelConfig(),
     var omnilingual: OfflineOmnilingualAsrCtcModelConfig = OfflineOmnilingualAsrCtcModelConfig(),
+    var medasr: OfflineMedAsrCtcModelConfig = OfflineMedAsrCtcModelConfig(),
     var canary: OfflineCanaryModelConfig = OfflineCanaryModelConfig(),
     var teleSpeech: String = "",
     var numThreads: Int = 1,
@@ -754,6 +759,16 @@ fun getOfflineModelConfig(type: Int): OfflineModelConfig? {
             )
         }
 
+        45 -> {
+            val modelDir = "sherpa-onnx-medasr-ctc-en-int8-2025-12-25"
+            return OfflineModelConfig(
+                medasr = OfflineMedAsrCtcModelConfig(
+                    model = "$modelDir/model.int8.onnx",
+                ),
+                tokens = "$modelDir/tokens.txt",
+            )
+        }
+
         9000 -> {
             val modelDir =
                 "sherpa-onnx-qnn-5-seconds-sense-voice-zh-en-ja-ko-yue-2024-07-17-int8-android-aarch64"

commit 2f8fb50a24b433316b088179ba6cb3e3d416da87
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:51:39 2025 +0800

    Add JavaScript API (node-addon) for Google MedAsr model (#2955)

diff --git a/.github/scripts/test-nodejs-addon-npm.sh b/.github/scripts/test-nodejs-addon-npm.sh
index 397935c6..938e4475 100755
--- a/.github/scripts/test-nodejs-addon-npm.sh
+++ b/.github/scripts/test-nodejs-addon-npm.sh
@@ -10,6 +10,16 @@ arch=$(node -p "require('os').arch()")
 platform=$(node -p "require('os').platform()")
 node_version=$(node -p "process.versions.node.split('.')[0]")
 
+echo "----------non-streaming ASR Google MedASR CTC----------"
+
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+node ./test_asr_non_streaming_medasr_ctc.js
+
+rm -rf sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+
 echo "----------non-streaming ASR Omnilingual ASR CTC----------"
 
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
diff --git a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
index 0e40f99e..6996d079 100644
--- a/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
+++ b/harmony-os/SherpaOnnxHar/sherpa_onnx/src/main/cpp/non-streaming-asr.cc
@@ -94,6 +94,22 @@ GetOfflineOmnilingualAsrCtcModelConfig(Napi::Object obj) {
   return c;
 }
 
+static SherpaOnnxOfflineMedAsrCtcModelConfig GetOfflineMedAsrCtcModelConfig(
+    Napi::Object obj) {
+  SherpaOnnxOfflineMedAsrCtcModelConfig c;
+  memset(&c, 0, sizeof(c));
+
+  if (!obj.Has("medasr") || !obj.Get("medasr").IsObject()) {
+    return c;
+  }
+
+  Napi::Object o = obj.Get("medasr").As<Napi::Object>();
+
+  SHERPA_ONNX_ASSIGN_ATTR_STR(model, model);
+
+  return c;
+}
+
 static SherpaOnnxOfflineDolphinModelConfig GetOfflineDolphinModelConfig(
     Napi::Object obj) {
   SherpaOnnxOfflineDolphinModelConfig c;
@@ -260,6 +276,7 @@ static SherpaOnnxOfflineModelConfig GetOfflineModelConfig(Napi::Object obj) {
   c.canary = GetOfflineCanaryModelConfig(o);
   c.wenet_ctc = GetOfflineWenetCtcModelConfig(o);
   c.omnilingual = GetOfflineOmnilingualAsrCtcModelConfig(o);
+  c.medasr = GetOfflineMedAsrCtcModelConfig(o);
 
   SHERPA_ONNX_ASSIGN_ATTR_STR(tokens, tokens);
   SHERPA_ONNX_ASSIGN_ATTR_INT32(num_threads, numThreads);
@@ -354,6 +371,7 @@ static void FreeConfig(const SherpaOnnxOfflineRecognizerConfig &c) {
 
   SHERPA_ONNX_DELETE_C_STR(c.model_config.wenet_ctc.model);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.omnilingual.model);
+  SHERPA_ONNX_DELETE_C_STR(c.model_config.medasr.model);
 
   SHERPA_ONNX_DELETE_C_STR(c.model_config.tokens);
   SHERPA_ONNX_DELETE_C_STR(c.model_config.provider);
diff --git a/nodejs-addon-examples/README.md b/nodejs-addon-examples/README.md
index de4d6ce7..c6afe1ed 100644
--- a/nodejs-addon-examples/README.md
+++ b/nodejs-addon-examples/README.md
@@ -126,6 +126,7 @@ The following tables list the examples in this folder.
 |[./test_asr_non_streaming_nemo_ctc.js](./test_asr_non_streaming_nemo_ctc.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) CTC model with greedy search|
 |[./test_asr_non_streaming_wenet_ctc.js](./test_asr_non_streaming_wenet_ctc.js)|Non-streaming speech recognition from a file using a [u2pp_conformer_yue](https://huggingface.co/ASLP-lab/WSYue-ASR/tree/main/u2pp_conformer_yue) CTC model with greedy search|
 |[./test_asr_non_streaming_omnilingual_asr_ctc.js](./test_asr_non_streaming_omnilingual_asr_ctc.js)|Non-streaming speech recognition from a file using a [Omnilingual-ASR](https://github.com/facebookresearch/omnilingual-asr) CTC model with greedy search|
+|[./test_asr_non_streaming_medasr_ctc.js](./test_asr_non_streaming_medasr_ctc.js)|Non-streaming speech recognition from a file using a [Google MedASR](https://github.com/google-health/medasr) CTC model with greedy search|
 |[./test_asr_non_streaming_nemo_canary.js](./test_asr_non_streaming_nemo_canary.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [Canary](https://k2-fsa.github.io/sherpa/onnx/nemo/canary.html#sherpa-onnx-nemo-canary-180m-flash-en-es-de-fr-int8-english-spanish-german-french) model|
 |[./test_asr_non_streaming_zipformer_ctc.js](./test_asr_non_streaming_zipformer_ctc.js)|Non-streaming speech recognition from a file using a Zipformer CTC model with greedy search|
 |[./test_asr_non_streaming_nemo_parakeet_tdt_v2.js](./test_asr_non_streaming_nemo_parakeet_tdt_v2.js)|Non-streaming speech recognition from a file using a [NeMo](https://github.com/NVIDIA/NeMo) [parakeet-tdt-0.6b-v2](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-parakeet-tdt-0-6b-v2-int8-english) model with greedy search|
@@ -428,6 +429,16 @@ npm install naudiodon2
 node ./test_vad_asr_non_streaming_nemo_ctc_microphone.js
 ```
 
+### Non-streaming speech recognition with Google MedASR CTC models
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+node ./test_asr_non_streaming_medasr_ctc.js
+```
+
 ### Non-streaming speech recognition with Omnilingual ASR CTC models
 
 ```bash
diff --git a/nodejs-addon-examples/test_asr_non_streaming_medasr_ctc.js b/nodejs-addon-examples/test_asr_non_streaming_medasr_ctc.js
new file mode 100644
index 00000000..c9c9631c
--- /dev/null
+++ b/nodejs-addon-examples/test_asr_non_streaming_medasr_ctc.js
@@ -0,0 +1,46 @@
+// Copyright (c)  2025  Xiaomi Corporation
+const sherpa_onnx = require('sherpa-onnx-node');
+
+// Please download test files from
+// https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+const config = {
+  'featConfig': {
+    'sampleRate': 16000,
+    'featureDim': 80,
+  },
+  'modelConfig': {
+    'medasr': {
+      'model': './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx',
+    },
+    'tokens': './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt',
+    'numThreads': 2,
+    'provider': 'cpu',
+    'debug': 1,
+  }
+};
+
+const waveFilename =
+    './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav';
+
+const recognizer = new sherpa_onnx.OfflineRecognizer(config);
+console.log('Started')
+let start = Date.now();
+const stream = recognizer.createStream();
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform({sampleRate: wave.sampleRate, samples: wave.samples});
+
+recognizer.decode(stream);
+const result = recognizer.getResult(stream);
+let stop = Date.now();
+console.log('Done')
+
+const elapsed_seconds = (stop - start) / 1000;
+const duration = wave.samples.length / wave.sampleRate;
+const real_time_factor = elapsed_seconds / duration;
+console.log('Wave duration', duration.toFixed(3), 'seconds')
+console.log('Elapsed', elapsed_seconds.toFixed(3), 'seconds')
+console.log(
+    `RTF = ${elapsed_seconds.toFixed(3)}/${duration.toFixed(3)} =`,
+    real_time_factor.toFixed(3))
+console.log(waveFilename)
+console.log('result\n', result)

commit a53d9eec12a628dbcaac1b91c55e70e43d458058
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:50:51 2025 +0800

    Add JavaScript API (WebAssembly) for Google MedAsr model (#2954)

diff --git a/.github/scripts/test-nodejs-npm.sh b/.github/scripts/test-nodejs-npm.sh
index 16f21b5f..0b929ca9 100755
--- a/.github/scripts/test-nodejs-npm.sh
+++ b/.github/scripts/test-nodejs-npm.sh
@@ -9,6 +9,14 @@ git status
 ls -lh
 ls -lh node_modules
 
+curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+node ./test-offline-medasr-ctc.js
+
+rm -rf sherpa-onnx-medasr-ctc-en-int8-2025-12-25
+
 curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
 tar xvf sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
 rm sherpa-onnx-omnilingual-asr-1600-languages-300M-ctc-int8-2025-11-12.tar.bz2
diff --git a/nodejs-examples/README.md b/nodejs-examples/README.md
index bc8c6832..72fba48b 100644
--- a/nodejs-examples/README.md
+++ b/nodejs-examples/README.md
@@ -203,6 +203,22 @@ rm sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03.tar.bz2
 node ./test-offline-zipformer-ctc.js
 ```
 
+## ./test-offline-medasr-ctc.js
+
+[./test-offline-medasr-ctc.js](./test-offline-medasr-ctc.js) demonstrates
+how to decode a file with a Google MedASR CTC model. In the code we use
+[sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2).
+
+You can use the following command to run it:
+
+```bash
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+node ./test-offline-medasr-ctc.js
+```
+
 ## ./test-offline-omnilingual-asr-ctc.js
 
 [./test-offline-omnilingual-asr-ctc.js](./test-offline-omnilingual-asr-ctc.js) demonstrates
diff --git a/nodejs-examples/test-offline-medasr-ctc.js b/nodejs-examples/test-offline-medasr-ctc.js
new file mode 100644
index 00000000..e9070576
--- /dev/null
+++ b/nodejs-examples/test-offline-medasr-ctc.js
@@ -0,0 +1,35 @@
+// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+//
+const fs = require('fs');
+const {Readable} = require('stream');
+const wav = require('wav');
+
+const sherpa_onnx = require('sherpa-onnx');
+
+function createOfflineRecognizer() {
+  let config = {
+    modelConfig: {
+      medasr: {
+        model: './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx',
+      },
+      tokens: './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt',
+    }
+  };
+
+  return sherpa_onnx.createOfflineRecognizer(config);
+}
+
+const recognizer = createOfflineRecognizer();
+const stream = recognizer.createStream();
+
+const waveFilename =
+    './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav';
+const wave = sherpa_onnx.readWave(waveFilename);
+stream.acceptWaveform(wave.sampleRate, wave.samples);
+
+recognizer.decode(stream);
+const text = recognizer.getResult(stream).text;
+console.log(text);
+
+stream.free();
+recognizer.free();
diff --git a/nodejs-examples/test-offline-tts-matcha-en.js b/nodejs-examples/test-offline-tts-matcha-en.js
index f5d1e6de..be129243 100644
--- a/nodejs-examples/test-offline-tts-matcha-en.js
+++ b/nodejs-examples/test-offline-tts-matcha-en.js
@@ -6,7 +6,6 @@ function createOfflineTts() {
   let offlineTtsMatchaModelConfig = {
     acousticModel: './matcha-icefall-en_US-ljspeech/model-steps-3.onnx',
     vocoder: './vocos-22khz-univ.onnx',
-    lexicon: './matcha-icefall-en_US-ljspeech/lexicon.txt',
     tokens: './matcha-icefall-en_US-ljspeech/tokens.txt',
     dataDir: './matcha-icefall-en_US-ljspeech/espeak-ng-data',
 
diff --git a/wasm/asr/sherpa-onnx-asr.js b/wasm/asr/sherpa-onnx-asr.js
index a77d61c1..876f947b 100644
--- a/wasm/asr/sherpa-onnx-asr.js
+++ b/wasm/asr/sherpa-onnx-asr.js
@@ -59,6 +59,10 @@ function freeConfig(config, Module) {
     freeConfig(config.omnilingual, Module)
   }
 
+  if ('medasr' in config) {
+    freeConfig(config.medasr, Module)
+  }
+
   if ('moonshine' in config) {
     freeConfig(config.moonshine, Module)
   }
@@ -776,6 +780,23 @@ function initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(config, Module) {
   }
 }
 
+function initSherpaOnnxOfflineMedAsrCtcModelConfig(config, Module) {
+  const n = Module.lengthBytesUTF8(config.model || '') + 1;
+
+  const buffer = Module._malloc(n);
+
+  const len = 1 * 4;  // 1 pointer
+  const ptr = Module._malloc(len);
+
+  Module.stringToUTF8(config.model || '', buffer, n);
+
+  Module.setValue(ptr, buffer, 'i8*');
+
+  return {
+    buffer: buffer, ptr: ptr, len: len,
+  }
+}
+
 function initSherpaOnnxOfflineWhisperModelConfig(config, Module) {
   const encoderLen = Module.lengthBytesUTF8(config.encoder || '') + 1;
   const decoderLen = Module.lengthBytesUTF8(config.decoder || '') + 1;
@@ -1052,6 +1073,12 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
     };
   }
 
+  if (!('medasr' in config)) {
+    config.medasr = {
+      model: '',
+    };
+  }
+
   if (!('whisper' in config)) {
     config.whisper = {
       encoder: '',
@@ -1139,10 +1166,13 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   const omnilingual = initSherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
       config.omnilingual, Module);
 
+  const medasr =
+      initSherpaOnnxOfflineMedAsrCtcModelConfig(config.medasr, Module);
+
   const len = transducer.len + paraformer.len + nemoCtc.len + whisper.len +
       tdnn.len + 8 * 4 + senseVoice.len + moonshine.len + fireRedAsr.len +
       dolphin.len + zipformerCtc.len + canary.len + wenetCtc.len +
-      omnilingual.len;
+      omnilingual.len + medasr.len;
 
   const ptr = Module._malloc(len);
 
@@ -1256,12 +1286,15 @@ function initSherpaOnnxOfflineModelConfig(config, Module) {
   Module._CopyHeap(omnilingual.ptr, omnilingual.len, ptr + offset);
   offset += omnilingual.len;
 
+  Module._CopyHeap(medasr.ptr, medasr.len, ptr + offset);
+  offset += medasr.len;
+
   return {
     buffer: buffer, ptr: ptr, len: len, transducer: transducer,
         paraformer: paraformer, nemoCtc: nemoCtc, whisper: whisper, tdnn: tdnn,
         senseVoice: senseVoice, moonshine: moonshine, fireRedAsr: fireRedAsr,
         dolphin: dolphin, zipformerCtc: zipformerCtc, canary: canary,
-        wenetCtc: wenetCtc, omnilingual: omnilingual
+        wenetCtc: wenetCtc, omnilingual: omnilingual, medasr: medasr
   }
 }
 
diff --git a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
index f5ded701..682a1cab 100644
--- a/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
+++ b/wasm/nodejs/sherpa-onnx-wasm-nodejs.cc
@@ -16,6 +16,7 @@ static_assert(sizeof(SherpaOnnxOfflineParaformerModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWenetCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) == 4, "");
+static_assert(sizeof(SherpaOnnxOfflineMedAsrCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineDolphinModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineNemoEncDecCtcModelConfig) == 4, "");
 static_assert(sizeof(SherpaOnnxOfflineWhisperModelConfig) == 5 * 4, "");
@@ -39,7 +40,8 @@ static_assert(sizeof(SherpaOnnxOfflineModelConfig) ==
                       sizeof(SherpaOnnxOfflineZipformerCtcModelConfig) +
                       sizeof(SherpaOnnxOfflineCanaryModelConfig) +
                       sizeof(SherpaOnnxOfflineWenetCtcModelConfig) +
-                      sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig),
+                      sizeof(SherpaOnnxOfflineOmnilingualAsrCtcModelConfig) +
+                      sizeof(SherpaOnnxOfflineMedAsrCtcModelConfig),
 
               "");
 static_assert(sizeof(SherpaOnnxFeatureConfig) == 2 * 4, "");
@@ -89,6 +91,7 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   auto canary = &model_config->canary;
   auto wenet_ctc = &model_config->wenet_ctc;
   auto omnilingual = &model_config->omnilingual;
+  auto medasr = &model_config->medasr;
 
   fprintf(stdout, "----------offline transducer model config----------\n");
   fprintf(stdout, "encoder: %s\n", transducer->encoder);
@@ -145,6 +148,9 @@ void PrintOfflineRecognizerConfig(SherpaOnnxOfflineRecognizerConfig *config) {
   fprintf(stdout, "----------offline Omnilingual ASR model config----------\n");
   fprintf(stdout, "model: %s\n", omnilingual->model);
 
+  fprintf(stdout, "----------offline MedASR model config----------\n");
+  fprintf(stdout, "model: %s\n", medasr->model);
+
   fprintf(stdout, "tokens: %s\n", model_config->tokens);
   fprintf(stdout, "num_threads: %d\n", model_config->num_threads);
   fprintf(stdout, "provider: %s\n", model_config->provider);

commit 14da0f6f76956c463b0468c0e059ada19a1c9409
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:49:06 2025 +0800

    Add Dart API for Google MedAsr model (#2953)

diff --git a/.github/scripts/test-dart.sh b/.github/scripts/test-dart.sh
index a1e1e615..cc89f163 100755
--- a/.github/scripts/test-dart.sh
+++ b/.github/scripts/test-dart.sh
@@ -74,6 +74,10 @@ popd
 
 pushd non-streaming-asr
 
+echo '----------MedASR CTC----------'
+./run-medasr-ctc.sh
+rm -rf sherpa-onnx-*
+
 echo '----------Omnilingual ASR CTC----------'
 ./run-omnilingual-asr-ctc.sh
 rm -rf sherpa-onnx-*
diff --git a/dart-api-examples/non-streaming-asr/bin/medasr-ctc.dart b/dart-api-examples/non-streaming-asr/bin/medasr-ctc.dart
new file mode 100644
index 00000000..65cfa397
--- /dev/null
+++ b/dart-api-examples/non-streaming-asr/bin/medasr-ctc.dart
@@ -0,0 +1,54 @@
+// Copyright (c)  2025  Xiaomi Corporation
+import 'dart:io';
+
+import 'package:args/args.dart';
+import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
+
+import './init.dart';
+
+void main(List<String> arguments) async {
+  await initSherpaOnnx();
+
+  final parser = ArgParser()
+    ..addOption('model', help: 'Path to the MedASR CTC model')
+    ..addOption('tokens', help: 'Path to tokens.txt')
+    ..addOption('input-wav', help: 'Path to input.wav to transcribe');
+
+  final res = parser.parse(arguments);
+  if (res['model'] == null ||
+      res['tokens'] == null ||
+      res['input-wav'] == null) {
+    print(parser.usage);
+    exit(1);
+  }
+
+  final model = res['model'] as String;
+  final tokens = res['tokens'] as String;
+  final inputWav = res['input-wav'] as String;
+
+  final medasr = sherpa_onnx.OfflineMedAsrCtcModelConfig(model: model);
+
+  final modelConfig = sherpa_onnx.OfflineModelConfig(
+    medasr: medasr,
+    tokens: tokens,
+    debug: true,
+    numThreads: 1,
+  );
+  final config = sherpa_onnx.OfflineRecognizerConfig(model: modelConfig);
+  final recognizer = sherpa_onnx.OfflineRecognizer(config);
+
+  final waveData = sherpa_onnx.readWave(inputWav);
+  final stream = recognizer.createStream();
+
+  stream.acceptWaveform(
+    samples: waveData.samples,
+    sampleRate: waveData.sampleRate,
+  );
+  recognizer.decode(stream);
+
+  final result = recognizer.getResult(stream);
+  print(result.text);
+
+  stream.free();
+  recognizer.free();
+}
diff --git a/dart-api-examples/non-streaming-asr/run-medasr-ctc.sh b/dart-api-examples/non-streaming-asr/run-medasr-ctc.sh
new file mode 100755
index 00000000..9b509389
--- /dev/null
+++ b/dart-api-examples/non-streaming-asr/run-medasr-ctc.sh
@@ -0,0 +1,17 @@
+#!/usr/bin/env bash
+
+set -ex
+
+dart pub get
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+dart run \
+  ./bin/medasr-ctc.dart \
+  --model ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx \
+  --tokens ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt \
+  --input-wav ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav
diff --git a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
index 4fba68bf..c1b86013 100644
--- a/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
+++ b/flutter/sherpa_onnx/lib/src/offline_recognizer.dart
@@ -31,10 +31,10 @@ class OfflineTransducerModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'encoder': encoder,
-        'decoder': decoder,
-        'joiner': joiner,
-      };
+    'encoder': encoder,
+    'decoder': decoder,
+    'joiner': joiner,
+  };
 
   final String encoder;
   final String decoder;
@@ -45,9 +45,7 @@ class OfflineParaformerModelConfig {
   const OfflineParaformerModelConfig({this.model = ''});
 
   factory OfflineParaformerModelConfig.fromJson(Map<String, dynamic> json) {
-    return OfflineParaformerModelConfig(
-      model: json['model'] as String? ?? '',
-    );
+    return OfflineParaformerModelConfig(model: json['model'] as String? ?? '');
   }
 
   @override
@@ -55,9 +53,7 @@ class OfflineParaformerModelConfig {
     return 'OfflineParaformerModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -76,9 +72,7 @@ class OfflineNemoEncDecCtcModelConfig {
     return 'OfflineNemoEncDecCtcModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -87,9 +81,7 @@ class OfflineDolphinModelConfig {
   const OfflineDolphinModelConfig({this.model = ''});
 
   factory OfflineDolphinModelConfig.fromJson(Map<String, dynamic> json) {
-    return OfflineDolphinModelConfig(
-      model: json['model'] as String? ?? '',
-    );
+    return OfflineDolphinModelConfig(model: json['model'] as String? ?? '');
   }
 
   @override
@@ -97,9 +89,7 @@ class OfflineDolphinModelConfig {
     return 'OfflineDolphinModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -118,9 +108,7 @@ class OfflineZipformerCtcModelConfig {
     return 'OfflineZipformerCtcModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -129,9 +117,7 @@ class OfflineWenetCtcModelConfig {
   const OfflineWenetCtcModelConfig({this.model = ''});
 
   factory OfflineWenetCtcModelConfig.fromJson(Map<String, dynamic> json) {
-    return OfflineWenetCtcModelConfig(
-      model: json['model'] as String? ?? '',
-    );
+    return OfflineWenetCtcModelConfig(model: json['model'] as String? ?? '');
   }
 
   @override
@@ -139,9 +125,7 @@ class OfflineWenetCtcModelConfig {
     return 'OfflineWenetCtcModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -149,7 +133,9 @@ class OfflineWenetCtcModelConfig {
 class OfflineOmnilingualAsrCtcModelConfig {
   const OfflineOmnilingualAsrCtcModelConfig({this.model = ''});
 
-  factory OfflineOmnilingualAsrCtcModelConfig.fromJson(Map<String, dynamic> json) {
+  factory OfflineOmnilingualAsrCtcModelConfig.fromJson(
+    Map<String, dynamic> json,
+  ) {
     return OfflineOmnilingualAsrCtcModelConfig(
       model: json['model'] as String? ?? '',
     );
@@ -160,20 +146,36 @@ class OfflineOmnilingualAsrCtcModelConfig {
     return 'OfflineOmnilingualAsrCtcModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
+
+  final String model;
+}
+
+class OfflineMedAsrCtcModelConfig {
+  const OfflineMedAsrCtcModelConfig({this.model = ''});
+
+  factory OfflineMedAsrCtcModelConfig.fromJson(Map<String, dynamic> json) {
+    return OfflineMedAsrCtcModelConfig(model: json['model'] as String? ?? '');
+  }
+
+  @override
+  String toString() {
+    return 'OfflineMedAsrCtcModelConfig(model: $model)';
+  }
+
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
 
 class OfflineWhisperModelConfig {
-  const OfflineWhisperModelConfig(
-      {this.encoder = '',
-      this.decoder = '',
-      this.language = '',
-      this.task = '',
-      this.tailPaddings = -1});
+  const OfflineWhisperModelConfig({
+    this.encoder = '',
+    this.decoder = '',
+    this.language = '',
+    this.task = '',
+    this.tailPaddings = -1,
+  });
 
   factory OfflineWhisperModelConfig.fromJson(Map<String, dynamic> json) {
     return OfflineWhisperModelConfig(
@@ -191,12 +193,12 @@ class OfflineWhisperModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'encoder': encoder,
-        'decoder': decoder,
-        'language': language,
-        'task': task,
-        'tailPaddings': tailPaddings,
-      };
+    'encoder': encoder,
+    'decoder': decoder,
+    'language': language,
+    'task': task,
+    'tailPaddings': tailPaddings,
+  };
 
   final String encoder;
   final String decoder;
@@ -206,12 +208,13 @@ class OfflineWhisperModelConfig {
 }
 
 class OfflineCanaryModelConfig {
-  const OfflineCanaryModelConfig(
-      {this.encoder = '',
-      this.decoder = '',
-      this.srcLang = 'en',
-      this.tgtLang = 'en',
-      this.usePnc = true});
+  const OfflineCanaryModelConfig({
+    this.encoder = '',
+    this.decoder = '',
+    this.srcLang = 'en',
+    this.tgtLang = 'en',
+    this.usePnc = true,
+  });
 
   factory OfflineCanaryModelConfig.fromJson(Map<String, dynamic> json) {
     return OfflineCanaryModelConfig(
@@ -229,12 +232,12 @@ class OfflineCanaryModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'encoder': encoder,
-        'decoder': decoder,
-        'srcLang': srcLang,
-        'tgtLang': tgtLang,
-        'usePnc': usePnc,
-      };
+    'encoder': encoder,
+    'decoder': decoder,
+    'srcLang': srcLang,
+    'tgtLang': tgtLang,
+    'usePnc': usePnc,
+  };
 
   final String encoder;
   final String decoder;
@@ -258,21 +261,19 @@ class OfflineFireRedAsrModelConfig {
     return 'OfflineFireRedAsrModelConfig(encoder: $encoder, decoder: $decoder)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'encoder': encoder,
-        'decoder': decoder,
-      };
+  Map<String, dynamic> toJson() => {'encoder': encoder, 'decoder': decoder};
 
   final String encoder;
   final String decoder;
 }
 
 class OfflineMoonshineModelConfig {
-  const OfflineMoonshineModelConfig(
-      {this.preprocessor = '',
-      this.encoder = '',
-      this.uncachedDecoder = '',
-      this.cachedDecoder = ''});
+  const OfflineMoonshineModelConfig({
+    this.preprocessor = '',
+    this.encoder = '',
+    this.uncachedDecoder = '',
+    this.cachedDecoder = '',
+  });
 
   factory OfflineMoonshineModelConfig.fromJson(Map<String, dynamic> json) {
     return OfflineMoonshineModelConfig(
@@ -289,11 +290,11 @@ class OfflineMoonshineModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'preprocessor': preprocessor,
-        'encoder': encoder,
-        'uncachedDecoder': uncachedDecoder,
-        'cachedDecoder': cachedDecoder,
-      };
+    'preprocessor': preprocessor,
+    'encoder': encoder,
+    'uncachedDecoder': uncachedDecoder,
+    'cachedDecoder': cachedDecoder,
+  };
 
   final String preprocessor;
   final String encoder;
@@ -305,9 +306,7 @@ class OfflineTdnnModelConfig {
   const OfflineTdnnModelConfig({this.model = ''});
 
   factory OfflineTdnnModelConfig.fromJson(Map<String, dynamic> json) {
-    return OfflineTdnnModelConfig(
-      model: json['model'] as String? ?? '',
-    );
+    return OfflineTdnnModelConfig(model: json['model'] as String? ?? '');
   }
 
   @override
@@ -315,9 +314,7 @@ class OfflineTdnnModelConfig {
     return 'OfflineTdnnModelConfig(model: $model)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-      };
+  Map<String, dynamic> toJson() => {'model': model};
 
   final String model;
 }
@@ -344,10 +341,10 @@ class OfflineSenseVoiceModelConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'model': model,
-        'language': language,
-        'useInverseTextNormalization': useInverseTextNormalization,
-      };
+    'model': model,
+    'language': language,
+    'useInverseTextNormalization': useInverseTextNormalization,
+  };
 
   final String model;
   final String language;
@@ -369,10 +366,7 @@ class OfflineLMConfig {
     return 'OfflineLMConfig(model: $model, scale: $scale)';
   }
 
-  Map<String, dynamic> toJson() => {
-        'model': model,
-        'scale': scale,
-      };
+  Map<String, dynamic> toJson() => {'model': model, 'scale': scale};
 
   final String model;
   final double scale;
@@ -393,6 +387,7 @@ class OfflineModelConfig {
     this.canary = const OfflineCanaryModelConfig(),
     this.wenetCtc = const OfflineWenetCtcModelConfig(),
     this.omnilingual = const OfflineOmnilingualAsrCtcModelConfig(),
+    this.medasr = const OfflineMedAsrCtcModelConfig(),
     required this.tokens,
     this.numThreads = 1,
     this.debug = true,
@@ -407,56 +402,74 @@ class OfflineModelConfig {
     return OfflineModelConfig(
       transducer: json['transducer'] != null
           ? OfflineTransducerModelConfig.fromJson(
-              json['transducer'] as Map<String, dynamic>)
+              json['transducer'] as Map<String, dynamic>,
+            )
           : const OfflineTransducerModelConfig(),
       paraformer: json['paraformer'] != null
           ? OfflineParaformerModelConfig.fromJson(
-              json['paraformer'] as Map<String, dynamic>)
+              json['paraformer'] as Map<String, dynamic>,
+            )
           : const OfflineParaformerModelConfig(),
       nemoCtc: json['nemoCtc'] != null
           ? OfflineNemoEncDecCtcModelConfig.fromJson(
-              json['nemoCtc'] as Map<String, dynamic>)
+              json['nemoCtc'] as Map<String, dynamic>,
+            )
           : const OfflineNemoEncDecCtcModelConfig(),
       whisper: json['whisper'] != null
           ? OfflineWhisperModelConfig.fromJson(
-              json['whisper'] as Map<String, dynamic>)
+              json['whisper'] as Map<String, dynamic>,
+            )
           : const OfflineWhisperModelConfig(),
       tdnn: json['tdnn'] != null
           ? OfflineTdnnModelConfig.fromJson(
-              json['tdnn'] as Map<String, dynamic>)
+              json['tdnn'] as Map<String, dynamic>,
+            )
           : const OfflineTdnnModelConfig(),
       senseVoice: json['senseVoice'] != null
           ? OfflineSenseVoiceModelConfig.fromJson(
-              json['senseVoice'] as Map<String, dynamic>)
+              json['senseVoice'] as Map<String, dynamic>,
+            )
           : const OfflineSenseVoiceModelConfig(),
       moonshine: json['moonshine'] != null
           ? OfflineMoonshineModelConfig.fromJson(
-              json['moonshine'] as Map<String, dynamic>)
+              json['moonshine'] as Map<String, dynamic>,
+            )
           : const OfflineMoonshineModelConfig(),
       fireRedAsr: json['fireRedAsr'] != null
           ? OfflineFireRedAsrModelConfig.fromJson(
-              json['fireRedAsr'] as Map<String, dynamic>)
+              json['fireRedAsr'] as Map<String, dynamic>,
+            )
           : const OfflineFireRedAsrModelConfig(),
       dolphin: json['dolphin'] != null
           ? OfflineDolphinModelConfig.fromJson(
-              json['dolphin'] as Map<String, dynamic>)
+              json['dolphin'] as Map<String, dynamic>,
+            )
           : const OfflineDolphinModelConfig(),
       zipformerCtc: json['zipformerCtc'] != null
           ? OfflineZipformerCtcModelConfig.fromJson(
-              json['zipformerCtc'] as Map<String, dynamic>)
+              json['zipformerCtc'] as Map<String, dynamic>,
+            )
           : const OfflineZipformerCtcModelConfig(),
       canary: json['canary'] != null
           ? OfflineCanaryModelConfig.fromJson(
-              json['canary'] as Map<String, dynamic>)
+              json['canary'] as Map<String, dynamic>,
+            )
           : const OfflineCanaryModelConfig(),
       wenetCtc: json['wenetCtc'] != null
           ? OfflineWenetCtcModelConfig.fromJson(
-              json['wenetCtc'] as Map<String, dynamic>)
+              json['wenetCtc'] as Map<String, dynamic>,
+            )
           : const OfflineWenetCtcModelConfig(),
       omnilingual: json['omnilingual'] != null
           ? OfflineOmnilingualAsrCtcModelConfig.fromJson(
-              json['omnilingual'] as Map<String, dynamic>)
+              json['omnilingual'] as Map<String, dynamic>,
+            )
           : const OfflineOmnilingualAsrCtcModelConfig(),
+      medasr: json['medasr'] != null
+          ? OfflineMedAsrCtcModelConfig.fromJson(
+              json['medasr'] as Map<String, dynamic>,
+            )
+          : const OfflineMedAsrCtcModelConfig(),
       tokens: json['tokens'] as String,
       numThreads: json['numThreads'] as int? ?? 1,
       debug: json['debug'] as bool? ?? true,
@@ -470,32 +483,33 @@ class OfflineModelConfig {
 
   @override
   String toString() {
-    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, omnilingual: $omnilingual, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
+    return 'OfflineModelConfig(transducer: $transducer, paraformer: $paraformer, nemoCtc: $nemoCtc, whisper: $whisper, tdnn: $tdnn, senseVoice: $senseVoice, moonshine: $moonshine, fireRedAsr: $fireRedAsr, dolphin: $dolphin, zipformerCtc: $zipformerCtc, canary: $canary, wenetCtc: $wenetCtc, omnilingual: $omnilingual, medasr: $medasr, tokens: $tokens, numThreads: $numThreads, debug: $debug, provider: $provider, modelType: $modelType, modelingUnit: $modelingUnit, bpeVocab: $bpeVocab, telespeechCtc: $telespeechCtc)';
   }
 
   Map<String, dynamic> toJson() => {
-        'transducer': transducer.toJson(),
-        'paraformer': paraformer.toJson(),
-        'nemoCtc': nemoCtc.toJson(),
-        'whisper': whisper.toJson(),
-        'tdnn': tdnn.toJson(),
-        'senseVoice': senseVoice.toJson(),
-        'moonshine': moonshine.toJson(),
-        'fireRedAsr': fireRedAsr.toJson(),
-        'dolphin': dolphin.toJson(),
-        'zipformerCtc': zipformerCtc.toJson(),
-        'canary': canary.toJson(),
-        'wenetCtc': wenetCtc.toJson(),
-        'omnilingual': omnilingual.toJson(),
-        'tokens': tokens,
-        'numThreads': numThreads,
-        'debug': debug,
-        'provider': provider,
-        'modelType': modelType,
-        'modelingUnit': modelingUnit,
-        'bpeVocab': bpeVocab,
-        'telespeechCtc': telespeechCtc,
-      };
+    'transducer': transducer.toJson(),
+    'paraformer': paraformer.toJson(),
+    'nemoCtc': nemoCtc.toJson(),
+    'whisper': whisper.toJson(),
+    'tdnn': tdnn.toJson(),
+    'senseVoice': senseVoice.toJson(),
+    'moonshine': moonshine.toJson(),
+    'fireRedAsr': fireRedAsr.toJson(),
+    'dolphin': dolphin.toJson(),
+    'zipformerCtc': zipformerCtc.toJson(),
+    'canary': canary.toJson(),
+    'wenetCtc': wenetCtc.toJson(),
+    'omnilingual': omnilingual.toJson(),
+    'medasr': medasr.toJson(),
+    'tokens': tokens,
+    'numThreads': numThreads,
+    'debug': debug,
+    'provider': provider,
+    'modelType': modelType,
+    'modelingUnit': modelingUnit,
+    'bpeVocab': bpeVocab,
+    'telespeechCtc': telespeechCtc,
+  };
 
   final OfflineTransducerModelConfig transducer;
   final OfflineParaformerModelConfig paraformer;
@@ -510,6 +524,7 @@ class OfflineModelConfig {
   final OfflineCanaryModelConfig canary;
   final OfflineWenetCtcModelConfig wenetCtc;
   final OfflineOmnilingualAsrCtcModelConfig omnilingual;
+  final OfflineMedAsrCtcModelConfig medasr;
 
   final String tokens;
   final int numThreads;
@@ -562,18 +577,18 @@ class OfflineRecognizerConfig {
   }
 
   Map<String, dynamic> toJson() => {
-        'feat': feat.toJson(),
-        'model': model.toJson(),
-        'lm': lm.toJson(),
-        'decodingMethod': decodingMethod,
-        'maxActivePaths': maxActivePaths,
-        'hotwordsFile': hotwordsFile,
-        'hotwordsScore': hotwordsScore,
-        'ruleFsts': ruleFsts,
-        'ruleFars': ruleFars,
-        'blankPenalty': blankPenalty,
-        'hr': hr.toJson(),
-      };
+    'feat': feat.toJson(),
+    'model': model.toJson(),
+    'lm': lm.toJson(),
+    'decodingMethod': decodingMethod,
+    'maxActivePaths': maxActivePaths,
+    'hotwordsFile': hotwordsFile,
+    'hotwordsScore': hotwordsScore,
+    'ruleFsts': ruleFsts,
+    'ruleFars': ruleFars,
+    'blankPenalty': blankPenalty,
+    'hr': hr.toJson(),
+  };
 
   final FeatureConfig feat;
   final OfflineModelConfig model;
@@ -594,19 +609,21 @@ class OfflineRecognizerConfig {
 }
 
 class OfflineRecognizerResult {
-  OfflineRecognizerResult(
-      {required this.text,
-      required this.tokens,
-      required this.timestamps,
-      required this.lang,
-      required this.emotion,
-      required this.event});
+  OfflineRecognizerResult({
+    required this.text,
+    required this.tokens,
+    required this.timestamps,
+    required this.lang,
+    required this.emotion,
+    required this.event,
+  });
 
   factory OfflineRecognizerResult.fromJson(Map<String, dynamic> json) {
     return OfflineRecognizerResult(
       text: json['text'] as String? ?? '',
       tokens: (json['tokens'] as List?)?.map((e) => e as String).toList() ?? [],
-      timestamps: (json['timestamps'] as List?)
+      timestamps:
+          (json['timestamps'] as List?)
               ?.map((e) => (e as num).toDouble())
               .toList() ??
           [],
@@ -622,13 +639,13 @@ class OfflineRecognizerResult {
   }
 
   Map<String, dynamic> toJson() => {
-        'text': text,
-        'tokens': tokens,
-        'timestamps': timestamps,
-        'lang': lang,
-        'emotion': emotion,
-        'event': event,
-      };
+    'text': text,
+    'tokens': tokens,
+    'timestamps': timestamps,
+    'lang': lang,
+    'emotion': emotion,
+    'event': event,
+  };
 
   final String text;
   final List<String> tokens;
@@ -662,7 +679,8 @@ class OfflineRecognizer {
 
     if (ptr == nullptr) {
       throw Exception(
-          "Failed to create offline recognizer. Please check your config");
+        "Failed to create offline recognizer. Please check your config",
+      );
     }
 
     freeConfig(c);
@@ -680,19 +698,20 @@ class OfflineRecognizer {
   }
 
   static Pointer<SherpaOnnxOfflineRecognizerConfig> convertConfig(
-      OfflineRecognizerConfig config) {
+    OfflineRecognizerConfig config,
+  ) {
     final c = calloc<SherpaOnnxOfflineRecognizerConfig>();
 
     c.ref.feat.sampleRate = config.feat.sampleRate;
     c.ref.feat.featureDim = config.feat.featureDim;
 
     // transducer
-    c.ref.model.transducer.encoder =
-        config.model.transducer.encoder.toNativeUtf8();
-    c.ref.model.transducer.decoder =
-        config.model.transducer.decoder.toNativeUtf8();
-    c.ref.model.transducer.joiner =
-        config.model.transducer.joiner.toNativeUtf8();
+    c.ref.model.transducer.encoder = config.model.transducer.encoder
+        .toNativeUtf8();
+    c.ref.model.transducer.decoder = config.model.transducer.decoder
+        .toNativeUtf8();
+    c.ref.model.transducer.joiner = config.model.transducer.joiner
+        .toNativeUtf8();
 
     // paraformer
     c.ref.model.paraformer.model = config.model.paraformer.model.toNativeUtf8();
@@ -715,30 +734,33 @@ class OfflineRecognizer {
 
     c.ref.model.senseVoice.model = config.model.senseVoice.model.toNativeUtf8();
 
-    c.ref.model.senseVoice.language =
-        config.model.senseVoice.language.toNativeUtf8();
+    c.ref.model.senseVoice.language = config.model.senseVoice.language
+        .toNativeUtf8();
 
     c.ref.model.senseVoice.useInverseTextNormalization =
         config.model.senseVoice.useInverseTextNormalization ? 1 : 0;
 
-    c.ref.model.moonshine.preprocessor =
-        config.model.moonshine.preprocessor.toNativeUtf8();
-    c.ref.model.moonshine.encoder =
-        config.model.moonshine.encoder.toNativeUtf8();
-    c.ref.model.moonshine.uncachedDecoder =
-        config.model.moonshine.uncachedDecoder.toNativeUtf8();
-    c.ref.model.moonshine.cachedDecoder =
-        config.model.moonshine.cachedDecoder.toNativeUtf8();
+    c.ref.model.moonshine.preprocessor = config.model.moonshine.preprocessor
+        .toNativeUtf8();
+    c.ref.model.moonshine.encoder = config.model.moonshine.encoder
+        .toNativeUtf8();
+    c.ref.model.moonshine.uncachedDecoder = config
+        .model
+        .moonshine
+        .uncachedDecoder
+        .toNativeUtf8();
+    c.ref.model.moonshine.cachedDecoder = config.model.moonshine.cachedDecoder
+        .toNativeUtf8();
 
     // FireRedAsr
-    c.ref.model.fireRedAsr.encoder =
-        config.model.fireRedAsr.encoder.toNativeUtf8();
-    c.ref.model.fireRedAsr.decoder =
-        config.model.fireRedAsr.decoder.toNativeUtf8();
+    c.ref.model.fireRedAsr.encoder = config.model.fireRedAsr.encoder
+        .toNativeUtf8();
+    c.ref.model.fireRedAsr.decoder = config.model.fireRedAsr.decoder
+        .toNativeUtf8();
 
     c.ref.model.dolphin.model = config.model.dolphin.model.toNativeUtf8();
-    c.ref.model.zipformerCtc.model =
-        config.model.zipformerCtc.model.toNativeUtf8();
+    c.ref.model.zipformerCtc.model = config.model.zipformerCtc.model
+        .toNativeUtf8();
 
     c.ref.model.canary.encoder = config.model.canary.encoder.toNativeUtf8();
     c.ref.model.canary.decoder = config.model.canary.decoder.toNativeUtf8();
@@ -747,7 +769,9 @@ class OfflineRecognizer {
     c.ref.model.canary.usePnc = config.model.canary.usePnc ? 1 : 0;
 
     c.ref.model.wenetCtc.model = config.model.wenetCtc.model.toNativeUtf8();
-    c.ref.model.omnilingual.model = config.model.omnilingual.model.toNativeUtf8();
+    c.ref.model.omnilingual.model = config.model.omnilingual.model
+        .toNativeUtf8();
+    c.ref.model.medasr.model = config.model.medasr.model.toNativeUtf8();
 
     c.ref.model.tokens = config.model.tokens.toNativeUtf8();
 
@@ -793,6 +817,7 @@ class OfflineRecognizer {
     calloc.free(c.ref.model.modelType);
     calloc.free(c.ref.model.provider);
     calloc.free(c.ref.model.tokens);
+    calloc.free(c.ref.model.medasr.model);
     calloc.free(c.ref.model.omnilingual.model);
     calloc.free(c.ref.model.wenetCtc.model);
     calloc.free(c.ref.model.canary.tgtLang);
@@ -836,15 +861,16 @@ class OfflineRecognizer {
   OfflineRecognizerResult getResult(OfflineStream stream) {
     final json =
         SherpaOnnxBindings.getOfflineStreamResultAsJson?.call(stream.ptr) ??
-            nullptr;
+        nullptr;
     if (json == nullptr) {
       return OfflineRecognizerResult(
-          text: '',
-          tokens: [],
-          timestamps: [],
-          lang: '',
-          emotion: '',
-          event: '');
+        text: '',
+        tokens: [],
+        timestamps: [],
+        lang: '',
+        emotion: '',
+        event: '',
+      );
     }
 
     final parsedJson = jsonDecode(toDartString(json));
@@ -852,12 +878,13 @@ class OfflineRecognizer {
     SherpaOnnxBindings.destroyOfflineStreamResultJson?.call(json);
 
     return OfflineRecognizerResult(
-        text: parsedJson['text'],
-        tokens: List<String>.from(parsedJson['tokens']),
-        timestamps: List<double>.from(parsedJson['timestamps']),
-        lang: parsedJson['lang'],
-        emotion: parsedJson['emotion'],
-        event: parsedJson['event']);
+      text: parsedJson['text'],
+      tokens: List<String>.from(parsedJson['tokens']),
+      timestamps: List<double>.from(parsedJson['timestamps']),
+      lang: parsedJson['lang'],
+      emotion: parsedJson['emotion'],
+      event: parsedJson['event'],
+    );
   }
 
   Pointer<SherpaOnnxOfflineRecognizer> ptr;
diff --git a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
index 9009be24..7bdf66b0 100644
--- a/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
+++ b/flutter/sherpa_onnx/lib/src/sherpa_onnx_bindings.dart
@@ -311,6 +311,10 @@ final class SherpaOnnxOfflineOmnilingualAsrCtcModelConfig extends Struct {
   external Pointer<Utf8> model;
 }
 
+final class SherpaOnnxOfflineMedAsrCtcModelConfig extends Struct {
+  external Pointer<Utf8> model;
+}
+
 final class SherpaOnnxOfflineWhisperModelConfig extends Struct {
   external Pointer<Utf8> encoder;
   external Pointer<Utf8> decoder;
@@ -392,6 +396,7 @@ final class SherpaOnnxOfflineModelConfig extends Struct {
   external SherpaOnnxOfflineCanaryModelConfig canary;
   external SherpaOnnxOfflineWenetCtcModelConfig wenetCtc;
   external SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+  external SherpaOnnxOfflineMedAsrCtcModelConfig medasr;
 }
 
 final class SherpaOnnxOfflineRecognizerConfig extends Struct {
@@ -681,282 +686,348 @@ final class SherpaOnnxSpokenLanguageIdentification extends Opaque {}
 
 final class SherpaOnnxOfflineSpeechDenoiser extends Opaque {}
 
-typedef SherpaOnnxCreateOfflineSpeechDenoiserNative
-    = Pointer<SherpaOnnxOfflineSpeechDenoiser> Function(
-        Pointer<SherpaOnnxOfflineSpeechDenoiserConfig>);
-
-typedef SherpaOnnxCreateOfflineSpeechDenoiser
-    = SherpaOnnxCreateOfflineSpeechDenoiserNative;
-
-typedef SherpaOnnxDestroyOfflineSpeechDenoiserNative = Void Function(
-    Pointer<SherpaOnnxOfflineSpeechDenoiser>);
-
-typedef SherpaOnnxDestroyOfflineSpeechDenoiser = void Function(
-    Pointer<SherpaOnnxOfflineSpeechDenoiser>);
-
-typedef SherpaOnnxOfflineSpeechDenoiserGetSampleRateNative = Int32 Function(
-    Pointer<SherpaOnnxOfflineSpeechDenoiser>);
-
-typedef SherpaOnnxOfflineSpeechDenoiserGetSampleRate = int Function(
-    Pointer<SherpaOnnxOfflineSpeechDenoiser>);
-
-typedef SherpaOnnxOfflineSpeechDenoiserRunNative
-    = Pointer<SherpaOnnxDenoisedAudio> Function(
-        Pointer<SherpaOnnxOfflineSpeechDenoiser>, Pointer<Float>, Int32, Int32);
-
-typedef SherpaOnnxOfflineSpeechDenoiserRun
-    = Pointer<SherpaOnnxDenoisedAudio> Function(
-        Pointer<SherpaOnnxOfflineSpeechDenoiser>, Pointer<Float>, int, int);
-
-typedef SherpaOnnxDestroyDenoisedAudioNative = Void Function(
-    Pointer<SherpaOnnxDenoisedAudio>);
-
-typedef SherpaOnnxDestroyDenoisedAudio = void Function(
-    Pointer<SherpaOnnxDenoisedAudio>);
-
-typedef SherpaOnnxCreateSpokenLanguageIdentificationNative
-    = Pointer<SherpaOnnxSpokenLanguageIdentification> Function(
-        Pointer<SherpaOnnxSpokenLanguageIdentificationConfig>);
-
-typedef SherpaOnnxCreateSpokenLanguageIdentification
-    = SherpaOnnxCreateSpokenLanguageIdentificationNative;
-
-typedef SherpaOnnxDestroySpokenLanguageIdentificationNative = Void Function(
-    Pointer<SherpaOnnxSpokenLanguageIdentification>);
-
-typedef SherpaOnnxDestroySpokenLanguageIdentification = void Function(
-    Pointer<SherpaOnnxSpokenLanguageIdentification>);
-
-typedef SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative
-    = Pointer<SherpaOnnxOfflineStream> Function(
-        Pointer<SherpaOnnxSpokenLanguageIdentification>);
-
-typedef SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream
-    = SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative;
-
-typedef SherpaOnnxSpokenLanguageIdentificationComputeNative
-    = Pointer<SherpaOnnxSpokenLanguageIdentificationResult> Function(
-        Pointer<SherpaOnnxSpokenLanguageIdentification>,
-        Pointer<SherpaOnnxOfflineStream>);
-
-typedef SherpaOnnxSpokenLanguageIdentificationCompute
-    = SherpaOnnxSpokenLanguageIdentificationComputeNative;
-
-typedef SherpaOnnxDestroySpokenLanguageIdentificationResultNative = Void
-    Function(Pointer<SherpaOnnxSpokenLanguageIdentificationResult>);
-
-typedef SherpaOnnxDestroySpokenLanguageIdentificationResult = void Function(
-    Pointer<SherpaOnnxSpokenLanguageIdentificationResult>);
+typedef SherpaOnnxCreateOfflineSpeechDenoiserNative =
+    Pointer<SherpaOnnxOfflineSpeechDenoiser> Function(
+      Pointer<SherpaOnnxOfflineSpeechDenoiserConfig>,
+    );
 
-typedef SherpaOnnxCreateOfflineSpeakerDiarizationNative
-    = Pointer<SherpaOnnxOfflineSpeakerDiarization> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>);
+typedef SherpaOnnxCreateOfflineSpeechDenoiser =
+    SherpaOnnxCreateOfflineSpeechDenoiserNative;
 
-typedef SherpaOnnxCreateOfflineSpeakerDiarization
-    = SherpaOnnxCreateOfflineSpeakerDiarizationNative;
+typedef SherpaOnnxDestroyOfflineSpeechDenoiserNative =
+    Void Function(Pointer<SherpaOnnxOfflineSpeechDenoiser>);
 
-typedef SherpaOnnxDestroyOfflineSpeakerDiarizationNative = Void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+typedef SherpaOnnxDestroyOfflineSpeechDenoiser =
+    void Function(Pointer<SherpaOnnxOfflineSpeechDenoiser>);
 
-typedef SherpaOnnxDestroyOfflineSpeakerDiarization = void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+typedef SherpaOnnxOfflineSpeechDenoiserGetSampleRateNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineSpeechDenoiser>);
 
-typedef SherpaOnnxCreateOfflinePunctuationNative
-    = Pointer<SherpaOnnxOfflinePunctuation> Function(
-        Pointer<SherpaOnnxOfflinePunctuationConfig>);
+typedef SherpaOnnxOfflineSpeechDenoiserGetSampleRate =
+    int Function(Pointer<SherpaOnnxOfflineSpeechDenoiser>);
 
-typedef SherpaOnnxCreateOnlinePunctuationNative
-    = Pointer<SherpaOnnxOnlinePunctuation> Function(
-        Pointer<SherpaOnnxOnlinePunctuationConfig>);
+typedef SherpaOnnxOfflineSpeechDenoiserRunNative =
+    Pointer<SherpaOnnxDenoisedAudio> Function(
+      Pointer<SherpaOnnxOfflineSpeechDenoiser>,
+      Pointer<Float>,
+      Int32,
+      Int32,
+    );
 
-typedef SherpaOnnxOfflineSpeakerDiarizationGetSampleRateNative = Int32 Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+typedef SherpaOnnxOfflineSpeechDenoiserRun =
+    Pointer<SherpaOnnxDenoisedAudio> Function(
+      Pointer<SherpaOnnxOfflineSpeechDenoiser>,
+      Pointer<Float>,
+      int,
+      int,
+    );
 
-typedef SherpaOnnxOfflineSpeakerDiarizationGetSampleRate = int Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+typedef SherpaOnnxDestroyDenoisedAudioNative =
+    Void Function(Pointer<SherpaOnnxDenoisedAudio>);
 
-typedef SherpaOnnxOfflineSpeakerDiarizationSetConfigNative = Void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>,
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>);
+typedef SherpaOnnxDestroyDenoisedAudio =
+    void Function(Pointer<SherpaOnnxDenoisedAudio>);
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakersNative = Int32
-    Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+typedef SherpaOnnxCreateSpokenLanguageIdentificationNative =
+    Pointer<SherpaOnnxSpokenLanguageIdentification> Function(
+      Pointer<SherpaOnnxSpokenLanguageIdentificationConfig>,
+    );
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers = int Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+typedef SherpaOnnxCreateSpokenLanguageIdentification =
+    SherpaOnnxCreateSpokenLanguageIdentificationNative;
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegmentsNative = Int32
-    Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+typedef SherpaOnnxDestroySpokenLanguageIdentificationNative =
+    Void Function(Pointer<SherpaOnnxSpokenLanguageIdentification>);
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments = int Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+typedef SherpaOnnxDestroySpokenLanguageIdentification =
+    void Function(Pointer<SherpaOnnxSpokenLanguageIdentification>);
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative
-    = Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+typedef SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative =
+    Pointer<SherpaOnnxOfflineStream> Function(
+      Pointer<SherpaOnnxSpokenLanguageIdentification>,
+    );
 
-typedef SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime
-    = SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative;
+typedef SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream =
+    SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative;
 
-typedef SherpaOnnxOfflineSpeakerDiarizationDestroySegmentNative = Void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment>);
+typedef SherpaOnnxSpokenLanguageIdentificationComputeNative =
+    Pointer<SherpaOnnxSpokenLanguageIdentificationResult> Function(
+      Pointer<SherpaOnnxSpokenLanguageIdentification>,
+      Pointer<SherpaOnnxOfflineStream>,
+    );
 
-typedef SherpaOnnxOfflineSpeakerDiarizationDestroySegment = void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment>);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationProcessNative
-    = Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarization>, Pointer<Float>, Int32);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationProcess
-    = Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarization>, Pointer<Float>, int);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative = Int32
-    Function(Int32, Int32);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArgNative
-    = Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarization>,
-        Pointer<Float>,
-        Int32,
-        Pointer<
-            NativeFunction<
-                SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative>>);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg
-    = Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
-        Pointer<SherpaOnnxOfflineSpeakerDiarization>,
-        Pointer<Float>,
-        int,
-        Pointer<
-            NativeFunction<
-                SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative>>);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationDestroyResultNative = Void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationDestroyResult = void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
-
-typedef SherpaOnnxOfflineSpeakerDiarizationSetConfig = void Function(
-    Pointer<SherpaOnnxOfflineSpeakerDiarization>,
-    Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>);
-
-typedef SherpaOnnxCreateOfflinePunctuation
-    = SherpaOnnxCreateOfflinePunctuationNative;
-
-typedef SherpaOnnxDestroyOfflinePunctuationNative = Void Function(
-    Pointer<SherpaOnnxOfflinePunctuation>);
-
-typedef SherpaOnnxDestroyOfflinePunctuation = void Function(
-    Pointer<SherpaOnnxOfflinePunctuation>);
-
-typedef SherpaOfflinePunctuationAddPunctNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxOfflinePunctuation>, Pointer<Utf8>);
-
-typedef SherpaOfflinePunctuationAddPunct
-    = SherpaOfflinePunctuationAddPunctNative;
+typedef SherpaOnnxSpokenLanguageIdentificationCompute =
+    SherpaOnnxSpokenLanguageIdentificationComputeNative;
+
+typedef SherpaOnnxDestroySpokenLanguageIdentificationResultNative =
+    Void Function(Pointer<SherpaOnnxSpokenLanguageIdentificationResult>);
+
+typedef SherpaOnnxDestroySpokenLanguageIdentificationResult =
+    void Function(Pointer<SherpaOnnxSpokenLanguageIdentificationResult>);
+
+typedef SherpaOnnxCreateOfflineSpeakerDiarizationNative =
+    Pointer<SherpaOnnxOfflineSpeakerDiarization> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>,
+    );
+
+typedef SherpaOnnxCreateOfflineSpeakerDiarization =
+    SherpaOnnxCreateOfflineSpeakerDiarizationNative;
+
+typedef SherpaOnnxDestroyOfflineSpeakerDiarizationNative =
+    Void Function(Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+
+typedef SherpaOnnxDestroyOfflineSpeakerDiarization =
+    void Function(Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+
+typedef SherpaOnnxCreateOfflinePunctuationNative =
+    Pointer<SherpaOnnxOfflinePunctuation> Function(
+      Pointer<SherpaOnnxOfflinePunctuationConfig>,
+    );
+
+typedef SherpaOnnxCreateOnlinePunctuationNative =
+    Pointer<SherpaOnnxOnlinePunctuation> Function(
+      Pointer<SherpaOnnxOnlinePunctuationConfig>,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationGetSampleRateNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationGetSampleRate =
+    int Function(Pointer<SherpaOnnxOfflineSpeakerDiarization>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationSetConfigNative =
+    Void Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakersNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers =
+    int Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegmentsNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments =
+    int Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative =
+    Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime =
+    SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative;
+
+typedef SherpaOnnxOfflineSpeakerDiarizationDestroySegmentNative =
+    Void Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationDestroySegment =
+    void Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationSegment>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationProcessNative =
+    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<Float>,
+      Int32,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationProcess =
+    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<Float>,
+      int,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative =
+    Int32 Function(Int32, Int32);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArgNative =
+    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<Float>,
+      Int32,
+      Pointer<
+        NativeFunction<
+          SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative
+        >
+      >,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg =
+    Pointer<SherpaOnnxOfflineSpeakerDiarizationResult> Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<Float>,
+      int,
+      Pointer<
+        NativeFunction<
+          SherpaOnnxOfflineSpeakerDiarizationProgressCallbackNoArgNative
+        >
+      >,
+    );
+
+typedef SherpaOnnxOfflineSpeakerDiarizationDestroyResultNative =
+    Void Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationDestroyResult =
+    void Function(Pointer<SherpaOnnxOfflineSpeakerDiarizationResult>);
+
+typedef SherpaOnnxOfflineSpeakerDiarizationSetConfig =
+    void Function(
+      Pointer<SherpaOnnxOfflineSpeakerDiarization>,
+      Pointer<SherpaOnnxOfflineSpeakerDiarizationConfig>,
+    );
+
+typedef SherpaOnnxCreateOfflinePunctuation =
+    SherpaOnnxCreateOfflinePunctuationNative;
+
+typedef SherpaOnnxDestroyOfflinePunctuationNative =
+    Void Function(Pointer<SherpaOnnxOfflinePunctuation>);
+
+typedef SherpaOnnxDestroyOfflinePunctuation =
+    void Function(Pointer<SherpaOnnxOfflinePunctuation>);
+
+typedef SherpaOfflinePunctuationAddPunctNative =
+    Pointer<Utf8> Function(
+      Pointer<SherpaOnnxOfflinePunctuation>,
+      Pointer<Utf8>,
+    );
+
+typedef SherpaOfflinePunctuationAddPunct =
+    SherpaOfflinePunctuationAddPunctNative;
 
 typedef SherpaOfflinePunctuationFreeTextNative = Void Function(Pointer<Utf8>);
 
 typedef SherpaOfflinePunctuationFreeText = void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxCreateOnlinePunctuation
-    = SherpaOnnxCreateOnlinePunctuationNative;
+typedef SherpaOnnxCreateOnlinePunctuation =
+    SherpaOnnxCreateOnlinePunctuationNative;
 
-typedef SherpaOnnxDestroyOnlinePunctuationNative = Void Function(
-    Pointer<SherpaOnnxOnlinePunctuation>);
+typedef SherpaOnnxDestroyOnlinePunctuationNative =
+    Void Function(Pointer<SherpaOnnxOnlinePunctuation>);
 
-typedef SherpaOnnxDestroyOnlinePunctuation = void Function(
-    Pointer<SherpaOnnxOnlinePunctuation>);
+typedef SherpaOnnxDestroyOnlinePunctuation =
+    void Function(Pointer<SherpaOnnxOnlinePunctuation>);
 
-typedef SherpaOnnxOnlinePunctuationAddPunctNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxOnlinePunctuation>, Pointer<Utf8>);
+typedef SherpaOnnxOnlinePunctuationAddPunctNative =
+    Pointer<Utf8> Function(Pointer<SherpaOnnxOnlinePunctuation>, Pointer<Utf8>);
 
-typedef SherpaOnnxOnlinePunctuationAddPunct
-    = SherpaOnnxOnlinePunctuationAddPunctNative;
+typedef SherpaOnnxOnlinePunctuationAddPunct =
+    SherpaOnnxOnlinePunctuationAddPunctNative;
 
-typedef SherpaOnnxOnlinePunctuationFreeTextNative = Void Function(
-    Pointer<Utf8>);
+typedef SherpaOnnxOnlinePunctuationFreeTextNative =
+    Void Function(Pointer<Utf8>);
 
 typedef SherpaOnnxOnlinePunctuationFreeText = void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxCreateAudioTaggingNative = Pointer<SherpaOnnxAudioTagging>
-    Function(Pointer<SherpaOnnxAudioTaggingConfig>);
+typedef SherpaOnnxCreateAudioTaggingNative =
+    Pointer<SherpaOnnxAudioTagging> Function(
+      Pointer<SherpaOnnxAudioTaggingConfig>,
+    );
 
 typedef SherpaOnnxCreateAudioTagging = SherpaOnnxCreateAudioTaggingNative;
 
-typedef SherpaOnnxDestroyAudioTaggingNative = Void Function(
-    Pointer<SherpaOnnxAudioTagging>);
+typedef SherpaOnnxDestroyAudioTaggingNative =
+    Void Function(Pointer<SherpaOnnxAudioTagging>);
 
-typedef SherpaOnnxDestroyAudioTagging = void Function(
-    Pointer<SherpaOnnxAudioTagging>);
+typedef SherpaOnnxDestroyAudioTagging =
+    void Function(Pointer<SherpaOnnxAudioTagging>);
 
-typedef SherpaOnnxAudioTaggingCreateOfflineStreamNative
-    = Pointer<SherpaOnnxOfflineStream> Function(
-        Pointer<SherpaOnnxAudioTagging>);
+typedef SherpaOnnxAudioTaggingCreateOfflineStreamNative =
+    Pointer<SherpaOnnxOfflineStream> Function(Pointer<SherpaOnnxAudioTagging>);
 
-typedef SherpaOnnxAudioTaggingCreateOfflineStream
-    = SherpaOnnxAudioTaggingCreateOfflineStreamNative;
+typedef SherpaOnnxAudioTaggingCreateOfflineStream =
+    SherpaOnnxAudioTaggingCreateOfflineStreamNative;
 
-typedef SherpaOnnxAudioTaggingComputeNative
-    = Pointer<Pointer<SherpaOnnxAudioEvent>> Function(
-        Pointer<SherpaOnnxAudioTagging>,
-        Pointer<SherpaOnnxOfflineStream>,
-        Int32);
+typedef SherpaOnnxAudioTaggingComputeNative =
+    Pointer<Pointer<SherpaOnnxAudioEvent>> Function(
+      Pointer<SherpaOnnxAudioTagging>,
+      Pointer<SherpaOnnxOfflineStream>,
+      Int32,
+    );
 
-typedef SherpaOnnxAudioTaggingCompute
-    = Pointer<Pointer<SherpaOnnxAudioEvent>> Function(
-        Pointer<SherpaOnnxAudioTagging>, Pointer<SherpaOnnxOfflineStream>, int);
+typedef SherpaOnnxAudioTaggingCompute =
+    Pointer<Pointer<SherpaOnnxAudioEvent>> Function(
+      Pointer<SherpaOnnxAudioTagging>,
+      Pointer<SherpaOnnxOfflineStream>,
+      int,
+    );
 
-typedef SherpaOnnxAudioTaggingFreeResultsNative = Void Function(
-    Pointer<Pointer<SherpaOnnxAudioEvent>>);
+typedef SherpaOnnxAudioTaggingFreeResultsNative =
+    Void Function(Pointer<Pointer<SherpaOnnxAudioEvent>>);
 
-typedef SherpaOnnxAudioTaggingFreeResults = void Function(
-    Pointer<Pointer<SherpaOnnxAudioEvent>>);
+typedef SherpaOnnxAudioTaggingFreeResults =
+    void Function(Pointer<Pointer<SherpaOnnxAudioEvent>>);
 
-typedef CreateKeywordSpotterNative = Pointer<SherpaOnnxKeywordSpotter> Function(
-    Pointer<SherpaOnnxKeywordSpotterConfig>);
+typedef CreateKeywordSpotterNative =
+    Pointer<SherpaOnnxKeywordSpotter> Function(
+      Pointer<SherpaOnnxKeywordSpotterConfig>,
+    );
 
 typedef CreateKeywordSpotter = CreateKeywordSpotterNative;
 
-typedef DestroyKeywordSpotterNative = Void Function(
-    Pointer<SherpaOnnxKeywordSpotter>);
+typedef DestroyKeywordSpotterNative =
+    Void Function(Pointer<SherpaOnnxKeywordSpotter>);
 
-typedef DestroyKeywordSpotter = void Function(
-    Pointer<SherpaOnnxKeywordSpotter>);
+typedef DestroyKeywordSpotter =
+    void Function(Pointer<SherpaOnnxKeywordSpotter>);
 
-typedef CreateKeywordStreamNative = Pointer<SherpaOnnxOnlineStream> Function(
-    Pointer<SherpaOnnxKeywordSpotter>);
+typedef CreateKeywordStreamNative =
+    Pointer<SherpaOnnxOnlineStream> Function(Pointer<SherpaOnnxKeywordSpotter>);
 
 typedef CreateKeywordStream = CreateKeywordStreamNative;
 
-typedef CreateKeywordStreamWithKeywordsNative = Pointer<SherpaOnnxOnlineStream>
-    Function(Pointer<SherpaOnnxKeywordSpotter>, Pointer<Utf8>);
+typedef CreateKeywordStreamWithKeywordsNative =
+    Pointer<SherpaOnnxOnlineStream> Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<Utf8>,
+    );
 
 typedef CreateKeywordStreamWithKeywords = CreateKeywordStreamWithKeywordsNative;
 
-typedef IsKeywordStreamReadyNative = Int32 Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef IsKeywordStreamReady = int Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef DecodeKeywordStreamNative = Void Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef DecodeKeywordStream = void Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef ResetKeywordStreamNative = Void Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef ResetKeywordStream = void Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef GetKeywordResultAsJsonNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxKeywordSpotter>, Pointer<SherpaOnnxOnlineStream>);
+typedef IsKeywordStreamReadyNative =
+    Int32 Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef IsKeywordStreamReady =
+    int Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef DecodeKeywordStreamNative =
+    Void Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef DecodeKeywordStream =
+    void Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef ResetKeywordStreamNative =
+    Void Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef ResetKeywordStream =
+    void Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef GetKeywordResultAsJsonNative =
+    Pointer<Utf8> Function(
+      Pointer<SherpaOnnxKeywordSpotter>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
 typedef GetKeywordResultAsJson = GetKeywordResultAsJsonNative;
 
@@ -964,103 +1035,134 @@ typedef FreeKeywordResultJsonNative = Void Function(Pointer<Utf8>);
 
 typedef FreeKeywordResultJson = void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxCreateOfflineTtsNative = Pointer<SherpaOnnxOfflineTts>
-    Function(Pointer<SherpaOnnxOfflineTtsConfig>);
+typedef SherpaOnnxCreateOfflineTtsNative =
+    Pointer<SherpaOnnxOfflineTts> Function(Pointer<SherpaOnnxOfflineTtsConfig>);
 
 typedef SherpaOnnxCreateOfflineTts = SherpaOnnxCreateOfflineTtsNative;
 
-typedef SherpaOnnxDestroyOfflineTtsNative = Void Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxDestroyOfflineTts = void Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxOfflineTtsSampleRateNative = Int32 Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxOfflineTtsSampleRate = int Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxOfflineTtsNumSpeakersNative = Int32 Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxOfflineTtsNumSpeakers = int Function(
-    Pointer<SherpaOnnxOfflineTts>);
-
-typedef SherpaOnnxOfflineTtsGenerateNative = Pointer<SherpaOnnxGeneratedAudio>
-    Function(Pointer<SherpaOnnxOfflineTts>, Pointer<Utf8>, Int32, Float);
-
-typedef SherpaOnnxOfflineTtsGenerate = Pointer<SherpaOnnxGeneratedAudio>
-    Function(Pointer<SherpaOnnxOfflineTts>, Pointer<Utf8>, int, double);
-
-typedef SherpaOnnxDestroyOfflineTtsGeneratedAudioNative = Void Function(
-    Pointer<SherpaOnnxGeneratedAudio>);
-
-typedef SherpaOnnxDestroyOfflineTtsGeneratedAudio = void Function(
-    Pointer<SherpaOnnxGeneratedAudio>);
-
-typedef SherpaOnnxGeneratedAudioCallbackNative = Int Function(
-    Pointer<Float>, Int32);
-
-typedef SherpaOnnxOfflineTtsGenerateWithCallbackNative
-    = Pointer<SherpaOnnxGeneratedAudio> Function(
-        Pointer<SherpaOnnxOfflineTts>,
-        Pointer<Utf8>,
-        Int32,
-        Float,
-        Pointer<NativeFunction<SherpaOnnxGeneratedAudioCallbackNative>>);
-
-typedef SherpaOnnxOfflineTtsGenerateWithCallback
-    = Pointer<SherpaOnnxGeneratedAudio> Function(
-        Pointer<SherpaOnnxOfflineTts>,
-        Pointer<Utf8>,
-        int,
-        double,
-        Pointer<NativeFunction<SherpaOnnxGeneratedAudioCallbackNative>>);
-
-typedef CreateOfflineRecognizerNative = Pointer<SherpaOnnxOfflineRecognizer>
-    Function(Pointer<SherpaOnnxOfflineRecognizerConfig>);
+typedef SherpaOnnxDestroyOfflineTtsNative =
+    Void Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxDestroyOfflineTts =
+    void Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxOfflineTtsSampleRateNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxOfflineTtsSampleRate =
+    int Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxOfflineTtsNumSpeakersNative =
+    Int32 Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxOfflineTtsNumSpeakers =
+    int Function(Pointer<SherpaOnnxOfflineTts>);
+
+typedef SherpaOnnxOfflineTtsGenerateNative =
+    Pointer<SherpaOnnxGeneratedAudio> Function(
+      Pointer<SherpaOnnxOfflineTts>,
+      Pointer<Utf8>,
+      Int32,
+      Float,
+    );
+
+typedef SherpaOnnxOfflineTtsGenerate =
+    Pointer<SherpaOnnxGeneratedAudio> Function(
+      Pointer<SherpaOnnxOfflineTts>,
+      Pointer<Utf8>,
+      int,
+      double,
+    );
+
+typedef SherpaOnnxDestroyOfflineTtsGeneratedAudioNative =
+    Void Function(Pointer<SherpaOnnxGeneratedAudio>);
+
+typedef SherpaOnnxDestroyOfflineTtsGeneratedAudio =
+    void Function(Pointer<SherpaOnnxGeneratedAudio>);
+
+typedef SherpaOnnxGeneratedAudioCallbackNative =
+    Int Function(Pointer<Float>, Int32);
+
+typedef SherpaOnnxOfflineTtsGenerateWithCallbackNative =
+    Pointer<SherpaOnnxGeneratedAudio> Function(
+      Pointer<SherpaOnnxOfflineTts>,
+      Pointer<Utf8>,
+      Int32,
+      Float,
+      Pointer<NativeFunction<SherpaOnnxGeneratedAudioCallbackNative>>,
+    );
+
+typedef SherpaOnnxOfflineTtsGenerateWithCallback =
+    Pointer<SherpaOnnxGeneratedAudio> Function(
+      Pointer<SherpaOnnxOfflineTts>,
+      Pointer<Utf8>,
+      int,
+      double,
+      Pointer<NativeFunction<SherpaOnnxGeneratedAudioCallbackNative>>,
+    );
+
+typedef CreateOfflineRecognizerNative =
+    Pointer<SherpaOnnxOfflineRecognizer> Function(
+      Pointer<SherpaOnnxOfflineRecognizerConfig>,
+    );
 
 typedef CreateOfflineRecognizer = CreateOfflineRecognizerNative;
 
-typedef OfflineRecognizerSetConfigNative = Void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>,
-    Pointer<SherpaOnnxOfflineRecognizerConfig>);
+typedef OfflineRecognizerSetConfigNative =
+    Void Function(
+      Pointer<SherpaOnnxOfflineRecognizer>,
+      Pointer<SherpaOnnxOfflineRecognizerConfig>,
+    );
 
-typedef OfflineRecognizerSetConfig = void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>,
-    Pointer<SherpaOnnxOfflineRecognizerConfig>);
+typedef OfflineRecognizerSetConfig =
+    void Function(
+      Pointer<SherpaOnnxOfflineRecognizer>,
+      Pointer<SherpaOnnxOfflineRecognizerConfig>,
+    );
 
-typedef DestroyOfflineRecognizerNative = Void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>);
+typedef DestroyOfflineRecognizerNative =
+    Void Function(Pointer<SherpaOnnxOfflineRecognizer>);
 
-typedef DestroyOfflineRecognizer = void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>);
+typedef DestroyOfflineRecognizer =
+    void Function(Pointer<SherpaOnnxOfflineRecognizer>);
 
-typedef CreateOfflineStreamNative = Pointer<SherpaOnnxOfflineStream> Function(
-    Pointer<SherpaOnnxOfflineRecognizer>);
+typedef CreateOfflineStreamNative =
+    Pointer<SherpaOnnxOfflineStream> Function(
+      Pointer<SherpaOnnxOfflineRecognizer>,
+    );
 
 typedef CreateOfflineStream = CreateOfflineStreamNative;
 
-typedef DestroyOfflineStreamNative = Void Function(
-    Pointer<SherpaOnnxOfflineStream>);
+typedef DestroyOfflineStreamNative =
+    Void Function(Pointer<SherpaOnnxOfflineStream>);
 
 typedef DestroyOfflineStream = void Function(Pointer<SherpaOnnxOfflineStream>);
 
-typedef AcceptWaveformOfflineNative = Void Function(
-    Pointer<SherpaOnnxOfflineStream>, Int32, Pointer<Float>, Int32);
+typedef AcceptWaveformOfflineNative =
+    Void Function(
+      Pointer<SherpaOnnxOfflineStream>,
+      Int32,
+      Pointer<Float>,
+      Int32,
+    );
 
-typedef AcceptWaveformOffline = void Function(
-    Pointer<SherpaOnnxOfflineStream>, int, Pointer<Float>, int);
+typedef AcceptWaveformOffline =
+    void Function(Pointer<SherpaOnnxOfflineStream>, int, Pointer<Float>, int);
 
-typedef DecodeOfflineStreamNative = Void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>, Pointer<SherpaOnnxOfflineStream>);
+typedef DecodeOfflineStreamNative =
+    Void Function(
+      Pointer<SherpaOnnxOfflineRecognizer>,
+      Pointer<SherpaOnnxOfflineStream>,
+    );
 
-typedef DecodeOfflineStream = void Function(
-    Pointer<SherpaOnnxOfflineRecognizer>, Pointer<SherpaOnnxOfflineStream>);
+typedef DecodeOfflineStream =
+    void Function(
+      Pointer<SherpaOnnxOfflineRecognizer>,
+      Pointer<SherpaOnnxOfflineStream>,
+    );
 
-typedef GetOfflineStreamResultAsJsonNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxOfflineStream>);
+typedef GetOfflineStreamResultAsJsonNative =
+    Pointer<Utf8> Function(Pointer<SherpaOnnxOfflineStream>);
 
 typedef GetOfflineStreamResultAsJson = GetOfflineStreamResultAsJsonNative;
 
@@ -1068,343 +1170,426 @@ typedef DestroyOfflineStreamResultJsonNative = Void Function(Pointer<Utf8>);
 
 typedef DestroyOfflineStreamResultJson = void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxCreateOnlineRecognizerNative
-    = Pointer<SherpaOnnxOnlineRecognizer> Function(
-        Pointer<SherpaOnnxOnlineRecognizerConfig>);
+typedef SherpaOnnxCreateOnlineRecognizerNative =
+    Pointer<SherpaOnnxOnlineRecognizer> Function(
+      Pointer<SherpaOnnxOnlineRecognizerConfig>,
+    );
 
-typedef SherpaOnnxCreateOnlineRecognizer
-    = SherpaOnnxCreateOnlineRecognizerNative;
+typedef SherpaOnnxCreateOnlineRecognizer =
+    SherpaOnnxCreateOnlineRecognizerNative;
 
-typedef SherpaOnnxDestroyOnlineRecognizerNative = Void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>);
+typedef SherpaOnnxDestroyOnlineRecognizerNative =
+    Void Function(Pointer<SherpaOnnxOnlineRecognizer>);
 
-typedef SherpaOnnxDestroyOnlineRecognizer = void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>);
+typedef SherpaOnnxDestroyOnlineRecognizer =
+    void Function(Pointer<SherpaOnnxOnlineRecognizer>);
 
-typedef SherpaOnnxCreateOnlineStreamNative = Pointer<SherpaOnnxOnlineStream>
-    Function(Pointer<SherpaOnnxOnlineRecognizer>);
+typedef SherpaOnnxCreateOnlineStreamNative =
+    Pointer<SherpaOnnxOnlineStream> Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+    );
 
 typedef SherpaOnnxCreateOnlineStream = SherpaOnnxCreateOnlineStreamNative;
 
-typedef SherpaOnnxCreateOnlineStreamWithHotwordsNative
-    = Pointer<SherpaOnnxOnlineStream> Function(
-        Pointer<SherpaOnnxOnlineRecognizer>, Pointer<Utf8>);
-
-typedef SherpaOnnxCreateOnlineStreamWithHotwords
-    = SherpaOnnxCreateOnlineStreamWithHotwordsNative;
-
-typedef IsOnlineStreamReadyNative = Int32 Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef IsOnlineStreamReady = int Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxDecodeOnlineStreamNative = Void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxDecodeOnlineStream = void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef GetOnlineStreamResultAsJsonNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
+typedef SherpaOnnxCreateOnlineStreamWithHotwordsNative =
+    Pointer<SherpaOnnxOnlineStream> Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<Utf8>,
+    );
+
+typedef SherpaOnnxCreateOnlineStreamWithHotwords =
+    SherpaOnnxCreateOnlineStreamWithHotwordsNative;
+
+typedef IsOnlineStreamReadyNative =
+    Int32 Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef IsOnlineStreamReady =
+    int Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef SherpaOnnxDecodeOnlineStreamNative =
+    Void Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef SherpaOnnxDecodeOnlineStream =
+    void Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef GetOnlineStreamResultAsJsonNative =
+    Pointer<Utf8> Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
 typedef GetOnlineStreamResultAsJson = GetOnlineStreamResultAsJsonNative;
 
-typedef ResetNative = Void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef Reset = void Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef IsEndpointNative = Int32 Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
-
-typedef IsEndpoint = int Function(
-    Pointer<SherpaOnnxOnlineRecognizer>, Pointer<SherpaOnnxOnlineStream>);
+typedef ResetNative =
+    Void Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef Reset =
+    void Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef IsEndpointNative =
+    Int32 Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
+
+typedef IsEndpoint =
+    int Function(
+      Pointer<SherpaOnnxOnlineRecognizer>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
 typedef DestroyOnlineStreamResultJsonNative = Void Function(Pointer<Utf8>);
 
 typedef DestroyOnlineStreamResultJson = void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxCreateVoiceActivityDetectorNative
-    = Pointer<SherpaOnnxVoiceActivityDetector> Function(
-        Pointer<SherpaOnnxVadModelConfig>, Float);
+typedef SherpaOnnxCreateVoiceActivityDetectorNative =
+    Pointer<SherpaOnnxVoiceActivityDetector> Function(
+      Pointer<SherpaOnnxVadModelConfig>,
+      Float,
+    );
 
-typedef SherpaOnnxCreateVoiceActivityDetector
-    = Pointer<SherpaOnnxVoiceActivityDetector> Function(
-        Pointer<SherpaOnnxVadModelConfig>, double);
+typedef SherpaOnnxCreateVoiceActivityDetector =
+    Pointer<SherpaOnnxVoiceActivityDetector> Function(
+      Pointer<SherpaOnnxVadModelConfig>,
+      double,
+    );
 
-typedef SherpaOnnxDestroyVoiceActivityDetectorNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxDestroyVoiceActivityDetectorNative =
+    Void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxDestroyVoiceActivityDetector = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxDestroyVoiceActivityDetector =
+    void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorAcceptWaveformNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>, Pointer<Float>, Int32);
+typedef SherpaOnnxVoiceActivityDetectorAcceptWaveformNative =
+    Void Function(
+      Pointer<SherpaOnnxVoiceActivityDetector>,
+      Pointer<Float>,
+      Int32,
+    );
 
-typedef SherpaOnnxVoiceActivityDetectorAcceptWaveform = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>, Pointer<Float>, int);
+typedef SherpaOnnxVoiceActivityDetectorAcceptWaveform =
+    void Function(
+      Pointer<SherpaOnnxVoiceActivityDetector>,
+      Pointer<Float>,
+      int,
+    );
 
-typedef SherpaOnnxVoiceActivityDetectorEmptyNative = Int32 Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorEmptyNative =
+    Int32 Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorEmpty = int Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorEmpty =
+    int Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorDetectedNative = Int32 Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorDetectedNative =
+    Int32 Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorDetected = int Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorDetected =
+    int Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorPopNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorPopNative =
+    Void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorPop = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorPop =
+    void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorClearNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorClearNative =
+    Void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorClear = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorClear =
+    void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorResetNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorResetNative =
+    Void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorReset = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorReset =
+    void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorFlushNative = Void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorFlushNative =
+    Void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorFlush = void Function(
-    Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorFlush =
+    void Function(Pointer<SherpaOnnxVoiceActivityDetector>);
 
-typedef SherpaOnnxVoiceActivityDetectorFrontNative
-    = Pointer<SherpaOnnxSpeechSegment> Function(
-        Pointer<SherpaOnnxVoiceActivityDetector>);
+typedef SherpaOnnxVoiceActivityDetectorFrontNative =
+    Pointer<SherpaOnnxSpeechSegment> Function(
+      Pointer<SherpaOnnxVoiceActivityDetector>,
+    );
 
-typedef SherpaOnnxVoiceActivityDetectorFront
-    = SherpaOnnxVoiceActivityDetectorFrontNative;
+typedef SherpaOnnxVoiceActivityDetectorFront =
+    SherpaOnnxVoiceActivityDetectorFrontNative;
 
-typedef SherpaOnnxDestroySpeechSegmentNative = Void Function(
-    Pointer<SherpaOnnxSpeechSegment>);
+typedef SherpaOnnxDestroySpeechSegmentNative =
+    Void Function(Pointer<SherpaOnnxSpeechSegment>);
 
-typedef SherpaOnnxDestroySpeechSegment = void Function(
-    Pointer<SherpaOnnxSpeechSegment>);
+typedef SherpaOnnxDestroySpeechSegment =
+    void Function(Pointer<SherpaOnnxSpeechSegment>);
 
-typedef SherpaOnnxCreateCircularBufferNative = Pointer<SherpaOnnxCircularBuffer>
-    Function(Int32);
+typedef SherpaOnnxCreateCircularBufferNative =
+    Pointer<SherpaOnnxCircularBuffer> Function(Int32);
 
-typedef SherpaOnnxCreateCircularBuffer = Pointer<SherpaOnnxCircularBuffer>
-    Function(int);
+typedef SherpaOnnxCreateCircularBuffer =
+    Pointer<SherpaOnnxCircularBuffer> Function(int);
 
-typedef SherpaOnnxDestroyCircularBufferNative = Void Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxDestroyCircularBufferNative =
+    Void Function(Pointer<SherpaOnnxCircularBuffer>);
 
-typedef SherpaOnnxDestroyCircularBuffer = void Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxDestroyCircularBuffer =
+    void Function(Pointer<SherpaOnnxCircularBuffer>);
 
-typedef SherpaOnnxCircularBufferPushNative = Void Function(
-    Pointer<SherpaOnnxCircularBuffer>, Pointer<Float>, Int32);
+typedef SherpaOnnxCircularBufferPushNative =
+    Void Function(Pointer<SherpaOnnxCircularBuffer>, Pointer<Float>, Int32);
 
-typedef SherpaOnnxCircularBufferPush = void Function(
-    Pointer<SherpaOnnxCircularBuffer>, Pointer<Float>, int);
+typedef SherpaOnnxCircularBufferPush =
+    void Function(Pointer<SherpaOnnxCircularBuffer>, Pointer<Float>, int);
 
-typedef SherpaOnnxCircularBufferGetNative = Pointer<Float> Function(
-    Pointer<SherpaOnnxCircularBuffer>, Int32, Int32);
+typedef SherpaOnnxCircularBufferGetNative =
+    Pointer<Float> Function(Pointer<SherpaOnnxCircularBuffer>, Int32, Int32);
 
-typedef SherpaOnnxCircularBufferGet = Pointer<Float> Function(
-    Pointer<SherpaOnnxCircularBuffer>, int, int);
+typedef SherpaOnnxCircularBufferGet =
+    Pointer<Float> Function(Pointer<SherpaOnnxCircularBuffer>, int, int);
 
 typedef SherpaOnnxCircularBufferFreeNative = Void Function(Pointer<Float>);
 
 typedef SherpaOnnxCircularBufferFree = void Function(Pointer<Float>);
 
-typedef SherpaOnnxCircularBufferPopNative = Void Function(
-    Pointer<SherpaOnnxCircularBuffer>, Int32);
-
-typedef SherpaOnnxCircularBufferPop = void Function(
-    Pointer<SherpaOnnxCircularBuffer>, int);
+typedef SherpaOnnxCircularBufferPopNative =
+    Void Function(Pointer<SherpaOnnxCircularBuffer>, Int32);
 
-typedef SherpaOnnxCircularBufferSizeNative = Int32 Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxCircularBufferPop =
+    void Function(Pointer<SherpaOnnxCircularBuffer>, int);
 
-typedef SherpaOnnxCircularBufferSize = int Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxCircularBufferSizeNative =
+    Int32 Function(Pointer<SherpaOnnxCircularBuffer>);
 
-typedef SherpaOnnxCircularBufferHeadNative = Int32 Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxCircularBufferSize =
+    int Function(Pointer<SherpaOnnxCircularBuffer>);
 
-typedef SherpaOnnxCircularBufferHead = int Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxCircularBufferHeadNative =
+    Int32 Function(Pointer<SherpaOnnxCircularBuffer>);
+
+typedef SherpaOnnxCircularBufferHead =
+    int Function(Pointer<SherpaOnnxCircularBuffer>);
+
+typedef SherpaOnnxCircularBufferResetNative =
+    Void Function(Pointer<SherpaOnnxCircularBuffer>);
+
+typedef SherpaOnnxCircularBufferReset =
+    void Function(Pointer<SherpaOnnxCircularBuffer>);
+
+typedef SherpaOnnxCreateSpeakerEmbeddingManagerNative =
+    Pointer<SherpaOnnxSpeakerEmbeddingManager> Function(Int32);
+
+typedef SherpaOnnxCreateSpeakerEmbeddingManager =
+    Pointer<SherpaOnnxSpeakerEmbeddingManager> Function(int);
 
-typedef SherpaOnnxCircularBufferResetNative = Void Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxDestroySpeakerEmbeddingManagerNative =
+    Void Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>);
 
-typedef SherpaOnnxCircularBufferReset = void Function(
-    Pointer<SherpaOnnxCircularBuffer>);
+typedef SherpaOnnxDestroySpeakerEmbeddingManager =
+    void Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+
+typedef SherpaOnnxSpeakerEmbeddingManagerAddNative =
+    Int32 Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+    );
 
-typedef SherpaOnnxCreateSpeakerEmbeddingManagerNative
-    = Pointer<SherpaOnnxSpeakerEmbeddingManager> Function(Int32);
+typedef SherpaOnnxSpeakerEmbeddingManagerAdd =
+    int Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+    );
 
-typedef SherpaOnnxCreateSpeakerEmbeddingManager
-    = Pointer<SherpaOnnxSpeakerEmbeddingManager> Function(int);
+typedef SherpaOnnxSpeakerEmbeddingManagerAddListFlattenedNative =
+    Int32 Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+      Int32,
+    );
 
-typedef SherpaOnnxDestroySpeakerEmbeddingManagerNative = Void Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+typedef SherpaOnnxSpeakerEmbeddingManagerAddListFlattened =
+    int Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+      int,
+    );
 
-typedef SherpaOnnxDestroySpeakerEmbeddingManager = void Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+typedef SherpaOnnxSpeakerEmbeddingManagerRemoveNative =
+    Int32 Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerAddNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>, Pointer<Float>);
+typedef SherpaOnnxSpeakerEmbeddingManagerRemove =
+    int Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerAdd = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>, Pointer<Float>);
+typedef SherpaOnnxSpeakerEmbeddingManagerContainsNative =
+    Int32 Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerAddListFlattenedNative
-    = Int32 Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>,
-        Pointer<Float>, Int32);
+typedef SherpaOnnxSpeakerEmbeddingManagerContains =
+    int Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerAddListFlattened = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>,
-    Pointer<Utf8>,
-    Pointer<Float>,
-    int);
+typedef SherpaOnnxSpeakerEmbeddingManagerSearchNative =
+    Pointer<Utf8> Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Float>,
+      Float,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerRemoveNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
+typedef SherpaOnnxSpeakerEmbeddingManagerSearch =
+    Pointer<Utf8> Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Float>,
+      double,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerRemove = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
+typedef SherpaOnnxSpeakerEmbeddingManagerFreeSearchNative =
+    Void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerContainsNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
+typedef SherpaOnnxSpeakerEmbeddingManagerFreeSearch =
+    void Function(Pointer<Utf8>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerContains = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Utf8>);
+typedef SherpaOnnxSpeakerEmbeddingManagerNumSpeakersNative =
+    Int32 Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+
+typedef SherpaOnnxSpeakerEmbeddingManagerNumSpeakers =
+    int Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+
+typedef SherpaOnnxSpeakerEmbeddingManagerVerifyNative =
+    Int32 Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+      Float,
+    );
+
+typedef SherpaOnnxSpeakerEmbeddingManagerVerify =
+    int Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingManager>,
+      Pointer<Utf8>,
+      Pointer<Float>,
+      double,
+    );
+
+typedef SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative =
+    Pointer<Pointer<Utf8>> Function(Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+
+typedef SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers =
+    SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative;
+
+typedef SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakersNative =
+    Void Function(Pointer<Pointer<Utf8>>);
+
+typedef SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers =
+    void Function(Pointer<Pointer<Utf8>>);
+
+typedef SherpaOnnxCreateSpeakerEmbeddingExtractorNative =
+    Pointer<SherpaOnnxSpeakerEmbeddingExtractor> Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingExtractorConfig>,
+    );
+
+typedef SherpaOnnxCreateSpeakerEmbeddingExtractor =
+    SherpaOnnxCreateSpeakerEmbeddingExtractorNative;
+
+typedef SherpaOnnxDestroySpeakerEmbeddingExtractorNative =
+    Void Function(Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
+
+typedef SherpaOnnxDestroySpeakerEmbeddingExtractor =
+    void Function(Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
+
+typedef SherpaOnnxSpeakerEmbeddingExtractorDimNative =
+    Int32 Function(Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
+
+typedef SherpaOnnxSpeakerEmbeddingExtractorDim =
+    int Function(Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
+
+typedef SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative =
+    Pointer<SherpaOnnxOnlineStream> Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerSearchNative = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Float>, Float);
+typedef SherpaOnnxSpeakerEmbeddingExtractorCreateStream =
+    SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative;
 
-typedef SherpaOnnxSpeakerEmbeddingManagerSearch = Pointer<Utf8> Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>, Pointer<Float>, double);
+typedef SherpaOnnxDestroyOnlineStreamNative =
+    Void Function(Pointer<SherpaOnnxOnlineStream>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerFreeSearchNative = Void Function(
-    Pointer<Utf8>);
+typedef SherpaOnnxDestroyOnlineStream =
+    void Function(Pointer<SherpaOnnxOnlineStream>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerFreeSearch = void Function(
-    Pointer<Utf8>);
+typedef OnlineStreamAcceptWaveformNative =
+    Void Function(
+      Pointer<SherpaOnnxOnlineStream>,
+      Int32,
+      Pointer<Float>,
+      Int32,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerNumSpeakersNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+typedef OnlineStreamAcceptWaveform =
+    void Function(Pointer<SherpaOnnxOnlineStream>, int, Pointer<Float>, int);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerNumSpeakers = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+typedef OnlineStreamInputFinishedNative =
+    Void Function(Pointer<SherpaOnnxOnlineStream>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerVerifyNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>,
-    Pointer<Utf8>,
-    Pointer<Float>,
-    Float);
+typedef OnlineStreamInputFinished =
+    void Function(Pointer<SherpaOnnxOnlineStream>);
 
-typedef SherpaOnnxSpeakerEmbeddingManagerVerify = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingManager>,
-    Pointer<Utf8>,
-    Pointer<Float>,
-    double);
+typedef SherpaOnnxSpeakerEmbeddingExtractorIsReadyNative =
+    Int32 Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative
-    = Pointer<Pointer<Utf8>> Function(
-        Pointer<SherpaOnnxSpeakerEmbeddingManager>);
+typedef SherpaOnnxSpeakerEmbeddingExtractorIsReady =
+    int Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers
-    = SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative;
+typedef SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative =
+    Pointer<Float> Function(
+      Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
+      Pointer<SherpaOnnxOnlineStream>,
+    );
 
-typedef SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakersNative = Void Function(
-    Pointer<Pointer<Utf8>>);
+typedef SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding =
+    SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative;
 
-typedef SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers = void Function(
-    Pointer<Pointer<Utf8>>);
+typedef SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbeddingNative =
+    Void Function(Pointer<Float>);
 
-typedef SherpaOnnxCreateSpeakerEmbeddingExtractorNative
-    = Pointer<SherpaOnnxSpeakerEmbeddingExtractor> Function(
-        Pointer<SherpaOnnxSpeakerEmbeddingExtractorConfig>);
+typedef SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding =
+    void Function(Pointer<Float>);
 
-typedef SherpaOnnxCreateSpeakerEmbeddingExtractor
-    = SherpaOnnxCreateSpeakerEmbeddingExtractorNative;
-
-typedef SherpaOnnxDestroySpeakerEmbeddingExtractorNative = Void Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
-
-typedef SherpaOnnxDestroySpeakerEmbeddingExtractor = void Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorDimNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorDim = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative
-    = Pointer<SherpaOnnxOnlineStream> Function(
-        Pointer<SherpaOnnxSpeakerEmbeddingExtractor>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorCreateStream
-    = SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative;
-
-typedef SherpaOnnxDestroyOnlineStreamNative = Void Function(
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxDestroyOnlineStream = void Function(
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef OnlineStreamAcceptWaveformNative = Void Function(
-    Pointer<SherpaOnnxOnlineStream>, Int32, Pointer<Float>, Int32);
-
-typedef OnlineStreamAcceptWaveform = void Function(
-    Pointer<SherpaOnnxOnlineStream>, int, Pointer<Float>, int);
-
-typedef OnlineStreamInputFinishedNative = Void Function(
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef OnlineStreamInputFinished = void Function(
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorIsReadyNative = Int32 Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorIsReady = int Function(
-    Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
-    Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative
-    = Pointer<Float> Function(Pointer<SherpaOnnxSpeakerEmbeddingExtractor>,
-        Pointer<SherpaOnnxOnlineStream>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding
-    = SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative;
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbeddingNative = Void
-    Function(Pointer<Float>);
-
-typedef SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding = void Function(
-    Pointer<Float>);
-
-typedef SherpaOnnxReadWaveNative = Pointer<SherpaOnnxWave> Function(
-    Pointer<Utf8>);
+typedef SherpaOnnxReadWaveNative =
+    Pointer<SherpaOnnxWave> Function(Pointer<Utf8>);
 
 typedef SherpaOnnxReadWave = SherpaOnnxReadWaveNative;
 
-typedef SherpaOnnxWriteWaveNative = Int32 Function(
-    Pointer<Float>, Int32, Int32, Pointer<Utf8>);
+typedef SherpaOnnxWriteWaveNative =
+    Int32 Function(Pointer<Float>, Int32, Int32, Pointer<Utf8>);
 
-typedef SherpaOnnxWriteWave = int Function(
-    Pointer<Float>, int, int, Pointer<Utf8>);
+typedef SherpaOnnxWriteWave =
+    int Function(Pointer<Float>, int, int, Pointer<Utf8>);
 
 typedef SherpaOnnxFreeWaveNative = Void Function(Pointer<SherpaOnnxWave>);
 
@@ -1421,67 +1606,67 @@ typedef SherpaOnnxGetGitDate = SherpaOnnxGetGitDateNative;
 
 class SherpaOnnxBindings {
   static SherpaOnnxCreateOfflineSpeechDenoiser?
-      sherpaOnnxCreateOfflineSpeechDenoiser;
+  sherpaOnnxCreateOfflineSpeechDenoiser;
 
   static SherpaOnnxDestroyOfflineSpeechDenoiser?
-      sherpaOnnxDestroyOfflineSpeechDenoiser;
+  sherpaOnnxDestroyOfflineSpeechDenoiser;
 
   static SherpaOnnxOfflineSpeechDenoiserGetSampleRate?
-      sherpaOnnxOfflineSpeechDenoiserGetSampleRate;
+  sherpaOnnxOfflineSpeechDenoiserGetSampleRate;
   static SherpaOnnxOfflineSpeechDenoiserRun? sherpaOnnxOfflineSpeechDenoiserRun;
   static SherpaOnnxDestroyDenoisedAudio? sherpaOnnxDestroyDenoisedAudio;
 
   static SherpaOnnxCreateSpokenLanguageIdentification?
-      sherpaOnnxCreateSpokenLanguageIdentification;
+  sherpaOnnxCreateSpokenLanguageIdentification;
   static SherpaOnnxDestroySpokenLanguageIdentification?
-      sherpaOnnxDestroySpokenLanguageIdentification;
+  sherpaOnnxDestroySpokenLanguageIdentification;
   static SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream?
-      sherpaOnnxSpokenLanguageIdentificationCreateOfflineStream;
+  sherpaOnnxSpokenLanguageIdentificationCreateOfflineStream;
   static SherpaOnnxSpokenLanguageIdentificationCompute?
-      sherpaOnnxSpokenLanguageIdentificationCompute;
+  sherpaOnnxSpokenLanguageIdentificationCompute;
   static SherpaOnnxDestroySpokenLanguageIdentificationResult?
-      sherpaOnnxDestroySpokenLanguageIdentificationResult;
+  sherpaOnnxDestroySpokenLanguageIdentificationResult;
 
   static SherpaOnnxCreateOfflineSpeakerDiarization?
-      sherpaOnnxCreateOfflineSpeakerDiarization;
+  sherpaOnnxCreateOfflineSpeakerDiarization;
   static SherpaOnnxDestroyOfflineSpeakerDiarization?
-      sherpaOnnxDestroyOfflineSpeakerDiarization;
+  sherpaOnnxDestroyOfflineSpeakerDiarization;
   static SherpaOnnxOfflineSpeakerDiarizationGetSampleRate?
-      sherpaOnnxOfflineSpeakerDiarizationGetSampleRate;
+  sherpaOnnxOfflineSpeakerDiarizationGetSampleRate;
   static SherpaOnnxOfflineSpeakerDiarizationSetConfig?
-      sherpaOnnxOfflineSpeakerDiarizationSetConfig;
+  sherpaOnnxOfflineSpeakerDiarizationSetConfig;
   static SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers?
-      sherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers;
+  sherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers;
   static SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments?
-      sherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments;
+  sherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments;
   static SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime?
-      sherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime;
+  sherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime;
   static SherpaOnnxOfflineSpeakerDiarizationDestroySegment?
-      sherpaOnnxOfflineSpeakerDiarizationDestroySegment;
+  sherpaOnnxOfflineSpeakerDiarizationDestroySegment;
   static SherpaOnnxOfflineSpeakerDiarizationProcess?
-      sherpaOnnxOfflineSpeakerDiarizationProcess;
+  sherpaOnnxOfflineSpeakerDiarizationProcess;
   static SherpaOnnxOfflineSpeakerDiarizationDestroyResult?
-      sherpaOnnxOfflineSpeakerDiarizationDestroyResult;
+  sherpaOnnxOfflineSpeakerDiarizationDestroyResult;
   static SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg?
-      sherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg;
+  sherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg;
 
   static SherpaOnnxCreateOfflinePunctuation? sherpaOnnxCreateOfflinePunctuation;
   static SherpaOnnxDestroyOfflinePunctuation?
-      sherpaOnnxDestroyOfflinePunctuation;
+  sherpaOnnxDestroyOfflinePunctuation;
   static SherpaOfflinePunctuationAddPunct? sherpaOfflinePunctuationAddPunct;
   static SherpaOfflinePunctuationFreeText? sherpaOfflinePunctuationFreeText;
 
   static SherpaOnnxCreateOnlinePunctuation? sherpaOnnxCreateOnlinePunctuation;
   static SherpaOnnxDestroyOnlinePunctuation? sherpaOnnxDestroyOnlinePunctuation;
   static SherpaOnnxOnlinePunctuationAddPunct?
-      sherpaOnnxOnlinePunctuationAddPunct;
+  sherpaOnnxOnlinePunctuationAddPunct;
   static SherpaOnnxOnlinePunctuationFreeText?
-      sherpaOnnxOnlinePunctuationFreeText;
+  sherpaOnnxOnlinePunctuationFreeText;
 
   static SherpaOnnxCreateAudioTagging? sherpaOnnxCreateAudioTagging;
   static SherpaOnnxDestroyAudioTagging? sherpaOnnxDestroyAudioTagging;
   static SherpaOnnxAudioTaggingCreateOfflineStream?
-      sherpaOnnxAudioTaggingCreateOfflineStream;
+  sherpaOnnxAudioTaggingCreateOfflineStream;
   static SherpaOnnxAudioTaggingCompute? sherpaOnnxAudioTaggingCompute;
   static SherpaOnnxAudioTaggingFreeResults? sherpaOnnxAudioTaggingFreeResults;
 
@@ -1501,9 +1686,9 @@ class SherpaOnnxBindings {
   static SherpaOnnxOfflineTtsNumSpeakers? offlineTtsNumSpeakers;
   static SherpaOnnxOfflineTtsGenerate? offlineTtsGenerate;
   static SherpaOnnxDestroyOfflineTtsGeneratedAudio?
-      destroyOfflineTtsGeneratedAudio;
+  destroyOfflineTtsGeneratedAudio;
   static SherpaOnnxOfflineTtsGenerateWithCallback?
-      offlineTtsGenerateWithCallback;
+  offlineTtsGenerateWithCallback;
 
   static CreateOfflineRecognizer? createOfflineRecognizer;
   static DestroyOfflineRecognizer? destroyOfflineRecognizer;
@@ -1522,7 +1707,7 @@ class SherpaOnnxBindings {
   static SherpaOnnxCreateOnlineStream? createOnlineStream;
 
   static SherpaOnnxCreateOnlineStreamWithHotwords?
-      createOnlineStreamWithHotwords;
+  createOnlineStreamWithHotwords;
 
   static IsOnlineStreamReady? isOnlineStreamReady;
 
@@ -1541,7 +1726,7 @@ class SherpaOnnxBindings {
   static SherpaOnnxDestroyVoiceActivityDetector? destroyVoiceActivityDetector;
 
   static SherpaOnnxVoiceActivityDetectorAcceptWaveform?
-      voiceActivityDetectorAcceptWaveform;
+  voiceActivityDetectorAcceptWaveform;
 
   static SherpaOnnxVoiceActivityDetectorEmpty? voiceActivityDetectorEmpty;
 
@@ -1578,21 +1763,21 @@ class SherpaOnnxBindings {
   static SherpaOnnxCircularBufferReset? circularBufferReset;
 
   static SherpaOnnxCreateSpeakerEmbeddingExtractor?
-      createSpeakerEmbeddingExtractor;
+  createSpeakerEmbeddingExtractor;
 
   static SherpaOnnxDestroySpeakerEmbeddingExtractor?
-      destroySpeakerEmbeddingExtractor;
+  destroySpeakerEmbeddingExtractor;
 
   static SherpaOnnxSpeakerEmbeddingExtractorDim? speakerEmbeddingExtractorDim;
 
   static SherpaOnnxSpeakerEmbeddingExtractorCreateStream?
-      speakerEmbeddingExtractorCreateStream;
+  speakerEmbeddingExtractorCreateStream;
 
   static SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding?
-      speakerEmbeddingExtractorComputeEmbedding;
+  speakerEmbeddingExtractorComputeEmbedding;
 
   static SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding?
-      speakerEmbeddingExtractorDestroyEmbedding;
+  speakerEmbeddingExtractorDestroyEmbedding;
 
   static SherpaOnnxDestroyOnlineStream? destroyOnlineStream;
 
@@ -1601,38 +1786,38 @@ class SherpaOnnxBindings {
   static OnlineStreamInputFinished? onlineStreamInputFinished;
 
   static SherpaOnnxSpeakerEmbeddingExtractorIsReady?
-      speakerEmbeddingExtractorIsReady;
+  speakerEmbeddingExtractorIsReady;
 
   static SherpaOnnxCreateSpeakerEmbeddingManager? createSpeakerEmbeddingManager;
 
   static SherpaOnnxDestroySpeakerEmbeddingManager?
-      destroySpeakerEmbeddingManager;
+  destroySpeakerEmbeddingManager;
 
   static SherpaOnnxSpeakerEmbeddingManagerAdd? speakerEmbeddingManagerAdd;
 
   static SherpaOnnxSpeakerEmbeddingManagerAddListFlattened?
-      speakerEmbeddingManagerAddListFlattened;
+  speakerEmbeddingManagerAddListFlattened;
 
   static SherpaOnnxSpeakerEmbeddingManagerRemove? speakerEmbeddingManagerRemove;
 
   static SherpaOnnxSpeakerEmbeddingManagerContains?
-      speakerEmbeddingManagerContains;
+  speakerEmbeddingManagerContains;
 
   static SherpaOnnxSpeakerEmbeddingManagerSearch? speakerEmbeddingManagerSearch;
 
   static SherpaOnnxSpeakerEmbeddingManagerFreeSearch?
-      speakerEmbeddingManagerFreeSearch;
+  speakerEmbeddingManagerFreeSearch;
 
   static SherpaOnnxSpeakerEmbeddingManagerNumSpeakers?
-      speakerEmbeddingManagerNumSpeakers;
+  speakerEmbeddingManagerNumSpeakers;
 
   static SherpaOnnxSpeakerEmbeddingManagerVerify? speakerEmbeddingManagerVerify;
 
   static SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers?
-      speakerEmbeddingManagerGetAllSpeakers;
+  speakerEmbeddingManagerGetAllSpeakers;
 
   static SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers?
-      speakerEmbeddingManagerFreeAllSpeakers;
+  speakerEmbeddingManagerFreeAllSpeakers;
 
   static SherpaOnnxReadWave? readWave;
 
@@ -1647,370 +1832,413 @@ class SherpaOnnxBindings {
   static void init(DynamicLibrary dynamicLibrary) {
     sherpaOnnxCreateOfflineSpeechDenoiser ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOfflineSpeechDenoiserNative>>(
-            'SherpaOnnxCreateOfflineSpeechDenoiser')
+          'SherpaOnnxCreateOfflineSpeechDenoiser',
+        )
         .asFunction();
 
     sherpaOnnxDestroyOfflineSpeechDenoiser ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOfflineSpeechDenoiserNative>>(
-            'SherpaOnnxDestroyOfflineSpeechDenoiser')
+          'SherpaOnnxDestroyOfflineSpeechDenoiser',
+        )
         .asFunction();
 
     sherpaOnnxOfflineSpeechDenoiserGetSampleRate ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeechDenoiserGetSampleRateNative>>(
-            'SherpaOnnxOfflineSpeechDenoiserGetSampleRate')
+          NativeFunction<SherpaOnnxOfflineSpeechDenoiserGetSampleRateNative>
+        >('SherpaOnnxOfflineSpeechDenoiserGetSampleRate')
         .asFunction();
 
     sherpaOnnxOfflineSpeechDenoiserRun ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOfflineSpeechDenoiserRunNative>>(
-            'SherpaOnnxOfflineSpeechDenoiserRun')
+          'SherpaOnnxOfflineSpeechDenoiserRun',
+        )
         .asFunction();
 
     sherpaOnnxDestroyDenoisedAudio ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyDenoisedAudioNative>>(
-            'SherpaOnnxDestroyDenoisedAudio')
+          'SherpaOnnxDestroyDenoisedAudio',
+        )
         .asFunction();
 
     sherpaOnnxCreateSpokenLanguageIdentification ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxCreateSpokenLanguageIdentificationNative>>(
-            'SherpaOnnxCreateSpokenLanguageIdentification')
+          NativeFunction<SherpaOnnxCreateSpokenLanguageIdentificationNative>
+        >('SherpaOnnxCreateSpokenLanguageIdentification')
         .asFunction();
 
     sherpaOnnxDestroySpokenLanguageIdentification ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxDestroySpokenLanguageIdentificationNative>>(
-            'SherpaOnnxDestroySpokenLanguageIdentification')
+          NativeFunction<SherpaOnnxDestroySpokenLanguageIdentificationNative>
+        >('SherpaOnnxDestroySpokenLanguageIdentification')
         .asFunction();
 
     sherpaOnnxSpokenLanguageIdentificationCreateOfflineStream ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative>>(
-            'SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream')
+          NativeFunction<
+            SherpaOnnxSpokenLanguageIdentificationCreateOfflineStreamNative
+          >
+        >('SherpaOnnxSpokenLanguageIdentificationCreateOfflineStream')
         .asFunction();
 
     sherpaOnnxSpokenLanguageIdentificationCompute ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpokenLanguageIdentificationComputeNative>>(
-            'SherpaOnnxSpokenLanguageIdentificationCompute')
+          NativeFunction<SherpaOnnxSpokenLanguageIdentificationComputeNative>
+        >('SherpaOnnxSpokenLanguageIdentificationCompute')
         .asFunction();
 
     sherpaOnnxDestroySpokenLanguageIdentificationResult ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxDestroySpokenLanguageIdentificationResultNative>>(
-            'SherpaOnnxDestroySpokenLanguageIdentificationResult')
+          NativeFunction<
+            SherpaOnnxDestroySpokenLanguageIdentificationResultNative
+          >
+        >('SherpaOnnxDestroySpokenLanguageIdentificationResult')
         .asFunction();
 
     sherpaOnnxCreateOfflineSpeakerDiarization ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxCreateOfflineSpeakerDiarizationNative>>(
-            'SherpaOnnxCreateOfflineSpeakerDiarization')
+          NativeFunction<SherpaOnnxCreateOfflineSpeakerDiarizationNative>
+        >('SherpaOnnxCreateOfflineSpeakerDiarization')
         .asFunction();
 
     sherpaOnnxDestroyOfflineSpeakerDiarization ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxDestroyOfflineSpeakerDiarizationNative>>(
-            'SherpaOnnxDestroyOfflineSpeakerDiarization')
+          NativeFunction<SherpaOnnxDestroyOfflineSpeakerDiarizationNative>
+        >('SherpaOnnxDestroyOfflineSpeakerDiarization')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationGetSampleRate ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationGetSampleRateNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationGetSampleRate')
+          NativeFunction<SherpaOnnxOfflineSpeakerDiarizationGetSampleRateNative>
+        >('SherpaOnnxOfflineSpeakerDiarizationGetSampleRate')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationSetConfig ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationSetConfigNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationSetConfig')
+          NativeFunction<SherpaOnnxOfflineSpeakerDiarizationSetConfigNative>
+        >('SherpaOnnxOfflineSpeakerDiarizationSetConfig')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakersNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers')
+          NativeFunction<
+            SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakersNative
+          >
+        >('SherpaOnnxOfflineSpeakerDiarizationResultGetNumSpeakers')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegmentsNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments')
+          NativeFunction<
+            SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegmentsNative
+          >
+        >('SherpaOnnxOfflineSpeakerDiarizationResultGetNumSegments')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime')
+          NativeFunction<
+            SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTimeNative
+          >
+        >('SherpaOnnxOfflineSpeakerDiarizationResultSortByStartTime')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationDestroySegment ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationDestroySegmentNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationDestroySegment')
+          NativeFunction<
+            SherpaOnnxOfflineSpeakerDiarizationDestroySegmentNative
+          >
+        >('SherpaOnnxOfflineSpeakerDiarizationDestroySegment')
         .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationProcess ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationProcessNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationProcess')
+          NativeFunction<SherpaOnnxOfflineSpeakerDiarizationProcessNative>
+        >('SherpaOnnxOfflineSpeakerDiarizationProcess')
         .asFunction();
 
-    sherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg ??= dynamicLibrary
-        .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArgNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg')
-        .asFunction();
+    sherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg ??=
+        dynamicLibrary
+            .lookup<
+              NativeFunction<
+                SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArgNative
+              >
+            >('SherpaOnnxOfflineSpeakerDiarizationProcessWithCallbackNoArg')
+            .asFunction();
 
     sherpaOnnxOfflineSpeakerDiarizationDestroyResult ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxOfflineSpeakerDiarizationDestroyResultNative>>(
-            'SherpaOnnxOfflineSpeakerDiarizationDestroyResult')
+          NativeFunction<SherpaOnnxOfflineSpeakerDiarizationDestroyResultNative>
+        >('SherpaOnnxOfflineSpeakerDiarizationDestroyResult')
         .asFunction();
 
     sherpaOnnxCreateOfflinePunctuation ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOfflinePunctuationNative>>(
-            'SherpaOnnxCreateOfflinePunctuation')
+          'SherpaOnnxCreateOfflinePunctuation',
+        )
         .asFunction();
 
     sherpaOnnxDestroyOfflinePunctuation ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOfflinePunctuationNative>>(
-            'SherpaOnnxDestroyOfflinePunctuation')
+          'SherpaOnnxDestroyOfflinePunctuation',
+        )
         .asFunction();
 
     sherpaOfflinePunctuationAddPunct ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOfflinePunctuationAddPunctNative>>(
-            'SherpaOfflinePunctuationAddPunct')
+          'SherpaOfflinePunctuationAddPunct',
+        )
         .asFunction();
 
     sherpaOfflinePunctuationFreeText ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOfflinePunctuationFreeTextNative>>(
-            'SherpaOfflinePunctuationFreeText')
+          'SherpaOfflinePunctuationFreeText',
+        )
         .asFunction();
 
     sherpaOnnxCreateOnlinePunctuation ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOnlinePunctuationNative>>(
-            'SherpaOnnxCreateOnlinePunctuation')
+          'SherpaOnnxCreateOnlinePunctuation',
+        )
         .asFunction();
 
     sherpaOnnxDestroyOnlinePunctuation ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOnlinePunctuationNative>>(
-            'SherpaOnnxDestroyOnlinePunctuation')
+          'SherpaOnnxDestroyOnlinePunctuation',
+        )
         .asFunction();
 
     sherpaOnnxOnlinePunctuationAddPunct ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOnlinePunctuationAddPunctNative>>(
-            'SherpaOnnxOnlinePunctuationAddPunct')
+          'SherpaOnnxOnlinePunctuationAddPunct',
+        )
         .asFunction();
 
     sherpaOnnxOnlinePunctuationFreeText ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOnlinePunctuationFreeTextNative>>(
-            'SherpaOnnxOnlinePunctuationFreeText')
+          'SherpaOnnxOnlinePunctuationFreeText',
+        )
         .asFunction();
 
     sherpaOnnxCreateAudioTagging ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateAudioTaggingNative>>(
-            'SherpaOnnxCreateAudioTagging')
+          'SherpaOnnxCreateAudioTagging',
+        )
         .asFunction();
 
     sherpaOnnxDestroyAudioTagging ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyAudioTaggingNative>>(
-            'SherpaOnnxDestroyAudioTagging')
+          'SherpaOnnxDestroyAudioTagging',
+        )
         .asFunction();
 
     sherpaOnnxAudioTaggingCreateOfflineStream ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxAudioTaggingCreateOfflineStreamNative>>(
-            'SherpaOnnxAudioTaggingCreateOfflineStream')
+          NativeFunction<SherpaOnnxAudioTaggingCreateOfflineStreamNative>
+        >('SherpaOnnxAudioTaggingCreateOfflineStream')
         .asFunction();
 
     sherpaOnnxAudioTaggingCompute ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxAudioTaggingComputeNative>>(
-            'SherpaOnnxAudioTaggingCompute')
+          'SherpaOnnxAudioTaggingCompute',
+        )
         .asFunction();
 
     sherpaOnnxAudioTaggingFreeResults ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxAudioTaggingFreeResultsNative>>(
-            'SherpaOnnxAudioTaggingFreeResults')
+          'SherpaOnnxAudioTaggingFreeResults',
+        )
         .asFunction();
 
     createKeywordSpotter ??= dynamicLibrary
         .lookup<NativeFunction<CreateKeywordSpotterNative>>(
-            'SherpaOnnxCreateKeywordSpotter')
+          'SherpaOnnxCreateKeywordSpotter',
+        )
         .asFunction();
 
     destroyKeywordSpotter ??= dynamicLibrary
         .lookup<NativeFunction<DestroyKeywordSpotterNative>>(
-            'SherpaOnnxDestroyKeywordSpotter')
+          'SherpaOnnxDestroyKeywordSpotter',
+        )
         .asFunction();
 
     createKeywordStream ??= dynamicLibrary
         .lookup<NativeFunction<CreateKeywordStreamNative>>(
-            'SherpaOnnxCreateKeywordStream')
+          'SherpaOnnxCreateKeywordStream',
+        )
         .asFunction();
 
     createKeywordStreamWithKeywords ??= dynamicLibrary
         .lookup<NativeFunction<CreateKeywordStreamWithKeywordsNative>>(
-            'SherpaOnnxCreateKeywordStreamWithKeywords')
+          'SherpaOnnxCreateKeywordStreamWithKeywords',
+        )
         .asFunction();
 
     isKeywordStreamReady ??= dynamicLibrary
         .lookup<NativeFunction<IsKeywordStreamReadyNative>>(
-            'SherpaOnnxIsKeywordStreamReady')
+          'SherpaOnnxIsKeywordStreamReady',
+        )
         .asFunction();
 
     decodeKeywordStream ??= dynamicLibrary
         .lookup<NativeFunction<DecodeKeywordStreamNative>>(
-            'SherpaOnnxDecodeKeywordStream')
+          'SherpaOnnxDecodeKeywordStream',
+        )
         .asFunction();
 
     resetKeywordStream ??= dynamicLibrary
         .lookup<NativeFunction<ResetKeywordStreamNative>>(
-            'SherpaOnnxResetKeywordStream')
+          'SherpaOnnxResetKeywordStream',
+        )
         .asFunction();
 
     getKeywordResultAsJson ??= dynamicLibrary
         .lookup<NativeFunction<GetKeywordResultAsJsonNative>>(
-            'SherpaOnnxGetKeywordResultAsJson')
+          'SherpaOnnxGetKeywordResultAsJson',
+        )
         .asFunction();
 
     freeKeywordResultJson ??= dynamicLibrary
         .lookup<NativeFunction<FreeKeywordResultJsonNative>>(
-            'SherpaOnnxFreeKeywordResultJson')
+          'SherpaOnnxFreeKeywordResultJson',
+        )
         .asFunction();
 
     createOfflineTts ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOfflineTtsNative>>(
-            'SherpaOnnxCreateOfflineTts')
+          'SherpaOnnxCreateOfflineTts',
+        )
         .asFunction();
 
     destroyOfflineTts ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOfflineTtsNative>>(
-            'SherpaOnnxDestroyOfflineTts')
+          'SherpaOnnxDestroyOfflineTts',
+        )
         .asFunction();
 
     offlineTtsSampleRate ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOfflineTtsSampleRateNative>>(
-            'SherpaOnnxOfflineTtsSampleRate')
+          'SherpaOnnxOfflineTtsSampleRate',
+        )
         .asFunction();
 
     offlineTtsNumSpeakers ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOfflineTtsNumSpeakersNative>>(
-            'SherpaOnnxOfflineTtsNumSpeakers')
+          'SherpaOnnxOfflineTtsNumSpeakers',
+        )
         .asFunction();
 
     offlineTtsGenerate ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOfflineTtsGenerateNative>>(
-            'SherpaOnnxOfflineTtsGenerate')
+          'SherpaOnnxOfflineTtsGenerate',
+        )
         .asFunction();
 
     destroyOfflineTtsGeneratedAudio ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxDestroyOfflineTtsGeneratedAudioNative>>(
-            'SherpaOnnxDestroyOfflineTtsGeneratedAudio')
+          NativeFunction<SherpaOnnxDestroyOfflineTtsGeneratedAudioNative>
+        >('SherpaOnnxDestroyOfflineTtsGeneratedAudio')
         .asFunction();
 
     offlineTtsGenerateWithCallback ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxOfflineTtsGenerateWithCallbackNative>>(
-            'SherpaOnnxOfflineTtsGenerateWithCallback')
+          'SherpaOnnxOfflineTtsGenerateWithCallback',
+        )
         .asFunction();
 
     createOfflineRecognizer ??= dynamicLibrary
         .lookup<NativeFunction<CreateOfflineRecognizerNative>>(
-            'SherpaOnnxCreateOfflineRecognizer')
+          'SherpaOnnxCreateOfflineRecognizer',
+        )
         .asFunction();
 
     destroyOfflineRecognizer ??= dynamicLibrary
         .lookup<NativeFunction<DestroyOfflineRecognizerNative>>(
-            'SherpaOnnxDestroyOfflineRecognizer')
+          'SherpaOnnxDestroyOfflineRecognizer',
+        )
         .asFunction();
 
     offlineRecognizerSetConfig ??= dynamicLibrary
         .lookup<NativeFunction<OfflineRecognizerSetConfigNative>>(
-            'SherpaOnnxOfflineRecognizerSetConfig')
+          'SherpaOnnxOfflineRecognizerSetConfig',
+        )
         .asFunction();
 
     createOfflineStream ??= dynamicLibrary
         .lookup<NativeFunction<CreateOfflineStreamNative>>(
-            'SherpaOnnxCreateOfflineStream')
+          'SherpaOnnxCreateOfflineStream',
+        )
         .asFunction();
 
     destroyOfflineStream ??= dynamicLibrary
         .lookup<NativeFunction<DestroyOfflineStreamNative>>(
-            'SherpaOnnxDestroyOfflineStream')
+          'SherpaOnnxDestroyOfflineStream',
+        )
         .asFunction();
 
     acceptWaveformOffline ??= dynamicLibrary
         .lookup<NativeFunction<AcceptWaveformOfflineNative>>(
-            'SherpaOnnxAcceptWaveformOffline')
+          'SherpaOnnxAcceptWaveformOffline',
+        )
         .asFunction();
 
     decodeOfflineStream ??= dynamicLibrary
         .lookup<NativeFunction<DecodeOfflineStreamNative>>(
-            'SherpaOnnxDecodeOfflineStream')
+          'SherpaOnnxDecodeOfflineStream',
+        )
         .asFunction();
 
     getOfflineStreamResultAsJson ??= dynamicLibrary
         .lookup<NativeFunction<GetOfflineStreamResultAsJsonNative>>(
-            'SherpaOnnxGetOfflineStreamResultAsJson')
+          'SherpaOnnxGetOfflineStreamResultAsJson',
+        )
         .asFunction();
 
     destroyOfflineStreamResultJson ??= dynamicLibrary
         .lookup<NativeFunction<DestroyOfflineStreamResultJsonNative>>(
-            'SherpaOnnxDestroyOfflineStreamResultJson')
+          'SherpaOnnxDestroyOfflineStreamResultJson',
+        )
         .asFunction();
 
     createOnlineRecognizer ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOnlineRecognizerNative>>(
-            'SherpaOnnxCreateOnlineRecognizer')
+          'SherpaOnnxCreateOnlineRecognizer',
+        )
         .asFunction();
 
     destroyOnlineRecognizer ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOnlineRecognizerNative>>(
-            'SherpaOnnxDestroyOnlineRecognizer')
+          'SherpaOnnxDestroyOnlineRecognizer',
+        )
         .asFunction();
 
     createOnlineStream ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOnlineStreamNative>>(
-            'SherpaOnnxCreateOnlineStream')
+          'SherpaOnnxCreateOnlineStream',
+        )
         .asFunction();
 
     createOnlineStreamWithHotwords ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateOnlineStreamWithHotwordsNative>>(
-            'SherpaOnnxCreateOnlineStreamWithHotwords')
+          'SherpaOnnxCreateOnlineStreamWithHotwords',
+        )
         .asFunction();
 
     isOnlineStreamReady ??= dynamicLibrary
         .lookup<NativeFunction<IsOnlineStreamReadyNative>>(
-            'SherpaOnnxIsOnlineStreamReady')
+          'SherpaOnnxIsOnlineStreamReady',
+        )
         .asFunction();
 
     decodeOnlineStream ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDecodeOnlineStreamNative>>(
-            'SherpaOnnxDecodeOnlineStream')
+          'SherpaOnnxDecodeOnlineStream',
+        )
         .asFunction();
 
     getOnlineStreamResultAsJson ??= dynamicLibrary
         .lookup<NativeFunction<GetOnlineStreamResultAsJsonNative>>(
-            'SherpaOnnxGetOnlineStreamResultAsJson')
+          'SherpaOnnxGetOnlineStreamResultAsJson',
+        )
         .asFunction();
 
     reset ??= dynamicLibrary
@@ -2019,248 +2247,272 @@ class SherpaOnnxBindings {
 
     isEndpoint ??= dynamicLibrary
         .lookup<NativeFunction<IsEndpointNative>>(
-            'SherpaOnnxOnlineStreamIsEndpoint')
+          'SherpaOnnxOnlineStreamIsEndpoint',
+        )
         .asFunction();
 
     destroyOnlineStreamResultJson ??= dynamicLibrary
         .lookup<NativeFunction<DestroyOnlineStreamResultJsonNative>>(
-            'SherpaOnnxDestroyOnlineStreamResultJson')
+          'SherpaOnnxDestroyOnlineStreamResultJson',
+        )
         .asFunction();
 
     createVoiceActivityDetector ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateVoiceActivityDetectorNative>>(
-            'SherpaOnnxCreateVoiceActivityDetector')
+          'SherpaOnnxCreateVoiceActivityDetector',
+        )
         .asFunction();
 
     destroyVoiceActivityDetector ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyVoiceActivityDetectorNative>>(
-            'SherpaOnnxDestroyVoiceActivityDetector')
+          'SherpaOnnxDestroyVoiceActivityDetector',
+        )
         .asFunction();
 
     voiceActivityDetectorAcceptWaveform ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxVoiceActivityDetectorAcceptWaveformNative>>(
-            'SherpaOnnxVoiceActivityDetectorAcceptWaveform')
+          NativeFunction<SherpaOnnxVoiceActivityDetectorAcceptWaveformNative>
+        >('SherpaOnnxVoiceActivityDetectorAcceptWaveform')
         .asFunction();
 
     voiceActivityDetectorEmpty ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorEmptyNative>>(
-            'SherpaOnnxVoiceActivityDetectorEmpty')
+          'SherpaOnnxVoiceActivityDetectorEmpty',
+        )
         .asFunction();
 
     voiceActivityDetectorDetected ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorDetectedNative>>(
-            'SherpaOnnxVoiceActivityDetectorDetected')
+          'SherpaOnnxVoiceActivityDetectorDetected',
+        )
         .asFunction();
 
     voiceActivityDetectorPop ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorPopNative>>(
-            'SherpaOnnxVoiceActivityDetectorPop')
+          'SherpaOnnxVoiceActivityDetectorPop',
+        )
         .asFunction();
 
     voiceActivityDetectorClear ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorClearNative>>(
-            'SherpaOnnxVoiceActivityDetectorClear')
+          'SherpaOnnxVoiceActivityDetectorClear',
+        )
         .asFunction();
 
     voiceActivityDetectorFront ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorFrontNative>>(
-            'SherpaOnnxVoiceActivityDetectorFront')
+          'SherpaOnnxVoiceActivityDetectorFront',
+        )
         .asFunction();
 
     destroySpeechSegment ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroySpeechSegmentNative>>(
-            'SherpaOnnxDestroySpeechSegment')
+          'SherpaOnnxDestroySpeechSegment',
+        )
         .asFunction();
 
     voiceActivityDetectorReset ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorResetNative>>(
-            'SherpaOnnxVoiceActivityDetectorReset')
+          'SherpaOnnxVoiceActivityDetectorReset',
+        )
         .asFunction();
 
     voiceActivityDetectorFlush ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxVoiceActivityDetectorFlushNative>>(
-            'SherpaOnnxVoiceActivityDetectorFlush')
+          'SherpaOnnxVoiceActivityDetectorFlush',
+        )
         .asFunction();
 
     createCircularBuffer ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateCircularBufferNative>>(
-            'SherpaOnnxCreateCircularBuffer')
+          'SherpaOnnxCreateCircularBuffer',
+        )
         .asFunction();
 
     destroyCircularBuffer ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyCircularBufferNative>>(
-            'SherpaOnnxDestroyCircularBuffer')
+          'SherpaOnnxDestroyCircularBuffer',
+        )
         .asFunction();
 
     circularBufferPush ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferPushNative>>(
-            'SherpaOnnxCircularBufferPush')
+          'SherpaOnnxCircularBufferPush',
+        )
         .asFunction();
 
     circularBufferGet ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferGetNative>>(
-            'SherpaOnnxCircularBufferGet')
+          'SherpaOnnxCircularBufferGet',
+        )
         .asFunction();
 
     circularBufferFree ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferFreeNative>>(
-            'SherpaOnnxCircularBufferFree')
+          'SherpaOnnxCircularBufferFree',
+        )
         .asFunction();
 
     circularBufferPop ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferPopNative>>(
-            'SherpaOnnxCircularBufferPop')
+          'SherpaOnnxCircularBufferPop',
+        )
         .asFunction();
 
     circularBufferSize ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferSizeNative>>(
-            'SherpaOnnxCircularBufferSize')
+          'SherpaOnnxCircularBufferSize',
+        )
         .asFunction();
 
     circularBufferHead ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferHeadNative>>(
-            'SherpaOnnxCircularBufferHead')
+          'SherpaOnnxCircularBufferHead',
+        )
         .asFunction();
 
     circularBufferReset ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCircularBufferResetNative>>(
-            'SherpaOnnxCircularBufferReset')
+          'SherpaOnnxCircularBufferReset',
+        )
         .asFunction();
 
     createSpeakerEmbeddingExtractor ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxCreateSpeakerEmbeddingExtractorNative>>(
-            'SherpaOnnxCreateSpeakerEmbeddingExtractor')
+          NativeFunction<SherpaOnnxCreateSpeakerEmbeddingExtractorNative>
+        >('SherpaOnnxCreateSpeakerEmbeddingExtractor')
         .asFunction();
 
     destroySpeakerEmbeddingExtractor ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxDestroySpeakerEmbeddingExtractorNative>>(
-            'SherpaOnnxDestroySpeakerEmbeddingExtractor')
+          NativeFunction<SherpaOnnxDestroySpeakerEmbeddingExtractorNative>
+        >('SherpaOnnxDestroySpeakerEmbeddingExtractor')
         .asFunction();
 
     speakerEmbeddingExtractorDim ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxSpeakerEmbeddingExtractorDimNative>>(
-            'SherpaOnnxSpeakerEmbeddingExtractorDim')
+          'SherpaOnnxSpeakerEmbeddingExtractorDim',
+        )
         .asFunction();
 
     speakerEmbeddingExtractorCreateStream ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative>>(
-            'SherpaOnnxSpeakerEmbeddingExtractorCreateStream')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingExtractorCreateStreamNative>
+        >('SherpaOnnxSpeakerEmbeddingExtractorCreateStream')
         .asFunction();
 
     speakerEmbeddingExtractorComputeEmbedding ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative>>(
-            'SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding')
+          NativeFunction<
+            SherpaOnnxSpeakerEmbeddingExtractorComputeEmbeddingNative
+          >
+        >('SherpaOnnxSpeakerEmbeddingExtractorComputeEmbedding')
         .asFunction();
 
     speakerEmbeddingExtractorDestroyEmbedding ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbeddingNative>>(
-            'SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding')
+          NativeFunction<
+            SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbeddingNative
+          >
+        >('SherpaOnnxSpeakerEmbeddingExtractorDestroyEmbedding')
         .asFunction();
 
     destroyOnlineStream ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroyOnlineStreamNative>>(
-            'SherpaOnnxDestroyOnlineStream')
+          'SherpaOnnxDestroyOnlineStream',
+        )
         .asFunction();
 
     onlineStreamAcceptWaveform ??= dynamicLibrary
         .lookup<NativeFunction<OnlineStreamAcceptWaveformNative>>(
-            'SherpaOnnxOnlineStreamAcceptWaveform')
+          'SherpaOnnxOnlineStreamAcceptWaveform',
+        )
         .asFunction();
 
     onlineStreamInputFinished ??= dynamicLibrary
         .lookup<NativeFunction<OnlineStreamInputFinishedNative>>(
-            'SherpaOnnxOnlineStreamInputFinished')
+          'SherpaOnnxOnlineStreamInputFinished',
+        )
         .asFunction();
 
     speakerEmbeddingExtractorIsReady ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingExtractorIsReadyNative>>(
-            'SherpaOnnxSpeakerEmbeddingExtractorIsReady')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingExtractorIsReadyNative>
+        >('SherpaOnnxSpeakerEmbeddingExtractorIsReady')
         .asFunction();
 
     createSpeakerEmbeddingManager ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxCreateSpeakerEmbeddingManagerNative>>(
-            'SherpaOnnxCreateSpeakerEmbeddingManager')
+          'SherpaOnnxCreateSpeakerEmbeddingManager',
+        )
         .asFunction();
 
     destroySpeakerEmbeddingManager ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxDestroySpeakerEmbeddingManagerNative>>(
-            'SherpaOnnxDestroySpeakerEmbeddingManager')
+          'SherpaOnnxDestroySpeakerEmbeddingManager',
+        )
         .asFunction();
 
     speakerEmbeddingManagerAdd ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxSpeakerEmbeddingManagerAddNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerAdd')
+          'SherpaOnnxSpeakerEmbeddingManagerAdd',
+        )
         .asFunction();
 
     speakerEmbeddingManagerAddListFlattened ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerAddListFlattenedNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerAddListFlattened')
+          NativeFunction<
+            SherpaOnnxSpeakerEmbeddingManagerAddListFlattenedNative
+          >
+        >('SherpaOnnxSpeakerEmbeddingManagerAddListFlattened')
         .asFunction();
 
     speakerEmbeddingManagerRemove ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxSpeakerEmbeddingManagerRemoveNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerRemove')
+          'SherpaOnnxSpeakerEmbeddingManagerRemove',
+        )
         .asFunction();
 
     speakerEmbeddingManagerContains ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerContainsNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerContains')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingManagerContainsNative>
+        >('SherpaOnnxSpeakerEmbeddingManagerContains')
         .asFunction();
 
     speakerEmbeddingManagerSearch ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxSpeakerEmbeddingManagerSearchNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerSearch')
+          'SherpaOnnxSpeakerEmbeddingManagerSearch',
+        )
         .asFunction();
 
     speakerEmbeddingManagerFreeSearch ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerFreeSearchNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerFreeSearch')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingManagerFreeSearchNative>
+        >('SherpaOnnxSpeakerEmbeddingManagerFreeSearch')
         .asFunction();
 
     speakerEmbeddingManagerNumSpeakers ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerNumSpeakersNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerNumSpeakers')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingManagerNumSpeakersNative>
+        >('SherpaOnnxSpeakerEmbeddingManagerNumSpeakers')
         .asFunction();
 
     speakerEmbeddingManagerVerify ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxSpeakerEmbeddingManagerVerifyNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerVerify')
+          'SherpaOnnxSpeakerEmbeddingManagerVerify',
+        )
         .asFunction();
 
     speakerEmbeddingManagerGetAllSpeakers ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakersNative>
+        >('SherpaOnnxSpeakerEmbeddingManagerGetAllSpeakers')
         .asFunction();
 
     speakerEmbeddingManagerFreeAllSpeakers ??= dynamicLibrary
         .lookup<
-                NativeFunction<
-                    SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakersNative>>(
-            'SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers')
+          NativeFunction<SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakersNative>
+        >('SherpaOnnxSpeakerEmbeddingManagerFreeAllSpeakers')
         .asFunction();
 
     readWave ??= dynamicLibrary
@@ -2269,7 +2521,8 @@ class SherpaOnnxBindings {
 
     writeWave ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxWriteWaveNative>>(
-            'SherpaOnnxWriteWave')
+          'SherpaOnnxWriteWave',
+        )
         .asFunction();
 
     freeWave ??= dynamicLibrary
@@ -2278,17 +2531,20 @@ class SherpaOnnxBindings {
 
     getVersionStr ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxGetVersionStrNative>>(
-            'SherpaOnnxGetVersionStr')
+          'SherpaOnnxGetVersionStr',
+        )
         .asFunction();
 
     getGitSha1 ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxGetGitSha1Native>>(
-            'SherpaOnnxGetGitSha1')
+          'SherpaOnnxGetGitSha1',
+        )
         .asFunction();
 
     getGitDate ??= dynamicLibrary
         .lookup<NativeFunction<SherpaOnnxGetGitDateNative>>(
-            'SherpaOnnxGetGitDate')
+          'SherpaOnnxGetGitDate',
+        )
         .asFunction();
   }
 }

commit d2b8492c5752cc13202ab0160da452d67b914327
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:47:58 2025 +0800

    Add Go API for Google MedAsr model (#2952)

diff --git a/.github/workflows/test-go.yaml b/.github/workflows/test-go.yaml
index 53faad2d..8bb715df 100644
--- a/.github/workflows/test-go.yaml
+++ b/.github/workflows/test-go.yaml
@@ -102,6 +102,7 @@ jobs:
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-canary-decode-files/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-decode-files/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-omnilingual-asr-ctc-decode-files
+            cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-medasr-ctc-decode-files
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-speaker-diarization/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/non-streaming-tts/
             cp -v ../scripts/go/_internal/lib/x86_64-pc-windows-gnu/*.dll ../scripts/go/_internal/speaker-identification/
@@ -141,6 +142,19 @@ jobs:
           name: ${{ matrix.os }}-libs
           path: to-upload/
 
+      - name: Test non-streaming decoding files with MedASR
+        shell: bash
+        run: |
+          cd scripts/go/_internal/non-streaming-medasr-ctc-decode-files
+          ls -lh
+          go mod tidy
+          cat go.mod
+          go build
+          ls -lh
+
+          ./run.sh
+          rm -rf sherpa-onnx-medasr-*
+
       - name: Test non-streaming decoding files with Omnilingual ASR
         shell: bash
         run: |
diff --git a/go-api-examples/non-streaming-medasr-ctc-decode-files/go.mod b/go-api-examples/non-streaming-medasr-ctc-decode-files/go.mod
new file mode 100644
index 00000000..9053a1b0
--- /dev/null
+++ b/go-api-examples/non-streaming-medasr-ctc-decode-files/go.mod
@@ -0,0 +1,3 @@
+module non-streaming-medasr-ctc-decode-files
+
+go 1.17
diff --git a/go-api-examples/non-streaming-medasr-ctc-decode-files/main.go b/go-api-examples/non-streaming-medasr-ctc-decode-files/main.go
new file mode 100644
index 00000000..67d658d4
--- /dev/null
+++ b/go-api-examples/non-streaming-medasr-ctc-decode-files/main.go
@@ -0,0 +1,97 @@
+package main
+
+import (
+	"bytes"
+	"encoding/binary"
+	"log"
+	"os"
+	"strings"
+
+	sherpa "github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx"
+	"github.com/youpy/go-wav"
+)
+
+func main() {
+	log.SetFlags(log.LstdFlags | log.Lmicroseconds)
+
+	config := sherpa.OfflineRecognizerConfig{}
+
+	config.ModelConfig.MedAsr.Model = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx"
+	config.ModelConfig.Tokens = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt"
+
+	waveFilename := "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav"
+
+	samples, sampleRate := readWave(waveFilename)
+
+	log.Println("Initializing recognizer (may take several seconds)")
+	recognizer := sherpa.NewOfflineRecognizer(&config)
+	log.Println("Recognizer created!")
+	defer sherpa.DeleteOfflineRecognizer(recognizer)
+
+	log.Println("Start decoding!")
+	stream := sherpa.NewOfflineStream(recognizer)
+	defer sherpa.DeleteOfflineStream(stream)
+
+	stream.AcceptWaveform(sampleRate, samples)
+
+	recognizer.Decode(stream)
+	log.Println("Decoding done!")
+	result := stream.GetResult()
+
+	log.Println("Text: " + strings.ToLower(result.Text))
+}
+
+func readWave(filename string) (samples []float32, sampleRate int) {
+	file, _ := os.Open(filename)
+	defer file.Close()
+
+	reader := wav.NewReader(file)
+	format, err := reader.Format()
+	if err != nil {
+		log.Fatalf("Failed to read wave format")
+	}
+
+	if format.AudioFormat != 1 {
+		log.Fatalf("Support only PCM format. Given: %v\n", format.AudioFormat)
+	}
+
+	if format.NumChannels != 1 {
+		log.Fatalf("Support only 1 channel wave file. Given: %v\n", format.NumChannels)
+	}
+
+	if format.BitsPerSample != 16 {
+		log.Fatalf("Support only 16-bit per sample. Given: %v\n", format.BitsPerSample)
+	}
+
+	reader.Duration() // so that it initializes reader.Size
+
+	buf := make([]byte, reader.Size)
+	n, err := reader.Read(buf)
+	if n != int(reader.Size) {
+		log.Fatalf("Failed to read %v bytes. Returned %v bytes\n", reader.Size, n)
+	}
+
+	samples = samplesInt16ToFloat(buf)
+	sampleRate = int(format.SampleRate)
+
+	return
+}
+
+func samplesInt16ToFloat(inSamples []byte) []float32 {
+	numSamples := len(inSamples) / 2
+	outSamples := make([]float32, numSamples)
+
+	for i := 0; i != numSamples; i++ {
+		s := inSamples[i*2 : (i+1)*2]
+
+		var s16 int16
+		buf := bytes.NewReader(s)
+		err := binary.Read(buf, binary.LittleEndian, &s16)
+		if err != nil {
+			log.Fatal("Failed to parse 16-bit sample")
+		}
+		outSamples[i] = float32(s16) / 32768
+	}
+
+	return outSamples
+}
diff --git a/go-api-examples/non-streaming-medasr-ctc-decode-files/run.sh b/go-api-examples/non-streaming-medasr-ctc-decode-files/run.sh
new file mode 100755
index 00000000..270c6851
--- /dev/null
+++ b/go-api-examples/non-streaming-medasr-ctc-decode-files/run.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+
+set -ex
+
+export CGO_ENABLED=1
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+go mod tidy
+go build
+./non-streaming-medasr-ctc-decode-files
diff --git a/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/.gitignore b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/.gitignore
new file mode 100644
index 00000000..f477ce34
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/.gitignore
@@ -0,0 +1 @@
+non-streaming-medasr-ctc-decode-files
diff --git a/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/go.mod b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/go.mod
new file mode 100644
index 00000000..7ee920cf
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/go.mod
@@ -0,0 +1,5 @@
+module non-streaming-medasr-ctc-decode-files
+
+go 1.17
+
+replace github.com/k2-fsa/sherpa-onnx-go/sherpa_onnx => ../
diff --git a/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/main.go b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/main.go
new file mode 120000
index 00000000..2bf87e85
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/main.go
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-medasr-ctc-decode-files/main.go
\ No newline at end of file
diff --git a/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/run.sh b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/run.sh
new file mode 120000
index 00000000..13ba7c9b
--- /dev/null
+++ b/scripts/go/_internal/non-streaming-medasr-ctc-decode-files/run.sh
@@ -0,0 +1 @@
+../../../../go-api-examples/non-streaming-medasr-ctc-decode-files/run.sh
\ No newline at end of file
diff --git a/scripts/go/sherpa_onnx.go b/scripts/go/sherpa_onnx.go
index 3e754f06..47c7864c 100644
--- a/scripts/go/sherpa_onnx.go
+++ b/scripts/go/sherpa_onnx.go
@@ -423,6 +423,10 @@ type OfflineOmnilingualAsrCtcModelConfig struct {
 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
 }
 
+type OfflineMedAsrCtcModelConfig struct {
+	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
+}
+
 type OfflineDolphinModelConfig struct {
 	Model string // Path to the model, e.g., model.onnx or model.int8.onnx
 }
@@ -485,6 +489,7 @@ type OfflineModelConfig struct {
 	Canary       OfflineCanaryModelConfig
 	WenetCtc     OfflineWenetCtcModelConfig
 	Omnilingual  OfflineOmnilingualAsrCtcModelConfig
+	MedAsr       OfflineMedAsrCtcModelConfig
 	Tokens       string // Path to tokens.txt
 
 	// Number of threads to use for neural network computation
@@ -589,6 +594,7 @@ func newCOfflineRecognizerConfig(config *OfflineRecognizerConfig) *C.struct_Sher
 	c.model_config.wenet_ctc.model = C.CString(config.ModelConfig.WenetCtc.Model)
 
 	c.model_config.omnilingual.model = C.CString(config.ModelConfig.Omnilingual.Model)
+	c.model_config.medasr.model = C.CString(config.ModelConfig.MedAsr.Model)
 
 	c.model_config.tokens = C.CString(config.ModelConfig.Tokens)
 
@@ -742,6 +748,11 @@ func freeCOfflineRecognizerConfig(c *C.struct_SherpaOnnxOfflineRecognizerConfig)
 		c.model_config.wenet_ctc.model = nil
 	}
 
+	if c.model_config.medasr.model != nil {
+		C.free(unsafe.Pointer(c.model_config.medasr.model))
+		c.model_config.medasr.model = nil
+	}
+
 	if c.model_config.omnilingual.model != nil {
 		C.free(unsafe.Pointer(c.model_config.omnilingual.model))
 		c.model_config.omnilingual.model = nil

commit 654b9fd502ef7d41084ccf34a1652c27e5647f54
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:45:09 2025 +0800

    Add Pascal API for Google MedASR model (#2950)

diff --git a/.github/workflows/pascal.yaml b/.github/workflows/pascal.yaml
index 905af440..44412c02 100644
--- a/.github/workflows/pascal.yaml
+++ b/.github/workflows/pascal.yaml
@@ -139,6 +139,10 @@ jobs:
           rm -rf sherpa-onnx-*
           echo "---"
 
+          ./run-medasr-ctc.sh
+          rm -rf sherpa-onnx-*
+          echo "---"
+
           ./run-omnilingual-asr-ctc.sh
           rm -rf sherpa-onnx-*
           echo "---"
diff --git a/pascal-api-examples/non-streaming-asr/.gitignore b/pascal-api-examples/non-streaming-asr/.gitignore
index d1917ed1..0ff47ac7 100644
--- a/pascal-api-examples/non-streaming-asr/.gitignore
+++ b/pascal-api-examples/non-streaming-asr/.gitignore
@@ -13,3 +13,4 @@ zipformer_ctc
 wenet_ctc
 nemo_canary
 omnilingual_asr_ctc
+medasr_ctc
diff --git a/pascal-api-examples/non-streaming-asr/medasr_ctc.pas b/pascal-api-examples/non-streaming-asr/medasr_ctc.pas
new file mode 100644
index 00000000..bd8fe3c1
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/medasr_ctc.pas
@@ -0,0 +1,76 @@
+{ Copyright (c)  2025  Xiaomi Corporation }
+
+{
+This file shows how to use a non-streaming Google MedASR CTC model
+to decode files.
+
+You can download the model files from
+https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
+}
+
+program medasr_ctc;
+
+{$mode objfpc}
+
+uses
+  sherpa_onnx,
+  DateUtils,
+  SysUtils;
+
+var
+  Wave: TSherpaOnnxWave;
+  WaveFilename: AnsiString;
+
+  Config: TSherpaOnnxOfflineRecognizerConfig;
+  Recognizer: TSherpaOnnxOfflineRecognizer;
+  Stream: TSherpaOnnxOfflineStream;
+  RecognitionResult: TSherpaOnnxOfflineRecognizerResult;
+
+  Start: TDateTime;
+  Stop: TDateTime;
+
+  Elapsed: Single;
+  Duration: Single;
+  RealTimeFactor: Single;
+begin
+  Initialize(Config);
+
+  Config.ModelConfig.MedAsr.Model := './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx';
+  Config.ModelConfig.Tokens := './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt';
+  Config.ModelConfig.Provider := 'cpu';
+  Config.ModelConfig.NumThreads := 1;
+  Config.ModelConfig.Debug := True;
+
+  WaveFilename := './sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav';
+
+  Wave := SherpaOnnxReadWave(WaveFilename);
+
+  Recognizer := TSherpaOnnxOfflineRecognizer.Create(Config);
+  Stream := Recognizer.CreateStream();
+  Start := Now;
+
+  Stream.AcceptWaveform(Wave.Samples, Wave.SampleRate);
+  Recognizer.Decode(Stream);
+
+  RecognitionResult := Recognizer.GetResult(Stream);
+
+  Stop := Now;
+
+  Elapsed := MilliSecondsBetween(Stop, Start) / 1000;
+  Duration := Length(Wave.Samples) / Wave.SampleRate;
+  RealTimeFactor := Elapsed / Duration;
+
+  WriteLn(RecognitionResult.ToString);
+  WriteLn(Format('NumThreads %d', [Config.ModelConfig.NumThreads]));
+  WriteLn(Format('Elapsed %.3f s', [Elapsed]));
+  WriteLn(Format('Wave duration %.3f s', [Duration]));
+  WriteLn(Format('RTF = %.3f/%.3f = %.3f', [Elapsed, Duration, RealTimeFactor]));
+
+  {Free resources to avoid memory leak.
+
+  Note: You don't need to invoke them for this simple script.
+  However, you have to invoke them in your own large/complex project.
+  }
+  FreeAndNil(Stream);
+  FreeAndNil(Recognizer);
+end.
diff --git a/pascal-api-examples/non-streaming-asr/run-medasr-ctc.sh b/pascal-api-examples/non-streaming-asr/run-medasr-ctc.sh
new file mode 100755
index 00000000..219cdd2a
--- /dev/null
+++ b/pascal-api-examples/non-streaming-asr/run-medasr-ctc.sh
@@ -0,0 +1,42 @@
+#!/usr/bin/env bash
+
+set -ex
+
+SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
+SHERPA_ONNX_DIR=$(cd $SCRIPT_DIR/../.. && pwd)
+
+echo "SHERPA_ONNX_DIR: $SHERPA_ONNX_DIR"
+
+if [[ ! -f ../../build/install/lib/libsherpa-onnx-c-api.dylib  && ! -f ../../build/install/lib/libsherpa-onnx-c-api.so && ! -f ../../build/install/lib/sherpa-onnx-c-api.dll ]]; then
+  mkdir -p ../../build
+  pushd ../../build
+  cmake \
+    -DCMAKE_INSTALL_PREFIX=./install \
+    -DSHERPA_ONNX_ENABLE_PYTHON=OFF \
+    -DSHERPA_ONNX_ENABLE_TESTS=OFF \
+    -DSHERPA_ONNX_ENABLE_CHECK=OFF \
+    -DBUILD_SHARED_LIBS=ON \
+    -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
+    ..
+
+  cmake --build . --target install --config Release
+  ls -lh lib
+  popd
+fi
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+fpc \
+  -dSHERPA_ONNX_USE_SHARED_LIBS \
+  -Fu$SHERPA_ONNX_DIR/sherpa-onnx/pascal-api \
+  -Fl$SHERPA_ONNX_DIR/build/install/lib \
+  ./medasr_ctc.pas
+
+export LD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$LD_LIBRARY_PATH
+export DYLD_LIBRARY_PATH=$SHERPA_ONNX_DIR/build/install/lib:$DYLD_LIBRARY_PATH
+
+./medasr_ctc
diff --git a/sherpa-onnx/pascal-api/sherpa_onnx.pas b/sherpa-onnx/pascal-api/sherpa_onnx.pas
index 257aa30e..40f4a485 100644
--- a/sherpa-onnx/pascal-api/sherpa_onnx.pas
+++ b/sherpa-onnx/pascal-api/sherpa_onnx.pas
@@ -340,6 +340,11 @@ type
     function ToString: AnsiString;
   end;
 
+  TSherpaOnnxOfflineMedAsrCtcModelConfig = record
+    Model: AnsiString;
+    function ToString: AnsiString;
+  end;
+
   TSherpaOnnxOfflineWhisperModelConfig = record
     Encoder: AnsiString;
     Decoder: AnsiString;
@@ -416,6 +421,7 @@ type
     Canary: TSherpaOnnxOfflineCanaryModelConfig;
     WenetCtc: TSherpaOnnxOfflineWenetCtcModelConfig;
     Omnilingual: TSherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
+    MedAsr: TSherpaOnnxOfflineMedAsrCtcModelConfig;
     class operator Initialize({$IFDEF FPC}var{$ELSE}out{$ENDIF} Dest: TSherpaOnnxOfflineModelConfig);
     function ToString: AnsiString;
   end;
@@ -828,6 +834,9 @@ type
   SherpaOnnxOfflineOmnilingualAsrCtcModelConfig = record
     Model: PAnsiChar;
   end;
+  SherpaOnnxOfflineMedAsrCtcModelConfig = record
+    Model: PAnsiChar;
+  end;
   SherpaOnnxOfflineWhisperModelConfig = record
     Encoder: PAnsiChar;
     Decoder: PAnsiChar;
@@ -886,6 +895,7 @@ type
     Canary: SherpaOnnxOfflineCanaryModelConfig;
     WenetCtc: SherpaOnnxOfflineWenetCtcModelConfig;
     Omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
+    MedAsr: SherpaOnnxOfflineMedAsrCtcModelConfig;
   end;
 
   SherpaOnnxOfflineRecognizerConfig = record
@@ -1719,6 +1729,12 @@ begin
     [Self.Model]);
 end;
 
+function TSherpaOnnxOfflineMedAsrCtcModelConfig.ToString: AnsiString;
+begin
+  Result := Format('TSherpaOnnxOfflineMedAsrCtcModelConfig(Model := %s)',
+    [Self.Model]);
+end;
+
 function TSherpaOnnxOfflineWhisperModelConfig.ToString: AnsiString;
 begin
   Result := Format('TSherpaOnnxOfflineWhisperModelConfig(' +
@@ -1810,7 +1826,8 @@ begin
     'ZipformerCtc := %s, ' +
     'Canary := %s, ' +
     'WenetCtc := %s, ' +
-    'Omnilingual := %s' +
+    'Omnilingual := %s, ' +
+    'MedAsr := %s' +
     ')',
     [Self.Transducer.ToString, Self.Paraformer.ToString,
      Self.NeMoCtc.ToString, Self.Whisper.ToString, Self.Tdnn.ToString,
@@ -1819,7 +1836,7 @@ begin
      Self.TeleSpeechCtc, Self.SenseVoice.ToString, Self.Moonshine.ToString,
      Self.FireRedAsr.ToString, Self.Dolphin.ToString,
      Self.ZipformerCtc.ToString, Self.Canary.ToString, Self.WenetCtc.ToString,
-     Self.Omnilingual.ToString
+     Self.Omnilingual.ToString, Self.MedAsr.ToString
      ]);
 end;
 
@@ -1900,6 +1917,7 @@ begin
 
   C.ModelConfig.WenetCtc.Model := PAnsiChar(Config.ModelConfig.WenetCtc.Model);
   C.ModelConfig.Omnilingual.Model := PAnsiChar(Config.ModelConfig.Omnilingual.Model);
+  C.ModelConfig.MedAsr.Model := PAnsiChar(Config.ModelConfig.MedAsr.Model);
 
   C.LMConfig.Model := PAnsiChar(Config.LMConfig.Model);
   C.LMConfig.Scale := Config.LMConfig.Scale;

commit 29c920d3d45f58b396a9537af438517d88380a5a
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:41:36 2025 +0800

    Add C# API for Google MedASR model (#2949)

diff --git a/.github/scripts/test-dot-net.sh b/.github/scripts/test-dot-net.sh
index f65c0f82..ccf61932 100755
--- a/.github/scripts/test-dot-net.sh
+++ b/.github/scripts/test-dot-net.sh
@@ -32,6 +32,9 @@ rm -rf sherpa-onnx-nemo-*
 
 cd ../offline-decode-files
 
+./run-medasr-ctc.sh
+rm -rf sherpa-onnx-*
+
 ./run-omnilingual-asr-ctc.sh
 rm -rf sherpa-onnx-*
 
diff --git a/dotnet-examples/offline-decode-files/Program.cs b/dotnet-examples/offline-decode-files/Program.cs
index 4b3d1893..0e2b8a48 100644
--- a/dotnet-examples/offline-decode-files/Program.cs
+++ b/dotnet-examples/offline-decode-files/Program.cs
@@ -90,6 +90,9 @@ class OfflineDecodeFiles
     [Option("omnilingual-asr-ctc", Required = false, HelpText = "Path to model.onnx. Used only for Omnilingual ASR CTC models")]
     public string Omnilingual { get; set; } = string.Empty;
 
+    [Option("medasr", Required = false, HelpText = "Path to model.onnx. Used only for Google MedASR CTC models")]
+    public string MedAsr { get; set; } = string.Empty;
+
     [Option("sense-voice-model", Required = false, HelpText = "Path to model.onnx. Used only for SenseVoice CTC models")]
     public string SenseVoiceModel { get; set; } = string.Empty;
 
@@ -265,6 +268,10 @@ to download pre-trained Tdnn models.
     {
       config.ModelConfig.Omnilingual.Model = options.Omnilingual;
     }
+    else if (!string.IsNullOrEmpty(options.MedAsr))
+    {
+      config.ModelConfig.MedAsr.Model = options.MedAsr;
+    }
     else if (!string.IsNullOrEmpty(options.WhisperEncoder))
     {
       config.ModelConfig.Whisper.Encoder = options.WhisperEncoder;
diff --git a/dotnet-examples/offline-decode-files/run-medasr-ctc.sh b/dotnet-examples/offline-decode-files/run-medasr-ctc.sh
new file mode 100755
index 00000000..915ed202
--- /dev/null
+++ b/dotnet-examples/offline-decode-files/run-medasr-ctc.sh
@@ -0,0 +1,14 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+dotnet run \
+  --medasr=./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx \
+  --tokens=./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt \
+  --files ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav
diff --git a/scripts/dotnet/OfflineMedAsrCtcModel.cs b/scripts/dotnet/OfflineMedAsrCtcModel.cs
new file mode 100644
index 00000000..9a4d872c
--- /dev/null
+++ b/scripts/dotnet/OfflineMedAsrCtcModel.cs
@@ -0,0 +1,18 @@
+/// Copyright (c)  2025  Xiaomi Corporation (authors: Fangjun Kuang)
+
+using System.Runtime.InteropServices;
+
+namespace SherpaOnnx
+{
+
+    [StructLayout(LayoutKind.Sequential)]
+    public struct OfflineMedAsrCtcModelConfig
+    {
+        public OfflineMedAsrCtcModelConfig()
+        {
+            Model = "";
+        }
+        [MarshalAs(UnmanagedType.LPStr)]
+        public string Model;
+    }
+}
diff --git a/scripts/dotnet/OfflineModelConfig.cs b/scripts/dotnet/OfflineModelConfig.cs
index 47205cfe..a6197fbd 100644
--- a/scripts/dotnet/OfflineModelConfig.cs
+++ b/scripts/dotnet/OfflineModelConfig.cs
@@ -31,6 +31,7 @@ namespace SherpaOnnx
             Canary = new OfflineCanaryModelConfig();
             WenetCtc = new OfflineWenetCtcModelConfig();
             Omnilingual = new OfflineOmnilingualAsrCtcModelConfig();
+            MedAsr = new OfflineMedAsrCtcModelConfig();
         }
         public OfflineTransducerModelConfig Transducer;
         public OfflineParaformerModelConfig Paraformer;
@@ -68,5 +69,6 @@ namespace SherpaOnnx
         public OfflineCanaryModelConfig Canary;
         public OfflineWenetCtcModelConfig WenetCtc;
         public OfflineOmnilingualAsrCtcModelConfig Omnilingual;
+        public OfflineMedAsrCtcModelConfig MedAsr;
     }
 }

commit 234bfe3629957561a086d014ec036bc015df00bc
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 12:40:38 2025 +0800

    Add Swift API for Google MedASR model (#2947)

diff --git a/.github/scripts/test-swift.sh b/.github/scripts/test-swift.sh
index 9944d861..c150b716 100755
--- a/.github/scripts/test-swift.sh
+++ b/.github/scripts/test-swift.sh
@@ -9,6 +9,9 @@ ls -lh
 
 ./run-test-version.sh
 
+./run-medasr-ctc-asr.sh
+rm -rf sherpa-onnx-medasr-*
+
 ./run-omnilingual-asr-ctc-asr.sh
 rm -rf sherpa-onnx-omnilingual-*
 
diff --git a/swift-api-examples/.gitignore b/swift-api-examples/.gitignore
index 52857338..22c8456e 100644
--- a/swift-api-examples/.gitignore
+++ b/swift-api-examples/.gitignore
@@ -25,3 +25,4 @@ tts-kitten-en
 compute-speaker-embeddings
 decode-file-t-one-streaming
 omnilingual-asr-ctc
+medasr-ctc
diff --git a/swift-api-examples/SherpaOnnx.swift b/swift-api-examples/SherpaOnnx.swift
index 1b1ca137..8c7993e8 100644
--- a/swift-api-examples/SherpaOnnx.swift
+++ b/swift-api-examples/SherpaOnnx.swift
@@ -376,6 +376,14 @@ func sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(
   )
 }
 
+func sherpaOnnxOfflineMedAsrCtcModelConfig(
+  model: String = ""
+) -> SherpaOnnxOfflineMedAsrCtcModelConfig {
+  return SherpaOnnxOfflineMedAsrCtcModelConfig(
+    model: toCPointer(model)
+  )
+}
+
 func sherpaOnnxOfflineNemoEncDecCtcModelConfig(
   model: String = ""
 ) -> SherpaOnnxOfflineNemoEncDecCtcModelConfig {
@@ -502,7 +510,9 @@ func sherpaOnnxOfflineModelConfig(
   wenetCtc: SherpaOnnxOfflineWenetCtcModelConfig =
     sherpaOnnxOfflineWenetCtcModelConfig(),
   omnilingual: SherpaOnnxOfflineOmnilingualAsrCtcModelConfig =
-    sherpaOnnxOfflineOmnilingualAsrCtcModelConfig()
+    sherpaOnnxOfflineOmnilingualAsrCtcModelConfig(),
+  medasr: SherpaOnnxOfflineMedAsrCtcModelConfig =
+    sherpaOnnxOfflineMedAsrCtcModelConfig()
 ) -> SherpaOnnxOfflineModelConfig {
   return SherpaOnnxOfflineModelConfig(
     transducer: transducer,
@@ -525,7 +535,8 @@ func sherpaOnnxOfflineModelConfig(
     zipformer_ctc: zipformerCtc,
     canary: canary,
     wenet_ctc: wenetCtc,
-    omnilingual: omnilingual
+    omnilingual: omnilingual,
+    medasr: medasr
   )
 }
 
diff --git a/swift-api-examples/medasr-ctc.swift b/swift-api-examples/medasr-ctc.swift
new file mode 100644
index 00000000..091a4a03
--- /dev/null
+++ b/swift-api-examples/medasr-ctc.swift
@@ -0,0 +1,42 @@
+func run() {
+  let model =
+    "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx"
+  let tokens =
+    "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt"
+
+  let medasr = sherpaOnnxOfflineMedAsrCtcModelConfig(
+    model: model
+  )
+
+  let modelConfig = sherpaOnnxOfflineModelConfig(
+    tokens: tokens,
+    debug: 1,
+    medasr: medasr
+  )
+
+  let featConfig = sherpaOnnxFeatureConfig()
+  var config = sherpaOnnxOfflineRecognizerConfig(
+    featConfig: featConfig,
+    modelConfig: modelConfig
+  )
+
+  let recognizer = SherpaOnnxOfflineRecognizer(config: &config)
+
+  let filePath = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav"
+  let audio = SherpaOnnxWaveWrapper.readWave(filename: filePath)
+
+  let result = recognizer.decode(samples: audio.samples, sampleRate: audio.sampleRate)
+  print("decode done")
+
+  print("\nresult is:\n\(result.text)")
+  if result.timestamps.count != 0 {
+    print("\ntimestamps is:\n\(result.timestamps)")
+  }
+}
+
+@main
+struct App {
+  static func main() {
+    run()
+  }
+}
diff --git a/swift-api-examples/run-medasr-ctc-asr.sh b/swift-api-examples/run-medasr-ctc-asr.sh
new file mode 100755
index 00000000..22c26190
--- /dev/null
+++ b/swift-api-examples/run-medasr-ctc-asr.sh
@@ -0,0 +1,34 @@
+#!/usr/bin/env bash
+
+set -ex
+
+if [ ! -d ../build-swift-macos ]; then
+  echo "Please run ../build-swift-macos.sh first!"
+  exit 1
+fi
+
+if [ ! -f ./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt ]; then
+  curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+  rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+fi
+
+if [ ! -e ./medasr-ctc ]; then
+  # Note: We use -lc++ to link against libc++ instead of libstdc++
+  swiftc \
+    -lc++ \
+    -I ../build-swift-macos/install/include \
+    -import-objc-header ./SherpaOnnx-Bridging-Header.h \
+    ./medasr-ctc.swift  ./SherpaOnnx.swift \
+    -L ../build-swift-macos/install/lib/ \
+    -l sherpa-onnx \
+    -l onnxruntime \
+    -o medasr-ctc
+
+  strip medasr-ctc
+else
+  echo "./medasr-ctc exists - skip building"
+fi
+
+export DYLD_LIBRARY_PATH=$PWD/../build-swift-macos/install/lib:$DYLD_LIBRARY_PATH
+./medasr-ctc

commit df0d4d0745585d99fc99f335840bd44d3c38b74f
Author: Sergio Rodrguez <35671015+SergioRt1@users.noreply.github.com>
Date:   Sun Dec 28 23:39:37 2025 -0500

    [TTS Engine] Fix engine speed (#2895)

diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
index f388b5f4..2de37255 100644
--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
+++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/MainActivity.kt
@@ -101,7 +101,7 @@ class MainActivity : ComponentActivity() {
                                             TtsEngine.speed = it
                                             preferenceHelper.setSpeed(it)
                                         },
-                                        valueRange = 0.2F..3.0F,
+                                        valueRange = MIN_TTS_SPEED..MAX_TTS_SPEED,
                                         modifier = Modifier.fillMaxWidth()
                                     )
                                 }
diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
index 649b7dce..25e65323 100644
--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
+++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsEngine.kt
@@ -13,6 +13,9 @@ import java.io.File
 import java.io.FileOutputStream
 import java.io.IOException
 
+const val MIN_TTS_SPEED = 0.1f
+const val MAX_TTS_SPEED = 5.0f
+
 object TtsEngine {
     var tts: OfflineTts? = null
 
diff --git a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsService.kt b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsService.kt
index 53a9ef2d..b1b5829f 100644
--- a/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsService.kt
+++ b/android/SherpaOnnxTtsEngine/app/src/main/java/com/k2fsa/sherpa/onnx/tts/engine/TtsService.kt
@@ -110,13 +110,24 @@ class TtsService : TextToSpeechService() {
         val country = request.country
         val variant = request.variant
         val text = request.charSequenceText.toString()
+        // Map Android TTS speech rate (where 100 == normal) to engine speed (1.0 == normal)
+        // Allow per-request override from external apps; fallback to engine default if absent.
+        val rate = runCatching { request.speechRate }.getOrDefault(-1)
+        val engineSpeed = if (rate > 0) {
+            // Map 100 -> 1.0f
+            val mapped = rate / 100.0f
+            mapped.coerceIn(MIN_TTS_SPEED, MAX_TTS_SPEED)
+        } else {
+            // Fallback to current engine/global setting
+            TtsEngine.speed
+        }
 
         val ret = onIsLanguageAvailable(language, country, variant)
         if (ret == TextToSpeech.LANG_NOT_SUPPORTED) {
             callback.error()
             return
         }
-        Log.i(TAG, "text: $text")
+        Log.i(TAG, "text: $text, engineSpeed: $engineSpeed")
         val tts = TtsEngine.tts!!
 
         // Note that AudioFormat.ENCODING_PCM_FLOAT requires API level >= 24
@@ -149,7 +160,7 @@ class TtsService : TextToSpeechService() {
         tts.generateWithCallback(
             text = text,
             sid = TtsEngine.speakerId,
-            speed = TtsEngine.speed,
+            speed = engineSpeed,
             callback = ttsCallback,
         )
 

commit 13b8b84a892fee9e0dabc56e696c7c9a79cea728
Author: Fangjun Kuang <csukuangfj@gmail.com>
Date:   Mon Dec 29 09:56:08 2025 +0800

    Add C and CXX API for Google MedASR model (#2946)

diff --git a/.github/workflows/c-api.yaml b/.github/workflows/c-api.yaml
index b8e1adea..d71e63d7 100644
--- a/.github/workflows/c-api.yaml
+++ b/.github/workflows/c-api.yaml
@@ -75,6 +75,36 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-c-api.dylib
           fi
 
+      - name: Test MedASR CTC
+        shell: bash
+        run: |
+          name=medasr-ctc-c-api
+          gcc -o $name ./c-api-examples/$name.c \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+          tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+          rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm $name
+          rm -rf sherpa-onnx-medasr-*
+
       - name: Test Omnilingual ASR CTC
         shell: bash
         run: |
diff --git a/.github/workflows/cxx-api.yaml b/.github/workflows/cxx-api.yaml
index 81866989..4f741a41 100644
--- a/.github/workflows/cxx-api.yaml
+++ b/.github/workflows/cxx-api.yaml
@@ -78,6 +78,40 @@ jobs:
             otool -L ./install/lib/libsherpa-onnx-cxx-api.dylib
           fi
 
+      - name: Test MedASR CTC
+        shell: bash
+        run: |
+          name=medasr-ctc-cxx-api
+          g++ -std=c++17 -o $name ./cxx-api-examples/$name.cc \
+            -I ./build/install/include \
+            -L ./build/install/lib/ \
+            -l sherpa-onnx-cxx-api \
+            -l sherpa-onnx-c-api \
+            -l onnxruntime
+
+          ls -lh $name
+
+          if [[ ${{ matrix.os }} == ubuntu-latest || ${{ matrix.os }} == ubuntu-22.04-arm ]]; then
+            ls -lh ./$name
+            ldd ./$name
+            echo "----"
+            readelf -d ./$name
+          fi
+
+          curl -SL -O https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+          tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+          rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+
+          echo "---"
+
+          export LD_LIBRARY_PATH=$PWD/build/install/lib:$LD_LIBRARY_PATH
+          export DYLD_LIBRARY_PATH=$PWD/build/install/lib:$DYLD_LIBRARY_PATH
+
+          ./$name
+
+          rm -rf sherpa-onnx-medasr-*
+          rm -v ./$name
+
       - name: Test Omnilingual ASR CTC
         shell: bash
         run: |
diff --git a/.gitignore b/.gitignore
index 612f82b2..2ee313d3 100755
--- a/.gitignore
+++ b/.gitignore
@@ -165,3 +165,4 @@ build-riscv64-linux-gnu-spacemit/
 spacemit-toolchain*
 sherpa-onnx-qnn-*
 matcha-icefall-*
+sherpa-onnx-medasr-ctc-en-int8-2025-12-25
diff --git a/c-api-examples/CMakeLists.txt b/c-api-examples/CMakeLists.txt
index b3baed3a..f73e7f40 100644
--- a/c-api-examples/CMakeLists.txt
+++ b/c-api-examples/CMakeLists.txt
@@ -86,6 +86,9 @@ target_link_libraries(wenet-ctc-c-api sherpa-onnx-c-api)
 add_executable(omnilingual-asr-ctc-c-api omnilingual-asr-ctc-c-api.c)
 target_link_libraries(omnilingual-asr-ctc-c-api sherpa-onnx-c-api)
 
+add_executable(medasr-ctc-c-api medasr-ctc-c-api.c)
+target_link_libraries(medasr-ctc-c-api sherpa-onnx-c-api)
+
 add_executable(streaming-zipformer-c-api streaming-zipformer-c-api.c)
 target_link_libraries(streaming-zipformer-c-api sherpa-onnx-c-api)
 
diff --git a/c-api-examples/medasr-ctc-c-api.c b/c-api-examples/medasr-ctc-c-api.c
new file mode 100644
index 00000000..27a02a9b
--- /dev/null
+++ b/c-api-examples/medasr-ctc-c-api.c
@@ -0,0 +1,82 @@
+// c-api-examples/medasr-ctc-c-api.c
+//
+// Copyright (c)  2025  Xiaomi Corporation
+
+//
+// This file demonstrates how to use MedASR with sherpa-onnx's C API.
+// clang-format off
+/*
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+*/
+//
+// clang-format on
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include "sherpa-onnx/c-api/c-api.h"
+
+int32_t main() {
+  // clang-format off
+  const char *wav_filename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav";
+  const char *model_filename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx";
+  const char *tokens_filename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt";
+  // clang-format on
+
+  const char *provider = "cpu";
+
+  const SherpaOnnxWave *wave = SherpaOnnxReadWave(wav_filename);
+  if (wave == NULL) {
+    fprintf(stderr, "Failed to read %s\n", wav_filename);
+    return -1;
+  }
+
+  SherpaOnnxOfflineMedAsrCtcModelConfig medasr;
+  memset(&medasr, 0, sizeof(medasr));
+  medasr.model = model_filename;
+
+  // Offline model config
+  SherpaOnnxOfflineModelConfig offline_model_config;
+  memset(&offline_model_config, 0, sizeof(offline_model_config));
+  offline_model_config.debug = 1;
+  offline_model_config.num_threads = 1;
+  offline_model_config.provider = provider;
+  offline_model_config.tokens = tokens_filename;
+  offline_model_config.medasr = medasr;
+
+  // Recognizer config
+  SherpaOnnxOfflineRecognizerConfig recognizer_config;
+  memset(&recognizer_config, 0, sizeof(recognizer_config));
+  recognizer_config.decoding_method = "greedy_search";
+  recognizer_config.model_config = offline_model_config;
+
+  const SherpaOnnxOfflineRecognizer *recognizer =
+      SherpaOnnxCreateOfflineRecognizer(&recognizer_config);
+
+  if (recognizer == NULL) {
+    fprintf(stderr, "Please check your config!\n");
+    SherpaOnnxFreeWave(wave);
+    return -1;
+  }
+
+  const SherpaOnnxOfflineStream *stream =
+      SherpaOnnxCreateOfflineStream(recognizer);
+
+  SherpaOnnxAcceptWaveformOffline(stream, wave->sample_rate, wave->samples,
+                                  wave->num_samples);
+  SherpaOnnxDecodeOfflineStream(recognizer, stream);
+  const SherpaOnnxOfflineRecognizerResult *result =
+      SherpaOnnxGetOfflineStreamResult(stream);
+
+  fprintf(stderr, "Decoded text: %s\n", result->text);
+
+  SherpaOnnxDestroyOfflineRecognizerResult(result);
+  SherpaOnnxDestroyOfflineStream(stream);
+  SherpaOnnxDestroyOfflineRecognizer(recognizer);
+  SherpaOnnxFreeWave(wave);
+
+  return 0;
+}
diff --git a/cxx-api-examples/CMakeLists.txt b/cxx-api-examples/CMakeLists.txt
index 2dafd987..88b3add8 100644
--- a/cxx-api-examples/CMakeLists.txt
+++ b/cxx-api-examples/CMakeLists.txt
@@ -42,6 +42,9 @@ target_link_libraries(wenet-ctc-cxx-api sherpa-onnx-cxx-api)
 add_executable(omnilingual-asr-ctc-cxx-api ./omnilingual-asr-ctc-cxx-api.cc)
 target_link_libraries(omnilingual-asr-ctc-cxx-api sherpa-onnx-cxx-api)
 
+add_executable(medasr-ctc-cxx-api ./medasr-ctc-cxx-api.cc)
+target_link_libraries(medasr-ctc-cxx-api sherpa-onnx-cxx-api)
+
 add_executable(nemo-canary-cxx-api ./nemo-canary-cxx-api.cc)
 target_link_libraries(nemo-canary-cxx-api sherpa-onnx-cxx-api)
 
diff --git a/cxx-api-examples/medasr-ctc-cxx-api.cc b/cxx-api-examples/medasr-ctc-cxx-api.cc
new file mode 100644
index 00000000..ccfee35b
--- /dev/null
+++ b/cxx-api-examples/medasr-ctc-cxx-api.cc
@@ -0,0 +1,75 @@
+// cxx-api-examples/medasr-ctc-cxx-api.cc
+// Copyright (c)  2025  Xiaomi Corporation
+
+//
+// This file demonstrates how to use MedASR with sherpa-onnx's C++ API.
+// clang-format off
+/*
+wget https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+tar xvf sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+rm sherpa-onnx-medasr-ctc-en-int8-2025-12-25.tar.bz2
+*/
+//
+// clang-format on
+
+#include <chrono>  // NOLINT
+#include <iostream>
+#include <string>
+
+#include "sherpa-onnx/c-api/cxx-api.h"
+
+int32_t main() {
+  using namespace sherpa_onnx::cxx;  // NOLINT
+  OfflineRecognizerConfig config;
+
+  // clang-format off
+  config.model_config.medasr.model = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/model.int8.onnx";
+  config.model_config.tokens = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/tokens.txt";
+
+  config.model_config.num_threads = 1;
+
+  std::cout << "Loading model\n";
+  OfflineRecognizer recognizer = OfflineRecognizer::Create(config);
+  if (!recognizer.Get()) {
+    std::cerr << "Please check your config\n";
+    return -1;
+  }
+  std::cout << "Loading model done\n";
+
+  std::string wave_filename = "./sherpa-onnx-medasr-ctc-en-int8-2025-12-25/test_wavs/0.wav";
+  // clang-format on
+
+  Wave wave = ReadWave(wave_filename);
+  if (wave.samples.empty()) {
+    std::cerr << "Failed to read: '" << wave_filename << "'\n";
+    return -1;
+  }
+
+  std::cout << "Start recognition\n";
+  const auto begin = std::chrono::steady_clock::now();
+
+  OfflineStream stream = recognizer.CreateStream();
+  stream.AcceptWaveform(wave.sample_rate, wave.samples.data(),
+                        wave.samples.size());
+
+  recognizer.Decode(&stream);
+
+  OfflineRecognizerResult result = recognizer.GetResult(&stream);
+
+  const auto end = std::chrono::steady_clock::now();
+  const float elapsed_seconds =
+      std::chrono::duration_cast<std::chrono::milliseconds>(end - begin)
+          .count() /
+      1000.;
+  float duration = wave.samples.size() / static_cast<float>(wave.sample_rate);
+  float rtf = elapsed_seconds / duration;
+
+  std::cout << "text: " << result.text << "\n";
+  printf("Number of threads: %d\n", config.model_config.num_threads);
+  printf("Duration: %.3fs\n", duration);
+  printf("Elapsed seconds: %.3fs\n", elapsed_seconds);
+  printf("(Real time factor) RTF = %.3f / %.3f = %.3f\n", elapsed_seconds,
+         duration, rtf);
+
+  return 0;
+}
diff --git a/sherpa-onnx/c-api/c-api.cc b/sherpa-onnx/c-api/c-api.cc
index c00286ac..8c293154 100644
--- a/sherpa-onnx/c-api/c-api.cc
+++ b/sherpa-onnx/c-api/c-api.cc
@@ -511,6 +511,9 @@ static sherpa_onnx::OfflineRecognizerConfig GetOfflineRecognizerConfig(
   recognizer_config.model_config.omnilingual.model =
       SHERPA_ONNX_OR(config->model_config.omnilingual.model, "");
 
+  recognizer_config.model_config.medasr.model =
+      SHERPA_ONNX_OR(config->model_config.medasr.model, "");
+
   recognizer_config.lm_config.model =
       SHERPA_ONNX_OR(config->lm_config.model, "");
   recognizer_config.lm_config.scale =
diff --git a/sherpa-onnx/c-api/c-api.h b/sherpa-onnx/c-api/c-api.h
index f622c26d..15a464b9 100644
--- a/sherpa-onnx/c-api/c-api.h
+++ b/sherpa-onnx/c-api/c-api.h
@@ -484,6 +484,10 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineOmnilingualAsrCtcModelConfig {
   const char *model;
 } SherpaOnnxOfflineOmnilingualAsrCtcModelConfig;
 
+SHERPA_ONNX_API typedef struct SherpaOnnxOfflineMedAsrCtcModelConfig {
+  const char *model;
+} SherpaOnnxOfflineMedAsrCtcModelConfig;
+
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineTransducerModelConfig transducer;
   SherpaOnnxOfflineParaformerModelConfig paraformer;
@@ -511,6 +515,7 @@ SHERPA_ONNX_API typedef struct SherpaOnnxOfflineModelConfig {
   SherpaOnnxOfflineCanaryModelConfig canary;
   SherpaOnnxOfflineWenetCtcModelConfig wenet_ctc;
   SherpaOnnxOfflineOmnilingualAsrCtcModelConfig omnilingual;
+  SherpaOnnxOfflineMedAsrCtcModelConfig medasr;
 } SherpaOnnxOfflineModelConfig;
 
 SHERPA_ONNX_API typedef struct SherpaOnnxOfflineRecognizerConfig {
diff --git a/sherpa-onnx/c-api/cxx-api.cc b/sherpa-onnx/c-api/cxx-api.cc
index 25bb9f36..33b3c8a1 100644
--- a/sherpa-onnx/c-api/cxx-api.cc
+++ b/sherpa-onnx/c-api/cxx-api.cc
@@ -272,6 +272,8 @@ static SherpaOnnxOfflineRecognizerConfig Convert(
   c.model_config.omnilingual.model =
       config.model_config.omnilingual.model.c_str();
 
+  c.model_config.medasr.model = config.model_config.medasr.model.c_str();
+
   c.lm_config.model = config.lm_config.model.c_str();
   c.lm_config.scale = config.lm_config.scale;
 
diff --git a/sherpa-onnx/c-api/cxx-api.h b/sherpa-onnx/c-api/cxx-api.h
index 7f46946c..42aaa73b 100644
--- a/sherpa-onnx/c-api/cxx-api.h
+++ b/sherpa-onnx/c-api/cxx-api.h
@@ -272,6 +272,10 @@ struct SHERPA_ONNX_API OfflineOmnilingualAsrCtcModelConfig {
   std::string model;
 };
 
+struct SHERPA_ONNX_API OfflineMedAsrCtcModelConfig {
+  std::string model;
+};
+
 struct SHERPA_ONNX_API OfflineMoonshineModelConfig {
   std::string preprocessor;
   std::string encoder;
@@ -302,6 +306,7 @@ struct SHERPA_ONNX_API OfflineModelConfig {
   OfflineCanaryModelConfig canary;
   OfflineWenetCtcModelConfig wenet_ctc;
   OfflineOmnilingualAsrCtcModelConfig omnilingual;
+  OfflineMedAsrCtcModelConfig medasr;
 };
 
 struct SHERPA_ONNX_API OfflineLMConfig {
